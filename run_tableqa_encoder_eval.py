import os
import re
import torch
import pandas as pd 
from encoder_models.encoder1.build_input import build_instruction, build_question 
from encoder_models.encoder2.build_input import build_encoder_input

from utils import (
    filter_code, 
    filter_cot, 
    save_json, 
    load_json,
    get_tool,
    timeout,
    TimeoutException,
    get_table_infos
)

from tqdm import tqdm

CODE_PREFIX = """import matplotlib.pyplot as plt
from mplfonts import use_font
import pandas as pd
import numpy as np
import seaborn as sns
import warnings

warnings.filterwarnings("ignore")
# Fixing Chinese font issues
use_font("Noto Serif CJK SC")
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus']=False\n"""

def execution_eval(observe: str) -> bool:
    """
    Test whether the code generated by eval_llm can be executed.
    :param output: output code of llm generation
    :return: True or False
    """
    # 只要执行结果中不出现error 或者 exception， 就认为代码可执行
    pattern = re.compile(r"error|exception", re.IGNORECASE)

    try:
        res = not pattern.search(observe)
    except:
        res = True
    return res


def eval_outputs(
    model, 
    test_datas:list[dict], 
    device:str= "cuda", 
    max_new_tokens: int=5000,
    temperature: float = 0.01,
    model_type: str = "1",
    num_sample: int = None
) -> list[dict]:
    model.to(device).eval()
    eval_answers = []
    for data in tqdm(test_datas[:num_sample]):
        output = {}
        csv_files_path = data["table_paths"]
        df_names = data["df_names"]
        query = data["query"]
        table_info = get_table_infos(csv_files_path)
        if model_type == "1":
            decoder_input = build_instruction(build_question(csv_files_path, df_names, query), model.tokenizer)
            encoder_input = None
            model_output = model.generate(
                                decoder_input, 
                                max_new_tokens=max_new_tokens, 
                                eos_token_id = model.tokenizer.eos_token_id, 
                                pad_token_id = model.tokenizer.eos_token_id,
                                path_csv=csv_files_path,
                                temperature = temperature,
                                do_sample=True
                                )
            output_content = model_output[1][0]
        elif model_type == "2":
            decoder_input = build_instruction(build_question(csv_files_path, df_names, query), model.tokenizer_decoder)
            encoder_input = build_encoder_input(table_info)
            model_output = model.generate(
                                decoder_input, 
                                max_new_tokens=max_new_tokens, 
                                eos_token_id = model.tokenizer_decoder.eos_token_id, 
                                pad_token_id=model.tokenizer_decoder.eos_token_id, 
                                table_str=encoder_input
                                )
            output_content = model.tokenizer_decoder.batch_decode(model_output, skip_special_tokens=True)[0]
        else:
            raise 

        output["llm_output"] = output_content
        output["query"] = query
        output["df_names"] = df_names
        output["table_paths"] = csv_files_path
        output["instruction"] = decoder_input
        output["encoder_input"] = encoder_input

        eval_answers.append(output)
    return eval_answers


@torch.inference_mode()
def run_eval(
    model_path: str,
    patch_model_path: str,
    decoder_model_path: str,
    test_data_path: str,
    device: str = "cuda",
    model_type: str = "1",
    num_samples: int = None,
    eval_result_save_path: str = "evalset/test_results/eval_results_encoder_qa.json",
):
    """
    Evaluate the model on the given data.
    If a conversation contains no CSV files, the answer will be based on given instruction. Otherwise, a new instruction will be generated based on the dataframes.
    Args:
        model_path (str): The path to the pretrained model.
        test_data_path (list): A list of data to evaluate the model on. Each element in the list should be a dictionary with the following keys:
            - 'table_paths' (list): A list of file paths to CSV files. If table_paths is not empty, this field will be ignored.
            - 'df_names' (list): A list of names for the dataframes.
            - 'instruction' (str): The user's instruction. If table_paths is not empty, this field will be ignored.

        device (str, optional): The device to run the evaluation on. Defaults to 'cuda'.

    Returns:
        list: A list of evaluated data with model answers. Each element in the list is a dictionary containing the following keys:
            - 'csv_abs_paths' (list): The same list of file paths to CSV files.
            - 'df_names' (list): The same list of names for the dataframes.
            - 'query' (str): The same user's query.
            - 'instruction' (str): The generated instruction text.
            - 'model_answer' (str): The model's answer to the user's query.

    """
    if model_type =="1":
        from encoder_models.encoder1.model_sft import Model
        model = Model.from_pretrained(
            path=model_path,
            sentence_transformer_path=patch_model_path,
            base_model_path=decoder_model_path
        )
    elif model_type == "2":
        from encoder_models.encoder2.model_encoderdecoder_t5 import Model
        model = Model.from_pretrained(
            encoder_path=model_path,
            decoder_path=decoder_model_path,
            projector_path=patch_model_path
        ).float()
    test_datas = load_json(test_data_path)
    print("Generating eval answers now..")

    if num_samples == None:
        num_samples = len(test_datas)

    model_outputs = eval_outputs(
        model=model,
        test_datas=test_datas,
        device=device,
        max_new_tokens=5000,
        num_sample=num_samples,
        model_type=model_type
    )
    print("Generating answers finished..")
    eval_answers = []
    for output in tqdm(model_outputs[:num_samples]):
        eval_sample = {}
        csv_files_path = output["table_paths"]
        input = output["instruction"]
        query = output["query"]
        df_names = output["df_names"]
        output_content = output["llm_output"]
        # input = build_instruction(build_question(csv_files_path, df_names, query), model.tokenizer)
        df = [pd.read_csv(path, low_memory=False) for path in csv_files_path]
        tool = get_tool(df,df_names)
        code, _ = filter_code(output_content)
        cot = filter_cot(output_content)

        # 运行超时代码，认为都是异常代码， 在tool.run()过程中，可能会print出额外的内容，不影响执行
        try:
            # 如果生成的代码为空（解析不到代码）， 也认为是llm没有理解observe内容或instruct， 输出为Code Error
            if not code:
                observe = "Code Error: output empty code.."
            else:
                with timeout(15):  # 设置超时时间为15秒
                    pure_code = CODE_PREFIX + code
                    observe = tool.run(pure_code)  # 需要监控超时的代码块
                    # observe = execute_with_timeout(pure_code, 15, tool)
                    if isinstance(observe, pd.DataFrame):
                        observe = observe.head().to_markdown(index=False)
                    else:
                        observe = str(observe)
        except TimeoutException as e:
            observe = f"Timeout Error: code running time exceed 15s.."
        except SystemExit as e:
            observe = f"SystemExit Error: {str(e)}"
        except Exception as e:
            observe = f"Unexpected Error: {str(e)}"

        eval_sample["code"] = CODE_PREFIX + code
        eval_sample["cot"] = cot
        eval_sample["observe"] = observe
        eval_sample["query"] = query
        eval_sample["instruction"] = input
        eval_sample["df_names"] = df_names
        eval_sample["table_paths"] = csv_files_path
        eval_sample["llm_output_content"] = output_content
        eval_answers.append(eval_sample)

    if not os.path.exists("./output"):
        os.mkdir("./output")

    save_json(eval_result_save_path, eval_answers)
    total_len = num_samples
    print("Eval answers construct complete..")
    execute_passed = 0
    for sample in eval_answers:
        observe = sample["observe"]
        execute_passed += 1 if execution_eval(observe) else 0
    
    print(f"Sample length: {total_len}. ")
    print(
        f"Execute Passed: {execute_passed}." f"\tExecute pass-rate is:",
        round(execute_passed / total_len, 3),
    )

def main(args):
    """main function"""
    patch_model_path = args.patch_model_path 
    model_type = args.model_type
    encoder_model_path = args.encoder_model_path
    decoder_model_path = args.decoder_model_path
    eval_results_save_path = args.eval_results_save_path
    eval_dataset_path = args.eval_dataset_path
    device=args.device
    num_samples = args.num_samples_to_eval
    # os.environ["CUDA_VISIBLE_DEVICES"] = "7"

    if model_type == "1":
        os.environ["SENTENCE_TRANSFORMER"] = os.path.split(patch_model_path)[-1]
        os.environ['MODELS_PATH'] = os.path.dirname(patch_model_path)
    run_eval(
        model_path=encoder_model_path,
        patch_model_path=patch_model_path,
        decoder_model_path=decoder_model_path,
        test_data_path=eval_dataset_path,
        device=device,
        eval_result_save_path=eval_results_save_path,
        num_samples=num_samples,
        model_type=model_type
    )

if __name__ == "__main__":
    import argparse
    from pathlib import Path
    import shutil
    output_dir = Path(__file__).parent / "images"
    if os.path.exists(output_dir):
        if not os.access(output_dir, os.W_OK):
            shutil.rmtree(output_dir)
            os.makedirs(output_dir)
            os.chmod(output_dir, 0o777)
            print("not write permission, makedir:", output_dir)
        else:
            print(f"{output_dir} exists!")
    else:
        os.makedirs(output_dir)
        os.chmod(output_dir, 0o777)
        print("makedir:", output_dir)
    
    parser = argparse.ArgumentParser(description="eval tableqa python code")

    # parser.add_argument(
    #     "--temperature", type=float, default=0.01, help="Temperature setting"
    # )

    parser.add_argument(
        "--decoder_model_path",
        type=str,
        # required=True,
        default="/data4/sft_output/qwen2-base-0717/checkpoint-2000",
        help="Decoder base model path.",
    )

    parser.add_argument(
        "--patch_model_path",
        type=str,
        default="/data0/workspace/tjm/review_code/checkpoints/cp271-qwen-ckpt_2000-only_proj_2e4_2epoch_table/projector.bin ",
        help="Patch model path, for encoder1 this path is sentence transformer path, for encoder2 this path is projector weigtht path",
    )

    parser.add_argument(
        "--encoder_model_path",
        type=str,
        default="/data0/workspace/liliyao/saved_models/checkpoint-271",
        help="Encoder model path",
    )

    parser.add_argument(
        "--model_type",
        choices=["1", "2"],
        default="2",
        help="Encoder1 or Encoder2",
    )

    parser.add_argument(
        "--eval_dataset_path",
        type=str,
        default="evalset/table_qa_execuate_test/test_datas_zuizong.json",
        help="Test Set Path",
    )

    parser.add_argument(
        "--eval_results_save_path",
        type=str,
        default="output/result_table_qa_encoder1.json",
        # help="Max iteration for llm to run each code correction task",
    )

    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="Use cuda or cpu to run eval",
    )

    parser.add_argument(
        "--num_samples_to_eval",
        type=int,
        default=None,
        help="Set eval samples number to eval",
    )

    args = parser.parse_args()
    main(args)

    # Encoder1 for LONGLIN
    """
    python run_tableqa_encoder_eval.py --decoder_model_path /data4/sft_output/qwen2-base-0802/checkpoint-2400 \
    --model_type "1" \
    --encoder_model_path /data0/gxj/sft_checkpoints/20col_-1/lr1e-5_constant_with_warmup_bs1024_bf16_freezedecoder_table4_nods_new/checkpoint-378 \
    --patch_model_path /data0/pretrained-models/all-MiniLM-L6-v2
    """
    # Encoder2 for LIYAO
    """
    python run_tableqa_encoder_eval.py --decoder_model_path /data4/sft_output/qwen2-base-0727/ \
    --model_type "2" \
    --encoder_model_path /data0/workspace/liliyao/saved_models/checkpoint-271 \
    --patch_model_path /data0/workspace/tjm/review_code/checkpoints/cp271-qwen-ckpt_2000-only_proj_2e4_2epoch_table/projector.bin
    """