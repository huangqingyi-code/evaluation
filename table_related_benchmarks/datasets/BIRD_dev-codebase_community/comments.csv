Id,PostId,Score,Text,CreationDate,UserId,UserDisplayName
1,3,5,"Could be a poster child fo argumentative and subjective.  At the least, need to define 'valuable'.",2010-07-19 19:15:52.0,13.0,
2,5,0,"Yes, R is nice- but WHY is it 'valuable'.",2010-07-19 19:16:14.0,13.0,
3,9,0,"Again- why?  How would I convince my boss to use this over, say, Excel.",2010-07-19 19:18:54.0,13.0,
4,5,11,"It's mature, well supported, and a standard within certain scientific communities (popular in our AI department, for example)",2010-07-19 19:19:56.0,37.0,
5,3,1,"Define ""valuable""...",2010-07-19 19:20:28.0,5.0,
6,14,9,"why ask the question here?  All are community-wiki, why not just fix the canonical answer?",2010-07-19 19:22:27.0,23.0,
7,18,1,also the US census data http://www.census.gov/main/www/access.html,2010-07-19 19:25:47.0,36.0,
9,16,0,Andrew Gelman has a nice R library that links Bugs to R.,2010-07-19 19:30:24.0,78.0,
10,23,4,I am not sure I understand the difficulty. If the functional form is known just take the derivative otherwise take differences. Am I missing something here?,2010-07-19 19:31:18.0,,user28
11,43,5,"There are many R GUI's available for Windows, so I'm not following your point.",2010-07-19 19:34:20.0,5.0,
12,38,0,"That's just an example - it might have a median that is much smaller, on the order of 200 (it depends on how I partition the data). That would preclude using a normal distribution, right?",2010-07-19 19:37:33.0,54.0,
13,20,2,What levels of kurtosis and skewdness are acceptable to meet the assumption of normality?,2010-07-19 19:38:01.0,24.0,
14,46,6,this is an incredibly unclear response. Please try to write in English.,2010-07-19 19:38:30.0,74.0,
15,3,2,"Maybe the focus shouldn't be on ""valuable"" but rather ""pros"" and ""cons"" of each project?",2010-07-19 19:44:47.0,24.0,
16,8,2,Probably better asked on meta.  People feel like they have to downvote it because it is an off-topic question.  But then the downvotes make it look like staticians have no sense of humor :(,2010-07-19 19:46:19.0,13.0,
18,36,10,http://xkcd.com/552/,2010-07-19 19:48:32.0,68.0,
19,8,0,"@Jason Punyon in particular gets a humorless downvote for removing my ""verboten"" tag! ;-)",2010-07-19 19:50:40.0,6.0,
20,54,3,"I am not sure if characterizing one or the other as the 'wrong' formula is the way to understand the issue. It is just that the second one is 'better' in in the sense that it is an unbiased estimator of the true standard deviation. So, if you care about unbiased estimates then the second one is 'better'/'correct'.",2010-07-19 19:51:06.0,,user28
21,77,1,I like the first example you give. That will certainly get the students talking ;),2010-07-19 19:56:18.0,8.0,
22,56,1,I like the analogy. I would find it very useful if there were a defined question (based on a dataset) in which an answer was derived using frequentist reasoning and an answer was derived using Bayesian - preferably with R script to handle both reasonings. Am I asking too much?,2010-07-19 19:56:21.0,104.0,
23,57,3,-1 for wikipedia cut and paste,2010-07-19 19:56:28.0,74.0,
24,73,0,"Very subjective question: this question cannot be answered, and is not suitable for a QA site.",2010-07-19 19:58:20.0,107.0,
25,76,1,+1 for ggplot2 - by far best graphs you can get,2010-07-19 19:59:04.0,22.0,
27,77,0,There's an interesting discussion by Steve Steinberg on his blog here: http://blog.steinberg.org/?p=11 about some of the implications of 1 and where it might lead in terms of Weak AI.,2010-07-19 20:01:15.0,55.0,
28,57,5,"@el chief: On Stackexchange sites, it is allowed to copy and paste answers from other resources. I give the source link along with the answers. It's pointless to give -1 to a right answer. See meta for what is allowed here and what is not; and a big -1 from me to your behavior!",2010-07-19 20:02:25.0,69.0,
29,79,0,"I wasn't suggesting it was, I was just curious as to why such a difference might have arisen, what sort of level of error following the wrong advice might give and whether there was a decent explanation of the difference I could give to my students.",2010-07-19 20:03:30.0,55.0,
30,57,3,"Wikipedia cut and paste doesn't bother me when cited, as it is  here [is it compatible with the cc-sa license?]. However, these explanations are complicated for a beginner",2010-07-19 20:03:31.0,87.0,
31,54,0,"I was characterising the formula as ""wrong"" purely in the sense that in an exam if you use the formula which isn't proscribed by the syllabus you'll end up with the ""wrong"" answer. Plus if the values are not a sample of population per se then surely the first formula gives the more accurate value.",2010-07-19 20:05:22.0,55.0,
32,73,2,Should probably be community wiki; useful question here but doesn't have definitive answer.,2010-07-19 20:05:57.0,5.0,
33,73,2,"@Shane: good point. moved.
@ Egon: subjective indeed. but if the answers come from knowledgeable people i don't mind dose of subjectivity.
i've started learning R quite recently and have couple of dozens installed to explore, however i notice that there are tools that I use much more often irrespectively of the task at hand.",2010-07-19 20:06:56.0,22.0,
34,56,8,"The simplest thing that I can think of that tossing a coin n times and estimating the probability of a heads (denote by p). Suppose, we observe k heads. Then the probability of getting k heads is:

P (k heads in n trials) = (n, k) p^k (1-p)^(n-k)

Frequentist inference would maximize the above to arrive at an estimate of p = k / n.

Bayesian would say: Hey, I know that p ~ Beta(1,1) (which is equivalent to assuming that p is uniform on [0,1]). So, the updated inference would be:

p ~ Beta(1+k,1+n-k) and thus the bayesian estimate of p would be 

p = 1+k / (2+n)

I do not know R, sorry.",2010-07-19 20:11:13.0,,user28
35,43,0,"I wasn't aware of rapidminer. That looks nice, thanks!",2010-07-19 20:11:28.0,33.0,
36,3,0,"Or maybe even ""How X will help you get Y done faster/cheaper and kill the germs that cause bad breath.""",2010-07-19 20:15:51.0,13.0,
38,73,0,It would be interesting if StackExchange could support some method of linking community wiki posts across sites.  Because I will bet this question has been asked on Stackoverflow and I also think that Statistical Analysis may attract some people that wouldn't usually visit SO.,2010-07-19 20:19:28.0,13.0,
39,75,2,This is really a Stackoverflow question as it has to do with learning the R programming language.  With the current wording the question is only associated with statistics by virtue of R's focus on statistical analysis.,2010-07-19 20:23:01.0,13.0,
41,16,3,"I'd rephrase that ""the most popular statistical tool in bioinformatics""... Bioinformaticians doing microarray analysis use it extensively, yes. But bioinformatics is not limited to that ;)",2010-07-19 20:25:52.0,120.0,
42,65,4,"Colin, an unbiased estimator of the standard deviation does not have a closed form representation in the general case. What does exist is the unbiased estimator of the <i>variance</i> (s<sup>2</sup> in this case).
Noteworthy that both are consistent estimators of the population variance - and so by the continuous mapping theorem, are the two estimators of the standard deviations.
A related point is that s<sub>n</sub><sup>2</sup> has a lower MSE than s<sup>2</sup>. The additional advantage from imposing unbiasedness is arguable.",2010-07-19 20:30:23.0,47.0,
43,75,0,I guess you're right. Voting to close my own question.,2010-07-19 20:32:02.0,69.0,
44,103,6,"i suggest everyone put their favourite image from the blog, so it's not just a collection of links...",2010-07-19 20:37:20.0,74.0,
45,73,0,"@Sharpie: there have been several interesting SO posts like http://stackoverflow.com/questions/1295955/what-is-the-most-useful-r-trick or http://stackoverflow.com/questions/1535021/whats-the-biggest-r-gotcha-youve-run-across however they are not focused on packages. and i agree, linkage of community wiki could be really useful.",2010-07-19 20:37:27.0,22.0,
46,44,6,This is an incredibly general question.,2010-07-19 20:38:14.0,46.0,
47,105,0,+1 Beat me by 10 seconds with that one...,2010-07-19 20:39:12.0,5.0,
48,103,0,"There is no single correct answer, should be wiki...",2010-07-19 20:39:39.0,122.0,
50,16,0,@Nicojo - Very good point!,2010-07-19 20:43:25.0,8.0,
51,65,0,@Tirthankar - very sloppy of me. I've altered the answer slightly. Thanks.,2010-07-19 20:45:49.0,8.0,
52,38,0,"The normal approximation to the Poisson distribution is pretty robust, the difference between the CDFs is bounded by something like 0.75/sqrt(lambda), if I recall correctly.  I wouldn't be too worried about using lambda=200, but if you're more risk-averse then definitely go with the negative binomial.",2010-07-19 20:46:12.0,61.0,
53,80,0,thank you user93!,2010-07-19 20:46:46.0,58.0,
54,80,0,"By ""January 1st"", I presume you mean the cut-off is an entire year and not 6 months period that applies to your children.",2010-07-19 20:48:04.0,58.0,
55,59,4,Do you get bonus points for the first graph in the stats exchange?,2010-07-19 20:53:53.0,8.0,
56,101,1,"Take your time! I won't be thinking about selecting a ""Best Answer"" for a week or so.",2010-07-19 20:54:37.0,13.0,
57,75,1,"I wouldn't vote to close it just yet- there could be a good question in there.  Perhaps something like ""Where can I find useful tutorials that focus on putting statistical concepts into practice using a tool such as R?"" or ""Where can I find useful tutorials that teach statistics by example using tools such as R?""",2010-07-19 20:58:26.0,13.0,
59,116,0,It doesn't have an RSS feed though :(,2010-07-19 21:11:30.0,8.0,
60,110,0,"Thanks for the great answer and for book advice. Also, do you know about relation of window function to generalized functions? It seems (from wikipedia article) they are suitable as domains for functionals.",2010-07-19 21:18:07.0,117.0,
62,116,0,http://cscs.umich.edu/~crshalizi/weblog/index.rss,2010-07-19 21:20:53.0,61.0,
64,118,11,"In a way, the measurement you proposed is widely used in case of error (model quality) analysis -- then it is called MAE, ""mean absolute error"".",2010-07-19 21:30:23.0,88.0,
65,130,7,"If you would taste R, it is highly probable that you will resign from MATLAB (as in my case).",2010-07-19 21:33:33.0,88.0,
66,116,0,@Rich - Thanks. I wonder why FF didn't pick it up?,2010-07-19 21:34:21.0,8.0,
67,130,0,"IMO, this should be community wiki (language ""versus"" type questions are pretty subjective).",2010-07-19 21:34:28.0,5.0,
68,130,0,This is definitely a question concerning programming languages and should be asked on Stack Overflow.,2010-07-19 21:35:25.0,13.0,
69,130,0,"I agree with Sharpie.  @Vivi: you should change the question title to be ""advantages and disadvantages for data munging"" or something along that line so that it's more on-topic.",2010-07-19 21:37:16.0,5.0,
70,134,1,I think this is the first candidate to be moved to Stack Overflow.,2010-07-19 21:38:27.0,88.0,
71,120,9,Nice analogy of euclidean space!,2010-07-19 21:38:48.0,83.0,
73,138,9,Should be community wiki.,2010-07-19 21:40:12.0,5.0,
74,116,1,"@Colin: The author of the blog would need to put `<link rel=""alternate"" type=""application/rss+xml"" title=""RSS"" href=""http://cscs.umich.edu/~crshalizi/weblog/index.rss"" />` in his the `<head>` section of the HTML document for Firefox to pick it up automatically :)",2010-07-19 21:42:04.0,66.0,
76,130,0,I don't know what munging is  :(,2010-07-19 21:51:17.0,90.0,
77,130,5,"@Sharpie, @Shane IMO to this extent it is a question about tools, so it is acceptable.",2010-07-19 21:54:08.0,88.0,
78,145,0,similar to http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/70#70,2010-07-19 21:54:47.0,22.0,
79,145,0,Voting to close as a duplicate.,2010-07-19 21:56:44.0,13.0,
80,130,0,"@mbq, we definitely need a set of community guidelines on these sorts of questions, help us decide on meta: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions",2010-07-19 21:59:52.0,13.0,
81,130,0,"@Sharpie It is not only in contex of R -- to this end I've made a separate meta discussion for that, http://meta.stats.stackexchange.com/questions/35/how-much-programming-here",2010-07-19 22:18:16.0,88.0,
82,20,5,"Most statistical methods assume normality, not of the data, but rather of an assumed random variable, e.g. the error term in a linear regression. Checking involves looking at the residuals, not the original data!",2010-07-19 22:24:14.0,,Statprof
83,145,0,"I agree, first duplicate ;-)",2010-07-19 22:28:22.0,88.0,
84,7,2,This should be community-wiki.,2010-07-19 22:31:13.0,88.0,
85,75,1,http://meta.stats.stackexchange.com/questions/35/how-much-programming-here,2010-07-19 22:39:45.0,88.0,
86,110,0,"The full technical explanation is complicated.  The shorter, less technical explanation is that, since they are typically smooth, and either of compact support or they decay exponentially fast, many (but definitely not all) windowing functions make good 'test functions' to examine operator behavior with.  A good starting point might be the chapter on distributions in http://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf",2010-07-19 22:42:33.0,61.0,
87,130,0,@Sharpie : check this out: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions,2010-07-19 22:52:59.0,90.0,
88,157,0,+1 that problem twisted my brain when I first read and thought about it- and the solution is pretty simple but teaches a lot about probability.,2010-07-19 23:01:30.0,13.0,
89,86,0,"It's probably best not to use the term ""normal variable"" here when you do not mean a normally distributed random variable.",2010-07-19 23:28:39.0,159.0,
90,143,0,Random Forest seems very interesting. Thanks. :),2010-07-19 23:32:20.0,131.0,
91,137,0,"This is a great answer! I'll look into your book suggestions, and the description of your process is also great. I especially like the suggestions for feature vectorizations.",2010-07-19 23:34:20.0,131.0,
92,137,0,"(If anyone would like to elaborate even further on the vectorization part, that'd be great.)",2010-07-19 23:35:23.0,131.0,
93,150,0,curious that that book has no reviews on Amazon,2010-07-19 23:54:17.0,74.0,
94,86,0,"Agreed.  Although I personally would look at someone funny for a few seconds if they said ""normal variable"" and didn't throw the word ""random"" or ""distributed"" in there somewhere to cue me that that is what they were discussing.  But I am also an engineer and not a statistician so I don't use that much domain-specific notation.",2010-07-19 23:56:45.0,13.0,
95,145,3,"Actually, I think the previous question is more a subset of this one. EAMann was asking for a datasets with some particular characteristics (although I'm not sure anybody is paying attention to those criteria); this question is wide-open.  For example, I feel comfortable voting up many of these answers, because they are, in fact, datasets, but not any of the answers on the previous question, because I haven't opened them up to see if they suit EAMann's requests.",2010-07-20 00:10:26.0,71.0,
96,101,0,Now that I've had a chance to come back and read the whole answer- a big +1 for the student height example.  Very clear and well laid out.,2010-07-20 00:12:40.0,13.0,
97,43,3,"""Windows-based"" may mean that it works on the Windows operating system (which R does) rather than meaning that it is a heavily GUI-oriented tool. Note the capital letter!",2010-07-20 01:01:12.0,173.0,
98,5,10,It's extensible and there's no statistical technique that can't be done in it.,2010-07-20 01:22:43.0,1356.0,
99,134,0,"Possibly, but it'd need a lot more explanation on SO.",2010-07-20 01:29:42.0,174.0,
100,86,5,"Random variables may be classified as *discreet* if they don't draw attention to themselves. If they're merely countable we say *discrete* :-P Also, you mean prescribe rather than proscribe, but I think *describe* might be more appropriate. Nice answer, anyway -- hopefully +1 will help mitigate the nitpicking!",2010-07-20 01:43:31.0,174.0,
101,46,1,"maybe so. is a person asking this question a person who walked in off the street, or a person who has at least opened a statistics book. 

Telling someone the standard deviation is just the square root of the variance is completely begging the question.",2010-07-20 01:59:53.0,62.0,
104,188,3,"That's a good explanation, but not for a nontechnical layperson. I suspect the OP wanted to know how to explain it to, say, the MBA who hired you to do some statistical analysis! How would you describe MCMC to someone who, at best, sorta understands the concept of a standard deviation (variance, though, may be too abstract)?",2010-07-20 02:18:02.0,6.0,
105,196,0,To give everyone a chance -- this should be wikified,2010-07-20 02:35:25.0,87.0,
106,188,2,"@Harlan: It's a hard line to straddle; if someone doesn't at least know what a random variable is, why we might want to estimate probabilities, and have some hazy idea of a density function, then I don't think it *is* possible to meaningfully explain the how or why of MCMC to them, only the ""what"", which in this case would boil down to ""it's a way of numerically solving an otherwise impossible problem by simulation, like flipping a coin a lot to estimate the probability that it lands on heads"".",2010-07-20 02:39:24.0,61.0,
108,25,0,"do you mean a GUI graphical tool that runs on Windows, or a command line based one that runs on Windows (or either)",2010-07-20 04:09:28.0,74.0,
109,200,0,"coming up with a persuasive theoretical framework that explains the patterns is very hard, especially when there are thousands of variables involved. Real world is a complicated business.",2010-07-20 04:33:45.0,175.0,
110,101,1,"Nice work ... but we need to add 

(C) our model (embodied in the formula/statistical routine) is wrong.",2010-07-20 05:07:08.0,187.0,
112,54,12,"Srikant, I don't think that the second one is an unbiased estimator.  The square of it *is* an unbiased estimator of the true variance.  However, Jensen's Inequality establishes that the expectation of a curvilinear function of a random variable is not the same as the function of the expectation of the random variable.  Hence the second formula can't be an unbiased estimator of the true standard deviation.",2010-07-20 05:28:20.0,187.0,
113,200,2,@Ngu Nobody promised it would be easy. Expecting meaningful patterns just to magically emerge from a sea of data is what gives data miners a bad name.,2010-07-20 05:29:24.0,174.0,
114,220,2,Answer can be checked from most textbooks / google.,2010-07-20 05:41:15.0,195.0,
115,167,0,Wouldn't that be FA then?,2010-07-20 05:52:50.0,144.0,
116,222,0,el chef has a condensed answer over here -> http://stats.stackexchange.com/questions/146/pca-scores-in-multiple-regression. HTH,2010-07-20 05:53:59.0,144.0,
117,224,0,"What do you mean, standalone application?",2010-07-20 06:05:54.0,5.0,
118,225,0,"The title says 99th percentile, but obviously it should be 98th; sorry about that.  The question still holds.",2010-07-20 06:08:48.0,196.0,
119,167,2,No. FA is not regression. I am referring to a response variable regressed against the principal components computed from a large number of explanatory variables. The principal components themselves are closely related to the factors of FA.,2010-07-20 06:14:32.0,159.0,
120,223,1,What's the significance of the fact that your friend is an MD?,2010-07-20 06:25:48.0,5.0,
121,225,1,You should be able to edit the title (I did a similar thing myself earlier).,2010-07-20 06:26:28.0,173.0,
122,224,0,By standalone application I mean an executable program.,2010-07-20 06:32:54.0,128.0,
123,223,0,"I think the significance of the MD is the person in question is a busy clinician, and not a researcher. Therefore we are being guided towards your less weighty tomes as source recommendations. Well, that's from the MD's I come across anyway.",2010-07-20 06:42:40.0,199.0,
124,25,0,I mean anything (GUI or command line based) that can run on Windows operating systems.,2010-07-20 06:57:11.0,69.0,
126,167,0,"I'm sorry, I should have been more precise in my comment. Your writing that the explanatory variables can be reduced to a small number of PC rang me ""factor analysis"" bell.",2010-07-20 07:32:47.0,144.0,
127,223,0,"Exactly! The fact that he is an MD poses a few restriction on (a) the volume of the introductory material (b) on what to assume about the ""starting point"" and (c) on the time willing to spend to reintroduce himself with basic stuff.",2010-07-20 07:48:49.0,79.0,
128,118,2,"In accepting an answer it seems important to me that we pay attention to whether the answer is circular.  The normal distribution is based on these measurements of variance from squared error terms, but that isn't in and of itself a justification for using (X-M)^2 over |X-M|.",2010-07-20 07:59:54.0,196.0,
129,10,2,Technically Likert scales are the sum of Likert-type items and as such end up being a reasonable approximation (at least according to many psychometricians in Psychology) of an interval data point.,2010-07-20 08:03:30.0,196.0,
130,240,0,"Univariate boxplots are useful for spotting univariate outliers. But they can completely miss multivariate outliers.

The regression idea is ok if I had a Y and a bunch of X variables. But as I said in the question, there is no Y so regression is inappropriate.",2010-07-20 08:12:10.0,159.0,
132,241,0,"Yes, I could compute the Mahalanobis distance of each observation from the mean of the data. But the observations with the greatest distance from the mean are not necessarily multivariate outliers. Think of a bivariate scatterplot with high correlation. An outlier can be outside the main cloud of points but not that far from the mean.",2010-07-20 08:14:21.0,159.0,
136,229,0,"Is there a way to express in plain language what the difference is between a given percentile and the maximum of N values?  From a lay perspective, it is hard to see why a datapoint that comes from a given (Y) percentile wouldn't be expected to be (on average) the same as the top scorer from a group of 100/Y.  

For example, if I found that your answers were ranked in the 90th percentile, I'd expect that your answer would usually be the top answer among any randomly selected group of 10 answers.",2010-07-20 08:45:52.0,196.0,
137,242,0,"could you edit the title to something like ""Using time series analysis to analyze/predict violent behavior""?",2010-07-20 08:56:46.0,87.0,
138,134,2,"Most programmers know ""median"".  (sort(array))[length/2] is a big enough hint for those who forgot.  Also at its most basic for each new point you only need to do a bisection/insert on one half of the array...",2010-07-20 09:04:20.0,87.0,
139,250,1,R is also lazy evaluated.,2010-07-20 09:04:48.0,88.0,
140,189,0,"Thats the same as taking the derivative, but just more inaccurate so why would you do it?",2010-07-20 09:39:03.0,214.0,
141,261,0,How about putting it in publication?,2010-07-20 09:59:57.0,217.0,
142,263,0,How to make a vector file usable in a publication using it?,2010-07-20 10:01:17.0,217.0,
143,261,4,"R produces some of the best quality graphics around. As an editor of an international research journal, I would love all our authors to use R.",2010-07-20 10:02:05.0,159.0,
144,263,0,@Łukasz I added it to my answer,2010-07-20 10:11:50.0,190.0,
145,263,0,Could you put a code snippet for completness? It would be very useful for people in the future finding this page.,2010-07-20 10:19:14.0,217.0,
146,262,0,How about saving to file?,2010-07-20 10:19:47.0,217.0,
147,254,0,Thank you. Would you please explain the two lines of R code for those of us (me included) who don't even understand R syntax.,2010-07-20 10:33:59.0,213.0,
148,250,0,"@mbq good to know, thanks!",2010-07-20 10:48:05.0,171.0,
149,257,2,"How do you define 'publication-quality'? Please elaborate on what aspects you like to see covered... e.g. color use, line widths, etc. Should answers focus on font size, instead?",2010-07-20 10:51:44.0,107.0,
150,263,0,"@Łukasz Hmm, some suggestion how to upload an svg figure?",2010-07-20 10:52:57.0,190.0,
151,261,1,".. see my comment on the question... how do you define 'publication-quality', or 'best quality'... from a editor perspective?",2010-07-20 10:53:25.0,107.0,
152,261,12,"I like to see vector graphics (no jpegs), graphical design following the principles of Tufte & Cleveland, readable fonts, uncluttered legends, no shaded backgrounds, sensible axis limits and tick intervals, labelled axes, no overlap of text and plotting characters or lines, etc. Most authors use the default settings of their software, so good software has good defaults. This is where Excel fails miserably and R does pretty well. But it is possible to produce lousy graphs in R and good graphs in Excel. It's just easier to produce high quality graphics in R.",2010-07-20 11:07:42.0,159.0,
154,267,0,"You should use Colin's answer, still your idea of making Monte Carlo simulation is also correct.",2010-07-20 11:14:06.0,88.0,
155,163,0,"It's a good example, but I'd rather see a proper definition (a function from a sample space to the real numbers, which you can make more general) followed up by an example.",2010-07-20 11:14:55.0,62.0,
156,254,0,Thank you again. Please see my edit to the original post that clarifies the problem even more,2010-07-20 11:21:08.0,213.0,
157,274,0,"Good answer. I think you should go further into what you mean by ""do your inference based on that"". That's kind of the second part of my question.",2010-07-20 11:36:16.0,62.0,
158,274,0,"mmm... I didn't really understand what you meant by what common variables and statistics... Oh, do you mean like you use z distribution if you have the population variance and the t-distribution if you only have the sample variance and the sample size is small? Something along those lines?",2010-07-20 12:03:37.0,90.0,
159,278,1,"Your random reordering reminded me of this AI koan: In the days when Sussman was a novice Minsky once came to him as he sat hacking at the PDP-6. ""What are you doing?"", asked Minsky. ""I am training a randomly wired neural net to play Tic-Tac-Toe."" ""Why is the net wired randomly?"", asked Minsky. ""I do not want it to have any preconceptions of how to play."" Minsky shut his eyes. ""Why do you close your eyes?"", Sussman asked his teacher. ""So the room will be empty."" At that moment, Sussman was enlightened.",2010-07-20 12:40:15.0,56.0,
160,263,3,You could have mentioned in your answer that matplotlib allows rendering of all typography in the plot with LaTeX so it perfectly integrates visually.,2010-07-20 12:49:37.0,56.0,
162,213,1,"If a scatterplot matrix won't catch it, you could try a 3D scatterplot.  That won't work out to 4D, of course, but then you could create a 4th dimension as time and make a movie.  :)",2010-07-20 13:06:49.0,5.0,
163,298,1,Are you asking about how to reduce the effect of outliers or when to use the log of some variable?,2010-07-20 13:14:06.0,56.0,
164,298,7,"I think that the OP is saying ""I've heard of people using the log on input variables: why do they do that?""",2010-07-20 13:24:23.0,5.0,
165,274,0,"What I was getting at was that mean and standard deviation are parameters associated with the population, but they're estimated by the sample mean ((1/N)*\\sum(x_i)) and the sample standard deviation ((1/(N-1))*\\sum(x_i - x^bar)^2).",2010-07-20 13:42:11.0,62.0,
168,249,3,"The m_i's do not have to be equal

Wikipedia has a simplified description of the model.",2010-07-20 14:17:12.0,8.0,
170,163,1,"@Baltimark That's a valid comment which I generally agree with but would someone who is new to statistics be scared away by a formal definition and then not proceed on to the more intuitive example? I have edited my answer, but have still kept it simple :)",2010-07-20 14:38:40.0,81.0,
171,262,0,+1 for R and ggplot2,2010-07-20 14:38:52.0,22.0,
172,119,4,I agree. Standard deviation is the *right* way to measure dispersion if you assume normal distribution. And a lot of distributions and real data are an approximately normal.,2010-07-20 14:40:02.0,217.0,
173,244,0,+1 for comprehensive collection,2010-07-20 14:40:51.0,22.0,
174,9,3,"If moving from Excel is the issue, you could try:

* http://www.coventry.ac.uk/ec/~nhunt/pottel.pdf 

* http://www.forecastingprinciples.com/files/McCullough.pdf 

* http://www.lomont.org/Math/Papers/2007/Excel2007/Excel2007Bug.pdf 

* http://www.csdassn.org/software_reports/gnumeric.pdf",2010-07-20 14:44:45.0,229.0,
175,163,0,"You're right, but I'm still confused as to whether answers here are being addressed to rank beginners, or someone with a minimum of knowledge. Correct me if I'm wrong, but when I was in college, we usually took stats after a certain amount of other math classes (for me, it was after calculus, so I was familiar and comfortable with the definition of functions).",2010-07-20 14:44:49.0,62.0,
176,301,0,"But still, using log changes the model -- for linear regression it is y~a*x+b, fo linear regression on log it is y~y0*exp(x/x0).",2010-07-20 14:50:05.0,88.0,
177,309,4,Especially where students are concerned.,2010-07-20 14:51:13.0,71.0,
178,163,2,"@Baltimark I'm not too sure myself tbh, I've just been basing the level of my answers on how the questions are asked, so e.g. if someone asks a question showing that they have some knowledge of the field, then you can say things that won't scare them away. This question seemed basic so I assume a very simple reply is appropriate. I'm sure http://meta.stats.stackexchange.com/ has some more guidance.",2010-07-20 14:51:44.0,81.0,
179,232,0,"+1 for Mondrian - very useful toy, especially for large data",2010-07-20 14:54:06.0,22.0,
180,310,0,"You're not missing something if all you're trying to do is estimate the parameter from a set of observations. That was definitely the main idea of the OP's question. However, she was also asking generally (if not rigorously) ""how to estimate poisson models"". Perhaps she wants to know the value of the pdf at a specific point. In that case, the normal approx. is probably going to be better than scaling the parameter, and the observations by 100, or whatever, if the observations are large enough to make calculating the factorial impractical.",2010-07-20 14:54:19.0,62.0,
182,220,0,I have voted to close this question for the reasons mentioned in this meta thread (http://meta.stats.stackexchange.com/questions/67/what-should-be-our-criteria-for-closing-questions). Please go to the meta thread if you think that we should not close questions like the one above.,2010-07-20 15:01:21.0,,user28
183,10,2,"@drknexus - So, multiple items serve as a measurement triangulation for construct scales?  If yes, what are the criteria for determining that a researcher has enough relevant data points (i.e., items) to use the scale as an interval measurement?",2010-07-20 15:06:18.0,24.0,
185,313,0,"First of all, this is just a big example and doesn't really explain explain the concept of p-value and test-statistic. Second, you're just claiming that if you get fewer than 5 or more than 15 white marbles, you reject the null hypothesis. What's your distribution that you're calculating those probabilities from? This can be approximated with a normal dist. centered at 10, with a standard deviation of 3. Your rejection criteria is not nearly strict enough.",2010-07-20 15:21:27.0,62.0,
186,224,4,This should probably be community wiki since there is no definitive answer.,2010-07-20 15:30:58.0,5.0,
188,304,1,"Answers will be reordered based on votes, so please try not to refer to other answers.",2010-07-20 15:54:34.0,220.0,
189,301,0,"I agree - taking log's changes your model. But if you have to transform your data, that implies that your model wasn't suitable in the first place.",2010-07-20 16:00:09.0,8.0,
190,99,0,+1 for multicore - very useful indeed,2010-07-20 16:02:04.0,22.0,
191,316,0,+1 for RODBC and sqldf - i find them essential as well,2010-07-20 16:02:52.0,22.0,
192,307,8,"I seen examples where a model has ten data points and nine parameters. On pointing out that the model has too many parameters, I was told that the R^2 was 0.999 so the model must be correct!",2010-07-20 16:03:44.0,8.0,
193,321,0,What other variants?  It might be helpful to tighten up this question a little (more specificity).,2010-07-20 16:03:59.0,5.0,
195,138,6,You should add your background. Programmers who came to R have different issues than people without a programming background.,2010-07-20 16:06:54.0,3807.0,
196,323,0,These were both listed in the original question...,2010-07-20 16:12:38.0,5.0,
197,114,3,I added the blogs in the question as answers to allow proper voting to find the most popular blogs.,2010-07-20 16:13:35.0,3807.0,
198,285,0,"Thanks Jeromy. I explored several possible transformations of data using Stata's gladder function for the ladder of powers.

Teoreically based solution will be one way to go, but before that I wanted to check for the data driven solution.",2010-07-20 16:19:56.0,22.0,
199,60,0,Thanks Reed. k-means is definitely a way to go and I explored several solutions. However I am trying to go for a solution without subjective decision of the number of clusters.   Hierarchical approach on the other hand produced huge classification and again it was hard for me to specify where to stop.,2010-07-20 16:22:43.0,22.0,
200,323,10,"By that reasoning another question which asks for the best blogs but lists other blogs in the starting question wouldn't be a duplicate of the question.
I think it makes sense to vote on all blogs to find out which are most popular including those already listed in the OP.",2010-07-20 16:22:54.0,3807.0,
201,268,0,Thanks Egon. SOM approach might be very interesting indeed. Will have a look at it.,2010-07-20 16:23:45.0,22.0,
202,254,0,"Colin, I tried to figure out how to expand the Wikipedia formulae to non-equal m_i's, but didn't have any success. Can you please help me with this? (sorry)",2010-07-20 16:38:50.0,213.0,
205,307,0,All worship the mighty R^2!,2010-07-20 17:00:03.0,229.0,
206,43,0,"fine, you win :)",2010-07-20 17:06:12.0,74.0,
209,321,0,Right; it is just one variant among others.,2010-07-20 17:11:25.0,88.0,
211,333,1,"These are good references, but I disagree with your assessment of Ed Thrope as the founder of this field.  Statistical analysis of financial data and statistical arbitrage are not the same thing: one would perform statistical analysis for most financial analysis (e.g. modern portfolio theory).",2010-07-20 17:20:49.0,5.0,
213,333,1,"I agree, Markowitz definitely invented portfolio theory",2010-07-20 17:55:23.0,74.0,
214,86,0,@walkytalky Thanks for the corrections- I have made some fixes.,2010-07-20 18:00:44.0,13.0,
215,134,0,@walkytalky I don't think it would require any more explanation than any other algorithm question on SO.  Probably less as the median is a relatively basic concept.,2010-07-20 18:15:39.0,13.0,
216,127,4,"Agree strongly. Both great books. Start with Bayesian Computation With R, then get Gelman et al.",2010-07-20 19:12:45.0,247.0,
217,321,0,"OK, I'll ask for comparison to adaboost since that is perhaps the best known.",2010-07-20 19:24:06.0,220.0,
218,306,0,"Agreed - just to throw out some additional ideas on modeling: logistic to predict which patients will have 1+ violent outbursts, Poisson(esque) regression to predict which patients will have many outbursts,  multilevel to examine variations from room-to-room and/or ward-to-ward...",2010-07-20 20:13:46.0,71.0,
219,7,0,I didn't even see that checkbox.  Done :-),2010-07-20 20:51:13.0,38.0,
220,310,1,"@Srikant, you are right, to estimate the parameters the factorial is not an issue, but in general you will want the value of the likelihood for a given model, and you would have to use the factorial for that. Also, for hypothesis testing (e.g. likelihood ratio test) you will need the value of the likelihood.",2010-07-20 21:31:03.0,90.0,
221,310,0,"@Baltimark: yes, I want to know in general, whether it is valid to change the unit of measurement of Poisson. I was asked this question and I didn't know what to say.",2010-07-20 21:33:48.0,90.0,
222,302,0,"the two answers look too different to me. One says 20 to 30, the other says 20 to 30 times slopes. So if you have 5 slopes, one rule tells you 20 to 30, the other 100 to 150 observations. That doesn't seem right to me....",2010-07-20 21:37:31.0,90.0,
223,313,0,"I would agree that this is just an example, and I it is true I just picked the numbers 5 and 15 out of the air for illustrative purposes. When I have time I will post a second answer, which I hope will be more complete.",2010-07-20 22:00:38.0,226.0,
224,337,0,"I know, however that doesn't help me to decide whether using sample entropy or shannon entropy or some other kind of entropy is appropriate for the data that I'm working with.",2010-07-20 22:31:17.0,3807.0,
226,337,1,"What I wrote in my post is just that for a certain type of data/process/system there is only one *true* entropy definition. Sample Entropy is *not* an entropy measure, it is just some statistic with a confusing name. Make a question where you define the data for which you want to calculate the entropy, and will get the formula.",2010-07-20 23:17:28.0,88.0,
227,302,0,They are pretty different guidelines.  I suspect the disconnect is whether you think that the test of the overall model matters (the lower N guideline) or the test of the individual slopes that matter (the higher N guideline).,2010-07-20 23:48:55.0,196.0,
228,358,1,The answer in that other thread don't really explain why 2 is a better value than other values that are very near to 2 but are no natural numbers.,2010-07-21 00:04:53.0,3807.0,
229,10,1,"I'm not sure; that might be a worthy question for the community in general.  I'd guess that it is probably in part a value judgement on the part of the researcher & area.  Some areas are completely willing to treat a single Likert item as interval even though it clearly is ordinal.  A reasonable answer might be to use a different analysis method, e.g. a permutation or bootstrapped test.  Another answer might be to conduct a simple test of normality, so long as the aggregate doesn't significantly depart from normality you are probably okay.",2010-07-21 00:07:20.0,196.0,
230,360,1,"Your point about month N's count not necessarily being correlated with N-1 is well-taken.  With a slow-growing disease like TB, that's something I'd have to look at carefully, but I'm pretty sure I could identify about how much lag there is between the time we report a source case and the time we report any secondary cases.",2010-07-21 00:11:05.0,71.0,
231,360,1,"However, it's your point about analyzing the distribution of monthly counts that's at the heart of my question.  There is a definite decline in TB, both nationally in the US and in my district.  For example, when I compare 2009 to the previous years, there are decidedly fewer cases.  2010 is on track to have fewer still.  What I'm trying to identify (which I did a poor job of explaining in the question) is whether or not these declines are part of an ongoing downward trend, or just a downward wobble.  Thanks - you've gotten me to think much more carefully about the problem.",2010-07-21 00:15:26.0,71.0,
232,358,0,I think it does; still I'll try to extend the answer.,2010-07-21 00:21:35.0,88.0,
234,135,3,"For R ks.test in the default ""stats"" package can conduct the KS test without installing additional packages.",2010-07-21 00:23:25.0,196.0,
235,10,1,... but in general it seems like one could evoke the central limit theorem and suggest that 20 to 30 items should be sufficient to use the scale as an interval measurement.,2010-07-21 00:26:48.0,196.0,
236,310,0,"@Vivi: I am not sure why you would want to compute the likelihood with k_i! included as in most applications (e.g., likelihood ratio test, bayesian estimation) the constant will not matter. In any case, I do not think you can re-scale as you suggested. If I feel otherwise I will update my answer.",2010-07-21 00:28:39.0,,user28
237,310,0,"@Srikant, I see your point, but some softwares (Eviews, for example) include this by default, and large numbers are an issue you like it or not. I guess I was really after an explanation of why you can or can't do it rather than a way around it, but the discussion has been interesting and instructive nonetheless  :)",2010-07-21 00:42:20.0,90.0,
239,217,0,Protovis looks awesome but do you know what browser support it has? particularly IE?,2010-07-21 01:39:03.0,191.0,
240,134,0,@Sharpie Perhaps not more than other SO algo questions. But certainly more than what it actually says here!,2010-07-21 02:58:14.0,174.0,
241,227,0,"Sorry, but what are loadings (c in your formula) and how do you determine them?",2010-07-21 03:17:32.0,191.0,
242,234,0,Would there be any benefit or could you plot them on a 3-d scatter plot?,2010-07-21 03:18:06.0,191.0,
243,226,0,"How does a higher Y value ""explain"" a bigger chunk of the variance? Is it how the PCA is computed? If so I think I've got another question to post ;)",2010-07-21 03:31:11.0,191.0,
244,287,1,See http://en.wikipedia.org/wiki/Method_of_moments_(statistics) and  http://en.wikipedia.org/wiki/Generalized_method_of_moments,2010-07-21 03:52:40.0,,user28
245,30,0,Similar question on SO: http://stackoverflow.com/questions/56411/how-to-test-randomness-case-in-point-shuffling,2010-07-21 05:09:44.0,68.0,
246,215,0,"I agree completely with what you're saying, but the reason for looking at tests here is to satisfy others. The situation is modelling possible extreme operational losses bassed on historical loss experience and the regulator needs to be convinced that the choice of distribution is supported by the data. What the regulator thinks is reasonable and what the business thinks is reasonable for the results can differ quite a bit! Using a (reasonably) standard statistical test may provide a somewhat independent approach to the justifying a particular choice.",2010-07-21 05:40:47.0,173.0,
247,372,2,"I think this is a too broad question. Because almost all statistics can be used in data mining, I don't see any reason for this question to exist.",2010-07-21 06:11:42.0,190.0,
249,375,2,Or this one http://stats.stackexchange.com/questions/30/,2010-07-21 06:16:52.0,56.0,
250,368,0,"I don't have any requirements how correct my ASR should be. For example, when I change my model a bit my error goes down from 30% to 29.8%. For that change I want to now if it's significant",2010-07-21 06:17:05.0,190.0,
252,215,0,Then using a 'distance between distribution test' (like chi square or Kolmogorov-Smirnov or ... is a better idea because it is easely understood by the end user.,2010-07-21 06:57:31.0,223.0,
253,372,1,"All statistics are usable, but if your goal is to study the *most* important (or often used) parts of statistics when applied to Data Mining - a subset would be useful.",2010-07-21 07:46:57.0,252.0,
254,372,1,Suggestion: why not made this a community wiki ? (p.s I wasn't the one who voted to close this - but I understand why someone voted for it),2010-07-21 07:56:08.0,253.0,
255,372,0,Good idea. Community wiki = on!,2010-07-21 08:05:44.0,252.0,
256,113,0,What is the difference between model selection http://www.modelselection.org/ (hot topic in statistic during the past 20 years) and method selection.,2010-07-21 08:30:20.0,223.0,
257,385,1,"I agree, but I was indeed worried about the k parameter -- if the unbalance will create some class-wise differences in the observation densities in the feature space, the same k will tend to a smaller sphere in feature space for an observation from denser class. Won't it influence the k parameter optimization then?",2010-07-21 08:45:38.0,88.0,
258,372,0,I agree with @Peter; way too vague.,2010-07-21 09:40:14.0,5.0,
261,368,0,"In that case you would use test 2. See the table for ""Two-proportion z-test, pooled for d0 = 0"" at the wiki: http://en.wikipedia.org/wiki/Statistical_hypothesis_testing#Common_test_statistics",2010-07-21 10:44:25.0,,user28
262,344,1,Can you give a description a problem where RNB gave you good results?,2010-07-21 10:57:06.0,217.0,
263,344,0,No ;-) This was only to revive the pool.,2010-07-21 11:05:37.0,88.0,
264,395,1,"Look at http://stats.stackexchange.com/questions/173/time-series-for-count-data-with-counts-20/ . It's not a duplicate, but the problem is similar at a first sight.",2010-07-21 11:20:20.0,88.0,
265,386,0,"How would this deal with cases where you don't know how many outliers you have, i.e. when the N-1 points still have a bias since they include outliers?",2010-07-21 11:58:10.0,56.0,
266,386,1,"if n is sufficiently large and the number of outlier is small then this bias is negligible. If there are a large number of outliers then, maibe it is not outliers and anyway, as I mentionned you can use leave k out strategy ... (in this case, you have to find out a strategy to avoid tracking all configurations which may be NP hard ... ) and if you don't know k, you can try many values for k and keep the most relevent.",2010-07-21 12:07:43.0,223.0,
267,154,3,Cool! Please post a link here to your thesis once it's complete and/or published!,2010-07-21 12:20:54.0,6.0,
268,293,2,"Yes, rather than starting with the data, start with the question and the data generating process.",2010-07-21 12:35:24.0,46.0,
269,192,3,This question doesn't make a lot of sense - where is your statistical/scientific question?,2010-07-21 12:35:50.0,46.0,
270,262,1,"Or a little more succinctly with melt and qplot: `m <- melt(d, id = ""x""); qplot(variable, value, data = m, colour = variable)`",2010-07-21 12:39:58.0,46.0,
271,400,13,I think you mean sent by fax at some point in the **past** ;),2010-07-21 12:42:57.0,46.0,
272,165,7,Here is my favorite paper about the topic: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pdf,2010-07-21 13:28:05.0,88.0,
273,217,1,"That's unfortunately one of Protovis's weak points (but it's an issue with any SVG library because IE doesn't support that).  Fortunately, Jamie Love has come up with a solution using SVGWeb.  See here: http://groups.google.com/group/protovis/browse_thread/thread/1a80f98a16736658?pli=1.",2010-07-21 13:37:53.0,5.0,
274,410,0,I've updated the question with the information on kind of time series/mean I'm talking about.,2010-07-21 13:41:09.0,219.0,
275,399,2,"Tufte's Visual Display of Quantitative Information (http://www.amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20) is better than Beautiful Evidence IMO. All four of his books are good though, and if you have an opportunity to attend one of his courses, do it.",2010-07-21 13:57:31.0,36.0,
276,415,0,Thanks!  I was out of town last week and missed that MO post.,2010-07-21 14:01:32.0,89.0,
277,133,0,I thought Chi-Sq was primarily for categorical data (contingency tables) vs. continuous?,2010-07-21 14:17:35.0,23.0,
278,13,2,"it's similar to using planet gravitational models to urban traffic. I find it absurd, but it works quiet accurately actually",2010-07-21 14:25:01.0,59.0,
279,405,0,We have done this in two different ways and it has proved very useful with the large movements this week lcearly being outside the histogram of typical results.,2010-07-21 14:47:33.0,210.0,
280,402,0,"Heh - this was actually one of the plots I made that didn't make it into the post.  The problem I had is deciding how to calculate the bounds - my initial attempt was with Poisson bounds, with lambda set to the mean of my data, but variance is too high to be a proper Poisson (high enough to matter in practice?  I don't know).",2010-07-21 15:01:32.0,71.0,
281,402,0,"A further problem is that the center of the distribution can change over time - for example, it wouldn't make sense to set those bounds using data from the early 1900s, when Colorado was a haven for TB patients.  So what's an appropriate way to keep the lines up-to-date with long-term changes in the process, while still being able to identify deviations?",2010-07-21 15:02:21.0,71.0,
282,217,0,IE 9 will also support SVG- so as long as you don't need to work with IE 6...,2010-07-21 15:08:29.0,13.0,
283,349,0,"The problem with the binning approach is that we do not have a good upper bound for the data, and so the midpoint for the largest bin would have to be huge. So, we'd need a huge number of bins (not enough memory for that), or have pretty wide bins (which would then lead to a fairly inaccurate answer.)

And the data is not very sparse.",2010-07-21 15:13:35.0,247.0,
284,404,0,"I'll have to read it more thoroughly later on, but yes, this package is definitely addressing the kinds of problems I'm facing here.  Thanks!  And also, thanks for the kind words about the plots ;p",2010-07-21 15:16:07.0,71.0,
285,352,2,"The data set is potentially too big to read in half of it...it is in a networking context where the device doing the processing can see tens of thousands of items per second, and probably has enough memory to store only a few hundred.

Also the data is definitely not Gaussian. In fact it does not fit well to any of the common distributions.",2010-07-21 15:17:56.0,247.0,
286,424,3,Absolutely love this one.,2010-07-21 15:23:11.0,13.0,
287,346,3,"Perhaps, asking on Stackoverflow may get better answers.",2010-07-21 15:33:20.0,,user28
288,113,2,"While model selection typically involves the scoring of models within a family of distributions, based on their fit and penalizing the number of parameters used (a la AIC and BIC), whereas method selection is more general.  Method selection involves being faced with a problem (e.g. test, classify, predict) for which we have some background knowledge (variables are known to be (e.g. independence, data type), and for which auxiliary assumptions are made (e.g. normality, homoscedasticity), and we must select a method.",2010-07-21 15:50:24.0,39.0,
289,113,1,"Now there are mathematical prescriptions along the lines of measurement type, convergence results, optimality, and time/space complexity, but no framework for their systematic application, that I am aware of, thus the question.",2010-07-21 15:53:08.0,39.0,
290,260,0,"Aptamer active motif selection, forest ground humidity forecasting, digit OCR, multispectral satellite image analysis, musical information retrieval, chemometry...",2010-07-21 15:58:58.0,88.0,
291,348,0,"The way I wrote it up, specifically with the bayesian not knowing much about cat reproduction, at the beginning only the frequentist would bet on there being kittens.  The relevant points of my *very crude example* were mostly that the frequentist made his prediction based on the data at the beginning, then sat back without incorporating new supplementary data, while the bayesian didn't have much data to begin with, but continued to incorporate relevant data as it become available.",2010-07-21 16:09:12.0,24.0,
292,135,0,"Thanks for the information, drknexus.",2010-07-21 16:53:25.0,39.0,
294,36,4,should be community wiki,2010-07-21 17:08:26.0,219.0,
295,428,0,"This is interesting, and where some statistical advice could come in! Assume in total I've got (say) 500,000 i.i.d. points and I look at groups of (say) 1,000 of them, and calculate the median of each group. Now I've got 500 medians. Is there theory that could allow me to calculate a confidence interval for the overall median based on these 500 medians?",2010-07-21 17:10:16.0,247.0,
298,223,2,"Is he looking to perform statistical analyses, interpret the output or critique published papers that use statistical methods?",2010-07-21 17:23:52.0,215.0,
300,437,0,I guess I should take the first suggestion as a vote of confidence.,2010-07-21 17:52:02.0,89.0,
301,410,0,@Silent: We would still need to guess what your problem really is. What is N? Number of sunflares in hundreds or the voltage you measure on some battery? It would suggest you plot your monthly means in a histogram. Fit this with a Gaussian an see if it fits and if you can justify a Gaussian fit from your model.,2010-07-21 19:24:05.0,56.0,
302,244,0,+1 for Graphviz which is often missed and really useful,2010-07-21 19:33:01.0,212.0,
303,457,1,"Are you referring to a particular paper? I imagine I could find an answer to my question if I researched, studied, read a lot, but so could 95% of the questions other people ask here... Also, in some cases, particularly with macroeconomics data (which is my area), there is no more data to be collected. Data is scarce (the number of observations, I mean), and you just have to live with it. There is no ""get more data"" solution. I was hoping someone here would know the topic, but it doesn't seem like. Maybe once the website is opened to the general public?",2010-07-21 20:07:46.0,90.0,
304,461,0,Indeed some people are taking Tufte as gospel and not being particularly flexible...,2010-07-21 20:31:16.0,259.0,
305,464,0,"No, nothing to do with Qnotifier, and that's not mine, it's a commercial product. The data I'm working with is very similar to that though.

Completely agree with the ""color the part over the threshold"" but the plotting tools in use currently here don't allow that level of control. :-(",2010-07-21 20:42:35.0,259.0,
306,450,0,Can you give examples of alternatives? I'd like to look into that.,2010-07-21 20:56:10.0,77.0,
307,450,5,"The mostly known and the simplest is the Median-Median regression, well known from smart calculators (Sigh!). Consult also Wikipedia  http://en.wikipedia.org/wiki/Robust_regression and maybe CRAN's Robust task view http://cran.r-project.org/web/views/Robust.html",2010-07-21 21:20:20.0,88.0,
308,464,0,Too bad... Still you can make the same trick only with the color of the threshold line.,2010-07-21 21:23:35.0,88.0,
310,133,1,Hmmm I actually like the KS test answer better than mine !,2010-07-22 00:07:38.0,139.0,
311,185,0,"I actually scanned that at work yesterday. It's an interesting read - I wish I had more time to absorb the material in it, but I had to get what I needed and move on.",2010-07-22 00:39:46.0,110.0,
312,457,0,I suspect the answer to your question will be domain/model specific and hence I am not sure I can recommend a specific paper.,2010-07-22 01:19:43.0,,user28
313,247,3,"More specifically, the asker may be interested in [JFreeChart](http://www.jfree.org/jfreechart/) which powers a lot of Incanter graphics.",2010-07-22 03:23:01.0,13.0,
314,262,0,"Actually, an even easier way is to use R+deducer with ggplot2 (there is a new release of this which is about to come out in the next few months.  A beta is currently available)",2010-07-22 03:57:44.0,253.0,
315,423,2,I do have to ask though- how come cartoons are in and jokes are out?,2010-07-22 05:09:53.0,13.0,
316,423,0,"@sharpie: are jokes out?  We obviously don't want the entire site to be humor, but everyone benefits from a little educational humor in small doses.",2010-07-22 05:15:40.0,5.0,
317,479,0,"Totally agree. By making it a background it is clearly not data, but my making it coloured it shows a clear change in 'state'.",2010-07-22 09:04:20.0,210.0,
318,461,0,Leaving the reader to figure things out has been a source of countless problems. You have to communicate your message properly and if your message is the data over this line is an issue you must shown the line which is concerning.,2010-07-22 09:47:28.0,210.0,
319,151,25,"""Squaring always gives a positive value, so the sum will not be zero."" and so does absolute values.",2010-07-22 09:54:23.0,223.0,
320,483,0,"Sorry, bu I don't really understand your answer. What do you mean by ""In detail, each tree is build on a sample of objects drawn with replacement from the original set""

 Can you give more precision on where I find the details ""here""?",2010-07-22 10:04:51.0,223.0,
321,262,2,"Nice example, but the plot is hardly publication quality. Or at least none of the journals I publish in would accept it.",2010-07-22 11:02:42.0,214.0,
322,411,1,"I like the question, there may be already most of the possible answer in the question... do you have an idea of the type of answer/developpement you want?",2010-07-22 11:18:44.0,223.0,
323,485,2,Should be community wiki.,2010-07-22 11:21:23.0,88.0,
324,483,0,This is how bagging works; check out http://en.wikipedia.org/wiki/Bootstrap_aggregating . Here is a link  (hardly visible in that theme I admit) to the detailed RF reference.,2010-07-22 11:24:12.0,88.0,
325,48,2,can you give one of those or advise a particular one in the book?,2010-07-22 11:30:13.0,223.0,
326,490,0,"Is it a question or a pool? If the latter, it should be community wiki. If the first, give more details about what you want to achieve? For instance, is it all-relevant or rather  minimal-optimal selection? How much is many? How   hard is the classification problem?",2010-07-22 11:38:20.0,88.0,
327,492,0,What about slow (standard) Fourier transform.,2010-07-22 11:45:24.0,88.0,
328,490,0,pool... many means 1000 features or more and less than 100 observations.,2010-07-22 11:58:08.0,223.0,
329,498,1,"Also, I know this might get flagged off-topic, or not statistical, but maybe not. I wouldn't mind seeing any SAS/R/SPPS/Stata question be fair game here. Statisticians should be the most experienced in these packages.",2010-07-22 11:59:44.0,62.0,
330,498,0,"imho offtopic. Even though it is an ""Analysis"" program, the question has definitely nothing to do with statistical analysis. Suggestion: contact the manufacturer of SAS, they can fix the problem.",2010-07-22 12:05:58.0,190.0,
331,498,1,"See also http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions . Option 3 seems to be favored here, which means that questions that have not anything to do with statistics are closed/move. I think this is the same case.",2010-07-22 12:12:06.0,190.0,
332,503,0,"Yes, certainly when I need to I output what I want into excel, or csv, or whatever, but at times, I just want a quick  C & P. It behave very oddly. . .not even giving me the option of ""copy"" from the ""edit"" drop-down.",2010-07-22 12:33:19.0,62.0,
333,168,1,"If I could vote up more than once, I would do it!",2010-07-22 12:38:01.0,223.0,
334,460,0,"The procedure I already gave can be applied in large dimension, as I said, using a gaussian assumption.  If the dimension is really large with respect to the sample size (i.e. p>>n) then you can  make some sparcity assumption (assume that the parameters of your gaussian distribution lie in a low dimensional space for example)  and use a thresholding estimation procedure for the estimation of the parameters...",2010-07-22 13:16:30.0,223.0,
335,151,13,"@robin girard: That is correct, hence why I preceded that point with ""The benefits of squaring include"". I wasn't implying that anything about absolute values in that statement. I take your point though, I'll consider removing/rephrasing it if others feel it is unclear.",2010-07-22 13:19:20.0,81.0,
336,498,2,"I think questions about how to implement a statistical analysis in SAS/R/whatever are ok. But, purely interface questions are off-topic.",2010-07-22 13:38:33.0,,user28
337,423,0,"@Sharpie, feel free to close or reopen according to your feelings! I agree with Shane, a bit is ok, but not too much. For example, this question already included a funny cartoon. The jokes question not really a funny joke....",2010-07-22 13:58:03.0,190.0,
338,423,25,These cartoons are useful too; they can be included in a lecture on a particular topic where you are trying to explain a concept (e.g. correlation/causation above).  A little humor can help to keep an audience engaged.,2010-07-22 14:22:11.0,5.0,
339,411,1,"Not very specifically.  I'm quite ignorant of statistics and one of my reasons for asking is to learn which criteria statisticians would use to pick between different metrics.  Since I did already describe one important practical advantage of 1 (you can actually compute it) I'm especially interested in theoretical motivations.  Say, is the information provided by estimates of Kolmogorov distance frequently of direct use in applications?",2010-07-22 14:50:34.0,89.0,
340,328,0,"Given the current state of national economies due to the rescue of the various financial institutions, one may question the value of accepted knowledge in this field, save for greater fool theory.",2010-07-22 15:06:12.0,229.0,
341,36,4,"That pirates / global warming chart is clearly cooked up by conspiracy theorists - anyone can see they have deliberately plotted even spacing for unequal time periods to avoid showing the recent sharp increase in temperature as pirates are almost entirely wiped out.
We all know that as temperatures rise it makes the rum evaporate and pirates cannot survive those conditions.
;-)",2010-07-22 16:08:37.0,270.0,
342,461,0,"... right.  I wasn't suggesting he leave it to the reader *if it was a part of his message*.  I was saying that if the threshold weren't a part of the plot, he could either leave it out entirely or incorporate a subtle visual cue to make the threshold easier to find if the reader wanted to look for it.",2010-07-22 16:09:04.0,71.0,
343,155,6,Should be community wiki.,2010-07-22 16:13:35.0,88.0,
344,517,6,"First, two lines from wiki: ""In computer science, semi-supervised learning is a class of machine learning techniques that make use of both labeled and unlabeled data for training - typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data)."" Does that help?",2010-07-22 16:25:36.0,,user28
345,11,0,Can you be a little more specific?,2010-07-22 16:26:29.0,88.0,
346,517,0,"What do you have in mind with ""Algorithmic approaches""? I gave some examples of applications in my answer, is that what you are looking for?",2010-07-22 16:49:10.0,190.0,
347,520,0,"Why the  downvote? Is it not a good answer, or do you think it's not a good tool?",2010-07-22 17:06:47.0,190.0,
348,411,0,"I forgot to end my previous comment with the more or less obvious: and if so, how?",2010-07-22 17:11:20.0,89.0,
349,479,1,"If you don't want too much colour, just show a band of a neutral eg light grey - but do it in the *foreground* with some high level of transparency. This means that any part of the data line which shows above this threshold will automatically ""pop"" as it has greater contrast to the background than to your 'normal' band.",2010-07-22 17:11:24.0,270.0,
350,26,21,"Is this really the kind of question we want on this site? A simple google search, opening any stats book, or checking out wikipedia would instantly provide an answer.",2010-07-22 17:36:12.0,247.0,
352,26,2,Totally agree with PeterR,2010-07-22 18:13:11.0,,jjgibaja
353,113,0,"Can you give an example of method selection with more details (a link to a page or a paper could be fine), this could help me to figure out more precisely. Thanks in advance",2010-07-22 18:14:49.0,223.0,
354,452,0,"I am not sure of what you meant by ""Robust Standard Errors are reported as a matter of course"" standard errors of what? You said testing for ""it"" what is the test you are talking about?",2010-07-22 18:21:09.0,223.0,
355,498,1,"Vote to keep. Stat software questions have to go somewhere. Stackoverflow is not the right place. Either we need to answer them here, or start statssoftware.stackexchange.com",2010-07-22 18:22:12.0,74.0,
356,520,0,"I second the python recommendation here, especially if you have trouble fitting the data set into r. If you do go the python route,  have a look at ""Programming Collective Intelligence"" by Toby Segaran",2010-07-22 18:29:15.0,247.0,
358,519,0,"Interesting example, because it looks, at first glance, like a probable cause-and-effect relationship, unlike many of the silliest examples.",2010-07-22 19:10:45.0,77.0,
359,498,0,"Chief makes a good point. If not here, where? Is this purely ""statistical analysis"" just because of the name? Or is it a place that people working in the field can come for answers. Sometimes I just need a piece of SAS syntax that I've forgotten. I can dig through my archives, but I wouldn't mind coming here to get an answer/an alternate/a better way/ or just leave mental residue for future questioners.",2010-07-22 19:51:37.0,62.0,
360,26,0,"The same concept may have different explanations at different levels... A smart sixth grader's understanding of the concept might be very different from that of a phd student, who has thought about it a lot more. It would be nice to see the basics revisited in light of more advanced concepts. That would help me understand how everything connects",2010-07-22 20:23:56.0,35.0,
361,494,0,"Even if the constant is included, the AIC (AICc) can be negative.",2010-07-22 23:15:41.0,159.0,
362,155,4,Should definitely be community wiki.,2010-07-23 01:54:10.0,5.0,
364,532,0,"Thanks for this answer (should be a comment ? ) 

As I already mentionned, as a comment to Rich answer's High dimension are not a problem (even 1000 could work) if you make parametric structural assumption.",2010-07-23 06:43:46.0,223.0,
365,532,0,"@rob ""I'm not sure what would be a good threshold "" this would be the purpose of the multiple testing procedure I mentionned .... but I fully agree that things have to be filled in and I really like the outlier detection in the outlier detection !  who wants to writte a paper :) ?",2010-07-23 06:47:22.0,223.0,
367,494,0,That's what I've written.,2010-07-23 08:17:51.0,88.0,
370,501,0,But the question says there are more variables than observations. So it is not possible to begin with the full set.,2010-07-23 09:50:30.0,159.0,
371,534,12,Correlation and a strong underlying reason for a link suggest causation until proven otherwise is probably the best you can get.,2010-07-23 09:54:58.0,229.0,
372,452,0,"Good point....I'm talking about the Standard Errors of regression coefficients in OLS regression and the problem of heteroscedasticity. The traditional approach would be to test for the presence of heteroscedasticity using, for example, White's test or Breusch Pagan test. If heteroscedasticity is found then one would report Robust Standard Errors, usually White Standard Errors.",2010-07-23 10:09:35.0,215.0,
373,486,2,Thanks for the helpful answers.,2010-07-23 10:37:02.0,266.0,
374,541,0,"""[T]hen I am making very strong assumptions about the relative difference between consecutive values of the ordinal variable.""

I think this is the key point, really.  i.e. how strongly can you argue that the difference between groups 1 and 2 is comparable to that between 2 and 3?",2010-07-23 10:43:29.0,266.0,
375,518,0,"Nice list, it will be very useful.",2010-07-23 11:31:31.0,166.0,
376,520,0,"What downvote? I'm not familiar with Python yet, but it seems promising, I'll take a look.",2010-07-23 11:32:09.0,166.0,
377,520,0,"@Victor, the first vote was a downvote (so it's now 2 upvotes/1 downvote). Maybe someone didn't like this tool. Happy I could help :)",2010-07-23 11:34:05.0,190.0,
378,26,7,"I don't think the purpose of this site is to answer 6th graders questions. And my kid, when faced with such a question, would google for the answer.

If there is a specific part of the definition you don't understand, ask away. But such an unfocused question on such a basic topic indicates (to me anyway) that the poster didn't even try to find an answer. What is going to be next ""What is a number and how are they used?""",2010-07-23 12:05:21.0,247.0,
382,519,1,"What I like is that you can provoke lots of discussion about whether the ""effect"" was to actually impact fertility (in a medical sense of ability to conceive) or was it social (""I don't want to bring a child into this bad world""). Then drop the bombshell about the Pill if no-one else has brought it up. And then point out that even this can only be one possible factor and discuss some of the others.",2010-07-23 13:32:48.0,270.0,
383,501,0,What's the problem?,2010-07-23 13:41:29.0,88.0,
384,486,0,So consider accepting one.,2010-07-23 13:48:15.0,88.0,
385,541,0,"I think you should make some assumption about how the continuous variable should be distributed and then try to fit this ""psudohistogram"" of each categorical variable frequency (I mean find bin widths which will transform it into a fitted histogram). Still, I'm not an expert in this field, its a fast&dirty idea.",2010-07-23 13:56:36.0,88.0,
387,555,2,"If I am not mistaken, 

linear regression is the estimation of coefficients that define a good linear map from X to Y.

 ANOVA is a test to know if there is significant differences in X when Y take two different values.  

Can you explain us why you think they are the same?",2010-07-23 15:29:16.0,223.0,
388,541,0,"Recasting binary categories as {0,1} makes sense, but turning that into a continuous [0,1] interval seems like a bit of a leap. On the broader front, I'm totally with your reluctance to weight ordinals equally unless there are powerful arguments from the model.",2010-07-23 15:29:27.0,174.0,
390,455,9,Statistical voyeurism? And there we were wondering what to call the site...,2010-07-23 15:48:25.0,174.0,
391,557,0,"Thanks for the Gelman reference. I will read his paper. But, can't we analyze multilevel models using classical maximum likelihood? I agree that OLS is inefficient/inappropriate for multi-level models.",2010-07-23 15:50:47.0,,user28
392,557,1,+1 just for the lovely lucid even-handedness of this answer.,2010-07-23 15:56:32.0,174.0,
393,556,6,"Data can be discrete without being restricted to integers. Or numbers, for that matter. It's always possible to *represent* discrete data with integers, but that doesn't mean the data can only take such values.",2010-07-23 16:06:37.0,174.0,
394,557,2,"@Srikant - there any many ways to deal with multilevel data and Gelman is ""the king"" of this field. His point is that ANOVA is a simple/clear method of capturing the key features of complex and hierarchical data structures or study designs and ANOVA is a simple/clear way of presenting the key results. In this sense it's role is complementary or exploratory.",2010-07-23 16:30:00.0,215.0,
395,565,0,That's a great example!,2010-07-23 17:03:23.0,5.0,
396,566,3,"Indeed, blending is one of the possible ensemble techniques. In particular, there are two when you combine the same sort of classifier, boosting (like Adaboost) and bagging (like Random Forest), and blending, where you combine different classifiers (what was Shane's question about).",2010-07-23 17:10:02.0,88.0,
397,470,6,"Why pay $70 for the book?  To support the authors and to have a physical copy.  That's why I did it, anyway!",2010-07-23 17:13:42.0,5.0,
398,411,0,"I just reread my long comment above and realized that the last question I raised is as much a practical consideration as theoretical.  In any case, that's one of the kinds of issues I'd be interested to learn about.",2010-07-23 17:23:48.0,89.0,
399,26,1,you are right.. this question is way too basic.. (but there are others like http://stats.stackexchange.com/questions/118 which are things I has confused by at some point and am glad to see discussions on on a website such as this),2010-07-23 17:32:09.0,35.0,
400,566,3,"For blending, this paper from the netflix competition is worth reading: http://www.the-ensemble.com/content/feature-weighted-linear-stacking.",2010-07-23 17:42:07.0,5.0,
401,563,3,Don't you think that the Wikipedia article about it is enough?,2010-07-23 17:59:06.0,88.0,
402,534,1,@James said it best.,2010-07-23 18:30:50.0,260.0,
404,563,1,Questions such as this require a wiki / blog post type of response. I do think questions should not require such long answers.,2010-07-23 19:50:07.0,,user28
406,563,0,I'm not sure the right thing to do is to simply ignore this question and refer the asker to the wiki - especially during beta where we are trying to build up the content of the site.  Perhaps the question asker should submit each of these questions individually so that they can be better addressed.,2010-07-23 20:52:00.0,196.0,
407,529,0,"Sorry, I was meaning an analytical measurement method.  I've re-worded the question.",2010-07-23 20:58:13.0,114.0,
408,529,0,"In that case, I think the two-sample test of equality for means/proportions is what you may want to do.",2010-07-23 21:04:24.0,,user28
412,579,0,Does that mean that for certain sample sizes BIC may be less stringent than AIC?,2010-07-23 21:36:49.0,196.0,
413,121,0,"said ""it's continuously differentiable (nice when you want to minimize it)"" do you mean that the absolute value is difficult to optimize ?",2010-07-23 21:40:12.0,223.0,
414,118,1,"Do you think the term standard means this is THE standard today ? Isn't it like asking why principal component are ""principal"" and not secondary ?",2010-07-23 21:44:37.0,223.0,
415,311,1,My opinion is that this book is THE book about extrem value theory,2010-07-23 21:56:20.0,223.0,
416,532,0,"The principal component is a good powerfull idea. It will work in a lot of case. However, it is to avoid when the distribution has a trend or is multimodal... in those cases the direction of largest variation are not related to outliers.",2010-07-23 22:00:48.0,223.0,
417,579,1,"Stringent is not a best word here, rather more tolerant for parameters; still, yup, for the common definitions (with natural log) it happens for 7 and less objects.",2010-07-23 22:13:56.0,88.0,
419,121,14,"@robin: while the absolute value function is continuous everywhere, its first derivative is not (at x=0). This makes analytical optimization more difficult.",2010-07-23 23:59:23.0,7.0,
420,470,0,I agree with Shane... and a hardcopy makes reading it much easier.,2010-07-24 00:07:57.0,7.0,
421,581,0,What do you mean by 'viterbi training' exactly?,2010-07-24 00:40:50.0,240.0,
422,579,0,AIC is asymptotically equivalent to cross-validation.,2010-07-24 01:47:58.0,159.0,
423,121,1,"Yeah, finding quantiles in general (which includes optimizing absolute values) tends to churn up linear programming type problems, which -- while they're certainly tractable numerically -- can get fiddly.  They typically don't have an analytical closed-form solution, and are a bit slower and a bit more difficult to implement than least-square-type solutions.",2010-07-24 02:55:02.0,61.0,
424,574,0,"There is a lot of training involved. Training a reasonable Finnish acoustic model can take 60-100 hours of computing time. Besides that, most corpora work with standardized training, development and evaluation sets.",2010-07-24 04:50:42.0,190.0,
425,588,1,If I'm right you mix up Viterbi training and Viterbi decoding.,2010-07-24 05:00:00.0,190.0,
427,121,1,"I do not agree with this. First, theoretically, the problem may be of different nature (because of the discontinuity) but not necessarily harder (for example the median is easely shown to be arginf_m E[|Y-m|]).  Second, practically, using a L1 norm (absolute value) rather than a L2 norm makes it piecewise linear and hence at least not more difficult. Quantile regression and its multiple variante is an example of that.",2010-07-24 06:01:42.0,223.0,
428,118,0,My understanding of this question is that it could be shorter just be something like: what is the difference between the MAE and the RMSE ? otherwise it is difficult to deal with.,2010-07-24 06:08:14.0,223.0,
429,527,0,"Can you give more details in your question? I don't understand what is ""the concentration of a particular molecule in a matrix"".",2010-07-24 06:41:14.0,223.0,
430,566,1,"IT is fun that meteorologist also use the word ""ensemble"" but not for combination: they use it for an ensemble of prediction (like scenario) obtained by perturbation of the initial conditions of the numerical model.",2010-07-24 06:46:49.0,223.0,
431,529,1,"Wouldn't a test of means/proportions only give you a point estimate of whether the two methods gave the same average response for a given set of responses?  Couldn't that approach yield a result of ""equal"" even if the two methods were actually negatively correlated with one another?",2010-07-24 07:44:03.0,196.0,
432,579,0,@Rob Can you give a reference? I doubt that it is general.,2010-07-24 08:03:12.0,88.0,
433,579,0,"@Rob For what I could found, this is true only for linear models.",2010-07-24 08:13:15.0,88.0,
434,587,0,"AIC is equivalent to K-fold cross-validation, BIC is equivalent to leve-one-out cross-validation. Still, both theorems hold _only_ in case of linear regression.",2010-07-24 08:23:58.0,88.0,
436,574,0,"E, 100 cores and one may live with that. Seriously, this looks dubious from a ML point of view, still I can understand the reasons why you do it in such a way. So then stick to the Srikant solution.",2010-07-24 08:44:37.0,88.0,
437,585,0,"I meant significantly different results. I also think there is none, at least real-world example. Still, I think I'll wait some time more.",2010-07-24 09:04:44.0,88.0,
438,590,0,This question is a repost of http://stats.stackexchange.com/questions/536/when-a-serious-statistician-calls-a-geometric-distribution-a-geometric-density-t (see comments there). @Hibernating is the original question-asker,2010-07-24 09:09:35.0,190.0,
439,121,11,"Yes, but finding the actual number you want, rather than just a descriptor of it, is easier under squared error loss.  Consider the 1 dimension case; you can express the minimizer of the squared error by the mean: O(n) operations and closed form.

You can express the value of the absolute error minimizer by the median, but there's not a closed-form solution that tells you what the median value is; it requires a sort to find, which is something like O(n log n).

Least squares solutions tend to be a simple plug-and-chug type operation, absolute value solutions usually require more work to find.",2010-07-24 09:10:00.0,61.0,
441,573,0,The question is interesting. The typos make it hard to read.,2010-07-24 09:24:27.0,200.0,
442,573,0,"@Ivo I am sorry, there were a lot of mistakes indeed. I corrected a lot of them.",2010-07-24 09:40:23.0,190.0,
444,587,1,"mbq, it's AIC/LOO (not LKO or K-fold) and I don't think the proof in Stone 1977 relied on linear models.  I don't know the details of the BIC result.",2010-07-24 11:01:35.0,251.0,
445,93,4,"I think that if you ask a question that can be understood by people that don't know what the hazard function is and if you elaborate a bit more on what you do (how do you estimate the parameter of you'r gausian process, how do you use the gaussian process at the end) you will increase the chances to get an answer and this will be an added value for stats.stackexchange :)",2010-07-24 11:01:43.0,223.0,
446,529,0,That is a good point.,2010-07-24 12:16:03.0,,user28
447,590,0,I am guessing it is a typo or an oversight.,2010-07-24 12:27:31.0,,user28
448,587,3,ars is correct. It's AIC=LOO and BIC=K-fold where K is a complicated function of the sample size.,2010-07-24 12:42:31.0,159.0,
449,512,0,We had a look at it. Worked okay but in this case the noise still seemed to be a bit too strong and if we changed the parameters to even out the distributions enough it appeared that the trend was damped down too much. Maybe in this case there just is no solution to the data and it is just a bit too noisy.,2010-07-24 17:10:43.0,210.0,
450,588,0,"You're right.  I wasn't aware that there was a procedure that used only the Viterbi algorithm to compute the transition probabilities as well.  It looks -- on further reading -- like there's some overlap of nomenclature between discrete time/discrete state HMM analysis, and discrete time/continuous state analysis using Gaussian mixture distributions.  My answer speaks to the DTDS HMM setup, and not the mixture model setup.",2010-07-24 19:08:38.0,61.0,
451,587,0,"Congratulations, you've got me; I was in hurry writing that and so I made this error, obviously it's how Rob wrote it. Neverthelss it is from Shao 1995, where was an assumption that the model is linear. I'll analyse Stone, still I think you, ars, may be right since LOO in my field has equally bad reputation as various *ICs.",2010-07-24 20:10:12.0,88.0,
453,603,0,"No, no, no, it is about machine learning *not* model selection.",2010-07-24 23:09:02.0,88.0,
454,605,0,"To get the behavior you wanted, try using a simple CART.",2010-07-24 23:29:48.0,88.0,
455,604,0,I am not sure why you feel that in regression you will get R2 = 1 if you try to predict the predicted variable. Can you clarify?,2010-07-25 01:37:03.0,,user28
457,603,1,"Interesting distinction. I thought model selection was central to machine learning, in almost all meanings of the term.",2010-07-25 02:40:53.0,30.0,
458,13,3,"I am interested in the last statement: ""the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome"". Which model was it? I believe that the first model was RiskCalc by Moody's, and even the first version was a logistic regression model. The developers of that model were not CS people with a background in ML, but rather in econometrics.",2010-07-25 02:58:55.0,30.0,
459,587,0,"The description on Wikipedia (http://en.wikipedia.org/wiki/Cross-validation_(statistics)#K-fold_cross-validation) makes it seem like K-fold cross-validation is sort of like a repeated simulation to estimate the stability of the parameters. I can see why AIC would be expected to be stable with LOO (since LOO can wasily be conducted exhaustively), but I don't understand why the BIC would be stable with K-fold unless K is also exhaustive. Does the complex formula underlying the value for K make it exhaustive?  Or is something else happening?",2010-07-25 03:18:18.0,196.0,
460,593,0,"Thanks, I think the Friedman test is interesting, but I can't quite figure out how it is doing that adjustment for Type I error in the post-hoc.  The comments say it is a ""Wilcoxon-Nemenyi-McDonald-Thompson test"" but I've never heard of that before could you explain it?",2010-07-25 04:29:21.0,196.0,
461,501,1,You can't fit a model that has more variables than observations. There are not enough degrees of freedom for parameter estimation.,2010-07-25 04:42:10.0,159.0,
462,411,0,I know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises  Anderson darling statistic (and include in particular higher criticism of Tukey)...,2010-07-25 06:38:39.0,223.0,
463,604,1,"You could improve clarity in your question. Note that A (good) classification rule may make classification errors in the learning set, this is sometime a clue to avoide overfitting.",2010-07-25 07:37:44.0,223.0,
464,596,1,Can you elaborate on what you really want to do (what is the model you want to fit) ? If you don't give a real question there won't be a real answer.,2010-07-25 07:40:01.0,223.0,
465,594,0,I am not sure but I think it is possible to interpret it as a stocastic gradient descent optimization procedure. I'll think about that...,2010-07-25 07:41:33.0,223.0,
466,534,6,Isn't it Karl Popper that said man can't establish causality: scientific theories are abstract in nature. They can be falsifiable and the fact that we encouter difficulties in falsifying something make us think about causality...,2010-07-25 07:48:30.0,223.0,
467,244,0,+1 for matplotlib and the collection... which one is your favorite ?,2010-07-25 07:51:32.0,223.0,
468,501,0,"This is machine learning, so you can.",2010-07-25 08:08:08.0,88.0,
469,603,0,"All those things work for trivial (mostly linear) models when you have few parameters and you just want to fit them to data to say something about it, like you have y and x and you want to check whether y=x^2 or y=x. Here I talk about estimating error of models like SVMs or RFs which can have thousands of parameters and are still not overfitting due to complex heuristics.",2010-07-25 08:22:10.0,88.0,
470,604,0,I think it is a good question; if I haven't known what I know I would thought the same.,2010-07-25 08:36:42.0,88.0,
471,590,0,Please fix spelling of Geometric. None of us have enough points yet to do editing.,2010-07-25 09:04:57.0,159.0,
473,512,1,Exponentially weighted moving averages are a special case of a kernel smoother (assuming you used a 2-sided MA rather than 1-sided). Better estimates that are generalizations of this are loess or splines -- see my answer.,2010-07-25 09:15:25.0,159.0,
474,50,2,"I think this question could be formulated with more precision. With your formulation, we don't know who is ""they"", and we don't know if you want a general abstract mathematical definition, or an intuitive explanation for someone that don't know anything about statistic or mathematic...",2010-07-25 11:10:57.0,223.0,
475,47,0,"I don't see why you want to cluster your users without any subjective input (I refer to a comment you made @reed), you said you want ""the most appropriate number of clusters"", but unfortunatly you don't have a clear objective, if you want to cluster your population to show something particular, you should tell us what you want to show ? If you want statistic (the data) to tell you what you want to show this is another problem :)",2010-07-25 11:24:42.0,223.0,
476,194,0,"Maybe you are asking a bit to much to statistic: you said ""patterns that you extract out from the data are indeed true patterns, not statistical fluke"" and then you ask if there is a statistical procedure to answer the question... can I send that to xkcd :) ? more seriously, I think you should try to see if the pattern are meaningfull by trying to understand if they mean something",2010-07-25 11:29:52.0,223.0,
477,242,1,"I really like this type of question, I think this type of precise real worl problem will increase the interest of the site. It would be even better if you had the possibility to add a link to the data, or to tell us (as a complement to the post) what you finally did, what was the conclusions .... however I understand that this can be confidential ...",2010-07-25 11:38:04.0,223.0,
478,242,0,I whish I could vote up again to make you pass over the question about the definition of a random variable ;),2010-07-25 11:41:11.0,223.0,
479,608,0,"+1 The code would be simple to write, still I'm very interested in seeing a clear, illustrative dataset.",2010-07-25 12:31:16.0,88.0,
481,614,3,Community wiki!,2010-07-25 15:02:22.0,88.0,
482,614,0,"On the other hand, can a book be open source? It rather applies to code, so probably the better word is ""open book"".",2010-07-25 15:08:59.0,88.0,
483,614,0,"Yes, any list question like this should be community wiki.",2010-07-25 15:25:40.0,5.0,
484,185,0,"agreed, that is an excellent book. It pretty much explains how Google works :)",2010-07-25 18:41:18.0,74.0,
485,615,1,"I'm still struggling to understand equations (biologist ahoy), which is why I turned to the community here, hoping it will help me explain the difference in layman's terms.",2010-07-25 19:18:23.0,144.0,
486,613,0,"Thanks for your answer, it makes sense :)",2010-07-25 19:29:14.0,90.0,
487,611,0,I am afraid I do not follow much of what you wrote as I am not a trained mathematician. Are you saying that all 'discrete distributions' are in some sense densities under a very general definition?,2010-07-25 19:39:03.0,,user28
488,611,2,"Without much mathematic you could say that a continuous variable has a density with respect to the Lebesgue measure, and a discrete random variable has a density with respect to the counting measure.",2010-07-25 19:56:22.0,223.0,
492,26,3,+1 to Peter... I think that we should directly close questions that find a direct answer on wikipedia http://en.wikipedia.org/wiki/Standard_deviation. I voted to close.,2010-07-25 20:07:35.0,223.0,
493,50,2,"In addition, did you try google before asking ? 
ranked second (after wikipedia which is a bit theoretical on this question) on my google: http://www.stats.gla.ac.uk/steps/glossary/probability_distributions.html",2010-07-25 20:10:04.0,223.0,
494,2,1,"did you try google/ wikipedia first ? 
http://en.wikipedia.org/wiki/Normal_distribution",2010-07-25 20:13:20.0,223.0,
495,206,2,"Did you try google first ? for me, it gives http://infinity.cos.edu/faculty/woodbury/stats/tutorial/Data_Disc_Cont.htm",2010-07-25 20:15:33.0,223.0,
496,109,3,Can you give a definition or a link to the definition of the different test. Can you state the hypothesis you want to test (otherwise it is difficult to discuss the power ...).,2010-07-25 20:20:36.0,223.0,
497,244,0,"I tend to use some more than others- the tool I use most often for visualization is R and associated packages, but I left it off of this list because there is no easy way to compile R scripts to stand-alone ""executables"" that the OP wanted.  I can't really claim a single favorite- I would have to say it depends on 1) The task at hand and 2) The tools I am using",2010-07-25 20:26:16.0,13.0,
498,603,0,"These results are valid for regression of general linear models with arbitrary number of independent variables. The variables can be arbitrary learners. The crucial assumption is that as the number of observations goes to infinity the number of learners describing the true model stays finite. All of this works for regression, so for a classification task like yours I am not sure it helps.",2010-07-25 20:57:47.0,30.0,
499,615,0,"I think the ideology is that FA assumes that the process is driven by some 'hidden factors', while the data we have consists of some combinations of them. Because of that, the problem of FA is to reconstruct the hidden factors somehow. And there goes PCA -- a method which iteratively builds a new variables (PCs) by mixing the old ones such to greedy absorb the variance of the data. One may say the PCs are equal to the FA's factors, and here they will be indistinguishable. But one may also make some changes to the PCA to make it a base of some other 'FA sort', and so the problem begins.",2010-07-25 22:56:50.0,88.0,
500,615,0,"So basically, you should think of what you want to do (not which buzzword you want to use). I know it is hard, especially while having biologists around (to some extend use-buzzword works well in biology, so they just assume that this is common to other disciplines); still this is the way science should be done. Than use Google (or this site) to assess the good algorithm for it. Finally, use the docks to find a function/button that does it and type/click it.",2010-07-25 23:19:04.0,88.0,
502,596,0,"Ok, I admit that now I am also in deep confusion. Probably you could translate some of the document-retrieval jargon.",2010-07-25 23:45:52.0,88.0,
503,566,0,"@robin It's not fun, its physics.",2010-07-26 00:03:50.0,88.0,
504,596,0,"Doc retrieval? None of that. However, I can elaborate.",2010-07-26 00:25:00.0,240.0,
505,608,0,"I'm not sure what all would need to be in a clear and illustrative dataset, but I've made an attempt to include a sample dataset.",2010-07-26 05:50:33.0,196.0,
506,566,1,@mbq in fact they call themselves forecaster and they use statistic quite a lot ...,2010-07-26 06:46:28.0,223.0,
507,457,1,"Sorry for the late reply. I like your suggestion of simulation. That is not really easy, though. The truth is, what I see in practice is that researchers just do the test that is computationally easier or that give them the result they want.",2010-07-26 07:41:27.0,90.0,
508,566,1,"@robin I know, this is just why it's called ""ensemble"" not a set or something like this.",2010-07-26 07:46:15.0,88.0,
509,608,0,"So look: what you provided is an example of an useless set, because the BIC and AIC give the same results: 340 v. 342 for AIC and 349 v. 353 for BIC -- so good.model wins in both cases. The whole idea with that convergence is that certain cross-validation will select the same model as its corresponding IC.",2010-07-26 07:53:50.0,88.0,
510,570,0,"The term ""SEM"" is vague. It could also mean ""Search Engine Marketing"", for instance, for someone looking for statistical analysis techniques for studying ad click data or evaluating advertising effectiveness. Consider making the title more verbose.",2010-07-26 09:10:42.0,87.0,
511,608,0,I've made a simple scanning and for instance for seed 76 the ICs disagree.,2010-07-26 09:45:12.0,88.0,
512,625,0,"Thanks for the suggestions, I've actually came across your book a couple of years ago.",2010-07-26 12:30:56.0,59.0,
513,600,1,"Two quick notes on your notes. 1. The C-vM distance is precisely the L^2 cousin of the Kolmogorov (L^infinity) and (univariate) K-R (L^1) distances, and hence interpolates between them.  2. One advantage I didn't mention of the K-R and B-L distances is that they generalize more naturally to higher dimensional spaces.",2010-07-26 13:31:30.0,89.0,
514,609,0,"Yes, what I called the Kantorovitch-Rubinstein distance is also called the L^1 Wasserstein distance or W1.  It goes by many other names too.",2010-07-26 13:37:04.0,89.0,
515,411,0,All the answers so far are very nice and add some nice perspective from different directions.  I won't accept any because I see no reasonable criterion for singling out just one.,2010-07-26 13:39:31.0,89.0,
517,25,0,Should be community wiki.,2010-07-26 14:54:10.0,5.0,
518,614,0,Done. Sorry about not doing it in the first place.,2010-07-26 15:32:04.0,107.0,
519,614,0,Any policy on how to aggregate the four answers into one wiki answer?,2010-07-26 15:32:58.0,107.0,
520,614,0,"They should be separate answers (so people can vote on them), but the moderator or the individuals should make their answers community wiki as well.  All new answers with be CW by default.",2010-07-26 15:36:44.0,5.0,
522,93,0,Upvoted for doing a bounty...,2010-07-26 15:37:59.0,107.0,
527,581,1,"In my problem I have an array of real valued data which I am modeling as a HMM (speficially a mixture of multiple density functions each with unknown parameters).   For now I assume that I know the state transition probabilites.   What I mean by Viterbi Trainig is the following algorithm.

1) Arbitrarily assign a state to each data point ( initialization)
2) Perform MLE of the density function parameters.
3) Re-estimate state for each point ( can be done with Viterbi Alg).
4) Goto step 2 and repeat unless stopping criteria is met.",2010-07-26 16:05:22.0,99.0,
528,632,0,"Isn't that a function of estimator is still an estimator? I still don't know \\sigma, only X_i.",2010-07-26 16:45:07.0,88.0,
529,632,0,"ok, then you will possibly estimate the square root of the variance of the estimation of the square root of the variance... right :) should be something like $\\hat{\\sigma}/n$ ?",2010-07-26 17:09:48.0,223.0,
530,567,0,"+1 For this resource. As the name says, excellent for a practical approach to engineering problems.",2010-07-26 17:42:34.0,77.0,
531,608,0,"Thanks mbq - I didn't understand what you meant/needed in terms of an illustrative dataset.  Also, in general, in terms of empirical demonstration I'd imagine that a single example from a single seed won't really do the trick.  I was imagining something like a metric from the employment of a cross-validation method being shown to be correlated with the calculated corresponding information criterion.  In that way the ICs don't necessarily need to give different answers in order to demonstrate the relation between the IC and the cross validation method.",2010-07-26 18:04:31.0,196.0,
532,636,1,"I didn't downvote, but I must say that some explanation or reasoning why to use random forest could make this answer very much more interesting. ;)",2010-07-26 18:14:28.0,190.0,
533,636,0,"I didn't downvote either, but why random forest over SVM (for instance)?",2010-07-26 18:16:06.0,5.0,
534,635,0,"Thanks a lot for your answer ! I have read this book from first to last page, but I think it was edition 1... I didn't know it was available online.",2010-07-26 18:24:35.0,223.0,
535,636,0,"I didn't downvote either, it didn't let me ;-) . @Peter extended. @Shane and why binomial regression? This is just my favorite algorithm and it may work well in this case.",2010-07-26 18:25:43.0,88.0,
536,636,0,"+1 @mbq because to some extent, that's the ""standard"" and/or ""obvious"" answer when it comes to regression. :) (hope that's vague enough...) When it comes to a machine-learning approach, it doesn't seem to me that there is an obvious answer; but your experience is more than enough.",2010-07-26 18:26:52.0,5.0,
537,625,1,"Rob, your book is great!",2010-07-26 18:37:01.0,74.0,
538,26,6,"I think this question is ok. Actually, it was the most upvoted example on topic question on Area 51. Basics are ok here!",2010-07-26 18:52:44.0,190.0,
539,50,5,"@robin We don't have a policy that we can not ask question, before you have googled them. We want to be the resource that comes on top when you google this question :)",2010-07-26 18:55:12.0,190.0,
540,609,0,Just to clarify for anyone unfamiliar with Wasserstein distances who reads this and gappy's answer: the L^2 Wasserstein distance (W2) is *not* the same as the Cramer-von Mises distance.,2010-07-26 18:55:17.0,89.0,
541,130,0,This should be community wiki since it is subjective.,2010-07-26 18:55:47.0,5.0,
542,641,0,Should be community wiki?,2010-07-26 19:25:49.0,5.0,
544,645,0,"This seems very vague to me.  What kind of data, and what kind of analysis?  Also, this should be community wiki if it is subjective.",2010-07-26 19:36:21.0,5.0,
545,650,0,"Peter, you beat me to the punch! I completely agree with storing data as text, though depending on the size (hundreds of millions of obs) it may be necessary to move into a map-reducible database (e.g., Pig, Cassandra, or one of the NoSQL options).",2010-07-26 19:37:03.0,302.0,
547,652,7,Related: http://stats.stackexchange.com/questions/421/what-book-would-you-recommend-for-non-statistician,2010-07-26 19:40:56.0,5.0,
549,652,3,Should be community wiki.,2010-07-26 19:41:32.0,5.0,
550,652,1,You tagged this as bayesian and machine-learning.  What kind of data analysis are you interested in?,2010-07-26 19:46:47.0,5.0,
551,650,0,Oh ok interesting! So just take data for each variable and lay it out in row-column format and get to number crunching eh? Are there any tools I should be looking at or should I just be programming something?,2010-07-26 19:47:31.0,9426.0,
552,652,0,"Shane: Honestly, I don't really know yet. What kind of data analysis is there? Should I be posing this as yet another question for the site?",2010-07-26 19:49:24.0,9426.0,
553,647,0,"I can do Monte Carlo, I just wanted to do in a more 'sciency' way; still you're right that the distribution is not normal, so this sd will be useless for testing.",2010-07-26 19:50:17.0,88.0,
554,650,0,R is a very extensive (and free) toolkit/programming language/library for statistics. My favorite for most things is however Python with SciPy / NumPy,2010-07-26 19:50:23.0,190.0,
555,648,0,I thought it shows the binomial distribution; I don't think that its asymptotics have a direct link with CLT.,2010-07-26 19:52:46.0,88.0,
556,655,1,May be a bit too academic for myself being such a beginner...,2010-07-26 19:53:40.0,9426.0,
558,652,0,"There are many kinds.  :)  Those are just two specific areas, so it seems odd for tags on such a general question.",2010-07-26 19:54:21.0,5.0,
559,652,0,"@Justin If you don't know it yet, this question is too broad and vague. A nice thing to learn what a field contains, is to look on the tags of this sites and see what questions match with it. Also wikipedia and the books you bought can give you and idea of what you want (although the books maybe not name the field wherein they operate, more on the practical overall part)",2010-07-26 19:54:54.0,190.0,
560,648,2,bean machine by the author of the package animation... http://yihui.name/en/wp-content/uploads/2010/07/animation-useR2010-Xie.pdf,2010-07-26 19:55:09.0,223.0,
562,643,0,This should be a community wiki.,2010-07-26 19:57:08.0,88.0,
565,648,1,@mbq take a look at http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation,2010-07-26 20:01:14.0,223.0,
566,658,0,what is the relation between the Cauchy distribution and the CLT or the failling of the CLT ?,2010-07-26 20:02:15.0,223.0,
567,652,0,Why is this question not closed yet as duplication ?,2010-07-26 20:03:09.0,223.0,
568,656,3,That's great.  The standard way of dealing with outliers.,2010-07-26 20:06:32.0,5.0,
569,660,0,"What do you mean by *types* of ""textual"" content?",2010-07-26 20:08:17.0,68.0,
570,648,0,"@robin I have wrote about it, what's the problem?",2010-07-26 20:09:41.0,88.0,
571,652,0,"@Peter Smit: Thanks I'll give that a shot.
@Shane: Thanks as well... I was trying to give some idea of context with my tags.

Beware the noobs!",2010-07-26 20:10:14.0,9426.0,
572,658,1,@robin Consult http://en.wikipedia.org/wiki/Cauchy_distribution#Properties,2010-07-26 20:12:30.0,88.0,
573,652,0,@robin There are only 7 users able to vote for closing... so it can take a small while,2010-07-26 20:17:54.0,190.0,
574,600,0,"Regarding 1., that's correct. Regarding 2. In principle all of the above distances could carry over to R^n, however I don't know of popular non-parametric tests based on *any* distance. It would be interesting to know if there are any.",2010-07-26 20:18:19.0,30.0,
575,641,0,"I think a general wiki for where to get data is great, with a section for survey data.",2010-07-26 20:21:15.0,302.0,
577,165,0,Should be community wiki?,2010-07-26 20:22:54.0,5.0,
579,660,0,Could you show some sample data?,2010-07-26 20:27:15.0,,user28
581,672,4,Related: http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english,2010-07-26 20:34:14.0,5.0,
583,658,0,great ! I did not know that !,2010-07-26 20:47:47.0,223.0,
589,262,3,"""Hardly publication quality""???? I realise that it isn't perfect - the phrase ""...should you get you started.."" covers that bit. But with a little additional work, i.e. axis labels, I would say it's fine. BTW, what journals do you publish in?",2010-07-26 21:20:59.0,8.0,
590,652,0,@robin @peter: It isn't clear to me that it's an exact duplicate either.  Are you both referring to the one I linked above or another one?,2010-07-26 21:23:14.0,5.0,
592,667,1,So probability is pure mathematics and statistics is applied mathematics?,2010-07-26 21:24:35.0,327.0,
593,673,0,So statistics is synonymous with data analysis?,2010-07-26 21:25:04.0,327.0,
594,652,0,"@shane I agree that it is not exact duplicate, but then it is too broad or not enough and the title is quite similar. In addition, my view is that someone asking for a book on stat.stack could at least say for what purpose and for what level.",2010-07-26 21:29:12.0,223.0,
595,652,0,@robin: I agree 100% with that.,2010-07-26 21:33:06.0,5.0,
597,675,0,"So statistics observes what happens in the physical world, theorizes about the underlying process, and then having found the process, uses it in the sense of probability to predict what will happen next?",2010-07-26 21:34:19.0,327.0,
598,651,0,"I work in medical/epidemiological research, and I see this book on colleagues' shelves all the time.  Still haven't read it myself, but I can attest to its popularity.",2010-07-26 21:35:13.0,71.0,
599,673,1,I don't see any distinction.,2010-07-26 21:39:17.0,25.0,
600,667,2,Statistics may be applied and may be not; still the concept of data is always present.,2010-07-26 21:42:34.0,88.0,
601,685,0,I feel that your question is not precise enough to get a reasonable answer.,2010-07-26 21:44:31.0,,user28
602,685,0,It could be rephrased as: In what ways are statistics misleadingly reported or cited?,2010-07-26 21:51:18.0,327.0,
603,479,0,"Found a way to do this (high transparency overlay on the ""danger"" part of the graph) with our toolset, thanks!",2010-07-26 21:53:17.0,259.0,
604,685,1,"Even if it is not off-topic, it should be community wiki.",2010-07-26 22:08:47.0,88.0,
605,687,0,"About the glass, I think that just the boundary between phases lies in the half of its height.",2010-07-26 22:10:48.0,88.0,
607,685,0,Your re-stated question is much better. I would either suggest asking another question along those lines or better still edit the current one along the lines of your comment.,2010-07-26 22:20:56.0,,user28
610,13,1,"I bet they used discriminant analysis before logistic regression, as DA was invented well before LR",2010-07-26 22:56:40.0,74.0,
612,194,0,"@robin, that would be the hard part. Sometimes when thousands of variables influencing something, it's hard to make sense of the patterns that emerged.",2010-07-27 00:09:31.0,175.0,
613,675,0,"I'm not a statistician, but from my understanding I'd say, yes, that *part* of what statistics does.",2010-07-27 00:10:53.0,89.0,
615,349,0,Since you are only interested in the median why couldn't you make the bins wider at higher values of your variable?,2010-07-27 00:23:15.0,196.0,
616,570,0,"drpaulbrewer, thanks for the suggestion.",2010-07-27 00:49:44.0,251.0,
617,702,1,"With a millon points and an 8 parameter model, a goodness of fit test like chi-squared tells me that there is essentially no chance that the model is correct. (Which is not surprising, as there are endless factors influencing reality that are not in the model)

RMSE gives me a sense as to how good the model fits the data, but does not give me a sense of whether there is a better model",2010-07-27 01:07:20.0,72.0,
618,702,0,"Well in order to find out if there is a better model, you could either experiment with different formulations or you could use various plots (e.g.,  exit times vs time) to see if the data is consistent with your model assumptions. You could also plot predicted exit times for a small sample selected at random vis-a-vis actual times to for model improvement ideas.",2010-07-27 01:17:13.0,,user28
619,612,0,Consider adding a tag that directly references the R package you are using.,2010-07-27 04:00:46.0,196.0,
620,712,6,"What is your data, and what do you want to do with the anonymized data?",2010-07-27 04:42:39.0,190.0,
621,495,2,I have watched all of those videos. It's a very good introduction to probability and counting.,2010-07-27 06:12:47.0,339.0,
623,732,12,"I don't mind the down vote, but I maintain that this is a deep statistical point, not to be taken lightly.  ;-)",2010-07-27 07:18:30.0,251.0,
624,675,0,great answer! +1 for Persi and Mark,2010-07-27 07:21:49.0,223.0,
625,242,0,"I will come back to tell you what the results were, but it will be a while as I am working my way through this alongside lots of other tasks. Wasn't sure what you meant about ""pass over the question about random variable""? Is there a question you recommend I look at?",2010-07-27 07:46:32.0,199.0,
626,744,13,"I like this one, could be put as an advise when people  write questions on this site ?",2010-07-27 08:48:41.0,223.0,
627,705,0,"Srikant, thanks for the response!  I'm not sure what you mean by contour plots of covariances (obs v est) -- could you elaborate?  Thanks.",2010-07-27 08:52:31.0,251.0,
628,612,0,Perhaps Peter can edit it - my rep is not high enough yet.,2010-07-27 08:54:56.0,144.0,
629,675,7,Induction vs Deduction?,2010-07-27 09:14:39.0,434.0,
630,686,0,I hadn't seen these ones before. They look good.,2010-07-27 09:40:01.0,183.0,
631,749,1,"+1 Wow, nice list.",2010-07-27 09:51:43.0,88.0,
632,734,1,"i think only one of the classes in the Iris data set is linearly separable. (From the OP's Question, i think he's after data sets w/ *only* linearly separable classes).",2010-07-27 09:52:30.0,438.0,
634,734,0,"@doug, good point, still I don't think that one can get any non-synthetic fully linearly separable problem. Nevertheless I'll wait for the OP's reaction.",2010-07-27 10:13:30.0,88.0,
636,705,0,"See this: http://en.wikipedia.org/wiki/Level_set. Let Sigma be a a 2 dimensional covariance matrix and Y ~ N(0, Sigma). An iso-contour line would plot the set of points Y for which f(Y|sigma) = c where c is a constant. Note that Y is a 2-dimensional vector. You would choose various values of c and hence obtain different iso-contour lines which would give you a sens of the spread of the distribution.",2010-07-27 10:54:50.0,,user28
637,759,0,thanks a lot for this answer!,2010-07-27 11:04:27.0,223.0,
638,759,0,You're welcome.,2010-07-27 11:16:21.0,88.0,
640,748,29,Still it looks promising.,2010-07-27 11:20:02.0,88.0,
642,741,0,"Thanks, this is just what I was looking for.

This is essentially a million subjects each with an entry and exit time.

Yes we are conditioning to account for the censoring.",2010-07-27 12:03:40.0,72.0,
643,693,3,+1 I really like this explanation and think it is very clear.,2010-07-27 12:16:21.0,81.0,
644,337,0,I'm not interested into *truth* but in getting a function that works. I'm a bioinformatician and taught not to seek dogmatic *truth* but to seek statistics that work. I don't think that there work done with the kind of data that I want to work with that specifics what entropy works best. That's kind of the point why I want to work with the data.,2010-07-27 12:17:57.0,3807.0,
645,337,1,"Right, but this is not a discussion about dogmatic truths but about words. You have asked about entropy, so I answered about entropy. Because now I see that you indeed need an answer about time series descriptors, write a question about time series descriptors, only then you'll get an useful answer.",2010-07-27 12:32:45.0,88.0,
646,695,1,Both of these are really excellent.,2010-07-27 12:35:16.0,247.0,
647,768,0,So we can only find patterns that we were looking for in the first place?,2010-07-27 12:49:35.0,327.0,
648,769,3,Statistics is not a subset of data analysis -- it is a theory that is used in data analysis.,2010-07-27 12:56:46.0,88.0,
649,618,0,"Nice, but I can't see anyone trying to draw a conclusion of causality there. Or are mexican lemon-truck drivers notoriously dangerous once they get over the border?",2010-07-27 12:57:59.0,270.0,
650,674,0,What if the random errors become greater than the observable factors over time?,2010-07-27 12:59:24.0,327.0,
652,579,0,"@mbq. I was thinking of Shao 1995 which is, indeed, only for linear models. I don't know if the result has been extended to other models.",2010-07-27 13:30:26.0,159.0,
653,769,4,Whatever you do in Excel does not count.  Just kidding...,2010-07-27 13:36:13.0,334.0,
654,769,1,@Dirk: Wow...your hatred towards Excel knows no bounds.  :),2010-07-27 13:37:37.0,5.0,
655,769,0,This should probably be community wiki since it is subjective/argumentative.,2010-07-27 13:40:46.0,5.0,
656,252,0,+1 for omegahat - very good source,2010-07-27 13:41:47.0,22.0,
657,780,2,+1 Nice pointer to the OR exchange site.,2010-07-27 13:59:54.0,5.0,
658,772,0,"Wow, Shane, thanks for the very fast response! I'll look into those references!",2010-07-27 14:02:33.0,445.0,
660,744,6,Absolutely...asking the right question is one of the most important skills.,2010-07-27 14:17:30.0,5.0,
661,790,1,maybe a reference to the paper could help ?,2010-07-27 14:52:37.0,223.0,
662,790,0,Slightly related question: http://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-var/,2010-07-27 14:53:37.0,5.0,
664,349,0,drknexus - because we don't know what the largest bin should be.,2010-07-27 14:58:50.0,247.0,
665,47,0,"@robin: thanks for comment. i haven't seen much research in this area and the input from data provider was minimal [so far they were not interested/capable of investigating it further]. after initial exploration of data it was quite clear for me that there are couple of distinctive patterns [examples being 'heavy downloaders' in just a few locations or 'frequent hoppers' with lots of small sessions in large number of locations].
my goal at this stage was to try to use data itself to tell me how can i divide it best and minimize subjective input.",2010-07-27 14:59:00.0,22.0,
666,790,0,"You may find this paper of interest, which discusses the log mean and how it refines the 'arithmetic mean - geometric mean' inequality: http://www.ias.ac.in/resonance/June2008/p583-594.pdf",2010-07-27 15:03:53.0,81.0,
667,704,1,"but then how to choose eta, and what doe sthis then mean statistically? i.e. how to form confidence intervals for the median from this result?",2010-07-27 15:08:20.0,247.0,
668,428,2,"So, according to a long lost colleague, the best apropoach seems to be Chiranjeeb Buragohain and Subhash Suri. Quantiles on Streams.
http://www.cs.ucsb.edu/~suri/psdir/ency.pdf

I also like Ian's approach, as these medians of smaller data sets will converge to a normal distribution, and so I can form conf intervals for the medians.",2010-07-27 15:10:34.0,247.0,
669,1,4,"Although I've accepted an answer, I would recommend that interested people should look at all the answers.",2010-07-27 15:22:18.0,8.0,
670,801,0,"sorry, i should have mentioned that i need to do this in an automated way. the option of ""doing it multiple times until i find the one that best suits my purpose"" won't work for me. has to be done computationally...",2010-07-27 15:34:03.0,476.0,
671,794,0,oh the bayesian is soooo Good...,2010-07-27 15:39:02.0,223.0,
672,799,2,"... and in that vein also the lme4 package (which I find easier to use than lme or nlme) and related packages from Baayen's above referenced book, languageR.",2010-07-27 15:40:33.0,196.0,
674,804,3,always these bayesian guys...,2010-07-27 15:45:32.0,442.0,
675,720,1,"I found all the answers to this question helpful, but I think that this one is the most practical.",2010-07-27 15:46:34.0,266.0,
676,486,3,"Oddly enough, I was rather interested in hearing what other people had to say...",2010-07-27 15:52:13.0,266.0,
677,779,0,"As I always understood it, the central limit theorem does not postulate something about averaging a large number of iid random variables. Rather, it states that when sampling means, the distribution of the means becomes normal (independent of the distribution underlying what is sampled from). So I question whether the antecedent for your question holds.",2010-07-27 15:57:43.0,442.0,
678,779,0,"But, if the sampling mean becomes normal irrespective of the distribution of the underlying distribution then is that not the same as saying 'averaging a large number of iid random variables' get us a normal distribution. To me they seem to be equivalent statements.",2010-07-27 16:11:46.0,,user28
679,660,0,"@Srikant Vadali - sample data could be press releases, news stories, etc .. the textual data would be free-form, likely obtained from rss feeds or similar. Market data for a given company is what I'm looking to analyze/correlate. So maybe Blogger Bill writes a story about an upcoming VMware feature release, and VMW jumps 10%. (Oversimplified, I know)",2010-07-27 16:11:57.0,292.0,
680,677,0,that looks like a really promising start :),2010-07-27 16:12:31.0,292.0,
681,750,13,"I hate this quote. It makes professions using statistics look like you could cheat. But, when someone profoundly uses statistics one knows that actually you cannot cheat. Because when provided with enough information about the statistical procedures used, one can draw a conclusion on the soundness of the procedures/results. If not enough information on the statistical (and other) procedures are provided, you should immediately question the results.",2010-07-27 16:15:40.0,442.0,
682,811,0,is it ? then we are all intelligent persons here:),2010-07-27 16:22:37.0,223.0,
683,799,0,"thanks for the comment, I totally agree with you. lme4 is simply the best around.",2010-07-27 16:22:39.0,447.0,
684,779,0,"Not in my eyes (but i would like to be convinced otherwise).
In the one case (the one i think of being meant by CLT) you draw samples from one distribution. Their means are normally distributed.
What i understand from the question and the quote ""average a large number of iid random variables"" is sth differnt: individual instantiations from different iid random variables determine (or make up) a trait. Hence, no averaging (i.e., computing a mean) from a single distribution and, hence, no application of the CLT.
I think mbq's answers points to the same issue.",2010-07-27 16:27:57.0,442.0,
685,815,0,+1 for harmonic oscillator.,2010-07-27 16:35:39.0,88.0,
686,779,1,Well the distribution need not be identical if some conditions hold. See: http://en.wikipedia.org/wiki/Central_limit_theorem#Lack_of_identical_distribution,2010-07-27 16:42:43.0,,user28
687,787,0,Hillaire Belloc?  Nice work on digging that up.,2010-07-27 16:43:12.0,5.0,
688,252,0,"Well, RCurl and XML and both on CRAN too...",2010-07-27 16:47:41.0,334.0,
689,307,3,"As can be read in my and dave's post, saturated models do not per definition lead to perfect fit. but if you use the n-1  polynominal as the model they will. see Sue Doe Nihm's seminal paper on this topic http://psych.fullerton.edu/mbirnbaum/papers/Nihm_18_1976.pdf",2010-07-27 16:52:10.0,442.0,
690,783,0,I would love to take this one as my accepted answer ! too good to be true !,2010-07-27 17:10:56.0,223.0,
691,825,1,"IMO, this question belongs on stackoverflow.  Voting to close as off-topic.",2010-07-27 17:11:36.0,5.0,
692,820,0,Some comments would be appreciated for the down-vote.,2010-07-27 17:12:21.0,488.0,
693,827,0,"Hah. You tricked me. I saw your first (one-line, no links) answer and then filled mine in.  We need mutexes here :)",2010-07-27 17:12:51.0,334.0,
694,825,1,"Dunno. This is middle ground and suitable for either.  Xrefs, maybe?",2010-07-27 17:13:41.0,334.0,
695,828,0,"What, are you copying my answers now?  :)  This is of course, needless to say, your domain.",2010-07-27 17:14:16.0,5.0,
696,828,0,"Well, mutexes needed. As I commented on your answer, I only saw the first (raw) version and figured well, I may expand on mc and Rmpi.  And then you did and I look like a copycat.  Such is life.",2010-07-27 17:15:55.0,334.0,
697,828,0,"On the other hand, my answer is derived from reading your paper/presentation in the past.  So I guess I'm copying you as well.",2010-07-27 17:16:37.0,5.0,
698,825,0,"@Shane Agree, this is for SO.",2010-07-27 17:18:46.0,88.0,
699,827,0,"Yep...with basic questions like this, I find that it's best to follow the ol' ""answer quickly and then improve"" routine...may not be a good practice for everyone else though.",2010-07-27 17:18:53.0,5.0,
700,825,0,I just don't see how the fact that importing data and running SVM has any relevance to the question.  That's why I think it's more of an SO question.  But I could see Xrefs as being a good long-term solution since it is R...,2010-07-27 17:20:58.0,5.0,
701,822,0,"""market basket analysis"" seems like it's what I'm looking for, thanks for input.",2010-07-27 17:22:33.0,488.0,
702,827,0,thanks for the prompt replies guys. multicore package looks like a good choice for my needs.,2010-07-27 17:27:49.0,480.0,
703,830,4,"I disagree somewhat. Revolution does a great sales job in getting mindshare (as evidenced by your post) but as of right now there is very little in the product you would not already get with the normal R (at least on Linux). Intel MKL, sure, but you can get Goto Blas.  On Windows, they offer doSMP which helps as multicore cannot be built there.",2010-07-27 17:42:14.0,334.0,
704,658,0,"The CLT requires that the MGF's exist in a neighborhood of 0. The Cauchy distribution does not have that property. CLT Win. 

Cauchy doesn't even satisfy the weaker requirements of a stronger version of CLT where all that is required is that mean and variance exist. The Cauchy distribution shows that the mean is required to exist for the CLT to hold. It doesn't make the CLT fail.",2010-07-27 17:46:38.0,62.0,
705,744,3,"I remember once where a private industry company commissioned a mathematician to solve a garbage collection routing problem. Long story short, the mathematician complained that the company was only interested in finding a ""close enough"" solution rather than an optimal solution. I think, ultimately he was fired, and an operations researcher was brought in instead.",2010-07-27 17:59:21.0,59.0,
706,658,0,"@Baltimark You have misunderstood my post -- its obvious that Cachy is not covered by CLT because of CLT assumptions, otherwise it would be impossible to prove CLT. I have gave this example because people believe that CLT works for all distributions; probably ""fail"" is not a perfect word, but still I don't think it is a reason for downvote. Ok, I have even changed it to not applicable.",2010-07-27 18:05:10.0,88.0,
707,658,0,I prefer your edit. The Cauchy distribution is definitely very cool.,2010-07-27 18:11:05.0,62.0,
708,834,3,I would suggest reviewing the wiki link: http://en.wikipedia.org/wiki/Akaike_information_criterion and then edit the question so that you can highlight the aspect of AIC that you do not understand.,2010-07-27 18:14:03.0,,user28
709,658,0,"Yup; the another nice ""paradox"" comes from basic physics -- if you have a particle that has flown between two detectors with an uniform speed and you've measured the distance and time-of-flight with a normally distributed error, the speed obtained from v=s/t is Cauchy distributed, so one may say that it is undefined ;-)",2010-07-27 18:19:06.0,88.0,
710,834,2,Read also this question: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other,2010-07-27 18:22:29.0,88.0,
711,827,0,"I mainly use multicore, still I like snowfall more than snow and Rmpi for its fault tolerance and clean interface.",2010-07-27 18:25:39.0,88.0,
712,754,0,"I guess I should remove it... poor Karl Pearson, one of the inventor of hypothesis testing not understood by the 21st century ... I would vote up if I could, but it's me that put it here :)",2010-07-27 18:31:32.0,223.0,
713,478,5,The Statistical sleuth is used a the textbook on that great introductory course (there are 64 lectures in total)  http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#,2010-07-27 18:36:31.0,339.0,
714,842,1,+1 Great point about how this doesn't deal with splitting up the cross-validation.,2010-07-27 18:45:28.0,5.0,
716,709,0,There are also more advanced treatments on the course pages for PS 236 and PS 239 (graduate-level political science methods courses) at my website: http://cgibbons.berkeley.edu/teaching.html,2010-07-27 19:09:24.0,401.0,
717,847,3,Have you tried asking Google itself?,2010-07-27 19:24:25.0,88.0,
718,846,0,Maybe Wikipedia will be sufficient here: http://en.wikipedia.org/wiki/F-test#Regression_problems,2010-07-27 19:34:49.0,88.0,
719,806,3,What do you precisely mean by normalization?,2010-07-27 20:13:18.0,88.0,
720,744,1,@dassouki I think the quote is more about the question .... something like science is not about finding good answer but about finding good questions !,2010-07-27 20:21:49.0,223.0,
721,680,0,"This was awesome. Thank you so much for this response. You've given me a great jumping off point. Any books you recommend since you seem to ""get"" where I'm at.",2010-07-27 20:24:17.0,9426.0,
722,731,0,"Thanks for the paper, I upvoted you for it.",2010-07-27 20:24:58.0,9426.0,
723,680,0,"you're very welcome. books:  

Statistics in Plain English to start. 
Multivariate Data Analysis by Hair after that .   
  
These are good web resources:  
  
http://www.itl.nist.gov/div898/handbook/  , 
http://www.statsoft.com/textbook/",2010-07-27 20:54:37.0,74.0,
724,845,4,"gappy specifically stated that he has paired, i.e. non-independent samples, so assumption 2 is violated right away.",2010-07-27 22:08:42.0,279.0,
725,349,0,"Do you have **any** intuition as to what the range will be?  If you're fairly sure that over half of the answers will be below number N, then you can make your last bin as large as you want.  Maybe your last bin is all numbers greater than 1 trillion - would that be high enough?  With the amount of memory in modern systems you can store a LOT of bins and achieve fairly high resolution. In terms of data structures, we're not talking anything fancy and memory intensive here.",2010-07-27 22:17:59.0,54.0,
726,850,0,"It's a good idea, but

    ""mtcars$dummy <- 1; lrm(am ~ dummy, data=mtcars);"" 

gives back:

    singular information matrix in lrm.fit (rank= 1 ) Offending variable(s):
    dummy 
    Error in lrm(am ~ dummy, data = mtcars) : 
    Unable to fit model using “lrm.fit”",2010-07-27 23:03:52.0,501.0,
727,858,0,This will due for now.  Is the fact that this doesn't work in the Design package a bug?,2010-07-27 23:06:16.0,501.0,
728,734,0,"yes - only one of the classes in the iris data set is linearly separable - so, I've now started experimenting with algorithmically generated datasets.",2010-07-27 23:22:51.0,130.0,
729,845,0,Sorry I missed that. I'll try and think of something else.,2010-07-27 23:41:17.0,8.0,
730,845,0,Thanks anyway. I learned something from the wrong answer as well.,2010-07-28 00:08:09.0,30.0,
731,845,1,@gappy: I've updated my answer. Hopefully this will get you started.,2010-07-28 00:09:12.0,8.0,
732,841,2,I'm not sure why the information that the observations are paired is important to the hypothesis being tested; could you explain?,2010-07-28 00:11:01.0,196.0,
733,834,1,Consider asking the general question about the AIC separately from the stata question.,2010-07-28 00:16:29.0,196.0,
735,674,0,In that case you re-work your model as it is no longer consistent with reality.,2010-07-28 01:06:13.0,,user28
736,815,0,"the maximum entropy principle is also another reason why the Gaussian distribution is used.
For example, what are good reasons for using Gaussian errors in the linear model, except tractability ?",2010-07-28 02:10:27.0,368.0,
737,868,0,Also help with laying out tables on this site is much welcomed and appreciated.,2010-07-28 03:15:57.0,9426.0,
738,869,0,Interesting... I'm not exactly sure how to apply that to what I've done but I'll think on it a bit more so I can articulate a question. Thanks!,2010-07-28 04:22:26.0,9426.0,
739,705,0,"@Srikant, thanks for the suggestion.  I spent some time trying it out and it seems like a good start at getting a quick visual comparison, especially when the fit is bad.",2010-07-28 04:35:31.0,251.0,
740,870,0,your reference about q-value should be http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1074290335,2010-07-28 05:54:26.0,223.0,
741,866,0,"""Say I want to estimate a large number of parameters"" this could be made more precise: What is the framework ? I guess it is linear regression?",2010-07-28 05:59:31.0,223.0,
742,870,0,"The Benjamini-Hochberg procedure is not for calculating the FDR, it is for controlling the FDR (keeping it under a predefined threshold)",2010-07-28 06:11:26.0,223.0,
743,870,0,"Your question, as it stands, is difficult to understand. What do you mean by ""referred to"" ?",2010-07-28 06:15:01.0,223.0,
744,858,1,"Rather a non-implemented feature, still you can send a bug report.",2010-07-28 09:23:10.0,88.0,
745,497,0,Change to community wiki.,2010-07-28 09:27:17.0,88.0,
747,835,0,aligatou gozaimasu,2010-07-28 09:35:29.0,223.0,
748,497,0,@mbq Done......,2010-07-28 09:36:32.0,190.0,
749,252,0,"@Dirk yes, that is correct, have edited my answer to include this information, cheers.",2010-07-28 10:00:28.0,81.0,
750,789,1,"This is Creative Commons but does not allow derivates... :( That way, I cannot use the bits in my own course material, just the bits I need them to know...",2010-07-28 11:00:41.0,107.0,
751,880,0,"But still, what do you want to measure with CV and in what purpose? To get a cutoff of attribute number?",2010-07-28 11:15:17.0,88.0,
752,880,0,"@mbq: thanks for the advice. I have edited the question accordingly, hope it is more clear now !",2010-07-28 11:26:47.0,223.0,
753,887,0,"What do you mean by: ""we would like to test whether it is an vis-a-vis the general population""? and ""single sample S""?",2010-07-28 12:06:44.0,,user28
755,886,2,I don't see the interest of these questions about trying to bridge a fictive gap. what is the aim of all that ? in addition there are a lot of others idea that are fundamental in statistic... and loss function is at least 100 years old. can you reduce statistic like that ? maybe your question is about fondamental concept in datamining/statistic/machine learning however you call it ... Then the question already exists and is too wide http://stats.stackexchange.com/questions/372/what-are-the-key-statistical-concepts-that-relate-to-data-mining/381#381.,2010-07-28 12:19:52.0,223.0,
756,890,0,Maybe you can precise your problem?,2010-07-28 12:44:10.0,88.0,
759,886,0,"Well, I do not know much about machine learning or its connections to statistics. In any case, look at this question: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning which suggests that at the very least that the approaches to answer the same questions are different. Is it that 'unnatural' to wonder if there is some sort of link between them? Yes, I agree that there lot of ideas in statistics. That is why I have fundamental in quotes and restricted the scope to estimating parameters of interest.",2010-07-28 13:13:32.0,,user28
760,886,0,"@Srikant link between what ? note that I really like to search link between to well defined objects, I find it really natural.",2010-07-28 13:20:05.0,223.0,
761,893,0,An excellent link. Thanks for sharing this one!,2010-07-28 13:43:41.0,1356.0,
762,894,7,"It is not quite correct to say ""you cannot change z"". In fact, you have to change z to make the sum equal 10. But you have no choice (no freedom) about what it changes to. You can change any two values, but not the third.",2010-07-28 14:29:49.0,25.0,
764,899,0,Can you assume that the other two groups are from different Normal distributions?,2010-07-28 14:37:00.0,8.0,
765,908,0,"said ""You miss one important issue -- there is almost never such thing as T[i]""

 I wanted the answer to focus on the problem of selecting the number of variables. Construction (which I agree are not perfect) of T[i]  are discussed here http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification

Sometime, it is also usefull to discuss problem separatly.",2010-07-28 15:21:55.0,223.0,
766,902,0,This is an interesting paper.  Could you add to your answer with some more references and include (for instance) the paper's title and author?  Are there any particular academic research labs working in this area that would be relevant?,2010-07-28 15:37:02.0,5.0,
767,908,1,"@robin But here you can't tear those apart. The most of algorithms mentions in that question were created to address this issue -- forward selection is to remove correlated features, backward elimination is to stabilize the importance measure, mcmc is to include correlated features...",2010-07-28 15:37:10.0,88.0,
768,908,0,"@robin the idea of making some exact importance measure was a base for so-called filter algorithms which are now mainly abandoned since they were just too weak. They have the advantage that they are computationally cheap, still this is not worth it.",2010-07-28 15:42:44.0,88.0,
769,910,0,I'm not sure that this is entirely correct.  In what sense do machine learning methods work without parameter estimation (within a parametric or distribution-free set of models)?,2010-07-28 15:48:18.0,39.0,
770,910,1,"You are estimating/calculating something (the exact term may be different). For example, consider a neural network. Are you not calculating the weights for the net when you are trying to predict something? In addition, when you say that you train to match output to reality, you seem to be implicitly talking about some sort of loss function.",2010-07-28 15:59:39.0,,user28
771,779,0,@Henrik Is there any meaningful difference between a single sample from each of N independent and *identically distributed* RVs and N independent measurements of a single RV?,2010-07-28 16:03:13.0,174.0,
772,899,0,"@cgillespie: it is the same group, just with two modes, I guess, and therefore I probably cannot assume this.",2010-07-28 16:03:55.0,219.0,
773,910,0,"@John, @Srikant Learners have parameters, but those are not the parameters in a statistical sense. Consider linear regression y=a*x (without free term for simp.). a is a parameter that statistical methods will fit, feed by the assumption that y=a*x. Machine learning will just try produce a*x when asked for x within the range of train (this makes sense, since it is not assuming y=a*x); it may fit hundreds of parameters to do this.",2010-07-28 16:07:24.0,88.0,
774,909,0,"`optimize` requires two distributions to be side-by-side as I understand. In my case one is inside the other, i.e., the values from the second population are on both side of the limits.",2010-07-28 16:09:25.0,219.0,
775,11,1,"Interpolation involves 3 things: 1) a class of functions to interpolate, e.g. sound, pictures, terrain; 2) grids of known / unknown points, 4 cases regular <-> scattered; and 3) a model of noise added to 1). There are many many interpolation methods for various cases, most ad hoc, not ""in common use"";
even IDW has variants. Can you describe what you're interpolating ?",2010-07-28 16:11:35.0,557.0,
776,910,0,"@Srikant About the second issue; yes, it can be called a loss, but on unseen data, not the training one. When statistical models are used for prediction, it is assumed that the train is a perfect representation of reality and contains the full scope of the true process, which is not the case of machine learning.",2010-07-28 16:18:22.0,88.0,
778,349,0,"Any intuition? yes. And your approach could work in general. However, in this case we can not have a lot of memory/computation. It is in a networking application where the device could see tens of thousands of items per second, and have VERY little processing left over for this purpose. Not the ideal/typical scenario, I know, but that is what makes it interesting!",2010-07-28 16:43:50.0,247.0,
781,894,0,"That's right, thanks for spotting an error!",2010-07-28 17:35:42.0,1356.0,
782,924,0,"By the way, is this a duplicate of what you posted here? http://stats.stackexchange.com/questions/920/test-if-probabilities-are-statistically-different",2010-07-28 17:42:49.0,,user28
783,924,0,I didn't think it was a duplicate. One question deals with probabilities and this one deals with a discrete variable.,2010-07-28 17:43:42.0,559.0,
784,926,0,"When I saw your original answer, for a second, I lost faith in my intuition! :-)",2010-07-28 17:49:37.0,,user28
785,926,0,It was surprising to me also!,2010-07-28 17:52:05.0,287.0,
787,926,0,"#JoFrhwld, would you happen to know the Matlab function to calculate this? I don't have access to R :(",2010-07-28 18:01:10.0,559.0,
788,930,0,Indeed we have suggested something like this in this topic; I'll dig for a link.,2010-07-28 18:05:47.0,88.0,
789,930,0,And I think your criterion for 'intervality' is valid only for a uniform distribution.,2010-07-28 18:10:53.0,88.0,
790,926,0,"Sorry, I don't have access to Matlab! I'd just google around for it. It's surprising you wouldn't have access to R, considering it's free and platform independent.",2010-07-28 18:14:50.0,287.0,
791,931,0,"And probably not easily put on an iPod, if that is essential to you.",2010-07-28 18:18:25.0,561.0,
792,929,1,I don't think it is a valid question here; just check out wiki and references there: http://en.wikipedia.org/wiki/Probability_interpretations,2010-07-28 18:20:32.0,88.0,
793,929,0,"That wikipedia article is filled with ""dubious-discuss"" and other tags, so the question would become how comprehensive is that wiki article, what interpretations are missing from it.",2010-07-28 18:26:20.0,560.0,
795,935,0,Not comprehensive at all then. Thanks.,2010-07-28 18:41:44.0,560.0,
796,928,0,"By the way, P70 - P50 represents the percentage of people who are between the 70th percentile and 50th percentile and that percentage is 20. Clearly that is the same as P50 -P30. When assessing if differences are equal I do not think you should look at the underlying scores.",2010-07-28 18:44:11.0,,user28
797,904,0,"Fully agree ! the questions that are asked are different. Registration, landmarking, estimation of derivatives can arise from the functional view. This convince me ! so the big deal with functional data (as it stands in statistical literature) would not be that it is defined on a continuous set but more that it is indexed on an ordered set?",2010-07-28 18:44:58.0,223.0,
798,929,1,"Still references are quite informative. And nevertheless even if it is discussive and mentions something that is not present in the book it is some kind of a clue, isn't it?",2010-07-28 18:45:07.0,88.0,
799,926,0,"@MJoFrhwld, I need to use Matlab so I can incorporate it into my simulation framework.",2010-07-28 18:53:16.0,559.0,
801,439,0,"I've accepted this answer on the somewhat capricious basis that I now remember Wasserman's book being recommended to me by someone else several years ago.  The same person also recommended ""The Cartoon Guide to Statistics"" by Gonick and Smith.",2010-07-28 19:13:50.0,89.0,
802,290,0,"I am tempted to say ""R for Stata users"", but I would get voted down for this :)",2010-07-28 19:19:36.0,253.0,
803,917,1,"I'd argue from a standpoint of trying to create good guess as to what video game a person likes coding 1s and 0s for like and didn't like isn't a good approach.  A scale of 1-5 or 1-7 is quite easy to elicit and will require fewer datapoints to generate a good model (because each data point provides more information).  With caveats about treating ordinal data as interval data of course applying, but probably not really that important in this context.",2010-07-28 19:23:52.0,196.0,
804,936,1,Econtalk is one of the most intelligent podcasts out there.,2010-07-28 19:36:06.0,319.0,
805,917,0,"Agreed. I just answered in terms of the question, which was about logistic regression. I've fit proportional odds logistic regressions for ordered data like this in R using `MASS:polr`.",2010-07-28 19:42:06.0,287.0,
806,918,0,"I was actually going to discuss this same thing. I had a job evaluating CEP/ESP tools, such as Aurora, STREAM, and Esper once...",2010-07-28 20:40:34.0,110.0,
808,939,0,Read carefully: http://en.wikipedia.org/wiki/Yates'_correction_for_continuity,2010-07-28 20:55:03.0,88.0,
810,898,0,"Bayesian methods are theoretically well suited for real time analyses because the essential Bayesian method is to take a prior belief, and using a bit of data, compute a posterior belief -- which can become the prior for a new cycle.  While that part sounds good, there are practical difficulties and significant hurdles in specifying interesting models on real, changing data that are tractable and easily computable.",2010-07-28 21:35:46.0,87.0,
811,944,6,IMO total off-topic. Voted to close.,2010-07-28 21:58:10.0,88.0,
812,948,0,Will t.test() compare the means across all levels of a factor?,2010-07-28 22:59:28.0,569.0,
813,949,0,"Till we have latex support, could you please avoid using latex. It is hard to make out what you are saying. See this meta thread: http://meta.stats.stackexchange.com/questions/5/what-typographic-support-is-available-to-support-display-of-statistical-formula",2010-07-28 23:03:58.0,,user28
814,917,0,Thanks guys! TONS O' INFO. :D So then what I'm doing now isn't exactly logistic regression and is instead proportional odds logistic regression?,2010-07-28 23:06:39.0,9426.0,
815,682,0,I ordered Statistics in Plain English... I'll accept your answer when I get it on Thursday if I dig it as much as I think I will.,2010-07-28 23:08:37.0,9426.0,
817,918,0,VFDT looks very interesting; thanks for the references!,2010-07-29 00:21:10.0,5.0,
818,928,0,"And the same stands for calculating correlation coefficients, I guess?",2010-07-29 00:26:04.0,1356.0,
819,928,0,"Yes, that would be correct. In fact correlation would be ratio as 0 means no correlation and such a conclusion is scale invariant. By the way, I suspect that percentiles would also be classified as ratio as the 0 point is scale invariant but it does not really matter. Most statistic applications require interval level measurements not necessarily ratio.",2010-07-29 00:53:56.0,,user28
820,939,0,@mbq I did read the Wikipedia article but I wanted to confirm and verify it,2010-07-29 00:55:08.0,559.0,
821,944,3,I agree. I think R programming question would be a better fit on StackOverlfow.com than here. http://stackoverflow.com/questions/tagged/r,2010-07-29 02:45:20.0,319.0,
822,808,0,Thanks. Had the opportunity to work with a great statistician for a while and was always amazed by how much information he could get out of the least amount of data by asking very pointed questions. This quotation so reminds me of hm,2010-07-29 03:30:12.0,482.0,
823,917,0,"What you _want_ to do is a proportional odds logistic regression. I'm not exactly sure what to call what you _have_ done,",2010-07-29 03:47:12.0,287.0,
824,952,0,How are you defining redundancy?,2010-07-29 04:15:57.0,,user28
825,957,0,"I am not sure I understand what you are saying. I am not sure there is any relationship between a new draw x and the current sample mean mean(S) as x ~ N(mu,sigma^2). Clearly, the draw of x can be anywhere in the support of the distribution. It is more likely to be around mu and less likely to be in the tails but it does not have anything to do with mean(S).",2010-07-29 04:20:42.0,,user28
826,955,1,Imo this question is offtopic on this site as it not goes about statistics itself.,2010-07-29 04:36:54.0,190.0,
827,917,0,Hahaha ok thanks man,2010-07-29 05:35:37.0,9426.0,
828,603,0,It does not; GLM is not machine learning. True machine learning methods are wise enough to hold their level of complexity independent of growing number of objects (if it is sufficient of course); even for linear models this whole theory works quite bad since the convergence is poor.,2010-07-29 07:43:01.0,88.0,
829,707,0,Have you even read this paper? Nevertheless it is works only for linear models (even the title shows it!) it is about asymptotic behavior for infinite number of objects. 100 is way not enough.,2010-07-29 07:49:16.0,88.0,
830,707,1,And I wish you luck making 10-fold cross validation on set with 9 objects.,2010-07-29 07:50:50.0,88.0,
831,644,2,"CV using full set for model selection, huh? It's a common error (still even Wikipedia mentions it), because it is a hidden overfit. You need to make a higher level CV or leave some test to do this right.",2010-07-29 08:00:22.0,88.0,
832,934,1,"I'd love to see this loss minimizing in clustering, kNN or random ferns...",2010-07-29 08:09:31.0,88.0,
833,922,0,Statistical crowd is clicking randomly in SPSS until desired p-value appears...,2010-07-29 08:17:05.0,88.0,
834,962,0,+1 Very nice paper.,2010-07-29 08:59:46.0,88.0,
835,955,0,Have you voted for close? It is strange that since without Shane we couldn't close any question...,2010-07-29 10:33:19.0,88.0,
836,750,3,"That would be true if everyone were knowledgeable enough in statistics to drive the correct conclusions. Alas, that quote is very applicable to many of those amusing human beings called politicians...",2010-07-29 11:22:35.0,582.0,
837,955,0,"I do think this could be rephrased in such a way that it would be on-topic (eg about the kind of work that statistical consultants do), but this is more like a job request.",2010-07-29 11:30:08.0,5.0,
838,607,7,Great post! Note that Vapnick had a PhD in statistics. I'm not sure there are a lot of computer scientist that know the name Talagrand and I'm sure 0.01% of them can state by memory one result of talagrand :) can you ? I don't know the work of Valiant :),2010-07-29 11:30:57.0,223.0,
840,955,0,"@mbq Sorry, I don't understand what you mean. Yes I voted to close. All 500+ rep users can (which means 8 users).",2010-07-29 11:45:13.0,190.0,
841,955,0,"@Shane So you're here... I've got an impression that you should be holidating.
@Shane, @Peter Still, the problem exists, because not all of this eight is active in closing.",2010-07-29 11:53:06.0,88.0,
843,955,0,"@mbq: I am vacationing, but occasionally my iPhone gets service.  And let's face it: I'm addicted to this.",2010-07-29 13:18:13.0,5.0,
844,929,0,See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.,2010-07-29 13:25:38.0,,user28
845,966,0,"Your last line should have been with the log(P)'s, right? And isn't this also a simple consequence of Markov processes and the related exponential distribution?",2010-07-29 13:47:51.0,56.0,
846,980,0,I forgot this: The points along the x-axis come with varying spacing.,2010-07-29 14:06:54.0,,Pete
847,980,0,I am not sure I understand. Don't you have a y-axis?,2010-07-29 14:07:59.0,,user28
848,966,0,In the notation I borrowed:  x_t := log(X_t) to the p_t are the logs of the P_t.,2010-07-29 14:08:04.0,334.0,
849,980,0,"Ah, sorry. I misstyped. I have now changed it above.",2010-07-29 14:11:05.0,,Pete
850,947,0,"Thanks, this worked.",2010-07-29 14:12:49.0,,anonymous
851,905,3,"Yes, and a slight clarification is that online learning algorithms, at least as studied in Machine Learning, mostly make the assumption that your ability to store examples is very limited compared to the size of the data set. In the most limiting case, you only get to see one example at a time, and then you have to forget it after you've used it to update your classifier.",2010-07-29 14:14:16.0,6.0,
852,944,1,"@mbq: @JohnD.Cook: This is not a programming question. It is also not a question about a common tool of programmers. It is a question about a common tool of statisticians. That is why I asked it here. I don't mind that it was closed, though, since I received and accepted the correct answer.",2010-07-29 14:16:46.0,,anonymous
853,981,0,"Ah, interesting, but I need it to be predictable, i.e. to have the same result each time I view the data.",2010-07-29 14:18:32.0,,Pete
854,980,0,"I also think you need to provide a bit more information. For example, I still cannot visualize the graph. What is your goal?",2010-07-29 14:19:04.0,,user28
855,981,0,"In that case, generate the *n* indexes of the points you choose, and store those indexes.",2010-07-29 14:21:07.0,8.0,
856,981,0,Or store the seed to the RNG before sampling.,2010-07-29 14:25:18.0,334.0,
857,981,0,Dirk's solution regarding the seed is probably the better option.,2010-07-29 14:31:10.0,8.0,
858,983,0,Is there any way to do this without knowing in advance what the relative varience will be or is it just a case of having to take a guess at the relative varience to do anything at all?,2010-07-29 14:38:55.0,210.0,
860,980,0,"Ok, sorry. I have added some more details above.",2010-07-29 15:11:17.0,,Pete
861,982,0,"Yep, visualization is what I want. I have added some more info in the question.",2010-07-29 15:11:55.0,,Pete
862,984,0,"Ok, sorry. I have added some more details above.",2010-07-29 15:12:12.0,,Pete
863,980,0,"Ok, just so I understand better- your x-axis is time with the 0 point being the time of the first sample. Your y-axis is beats per minute. Is that right? I still do not know your goal: Why do you want to reduce the number of data points? - Reduce clutter? or See patterns better? or the existing points are too close to each other for you to figure out what is happening?",2010-07-29 15:17:06.0,,user28
864,911,0,Thanks! But I hoped for a statistic that is faster and more stable to calculate...,2010-07-29 15:25:23.0,506.0,
866,963,3,"You should be very careful in interpreting p-values from these t-tests with this looping approach. If you have 20 columns, and there is no real difference between any of them, at least one comparison is likely to appear significantly different at p < 0.05. You should at least multiply the p-values by the number of tests, to produce the Bonferronoi correction. Or, you could multiply them by their inverse rank for the Holm correction.",2010-07-29 15:36:54.0,287.0,
867,982,0,seconding plotting raw data with a smoothing line.,2010-07-29 15:43:10.0,287.0,
869,685,0,See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.,2010-07-29 15:48:02.0,,user28
870,971,0,"Thank you, that's exactly what I needed. I'm not completely sure of what you mean when you talk about the overdispersion (sorry, I'm not a statistician, maybe it's something very basic)... You say that the residual deviance should be equal to the residual degrees of freedom... how would I check that?",2010-07-29 16:10:48.0,582.0,
871,944,0,"It is a question about using a computer program use; so I think you should try SuperUser. Dirk is there, so R questions will be answered in a blink of eye ;-)",2010-07-29 16:13:58.0,88.0,
872,981,0,"Calculating averages per each second is ok, but what I do when there is no data for a specific second. I guess I could do some interpolation from the seconds before and after it, but it would be great with some specific (named) method for this, so I don't try to invent something already invented.",2010-07-29 16:24:46.0,,Pete
873,948,0,"I'm not sure what you mean. If you have some continuous variable column A (say people's heights) and some grouping factor column B (say Country of origin), you really want to do a `pairwise.t.test()`. You would do this like `pairwise.t.test(df$A , df$B)`. This function will automatically correct for multiple comparisons.",2010-07-29 16:28:03.0,287.0,
874,997,0,"excellent logic, but wouldn't that be binom.test(3,13,0.5) (which is the same as binom.test(10,13,0.5)",2010-07-29 17:31:51.0,605.0,
875,982,0,thirding plotting raw data with a smoothing line --- You might want to also plot the change in BPM over time as a separate visualization.,2010-07-29 17:52:51.0,601.0,
876,981,0,The interpolation you are talking about is the moving average or smoothing bit.,2010-07-29 18:21:06.0,8.0,
877,993,0,You should really should state would language you sample code is for.,2010-07-29 18:33:14.0,8.0,
878,730,1,I use this quote a lot to explain the difficulties in mathematicians transitioning to statistics,2010-07-29 18:48:03.0,549.0,
879,971,0,If you give `summary(model1)` you'll see something like `Residual deviance: -2.7768e-28  on 0  degrees of freedom`,2010-07-29 18:55:47.0,339.0,
882,138,3,Refer to SO. http://stackoverflow.com/questions/192369/books-for-learning-the-r-language/2270793,2010-07-29 19:17:06.0,1356.0,
884,981,0,"But moving average will not reduce the number of data points, will it? As far as I know it will _only_ smoothen. Or are there optional moving averages that does both?",2010-07-29 19:29:56.0,,Pete
885,763,0,this is great information.  Do you know of any papers that cover this?,2010-07-29 19:39:40.0,5.0,
886,981,0,I rewrote the answer. Hopefully it's clear and useful now.,2010-07-29 20:07:35.0,8.0,
887,993,0,"@csgillespie: sorry; it's Matlab code, as Elpezmuerto requested in his comment to the other answer.",2010-07-29 20:44:37.0,506.0,
888,899,1,Do you know that members of the second group aren't included in the first group or are you just willing to mistakenly label those members as belonging to the first group?,2010-07-29 20:54:53.0,3807.0,
890,926,1,There is a chi-square test function in Matlab Central File Exchange: http://www.mathworks.com/matlabcentral/fileexchange/4779,2010-07-29 21:19:19.0,128.0,
891,1004,0,"KS test always turns out to be non-normal with very large sets (I have dataset larger than 1 million data points). Also, I would like to have a quantifying measure that tells me about goodness of fit rather than just a test.

Am I asking for too much?

Thanks in advance,
A",2010-07-29 21:21:41.0,608.0,
892,1004,0,"I agree with the question answerer.  What you are looking for is exactly a KS test.  The heightened ability of a KS test to detect violations of normality in large datasets is not a reason to toss it aside.  But, you may want to set different thresholds in terms of the D statistic depending on your sample size or get into the guts of the equation and see if you can remove the increased likelihood of rejecting the null as a function of sample size.",2010-07-29 21:50:42.0,196.0,
893,1001,0,Spearman's Correlation Coefficient is used to compare relative rank orders.  It strikes me that when comparing normal distributions in this way you are particularly unlikely to detect differences in kurtosis.,2010-07-29 21:53:37.0,196.0,
894,981,0,"Thanks, very clear! Random sampling sounds like a great tool. But what can I do if the original data points are very unevenly spread out - I still want, for example, one final data point per 5 second interval, and that might not be present (maybe not even before the random sampling). That means I need to do some kind of interpolation.",2010-07-29 22:09:51.0,,Pete
895,1001,0,"Not adding that this code is a total junk; you must normalize histograms somehow, cutting the end is not a good idea (still I have no idea how to do it). And make code a code (indent with 4 spaces).",2010-07-29 22:13:58.0,88.0,
896,1004,0,"And as drknexus wrote, it is easy to extract the statistic and use it for comparison. Even the p-values will do.",2010-07-29 22:15:59.0,88.0,
898,1004,0,Is it actually valid to use p-values as a qualitative measure if they are not statistically significant (i.e. below .05)?  Wouldn't it be much better to have some kind of effect size?,2010-07-29 23:25:52.0,608.0,
899,1012,1,"What is ""not meaningful"" about the standard deviation of a uniform distribution? It is a measure of spread for the uniform, just as it is for almost every other distribution. It may not be the best measure of spread, but it is certainly meaningful.",2010-07-29 23:32:38.0,159.0,
900,965,0,"+1 from me--domain-independent answers are particularly useful here. It seems to me that in interdisciplinary domains, the ""why"" is often ignored because of the (understandable) emphasis on practical application.",2010-07-29 23:32:42.0,438.0,
901,1023,3,community wiki?,2010-07-29 23:55:06.0,,user28
902,1004,4,"Why below 0.05, not 0.04973? Statistical significance does not have any in-depth meaning, it is just an accepted probability of analysis failure.  The operation of transforming statistic into a p-value is monotonic, so there is no problem with comparison. Still obtaining a significance level of this comparison is problematic (I have no better idea than bootstrap).",2010-07-30 00:20:42.0,88.0,
903,1004,0,"Hi drknexus, you said ""set different thresholds in terms of the D statistic depending on your sample size"". How can this be achieved with kstest. I did not find a way to manipulate thresholds. I assume that with ""getting into the guts of the equation"" you mean transformations before testing. My distribution looks multi-modal and I have already an idea what these multi-modal tendencies might be i.e. how I can single them out...",2010-07-30 00:42:25.0,608.0,
904,429,1,"Besides... Shapiro-Wilk's test is often used when estimating departures from normality in small samples. Great answer, John! Thanks.",2010-07-30 01:24:13.0,1356.0,
905,871,0,See the meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed where this question is proposed to be closed.,2010-07-30 02:00:21.0,,user28
906,997,0,"Thanks Andreas. I fixed it. You are right of course, and that is what I entered into R. I am not sure how I copy and pasted something totally different into this answer, but it is fixed now.",2010-07-30 03:47:01.0,25.0,
907,957,0,"But you don't and can't know mu or sigma. What you know are the mean and SD of the sample, mean(S) and stdev(S).",2010-07-30 03:51:27.0,25.0,
908,1027,0,"Thanks for your thoughts. Maybe I wasn't clear though. I do have paired sets. They are the standardized quadratic scores of the test-retest results, ie some distributions were elicited twice. I will edit the question.",2010-07-30 06:42:57.0,108.0,
909,971,0,thanks!,2010-07-30 06:49:40.0,582.0,
910,1012,0,"All I meant was that the standard deviation is not really a parameter that defines or describes the uniform distribution well.  In my mind, the standard deviation refers to the spread of a normal, or near normal distribution.  

Simply because a value is calculable does not mean that it is interesting or meaningful.  For example, I might be able to calculate what rate parameter from an exponential distribution best matches a normal distribution, but to me such a value would not be particularly meaningful because the distribution being described is not actually exponential.",2010-07-30 06:51:57.0,196.0,
911,949,1,Could you put $ signs around the LaTex? It now will be rendered.,2010-07-30 07:04:38.0,190.0,
912,916,0,"It also seems as if $y$ is always larger than some linear function of $x$, $y>C x$.",2010-07-30 10:11:17.0,56.0,
914,981,0,@Pete - see answer,2010-07-30 10:45:12.0,8.0,
915,916,0,"Possibly; still it is hard to tell without a zoom on this dense area (points are not transparent, so one cannot judge the density, and so this plot may be deceiving).",2010-07-30 10:48:57.0,88.0,
917,445,0,"In referring to other questions, don't say ""above"" or ""below"" as the order can change depending on votes or whether the answer is accepted, for example.",2010-07-30 11:33:28.0,159.0,
918,1012,5,"I can define a uniform distribution with mean 0.5 and standard deviation 1/12. It is perfectly well defined, but it is not the most natural parameterization. There is nothing about standard deviations that implies normality.",2010-07-30 11:36:04.0,159.0,
921,1031,0,Please add $ around your LaTeX. It should now be rendered ok. See http://meta.math.stackexchange.com/questions/2/tex-math-markup-is-sorely-needed,2010-07-30 13:10:35.0,159.0,
922,21,0,What kind of demographic data are you trying to forecast? Every data could (and should) be treated differently.,2010-07-30 13:43:17.0,614.0,
923,1040,0,You can latex on this site. Please enclose the tex with $ $. See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats,2010-07-30 14:29:26.0,,user28
924,1027,0,"Could you clarify how you are computing standardized quadratic scores? And, what exactly your are correlating?",2010-07-30 14:44:55.0,,user28
927,1027,0,@Srikant: I updated the question with some background information,2010-07-30 17:16:03.0,108.0,
928,1045,0,"Not an answer (because I don't know) but I just saw this new book on the Springer web site: ""A Comparison of the Bayesian and Frequentist Approaches to Estimation"" http://www.springer.com/statistics/statistical+theory+and+methods/book/978-1-4419-5940-9 that (one would assume) may have some answers.",2010-07-30 17:20:19.0,247.0,
929,1019,0,I don't understand... one set per person?,2010-07-30 17:40:15.0,88.0,
930,1019,0,@robin & @mbq I would suggest keeping it one dataset per post. This so people can indicate with votes which of the suggested ones there also suggest/support,2010-07-30 17:59:24.0,190.0,
931,1049,0,"Welcome -- Nice to see you here, Pat!",2010-07-30 18:35:19.0,334.0,
932,949,0,"cool, mostly works, although \\{ is not recognized and \\ldots looks strange",2010-07-30 18:43:31.0,511.0,
933,949,0,"mostly worked here on firefox/Linux.  the subscript 1 on x1,...,xd didnt appear as a subscript but the others did.  The issues with formatting are minor, mostly the latex appears higher than other characters in the line.",2010-07-30 19:52:01.0,87.0,
934,1051,0,"I understand what you wrote but I am not sure that answers my question. The method you describe and the methods in the linked question in my ps avoid estimating the nuisance parameter on the grounds that the nuisance parameter is not of interest. Such an approach is fine but then without estimating the nuisance parameter we cannot construct a confidence interval for this parameter. Thus, the sentence in the wiki makes sense only if we estimate the nuisance parameter using standard maximum likelihood.",2010-07-30 22:45:06.0,,user28
935,1051,0,"So my question is: if we estimate the nuisance parameter using maximum likelihood how is its treatment any different than estimating any other parameter. By the way there seems to be a typo in your eqns as the second term f(z|-) should not depend on theta, right?",2010-07-30 22:47:15.0,,user28
937,1051,0,"The equation looks right to me -- I hope I'm not having a blind moment staring at the screen here.  In each case, we have one component that depends on theta and the other (nuisance) component may or may not.  The crucial point is that we must find a transformation that isolates theta to *some* extent to obtain either a marginal or conditional.  When we can't, we must work with profile and other estimated likelihoods.  We could argue that ML is losing information, but that's another question entirely.  Still, perhaps this sheds light on the differences?",2010-07-30 23:33:50.0,251.0,
939,934,0,"Well, for a loss function characterization of k-means nearest neighbor, see the relevant subsection (2.5) of this paper: http://www.hpl.hp.com/conferences/icml2003/papers/21.pdf",2010-07-30 23:41:56.0,39.0,
940,628,0,"Note that the last definition here is an *Integrated* (or Bayesian) Likelihood, not a Marginal Likelihood.",2010-07-31 00:09:58.0,251.0,
941,628,0,"Is this correct in the RHS for partial likelihood: ""L2(θ|theta)""?",2010-07-31 00:13:14.0,461.0,
942,628,0,oops. Thanks for spotting the typo. Fixed it now.,2010-07-31 00:34:57.0,,user28
943,728,0,It has actually led to very interesting articles... :),2010-07-31 01:07:04.0,253.0,
944,1053,2,community wiki please. Your question does not have an 'objective best' answer.,2010-07-31 02:04:28.0,,user28
946,1055,1,"Correlation is not necessarily linear - Spearman's rho relies on the monotonic function, and yet, we refer to it as a ""correlation coefficient"", not ""mutual information coefficient"". And for a good reason: it provides an information about association between two variables. Mutual information, redundant information, mutual variance, correlation - these terms are so similar, and this question refers to _network reconstruction_, so I guess that we ended up in the wrong area with right terminology. This is quite specific question...",2010-07-31 02:26:27.0,1356.0,
947,1055,0,Good point. I've edited my answer to include monotonic relationships. I don't know anything about network reconstruction.,2010-07-31 03:04:58.0,159.0,
948,1051,0,"I am not sure how far we should take this discussion as SE is not a good outlet for discussions. I agree with what you said but I am not sure you are addressing the issue I raised. If you use a nuisance free likelihood how can you construct a confidence interval for the nuisance parameter? In any case, I should probably stop here and I will let you have the last comment.",2010-07-31 04:27:08.0,,user28
949,1015,0,Is your your probability score identical to what is called the Brier score? (See: http://en.wikipedia.org/wiki/Brier_score),2010-07-31 04:34:21.0,,user28
951,1051,0,"Sorry, I wrote we don't estimate the nuisance, let me try again with some examples that address it.  Partial likelihood just discards information about the nuisance.  Profile replaces it with the MLE at fixed theta, but this doesn't account for uncertainty in lambda.  Contrast with the Bayesian and specification of a prior.  I think I get what you're saying that it's just estimation like any other parameter.  But the treatment matters because it's how you account for the uncertainty due to the nuisance which affects the intervals, whether through the posterior or the likelihood.",2010-07-31 04:54:33.0,251.0,
952,894,0,+1 I think this is the nicest/simplest example here.,2010-07-31 05:18:59.0,251.0,
953,1019,0,"@Peter, OK, I follow your idea, I have changed the question accordingly.",2010-07-31 06:14:49.0,223.0,
954,728,0,@Tal: Fully agree! I think the whole area on optimal separation in minimax testing is starting from this idea ... and it is still so confused for a lot of statistician. For those interested see the paper of donoho http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492 (and the references in the paper ! since things are much older than donoho's paper),2010-07-31 06:19:15.0,223.0,
955,1015,0,@Srikant: Yes it is. It is known by both names.,2010-07-31 06:49:42.0,108.0,
956,659,0,+1 from me--nice explanation.,2010-07-31 10:37:32.0,438.0,
957,1061,0,"Aww, you've beat me. +1",2010-07-31 10:44:50.0,88.0,
958,1061,0,"Thanks! Is it the same as the df=6 that LogLik returns? Could I have used -2*logLik(fit)+2*(attr(logLik(fit),""df""))  ??",2010-07-31 10:58:19.0,339.0,
959,255,4,Check out the outlier function in the randomForest package http://lib.stat.cmu.edu/R/CRAN/web/packages/randomForest/randomForest.pdf,2010-07-31 11:01:22.0,339.0,
960,213,8,"5th D in color hue, 6th D in color intensity, 7th D in point size and we can go ;-)",2010-07-31 12:36:14.0,88.0,
961,963,0,You are right. I was feeling that something was wrong. They constitute a family of confidence intervals. Perhaps its even better to adjust using Tukey Honest Significant Differences (TukeyHSD in R),2010-07-31 15:45:10.0,339.0,
962,1062,0,Orthogonal parameterization has a broad range of uses.  Please be more specific.  What are you applying it to?,2010-07-31 15:48:18.0,601.0,
963,1061,0,"@user603 Also, how did you ""draw"" that indexed sigma?",2010-07-31 15:48:24.0,339.0,
964,1065,0,"On 2) I believe the confusion is what p(X=.01) means when X is a continuous random variable.  Intuitively, the probability seem to be zero everywhere because there is no chance X is exactly .01. The questioner should review the definition of a density function in the continuous case, which is defined as the derivative of the cumulative density function.",2010-07-31 17:15:21.0,493.0,
965,827,0,@mbq +1 for snowfall- abstracts snow even further and makes parallel computing with R pretty simple.,2010-07-31 18:14:41.0,13.0,
966,823,0,It's unfortunate that consensus has so much control over what kind of science gets funded.,2010-07-31 18:26:15.0,13.0,
967,827,0,You could give it to my answer not Shane's... still thanks ;-),2010-07-31 18:28:48.0,88.0,
968,1066,0,"You mean that you are looking at the distribution of subsample **lengths** of subsegments covering each position, right? On the other hand (probably I'm wrong) I'm guessing it has something to do with the sequencing?",2010-07-31 18:40:22.0,88.0,
969,1066,0,"No, I'm looking at the distribution of the **number of sub-segments** covering a specific position. For example, if we focus on position 1 in the large segment, in the first simulation we have 4 sub-segments covering it; in the second simulation we have 1 sub-segment covering it, etc.

I don't care what are the lengths of the sub-segments covering each position, just how many sub-segments there are.

You are not completely wrong, this has started as a part of an exercise I've been working on in a biology course, but has gone to another domain :)",2010-07-31 18:45:20.0,634.0,
971,1066,0,"Still, one more doubt -- what is a distribution of the lengths of subsamples? I think it is crucial to the answer. (I'm suspecting it is a binomial distribution)",2010-07-31 18:58:00.0,88.0,
972,1066,0,"the lengths of the sub-segments? A list of lengths is given. I believe the lengths distribute quite normally, but I did not check it.",2010-07-31 19:01:45.0,634.0,
973,1066,0,sounds like a poisson distribution...,2010-07-31 19:08:22.0,601.0,
974,1068,0,"I think I got the idea, although I will have to think about it some more. One other thing I intentionally did not mention at the beginning, is the fact we might have predefined ""hot"" subranges along our long segment. This means that every subrange that is drawn and completely includes one of the ""hot"" subranges will not be counted (it will be totally elimnated, not drawn again).",2010-07-31 19:12:59.0,634.0,
975,1068,0,"For example, if we have a predefined ""hot"" subrange 20..25 and we are now drawing a subrange of length  40, and we happen to draw 11..50, than we don't count anything (we throw this subrange). However, if we happen to draw 21..60 we count as normal (since our subrange does not include the complete hot subrange, only part of it).",2010-07-31 19:13:25.0,634.0,
976,1068,0,"If you want the pmf, then it seems the only sensible thing to do is to actually construct it.  Some sort of counting approach probably works best, similar to what I suggested where you don't need to actually count everything.  It's not clear what your goal is though.  If you're interested in the pmf, then an exact pmf is obviously better than some ad hoc approximation.  If you're interested in some underlying parameter, the fact that you can easily simulate from your distribution opens other possibilities.",2010-07-31 19:25:14.0,493.0,
977,1068,0,"OK, so what I really want is the following: I do not know if where the hotspots are (how many are there or how long is each hotspit), but I do have a few hypotheses (each hypothesis is a set of hotspots; each hotspot is just a subrange). These are hidden variables.

I also have one ""true"" mapping - where all the lengths were drawn and mapped to the long range. This is my data.

What I aim to do is to check which of my hypotheses is most likely given the data (the ""true"" mapping).",2010-07-31 19:36:44.0,634.0,
978,1068,0,"(and sorry I can't vote-up yet, I'm still new here :))",2010-07-31 19:40:30.0,634.0,
979,1068,0,"I don't exactly understand your data.  It sounds like you'll end up wanting to do something like EM or set up a full Bayesian model. If the probability of exactly matching your data using simulation is reasonable (above one in a million or something), you could just do rejection sampling to get the posterior probabilities of your hypotheses.  Unless your application is simple though rejection sampling may not be feasible since the match probability is too small.  In that case, having the pmf will help.",2010-07-31 19:57:37.0,493.0,
980,893,2,"It would be nice to have an explanation for *why* degrees of freedom is important, rather than just what it is.  For instance, showing that the estimate of variance with 1/n is biased but using 1/(n-1) yields an unbiased estimator.",2010-07-31 20:12:29.0,493.0,
981,1068,0,"What I thought of doing is: for each scenario, get the pmf for each position, then calculate the likelihood of the data to be produced given the scenario (multiple probability of each position, assuming positions are independent, although they actually are not). Then we have a likelihood for the data given each of the scenarios and we can choose the most likely scenario (from the set of given scenarios).

There is no chance I will fully match my data using simulations. There is too much noise and the segments are quite long.",2010-07-31 20:20:15.0,634.0,
982,1070,0,"I like your explanation (+1)-- it is probably more to the point for Alekk.  I'll leave mine up in the hope that it casts a different, perhaps useful, light on the role of orthogonality.",2010-07-31 20:41:58.0,39.0,
983,1068,0,"I've done some thinking. If I understand your suggestion correctly, I need to keep for each position in my long range m bernouli variables (one for each length, saying what is the probabilty the i-th subrange will cover this position).
Since I usually work with n=200k and m=20k, this means 4*10^9 variables, which is way too much. This could perhaps be ""compressed"" since many subsequent positions will have the exact same set of variables, but I can not think immediately of an easy way to do that. On the other hand, perhaps I can do all the calculations on the fly and not save those variables...",2010-07-31 20:44:17.0,634.0,
985,1026,0,"Actually, to the best of my memory, Harrell strongly discourages the use of AIC.  I guess cross-validation would probably be the safest method around.",2010-08-01 01:54:35.0,253.0,
986,1072,1,"Is there some way to quantify what is "" increasingly prefered "" these days ?",2010-08-01 01:55:20.0,253.0,
987,1070,0,Can you comment on how your definition of marginal likelihood relates to the one given in the wiki link? (See: http://en.wikipedia.org/wiki/Marginal_likelihood),2010-08-01 01:57:42.0,,user28
988,1026,1,"AIC is asymptotically equivalent to CV. See answers to http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other. I checked Harrell before I wrote that answer, and I didn't see any discouragement of the AIC. He does warn about significance testing after variable selection, with the AIC or any other method.",2010-08-01 02:54:39.0,159.0,
989,282,4,"Effort is commendable -- BUT -- neither PC1 nor PC2 tells you who did best in all subjects.  To do so the PC subject coeffcients would all have to be positive.  PC1 has positive weights for Math and Music but negative for Science and English.  PC2 has positive weights for Math and English but negative for Science and Music. What the PCs tell you is where the largest variance in the dataset lies.  So by weighting the subjects by the coefficients in PC1, and using that to score the students, you get the biggest variance or spread in student behaviors. It can classify types but not performance.",2010-08-01 02:58:57.0,87.0,
991,1073,1,"Nice explanation.  Doesn't explain people's cognitive failings, but +1 anyway.",2010-08-01 03:17:33.0,87.0,
997,1051,0,"[moving this comment from the orthogonal question] You might find this paper by Berger et al. interesting: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009211804 -- marginal likelihood is defined on p.5 before they try to sell the integrated form to ""likelihoodists"". Also check out the historical references on that page, Cox 1975 and Basu 1977. The former is the origin of partial likelihood and the latter is a critique of Fisher likelihood w.r.t. nuisance parameters.",2010-08-01 04:34:25.0,251.0,
998,1070,0,"All the cites on that page (Bos; MacKay/p.29) are to Bayesian concepts of likelihood (post ~ lik x prior). There's a connection, but it's not the marginal likelihood in the sense of Fisher and Likelihood theory. [see comment on Berger paper here: http://stats.stackexchange.com/questions/1045/is-there-a-radical-difference-in-how-bayesian-and-frequentist-approaches-treat-nu/1051#1051 ]",2010-08-01 04:37:58.0,251.0,
999,1026,0,"@Tal: Perhaps from one of his papers rather than the RMS book, I remember Harrell objecting to the use of AIC for simply choosing among a pool of *many* models.  I think his point was that you must add a variable at a time and compare two models methodically or use some similar strategy.  (To be clear, this is in line with Rob's answer.)",2010-08-01 05:14:48.0,251.0,
1001,282,0,"+1 good comment, cheers. You are of course correct, I should have have written that better and have now edited the offending line to make it clear I hope.",2010-08-01 09:29:48.0,81.0,
1002,1081,0,One remark -- expand nMDS. Is it just non-metric or something more exotic?,2010-08-01 09:35:29.0,88.0,
1003,825,2,"I have no problem with this sort of Q&A here.  R isn't such a mainstream language (like Python or Java) that a quant would naturally say, ""Oh this is a general programming question so I should go to StackOverflow or similar and ask this or look there for solutions"".  Actually it is more a question for an R mailing list or group site.  To serve those budding analysts who want to learn R we should be glad to have an answer here as well.",2010-08-01 10:32:26.0,87.0,
1006,226,1,"That's right - if the variance of PC is, say 3.5, then that PC ""explains"" variability of 3.5 variables from the initial set. Since PCs are additive, `PC1 > PC2 > ... > PCn`, and the sum of their variances is equal to the sum of the variances of the initial variable set, since PCA is computed upon covariance matrix, i.e. variables are standardised (SD = 1, VAR = 1).",2010-08-01 11:51:45.0,1356.0,
1007,282,0,"You could standardise the vars, hence calculate the sum, in order to see who's the best, or if you prefer, in R: `apply(dtf, 1, function(x) sum(scale(x)))`",2010-08-01 12:08:34.0,1356.0,
1008,1072,0,I think that it is recognized to be scientifically more correct in many field in the sense that the shrinkage approach is used more in recent applied stat papers than the *.IC approach. That shows a certain -at least tacit- theoretical consensus.,2010-08-01 12:12:41.0,603.0,
1011,1086,0,"moving average is exactly what I needed, thanks!",2010-08-01 14:40:31.0,642.0,
1012,981,0,Great method for dealing with uneaven data. I will try that. A huge thanks for all your help!,2010-08-01 14:50:13.0,,Pete
1014,632,0,"What Srikant found (and what seems confirmed at PhysicsForums) there should be $\\sqrt{2}$, so rather $\\hat{\\sigma}\\frac{\\sqrt{2}}{2n}$.",2010-08-01 15:34:01.0,88.0,
1015,632,0,"Aww, those comments locks; $\\frac{\\hat{\\sigma}}{\\sqrt{2n}}$. At least this one gives the result in agreement with bootstrap.",2010-08-01 15:58:24.0,88.0,
1016,1069,0,"[Ack, deleted a previous comment when I thought it would edit, back to try again.]  This is interesting though I don't understand information geometry +1.  I'm intrigued by this question at quora, which is unfortunately unanswered: http://www.quora.com/What-does-it-mean-to-say-In-information-geometry-the-E-step-and-the-M-step-are-interpreted-as-projections-under-dual-affine-connections",2010-08-01 17:59:05.0,251.0,
1017,1069,0,"I had wondered where that went!  If I get some time, I think I'll try to answer that question over at quora, though I should admit that I an information geometry novice.  Most of what I know of it is incidental to an an effort to pin down how the features of mathematical spaces (topological, algebraic, and otherwise) enable the application of learning methods.  This, in turn, is part of a quite difficult pet project of mine.",2010-08-01 18:51:44.0,39.0,
1018,1069,0,If you would like to correspond about any of these subjects feel free to email me at johnnylogic at gmail.,2010-08-01 19:09:46.0,39.0,
1019,113,0,"The aforementioned paper addresses method selection, generally. As for specific examples and more details they may be found scattered among specific meta-methodological disciplines (measurement theory, algorithmic learning theory, statistical learning theory, complexity theory), but I have not found a systematic treatment, thus the question. If you wish to discuss these issues generally, you may email me at johnnylogic at gmail.",2010-08-01 19:11:47.0,39.0,
1020,1040,0,Thanks for the pointer on Latex,2010-08-01 20:12:49.0,168.0,
1021,1095,0,Reg Latex: See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats,2010-08-01 22:58:47.0,,user28
1022,1094,0,"I am already modeling the probability of each cell as a Dirichlet, $p \\sim Dirichlet(\\alpha)$.  Normalizing by $n$ wouldn't quite work since a cell with large $p_i$ would have small likelihood whenever $n$ is fairly large, even when we indeed observed $x_i=1$ (unnormalized).  Does that make any sense?",2010-08-01 23:08:31.0,647.0,
1023,1094,0,I am afraid you lost me by the mention of a 'cell' when the question does not mention this word at all. Could you provide some more context so that I can understand better?,2010-08-01 23:13:24.0,,user28
1024,1094,0,Sorry: By cell I mean each $x_i$.,2010-08-01 23:14:44.0,647.0,
1025,1093,0,"By the way, will the binomial not work for you? You have k trials and n successes. right?",2010-08-01 23:16:12.0,,user28
1026,1094,0,"In other words, I want to sample $n$ integers between 1 and $k$ without replacement using the probability vector $p$.",2010-08-01 23:17:14.0,647.0,
1027,1094,0,This getting confusing to me. Your $X_i$ is either 0 or 1 but now you want to sample $n$ integers between 1 and $k$?,2010-08-01 23:23:32.0,,user28
1028,1094,0,Sorry: I'm saying that it's an equivalent formulation to sample the indices of $X$ without replacement.  Then $x_i=1$ for those integers that are sampled and $x_i=0$ for those integers that are not.,2010-08-01 23:39:34.0,647.0,
1029,1094,0,"In that case, you have basically the following scenario: You flip a coin $k$ times and your constraint is that you need $n$ successes as that will ensure that $X_i$ sum to $n$. This is a binomial distribution. If this satisfies your requirements I will change my answer. If it does not can you explain why this will not work?",2010-08-02 00:04:19.0,,user28
1030,219,0,"Would this be good for implementing on GPUs?  Link to details, references?",2010-08-02 00:27:47.0,646.0,
1031,146,0,"Why did you put `r` tag and what do you mean by ""why this is so""? PC's are not correlated, i.e. they're orthogonal, additive, you cannot predict one PC with the another. Are you looking for a formula?",2010-08-02 00:38:33.0,1356.0,
1032,167,0,"In the set with n variables, n PCs can be extracted, but you can decide how many you'd like to keep, e.g. Guttman-Keiser criterion says: keep all PCs that have eigenvalue (variance) larger than 1. So there...",2010-08-02 00:43:29.0,1356.0,
1033,1094,0,"Slightly different: I have $k$ different coins, each with a different probability (ie. $p_i$), and I have $n$ successes.  Sorry for the confusion!",2010-08-02 01:14:41.0,647.0,
1035,1093,0,"Slightly different: I have $k$ different coins, each with a different probability (ie. $p_i$), and I have $n$ successes. Sorry for the confusion!",2010-08-02 01:43:26.0,647.0,
1036,1094,0,"For the second proposal, what is the form of this posterior?  Another Dirichlet?  It makes sense that the Dirichlet would be conjugate here, just like if we were using a multinomial.  But we're not using a multinomial.  So what is the pdf of the distribution you propose?  (It's a multivariate bernoulli that has been conditioned on observing n successes, right?)",2010-08-02 02:07:56.0,647.0,
1037,1094,0,"Unfortunately, it is not a dirichlet. The bernoulli can be written as: $f(X_i;p_i) = p_i^{X_i} (1-p_i)^{1-X_i}$ which is not conjugate to the dirichlet. You will have to use Metropolis-Hastings to estimate the parameters.",2010-08-02 02:14:54.0,,user28
1038,1094,0,Thanks for your help.,2010-08-02 02:19:15.0,647.0,
1039,910,2,"[citation needed]. In other words, intriguing answer, although it doesn't jive (at least) with a lot of ML literature.",2010-08-02 05:03:16.0,30.0,
1040,1081,0,"Indeed, its non-metric multidimensional scaling.",2010-08-02 06:09:19.0,144.0,
1041,146,0,I was wondering about the principles behind the logic (in my quest to understand PCA). I used R tag because R people might read this and maybe show R examples. :),2010-08-02 06:13:21.0,144.0,
1042,910,1,"Classical one is Breiman's ""Statistical Modeling: The Two Cultures"".",2010-08-02 06:42:56.0,88.0,
1043,1025,0,Interesting posting. I will look into that too. You are probably right.,2010-08-02 07:53:58.0,,Pete
1045,146,0,"Oh, why didn't you say so? Have you seen http://www.statmethods.net/advstats/factor.html",2010-08-02 11:01:14.0,1356.0,
1046,1113,0,"Thanks for your answer. That solves the problem of bounding.

For my data it goes to 1 very quickly for my data so I guess the next thing I need to do is to scale this information to concentrate on the interesting range which I could do based on the history of it without fear of leaving the bound, just hitting the limit.",2010-08-02 15:19:49.0,652.0,
1047,1114,0,"erf is not a very handy function, provided you don't want to rather use it for its derivative.",2010-08-02 15:26:48.0,88.0,
1048,1026,0,"Doing a quick search, I found Harrell writing the following ""Beware of doing model selection on the basis of P-values, R-square, partial R-square, AIC, BIC, regression coefficients, or Mallows' Cp.""  He wrote that on 12/14/08, on a mailing list titled [R] Obtaining p-values for coefficients from LRM function (package Design) - plaintext.  I guess I misunderstood his meaning.",2010-08-02 16:20:19.0,253.0,
1049,1115,3,Could you please make your question a little more understandable?,2010-08-02 16:38:41.0,88.0,
1050,1118,0,"Thank you- mbq and stoplan - I computed mean in time1 and time2, and percent change. There are several obs for each idno, but not an equal  no. of obs for each idno in each time period.  There are 2 or 3 vars that cause change in dep. var, but the means are diff from time1 to time2. I am not clear on repeated measures - in SAS I used proc mixed w/repeated option but I am not clear on results!",2010-08-02 17:32:45.0,474.0,
1051,1118,0,"There is a specific change in 1 indep. var, X1 in time2. What I want to do is show statistically what is causing change in dep var from time1 to time2.  How much is due to change in X1? Am I still unclear?",2010-08-02 17:37:02.0,474.0,
1052,1118,0,"@Tailltu I think, ideally, editing the question itself to lend clarity is preferable. That way your question would show up in searches by other people who may have an issue similar to yours.",2010-08-02 17:58:52.0,,user28
1053,1126,2,Community wiki? I do not think there is a 'correct' answer for your questions.,2010-08-02 18:28:30.0,,user28
1054,763,0,"(didn't get a notifier from SA of your comment) Well, i wasn't referring to any papers when i wrote that, rather just informally summing pieces of my experience relevant to your Question. I'll look through my files and see what i have that is relevant though.",2010-08-02 18:32:24.0,438.0,
1055,1126,2,Community wiki...,2010-08-02 19:18:14.0,5.0,
1056,1128,0,+1 Very good point; my number 1.,2010-08-02 19:29:21.0,88.0,
1057,1126,1,Fascinating question - thank you for it!,2010-08-02 20:27:48.0,253.0,
1058,1115,0,"Or for example mention what stat package you are using ? (R, SAS, SPSS, etx ?)",2010-08-02 20:30:33.0,253.0,
1059,1133,0,"I once asked the same question on the R mailing list, and didn't get a response.  I'd suggest you to change your title since your question is regarding ""post hoc analysis of chi square - to detect the cause of the significance"" (a shorter titles then the one I proposed would be better :) )",2010-08-02 20:33:00.0,253.0,
1060,1144,0,"Thank you for your response, but what if the signal exhibits a high seasonality (i.e. a lot of network measurements are characterized by a daily and weekly pattern at the same time, for example night vs day or weekend vs working days)? An approach based on standard deviation will not work in that case.",2010-08-02 20:57:49.0,667.0,
1061,1144,0,"For example, if I get a new sample every 10 minutes, and I'm doing an outlier detection of the network bandwidth usage of a company, basically at 6pm this measure will fall down (this is an expected an totaly normal pattern), and a standard deviation computed over a sliding window will fail (because it will trigger an alert for sure). At the same time, if the measure falls down at 4pm (deviating from the usual baseline), this is a real outlier.",2010-08-02 20:58:18.0,667.0,
1062,1132,2,"I reckon loads of computer games are modelling problems dressed in disguise. SimCity for example - the goal of the game is to build as good a model as possible of the hidden game mechanics, then use that model to build a functioning city! (This is all probably a gross over-justification for wasting my youth playing SimCity)",2010-08-02 21:03:44.0,668.0,
1065,1147,1,"Yeah, this is exactly what I am doing: until now I manually split the signal into periods, so that for each of them I can define a confidence interval within which the signal is supposed to be stationary, and therefore I can use standard methods such as standard deviation, ...
The real problem is that I can not decide the expected pattern for all the signals I have to analyze, and that's why I'm looking for something more intelligent.",2010-08-02 21:37:03.0,667.0,
1066,1142,1,"Just for clarity, here's the original question on SO: http://stackoverflow.com/questions/3390458/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series",2010-08-02 21:42:06.0,71.0,
1067,1142,1,I think we should encourage posters to post links as part of the question if they have posted the same question at another SE site.,2010-08-02 21:47:31.0,,user28
1068,886,2,"As, arguably, a machine learner, I'm here to tell you we maximise the heck out of likelihoods. All the time. Loads of machine learning papers start with ""hey look at my likelihood, look how it factorises, watch me maximise"". I'd suggest that it's dangerous to claim a fundamental basis of either discipline in terms of inference techniques. It's more about which conference you go to!",2010-08-02 21:47:52.0,668.0,
1069,1142,0,"yes, you're completely right. Next time I'll mention that the message is crossposted.",2010-08-02 21:53:14.0,667.0,
1070,1148,0,"Again, this works pretty well if the signal is supposed to have a seasonality like that, but if I use a completely different time series (i.e. the average TCP round trip time over time), this method will not work (since it would be better to handle that one with a simple global mean and standard deviation using a sliding window containing historical data).",2010-08-02 22:02:16.0,667.0,
1071,1148,1,Unless you are willing to implement a general time series model (which brings in its cons in terms of latency etc) I am pessimistic that you will find a general implementation which at the same time is simple enough to work for all sorts of time series.,2010-08-02 22:06:07.0,,user28
1072,1148,0,"Another comment: I know a good answer might be ""so you might estimate the periodicity of the signal, and decide the algorithm to use according to it"", but I didn't find a real good solution to this other problem (I played a bit with spectral analysis using DFT and time analysis using the autocorrelation function, but my time series contain a lot of noise and such methods give some crazy results mosts of the time)",2010-08-02 22:06:34.0,667.0,
1073,1148,0,"A comment to your last comment: that's why I'm looking for a more generic approach, but I need a kind of ""black box"" because I can't make any assumption about the analyzed signal, and therefore I can't create the ""best parameter set for the learning algorithm"".",2010-08-02 22:09:22.0,667.0,
1074,1147,0,"Here is a one idea: Step 1: Implement and estimate a generic time series model on a one time basis based on historical data. This can be done offline. Step 2: Use the resulting model to detect outliers. Step 3: At some frequency (perhaps every month?), re-calibrate the time series model (this can be done offline) so that your step 2 detection of outliers does not go too much out of step with current traffic patterns. Would that work for your context?",2010-08-02 22:24:42.0,,user28
1075,108,0,I love that blog!,2010-08-02 22:31:03.0,582.0,
1076,1147,0,"Yes, this might work. I was thinking about a similar approach (recomputing the baseline every week, which can be CPU intensive if you have hundreds of univariate time series to analyze).
BTW the real difficult question is ""what is the best blackbox-style algorithm for modeling a completely generic signal, considering noise, trend estimation and seasonality?"".
AFAIK, every approach in literature requires a really hard ""parameter tuning"" phase, and the only one automatic method I found is an ARIMA model by Hyndman (http://robjhyndman.com/software/forecast/). Am I missing something?",2010-08-02 22:38:45.0,667.0,
1077,1147,0,"Please keep in mind I'm not too lazy for investigating these parameters, the point is that these values need to be set according to the expected pattern of the signal, and in my scenario I can't make any assumption.",2010-08-02 22:40:16.0,667.0,
1078,1147,0,ARIMA models are classic time series models that can be used to fit time series data. I would encourage you to explore the application of ARIMA models. You could wait for Rob to be online and perhaps he will chime in with some ideas.,2010-08-02 22:44:41.0,,user28
1079,1026,0,"@Tal. Thanks for finding that. While it sounds like a general statement, I *think* he probably meant ""Beware of looking at p-values after doing model selection on the basis of ...."" I don't think it makes sense otherwise. Certainly the discussion in that thread was all about p-values",2010-08-02 23:12:30.0,159.0,
1080,1128,1,"+1 Ha. Speaking of overlap, this may be the only answer with none over Epstein's reasons.",2010-08-02 23:49:07.0,251.0,
1081,1069,0,"Cool, thanks. :)",2010-08-02 23:49:50.0,251.0,
1082,1098,0,"Interesting, thanks.  I wonder though if it's not the Fisher that's more appropriate here -- since in the urn formulation of the problem, N is observed after the experiment, and you condition on the sum.  Quite likely, I'm one of the ""confused"" the wikipedia article refers to. :-)",2010-08-02 23:52:10.0,251.0,
1083,795,0,"Do you mean normalize to (0,1)? Please explain clearly what you are asking.",2010-08-03 01:08:43.0,159.0,
1084,1126,0,I think I like the overall classification given by the answers here more than Epstein's.,2010-08-03 01:44:08.0,251.0,
1086,1149,1,Really great question.  The best way to understand something is from multiple direction of explanation.,2010-08-03 02:29:31.0,253.0,
1087,1026,2,"@Tal, @Rob: In that thread, he does say ""Be sure to use the hierarchy principle"".  Perhaps of interest, this discussion from medstats (scroll down for Harrell's response): http://groups.google.com/group/medstats/browse_thread/thread/86c44163b849572",2010-08-03 02:38:20.0,251.0,
1088,1153,0,"+1 from me, excellent. So > 1.5 X inter-quartile range is the consensus definition of an outlier for time-dependent series? That would be nice to have a scale-independent reference.",2010-08-03 03:06:39.0,438.0,
1089,1153,0,"The outlier test is on the residuals, so hopefully the time-dependence is small. I don't know about a consensus, but boxplots are often used for outlier detection and seem to work reasonably well. There are better methods if someone wanted to make the function a little fancier.",2010-08-03 03:45:06.0,159.0,
1090,538,0,"I have some trouble understanding why arrow directions in a corresponding Bayesian network have any relation to causation. For instance, A->B and B->A represent different directions for causality, but Bayesian networks for those two structures are equivalent",2010-08-03 06:41:21.0,511.0,
1091,1160,0,Probably better suited to stackoverflow since it has no particular data analysis relevance.,2010-08-03 08:28:02.0,5.0,
1092,795,0,This question is proposed to be closed. See this meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed,2010-08-03 08:58:14.0,,user28
1093,1160,0,This question is proposed to be closed. See this meta thread: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed,2010-08-03 08:58:48.0,,user28
1102,1171,0,"Ah, yes, I knew that overlapping CI's didn't necessitate zero difference, but hadn't connected that to this example, where clearly the above procedure would always compute difference ""CI"" that includes zero whenever the individual CIs overlap. Thanks!",2010-08-03 12:39:56.0,364.0,
1103,1061,1,@gd047. LaTeX code: `$\\sigma_{\\epsilon}$`,2010-08-03 13:23:24.0,159.0,
1104,353,0,"I think that you're right, Rich.  I can't think of any other reasons.  Thanks!",2010-08-03 13:36:15.0,187.0,
1105,1170,2,"""sometimes"" is an understatement... the authors logic isn't often this direct but the stimulus / reward scenario is such that people will do this as a matter of conditioning",2010-08-03 13:52:52.0,601.0,
1106,1175,0,The data isn't necessary rates. It could be anything.,2010-08-03 14:07:10.0,8.0,
1107,1171,2,A useful fact given in these references is that non-overlapping 84% confidence intervals *do* approximate a 95% level test.,2010-08-03 14:08:29.0,279.0,
1108,1176,0,Is it available as a video? It sounds great.,2010-08-03 14:12:15.0,442.0,
1109,1176,0,"I think the word is ""will be eventually"" -- keynotes got recorded.",2010-08-03 14:18:07.0,334.0,
1110,1173,3,Helping your field come to a consensus on just the se v. sd question would be a huge advance.  They mean completely different things.,2010-08-03 14:41:26.0,601.0,
1111,1170,0,"I don't researchers are being dishonest so much as acting out of ignorance.  They don't understand what statistics mean or what assumptions they require, but as you said they clearly understand the stimulus/reward: p > 0.05 => no publication.",2010-08-03 14:44:25.0,319.0,
1112,1175,0,"Subscription link, unfortunately.",2010-08-03 14:57:26.0,71.0,
1113,1176,0,"this is easy in ggplot I think, i.e. http://had.co.nz/ggplot2/geom_jitter.html",2010-08-03 14:58:29.0,668.0,
1115,1175,0,... but here's the Wikipedia link on funnel plots: http://en.wikipedia.org/wiki/Funnel_plot,2010-08-03 14:59:02.0,71.0,
1116,1180,2,"And so if the mean is the same as the variance, could you conclude that the data was Poisson? Hardly!",2010-08-03 14:59:46.0,247.0,
1118,1182,0,"I should add that my ""plot only the data and uncertainty"" recommendation should be qualified: when presenting data to an audience that has experience/expertise with the variable being plotted, plot only the data and uncertainty. When presenting data to a naieve audience and when zero is a meaningful data point, I'd first show the data extending to zero so that the audience can get oriented to the scale, then zoom in to show just the data and uncertainty.",2010-08-03 15:10:49.0,364.0,
1119,1176,0,`jitter` is also in plain R.,2010-08-03 15:14:10.0,88.0,
1120,1176,0,"Well, I didn't mention a need to jitter. Frank's example had relatively few points.  I simply mentions alpha blending because it may make mixing the dots and the bars easier.",2010-08-03 15:18:15.0,334.0,
1121,1173,0,I agree - se is usually chosen because it gives a smaller region!,2010-08-03 15:20:39.0,8.0,
1122,1182,0,"since you've went to trouble of writing R code, could you include a jpeg image of the final plot. I find just uploading the image to http://img84.imageshack.us/ and linking to it is fairly easy. Oh thanks for the answer :)",2010-08-03 15:26:43.0,8.0,
1123,1179,0,I've added a small section on why I dislike these plots.,2010-08-03 15:34:58.0,8.0,
1124,1121,0,This can all be implemented in Max/MSP/Jitter with the [peak] and [trough] objects for the first example and with [jit.3m] for the second example.,2010-08-03 15:40:01.0,162.0,
1126,1182,0,@csgillespie: done.,2010-08-03 15:46:09.0,364.0,
1127,1153,0,"Really thank you for your help, I really appreciate. I'm quite busy at work now, but I'm going to test an approach like yours as soon as possible, and I will come back with my final considerations about this issue. One only thought: in your function, from what I see, I have to manually specify the frequency of the time series (when constructing it), and the seasonality component is considered only when the frequency is greater than 1. Is there a robust way to deal with this automatically?",2010-08-03 15:59:18.0,667.0,
1128,1180,0,True. Necessary but not sufficient.,2010-08-03 16:44:18.0,319.0,
1129,1173,0,Maybe some more informative title?,2010-08-03 18:36:41.0,88.0,
1130,1182,0,"I've found that it's easier to read a plot like this with `geom_ribbon()` indicating the error. If you don't like producing apparent estimates for regions between 1 and 2, at least reduce the width of the error bar.",2010-08-03 19:20:49.0,287.0,
1131,752,1,"I don't get what is so deep in that one, is it only playing with words ?",2010-08-03 19:29:45.0,223.0,
1132,1187,0,"Thank you, I will check these out.",2010-08-03 19:40:35.0,573.0,
1133,595,0,"Agreed; wavelets are excellent for picking out non-stationary behavior in high amounts of noise.  You do have to be careful with the DWT, though.  It's not rotation-invariant (although there are modifications of the DWT that are, see e.g. Percival and Walden 2000), so you can lose sharp transients depending on the starting point for your data.  Also, most implementations of the DWT do implicit circularization of the data, so you'd still need to control for that.",2010-08-03 19:57:01.0,61.0,
1134,1193,0,Edited to disambiguate 'above.',2010-08-03 20:10:46.0,455.0,
1135,1194,2,This question is proposed to be closed. See: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed I see that it has 2 votes. Could the up-voters or the OP comment on why they would like to see the question stay open at the meta thread?,2010-08-03 20:33:45.0,,user28
1136,1194,3,"Rather than saying ""this should be closed. Someone should defend it"" how about starting with explaining why you want it closed. Too vague? Then ask for clarification. This seems a reasonable question to me. The asker presents a paper and asks about the difference is between predictive and explanatory statistics. The only change I would make to the question is to clarify exactly the question thus making it easier to vote.",2010-08-03 20:39:24.0,29.0,
1137,1194,2,I have already offered a reason on the meta thread. I feel that 'meta discussions' about the question would clutter up this particular page.,2010-08-03 20:41:09.0,,user28
1138,1160,1,I wish we could move a question to Stack Overflow the way you can move a question from Super User...,2010-08-03 20:42:08.0,29.0,
1139,1194,1,@Srikant @JD I'll beef up the question. Thanks for the feedback. I do think that this is a topic that merits discussion.,2010-08-03 20:44:41.0,11.0,
1140,1194,1,"Your question would good for the community if, instead of telling us your life,  you could define what is (according to you) predictive model and explanatory model. I think nice debates start with clear definitions...",2010-08-03 20:49:23.0,223.0,
1141,1194,1,Could you pose a question here?  Is the question whether the paper is right?,2010-08-03 20:52:48.0,5.0,
1142,595,0,"If my memory is good, package wavethresh contains translation invariant denoising (my reference was Coifman 1995) (Note that you talked about rotation, arn't we talking about temporal signals?).",2010-08-03 20:53:23.0,223.0,
1143,1194,0,"@srikant you seem to not understand the nature of comments under the questions. They are, by definition, meta. They are not answers. They are not questions. They are meta. Having a convention where comments become a pointer to meta conversations in some other location is wasteful and silly.",2010-08-03 20:59:14.0,29.0,
1144,1194,0,I've made the question more specific,2010-08-03 21:18:07.0,11.0,
1145,1194,1,"@JD Perhaps. But, in order to get some control on the process of closing questions there is a meta thread devoted to this issue. If I do not make a mention of the fact that this question is proposed to be closed then the community will not get a chance to have a say whether it should stay open or not. One other issue is consider a person who stumbles on this question in the distant future. All this discussion about whether and why we should keep the question open is a bit irrelevant. I feel that comments should be used to clarify the question not to debate its merits.",2010-08-03 21:19:05.0,,user28
1146,1194,0,"@srikant. That's articulately stated and clear. This discussion probably belongs in the Meta area, however, as it is not specific to the question above. :) yeah, ok, that was a bit of a jackass comment... I could not resist it!   You make a good point. I think we can agree that @wahalulu needed to clarify his question. I think he's moving in the right direction.",2010-08-03 21:46:16.0,29.0,
1147,1194,4,"Nevertheless, it should be community wiki.",2010-08-03 22:17:23.0,88.0,
1148,1031,0,"Since I am not a mathematician nor a statistician, I would like to restate what you were saying to make sure I did not mis-understand. 

So, you are saying that taking ds^2 (twice the KL) would have a similar meaning as R^2 (in a regression model) for a general distribution. And that this could actually be used to quantify  distances geometrically? Does ds^2 have a name so I can do more reading about this. Is there a paper that directly describes this metric and shows applications and examples?",2010-08-03 22:22:05.0,608.0,
1149,608,1,"Wow, this is something that will be even harder do obtain I'm afraid; my general point in the whole discussion is that the convergence of those theorems is too weak so the difference may emerge from random fluctuations. (And that it is not working for machine learning, but I hope this is obvious.)",2010-08-03 23:19:47.0,88.0,
1150,1153,0,"Yes, I have assumed the frequency is known and specified. There are methods to estimate the frequency automatically, but that would complicate the function considerably. If you need to estimate the frequency, try asking a separate question about it -- and I'll probably provide an answer! But it needs more space than I have available in a comment.",2010-08-03 23:40:47.0,159.0,
1151,608,0,"That is interesting.  If the convergence is so weak I wonder why people even brought it up in the other question about AIC/BIC, or how they figured out that the cross validation and IC methods were actually convergent; I'm not really involved in machine learning, so the last bit that you hope is obvious isn't obvious to me, could you expand on that point?  Besides, the entire thing may be moot, as I say below - a correlational approach to the problem doesn't look like it will work (at least not while sample sizes are constant).",2010-08-04 00:05:11.0,196.0,
1152,1153,0,"Thank you, I'll post a different question.",2010-08-04 00:21:03.0,667.0,
1153,109,0,"I've been watching this question for a while waiting for clarification from the question asker, but none has been forthcoming.  I've nominated this question to be closed: http://meta.stats.stackexchange.com/questions/213/list-of-candidate-questions-to-be-closed",2010-08-04 00:38:26.0,196.0,
1154,1185,1,"But this raises an interesting pedagogical problem, at least in Psychology, because as far as I know most introductory statistics books being used in my field do not really discuss robust measures except as an aside.",2010-08-04 00:43:19.0,196.0,
1155,1205,0,What is N? What is the nature of the weighting scheme? Are the weights known?,2010-08-04 01:25:01.0,,user28
1156,1182,0,"@JoFrwld: I like ribbons too, though I tend to reserve them for cases where the x-axis variable it truly numeric; my version of the ""don't draw lines unless the x-axis variable is numeric"" rule that I profess violating in my answer above :Op",2010-08-04 01:40:54.0,364.0,
1157,1206,1,Why/how would an important explanatory variable reduce predictive accuracy?,2010-08-04 02:34:10.0,,user28
1158,1210,0,Interesting.  Did you ever come across an R implementation of this ?,2010-08-04 03:16:52.0,253.0,
1159,1206,1,"@Srikant. This can happen when the explanatory variable has a weak but significant relationship with the response variable. Then the coefficient can be statistically significant but hard to estimate. Consequently, the MSE of predictions can increase when the variable is included compared to when it is omitted. (The bias is reduced with its inclusion but the variance is increased.)",2010-08-04 05:15:22.0,159.0,
1160,1181,0,+1 thanks for the thorough example!,2010-08-04 06:49:26.0,634.0,
1161,1178,0,"+1 thank you. often I get some ""weired"" results, for example, a normal distribution gets a higher p-value then a poisson one, where lambda is relatively small (so by looks only the normal and poisson are not similiar at all)",2010-08-04 06:51:10.0,634.0,
1162,1212,0,+1 thanks for the suggestions!,2010-08-04 07:04:35.0,634.0,
1163,562,0,"i added what i think is a pretty good source (unfortunately a Textbook) w/ annotation, in light of your comment/question below my answer. I edited my original answer, so it appears at the end.",2010-08-04 07:07:48.0,438.0,
1164,1216,0,"ok, let's size I need to calculate for size 45. Every delivery is about once every 3 months.
I'm using excel here. =POISSON(4;98;FALSE). Ressult: 0... Could you give me some more clues?",2010-08-04 07:16:39.0,698.0,
1165,1216,0,"If the deliveries are every 3 months, and the data in the question are annual, then the average sales between deliveries is 95/4. So you want y such that 1-POISSON(y,95/4,TRUE) is smaller than 0.05. Choosing y=32 will give the out-of-stock probability of 4.1%.",2010-08-04 08:12:41.0,159.0,
1166,752,0,"I like to think of it as the statisticians equivalent of ""guns don't kill people, people kill people"" not very deep, but important to realise from time to time",2010-08-04 09:17:53.0,127.0,
1167,1222,0,"Good point, still the main differences are somewhere else;  first, statistics is about fitting a model to the data one has, ML is about fitting a model to data one will have; second, statistics ASSUME that a process one observes is fully driven by some embarassingly trivial ""hidden"" model that they want to excavate, while ML TRIES to make some complex enough to be problem-independent model behave like reality.",2010-08-04 09:28:38.0,88.0,
1168,934,0,"@John Still, this is mixing aims with reasons. To great extent you can explain each algorithm in terms of minimizing something and call this something ""loss"". kNN wasn't invented in such a way: Guys, I have thought of loss like this, let's optimize it and see what will happen!; rather Guys, let's say that decision is more less continuous over the feature space, then if we would have a good similarity measure... and so on.",2010-08-04 09:38:57.0,88.0,
1169,1176,0,@Dirk I was commenting Mike's comment about ggplot. As a general comment I also used to like these seas of dots for the sake that nothing will better represent the distribution -- until we've made figures that were crushing Acrobat Reader ;-) .,2010-08-04 09:54:30.0,88.0,
1170,1222,0,@mbq. That's a rather harsh caricature of statistics. I've worked in five university statistics departments and I don't think I've met anybody who would think of statistics like that.,2010-08-04 09:59:17.0,159.0,
1172,1223,1,I don't think this is a duplicate because of the nature of the data. The problem discussed on the other question concerned regularly observed time series with occasional outliers (at least that's how I interpreted it). The nature of tick-by-tick data would lead to different solutions due to the exchange opening effect.,2010-08-04 10:09:10.0,159.0,
1173,1223,0,possible duplicate of [Simple algorithm for online outlier detection of a generic time series](http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series) This question is proposed to be closed as a duplicate. Could you please let us know at the meta thread if and how your context is different from the question I linked?,2010-08-04 10:10:59.0,,user28
1174,1223,0,"@Rob But the exchange opening effect only determines when you have to run the algorithm. The fundamental issue remains the same. Even in network data you have the 'office opening effect' where traffic peaks as soon as an office opens. At the very least, the OP should link to that question, scan the answers there and explain why the solutions there do not work so that a suitable answer can be posted for this question.",2010-08-04 10:12:49.0,,user28
1176,1224,1,I think we need a bit more information. What are you comparing? What sort of data do you have? Is it a single outcome you a measuring per centre?,2010-08-04 10:41:18.0,8.0,
1177,1223,1,"I agree with @Rob. This kind of data can pose unique challanges, so this is not a duplicate.",2010-08-04 10:46:15.0,5.0,
1178,1222,0,"@Rob Caricature? I think this is what makes statistics beautiful! You assume all those gaussians and linearities and it just works -- and there is a reason for it which is called Taylor expansion. World is hell of a complex, but in linear approx. (which is often ninety-something% of complexity) embarrassingly trivial. ML (and nonparametric statistics) comes in in these few per cent of situations where some more subtle approach is needed. This is just no free lunch -- if you want theorems, you need assumptions; if you don't want assumptions, you need approximate methods.",2010-08-04 10:54:35.0,88.0,
1179,1223,0,This question might eventually get served better here due to its domain-specificity: http://area51.stackexchange.com/proposals/117/quantitative-finance,2010-08-04 11:09:19.0,5.0,
1180,1222,0,@mbq. Fair enough. I must have misinterpreted your comment.,2010-08-04 11:21:07.0,159.0,
1181,1205,0,see also this related question: http://stats.stackexchange.com/questions/856/fishers-exact-test-with-weights,2010-08-04 11:22:20.0,442.0,
1182,1126,0,"I don't want to pick just one right answer, so I've made this a community wiki page, based on two suggestions above. Thanks for all the ideas!",2010-08-04 11:41:05.0,660.0,
1183,718,1,"+1 for David Howell, because his Intro to Statistics for Psychologists is the best you can get as an undergrad!!",2010-08-04 12:01:14.0,442.0,
1185,1223,1,"I think it belongs here. The question is about analyzing irregularly spaced, very noisy time series.

Have you had a look at ""An Introduction to High-Frequency Finance"" by Dacorogna, Olsen and a bunch of others? Or the papers by the same authors?",2010-08-04 12:30:40.0,247.0,
1186,1228,1,I took the liberty of trying to fix the grammar. I hope I understood the question correctly.,2010-08-04 12:35:14.0,159.0,
1187,1183,2,I'm not sure boxplots or vioplots would be suitable with such a small sample size (n = 6),2010-08-04 12:39:43.0,8.0,
1188,1099,0,I think asking another question separately is much better than updating your previous question.,2010-08-04 12:42:04.0,,user28
1189,1223,0,I saw the other answer and followed Rob's reasoning. I amended my question to address differences I see.,2010-08-04 13:18:51.0,127.0,
1190,1170,8,"You must also present something that those ""in power"" (decision makers, supervisors, reviewers) understand. Therefore it has to be in the common language which evolves quite slowly, as those people tend to be older and more resistant to change, largely as it may invalidate their careers hitherto!",2010-08-04 13:29:34.0,229.0,
1191,1164,8,The Black Swann by Nassim Nicholas Taleb explains why simple models have been used in the financial world and the dangers this has led to. A particular fault is equating very-low probabilities with zero and blindly applying the normal distribution in risk management!,2010-08-04 13:38:31.0,229.0,
1192,1223,0,@jilles I do not see any edits to your question. Did you save your edits? It may help if you post a link to that question as well and indicate the changes to your question by something like 'Edit'.,2010-08-04 13:39:08.0,,user28
1193,1223,0,I have the Olsen book and it doesn't address the exchange open/close question.,2010-08-04 13:39:26.0,5.0,
1194,1223,0,"@Srikant: done, @PeterR: do you know of any specific paper of those authors that addresses this question?",2010-08-04 13:46:53.0,127.0,
1195,1223,0,I wish there is a way to undo my close vote! I think it is clear now that it is not a duplicate.,2010-08-04 13:49:44.0,,user28
1196,1234,0,"I've tried something like this, but this method is not very good at dealing with abrupt changes in the volatility. This leads to underfiltering in quiet periods and overfiltering during more busy times.",2010-08-04 13:53:00.0,127.0,
1197,1183,0,"Right, I admit I haven't read the question carefully enough, so it was rather a general idea; nevertheless I think that 6 points is minimal but enough for a boxplot. I have made some experiments and they were meaningful. On the other hand, obviously boxplot does not indicate the number of observations (which is an important bit of information here), so I would rather use a combination of it and points.",2010-08-04 13:55:26.0,88.0,
1199,1235,0,"By using only the returns you become very vulnerable to ladders (i.e. a sequence of prices that climbs or drops away from the norm, where each individual return is acceptable, but as a group they represent an outlier). Ideally you'd use both the return and the absolute level.",2010-08-04 14:01:02.0,127.0,
1200,1224,0,Agree; what is the null hypothesis?,2010-08-04 14:01:59.0,88.0,
1201,1206,0,"First paragraph is a very, very good point. Still sometimes is even worse; here PMID: 18052912 is a great example that sometimes a better model can be made on the noise part of the set than on a true one -- it is obvious that one can do a good model on random data, but this is a bit shocking.",2010-08-04 14:20:30.0,88.0,
1202,825,1,Vote to keep open; very relevant to statisticians because the ways in which our problems can or can not be broken down into parallel streams is of relevance to the question being asked.,2010-08-04 14:40:38.0,196.0,
1203,830,1,"But, of course, doSMP is exactly what the OP would be looking for if they were working in a Windows environment.",2010-08-04 14:44:16.0,196.0,
1204,1238,0,"Spam filter prevented my posting the TestU01 link:

http://www.iro.umontreal.ca/~simardr/testu01/tu01.html",2010-08-04 14:48:00.0,729.0,
1205,1238,0,Fixed the link to TestU01,2010-08-04 14:49:37.0,,user28
1206,1214,0,"Thank you. Again, I will try this approach as soon as possible and will write here the final results.",2010-08-04 14:49:55.0,667.0,
1208,1224,0,Comparing a biomarker using hazard ratio as a measure of effect (will be adjusing for other covariates also),2010-08-04 15:18:46.0,,user712
1209,1241,0,What is a ROC curve? Could you please provide a link?,2010-08-04 15:19:32.0,,user28
1210,1243,0,Your last point about regression would only be true if X is random. If X is fixed then Y would also be a normal. no?,2010-08-04 15:28:24.0,,user28
1211,1210,0,"No, not directly.  However, R will give you everything you need to do this--such as: the observed counts, the expected values, and the residuals for each cell.

x <- matrix(c(12, 5, 7, 7), ncol = 2)
chisq.test(x)$expected
chisq.test(x)$observed
chisq.test(x)$residuals",2010-08-04 15:55:25.0,485.0,
1212,1245,0,"+1 yes, nothing is perfect. Tickdata.com (whose paper is mentioned) also includes outliers and they also strip out too much good data (when compared with another source). Olsen's data is close to being terrible, and I generally just indicative. There's a reason that banks pay big operations teams to work on this.",2010-08-04 16:10:44.0,5.0,
1213,1241,0,http://en.wikipedia.org/wiki/Receiver_operating_characteristic,2010-08-04 16:54:06.0,88.0,
1214,1210,0,"I'll give you the tick, since this should be useful for my research life. However, this approach is applicable to an i x j matrix. However, my question involves an i x j x k matrix,",2010-08-04 17:02:58.0,287.0,
1215,1183,0,With 6 points - scatter plot is probably best (maybe with adding a red dot for the mean),2010-08-04 17:20:09.0,253.0,
1216,1031,0,"I think you are far from understanding the point, and I am not sure you should try to go further now. If you are motivated, you can read the paper from Bradley Efron I mentionned or that paper from Amari http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176345779.",2010-08-04 17:25:16.0,223.0,
1217,1183,2,"I generally use boxplots with superimposed points, I find it very ""visual"". Violin plots, instead, are a bit hard to understand in my opinion.",2010-08-04 17:30:20.0,582.0,
1219,1254,1,"Something similar to that but includes more information rather than just the name of the test. We have some of these charts in urban and transportation modelling. They'll show a large table where they specify tests per type of problem. They also list caveats, expected time/duration, input and outputs, etc",2010-08-04 17:35:31.0,59.0,
1220,1255,0,"To compute similarity, I need to count the number of times two items were rated together by the same person.  That count becomes my numerator.  The formula I'm using to calculate similarity between items A & B is 

    AB / A + B - AB

Where AB is the number of times items A and B were rated by the same person, and A and B are the number of times each item was rated overall.

In order to perform that count, I need to generate all of the itemA,itemB pairs in my mapper, and then sum the counts in my reducer.

The items are rated in a binary fashion-if a (userid,itemid) exists, then 1.",2010-08-04 17:43:38.0,738.0,
1221,1249,2,This question is for Mathoverflow: http://mathoverflow.net/. Seems to be a good question anyway. I remember I saw a generalization of the results in Baraud's paper done by Rozenholc an other collaborators but I can't find the paper anymore...,2010-08-04 17:44:19.0,223.0,
1222,1210,1,"Chi-square partitioning is extensible to multi-way contingency tables.  Here's the article that Agresti cites in his book, in fact...

H. O. Lancaster (1951) ""Complex Contingency Tables Treated by the Partition of χ2"" Journal of the Royal Statistical Society. Series B (Methodological), Vol. 13, No. 2",2010-08-04 18:13:20.0,485.0,
1223,1214,1,"Your idea is quite good, but in my case, it fails to detect the periodicity of a really simple (and not so noisy) time series like http://dl.dropbox.com/u/540394/chart.png. With my ""empirical"" approach (based on the autocorrelation), the simple algorithm I wrote returns an exact period of 1008 (having a sample every 10 minute, this means 1008/24/6 = 7, so a weekly periodicity). My main problems are:
1) It's too slow to converge (it requires a lot of historical data) and I need a reactive, online approach;
2) It's inefficient as hell from a memory usage point of view;
3) It's not robust at all;",2010-08-04 18:14:17.0,667.0,
1224,1257,0,PS: This was posted on mathoverflow.net sometime back. See: http://mathoverflow.net/questions/18302/likelihood-function-for-sequential-random-variables,2010-08-04 18:29:49.0,,user28
1226,1102,0,"Didn't see this for a while. Users don't get notifications about new answers to questions they've contributed answers to themselves. If you leave a followup as a comment instead, you might get an additional response sooner.",2010-08-04 19:18:37.0,287.0,
1227,1102,0,"Actually, `P(y | grassland)` would be `exp(-2.8) / (1 + exp(-2.8))` A point in the forest would be `exp(-2.8 - 3) / (1 + exp(-2.8  - 3))`

However, you could estimate those probabilities without doing a logistic regression. What would be more interesting would be to look at `exp(-3)`, or `1/exp(-3)`. This says that an animal is 20x more likely on grassland than forest, and if the p-value is <0.05, then that difference is significant.",2010-08-04 19:23:49.0,287.0,
1228,1249,0,Do you want a lower or a upper bound? The first sentence says upper but the last sentence mentions lower.,2010-08-04 19:24:01.0,,user28
1229,1256,0,I assume that 'without multicore' means without parallelism at all?,2010-08-04 19:37:59.0,88.0,
1230,1249,0,@Srikant: you should read the mentionned paper by Barraud which is great to understand the whole thing... heuristically lower bounds are larger (testing is more difficult) when distributions (from the null and alternative) are close. Hence the upper bound asked by mkolar is related to the chi square distance between two distributions (the least favorable in the alternative and the null) if this distance is small the minimum error will be large...,2010-08-04 19:40:09.0,223.0,
1231,1263,1,"And actually the sapply, could be coded into a multi core framework...",2010-08-04 19:49:36.0,253.0,
1232,1160,0,I moved it manually to stackoverflow. JD - I agree with you.,2010-08-04 20:14:47.0,253.0,
1233,1263,0,Agree; still OP wrote that this is not an option.,2010-08-04 20:33:25.0,88.0,
1235,1268,0,"Good question! I think you can improve the title, there could be something like ""missing data"" somewhere or ""times series of different length"".",2010-08-04 21:19:03.0,223.0,
1236,1269,0,+1 replication/resampling are definitely better than zero-padding.. still I'll wait and see if there are any other ideas out there :),2010-08-04 21:43:04.0,170.0,
1237,1268,1,"I wouldn't call it ""missing data"", perhaps ""SVD dimensionality reduction for time series of different length""?",2010-08-04 21:45:44.0,170.0,
1238,1261,0,"Do i, j, k have to be integers? In general you have an unidentifiable model (that is the solution is not unique) because if you let p2=p^i, then p^j=p2^j2, where j2=j/i.",2010-08-04 21:45:48.0,279.0,
1239,1263,0,"#Actually sapply is about 3 times slower than my baseline method
library(rbenchmark);
baseline <- function()
{;
	ncol <- 100;
	nrow <- 100;
	x <- matrix(rnorm(ncol*nrow),nrow,ncol);
	y <- matrix(rnorm(ncol*nrow),nrow,ncol);
	return(diag(cor(t(x),t(y))));
};
sapply.method <- function()
{;
	ncol <- 100;
	nrow <- 100;
	x <- matrix(rnorm(ncol*nrow),nrow,ncol);
	y <- matrix(rnorm(ncol*nrow),nrow,ncol);
	return(sapply(1:100,function(i) cor(x[i,],y[i,])));
};
benchmark(baseline=baseline(),sapply.method(),replications=10);",2010-08-04 21:49:38.0,196.0,
1240,1263,0,(your mileage may vary on by what factor it is slower),2010-08-04 22:10:15.0,196.0,
1241,1265,0,"Simulation supports your answer, and even with a transpose operation added to handle cases with more items than subjects, sapply beats diag(cor()), but only when there are notable asymmetries between the N of items and the N of subjects.",2010-08-04 22:21:18.0,196.0,
1242,1243,0,"Yes, this is true, but for general regression problems (as opposed to anova or designed problems), X really isn't fixed but are observations from the underlying process. However, for the Poisson case, the point still holds, since mixtures of Poissons aren't necessarily Poisson.",2010-08-04 22:23:54.0,732.0,
1243,1274,4,Correlated with what? Each obs to the one before it?,2010-08-04 22:26:35.0,287.0,
1244,1263,1,"For 100x100 matrix it is obvious; I believe that the problem requires a lot larger matrices and there my method will be faster. Still, I have written ""your code is equivalent to:"" instead of ""use:"" having that in mind -- this is how possible C/Fortran chunk should work.",2010-08-04 22:29:35.0,88.0,
1245,1256,0,That is correct.,2010-08-04 22:32:32.0,196.0,
1246,1263,0,"Also, from no-multicore requirement one can conclude that this is indeed memory-bounded, so reducing memory complexity from $N^2$ to $N$ is also something worth playing.",2010-08-04 22:35:12.0,88.0,
1247,1194,0,I've got this 3 votes as an agreement for making it CV.,2010-08-04 22:37:49.0,88.0,
1248,1261,0,I think to demonstrate non-identifiability you have to show that the likelihood is invariant to a transformation of parameters. I am not sure if what you did is sufficient to demonstrate lack of  identification.,2010-08-04 22:50:19.0,,user28
1250,1170,9,"Good point. ""I understand p-values.  Just give me a p-value.""  Ironically, they probably do *not* understand p-values, but that's another matter.",2010-08-05 00:22:37.0,319.0,
1251,1277,0,Will the usual dw test be appropriate?,2010-08-05 00:33:10.0,273.0,
1252,752,0,"So it's an inane platitude used for quibbling over semantics? ""Cigarettes don't cause cancer; people cause cancer."" ""Landmines don't maim people; people maim people.""",2010-08-05 00:36:02.0,753.0,
1253,823,4,"That's because consensus is a practical means of establishing the validity of a scientific position. Sound science cannot be conducted by a single person. It requires peer review. Having achieved a popular consensus implies that the science in question has passed peer review. Whereas a theory or position held by a lone individual and opposed by the rest of the scientific community is likely to have failed the process of peer review.

If you're not an expert in field X, then statistically you're better off following popular consensus within field X.",2010-08-05 00:50:10.0,753.0,
1254,1277,1,"The dw test is designed for checking autocorrelation in the residuals from a regression. There is no regression here, so it is not appropriate.",2010-08-05 00:57:48.0,159.0,
1255,823,2,"To elaborate: if you're a politician who has no background in field X, then it is far better that you simply consult the consensus of experts in that field of research than to misinterpret the data first-hand.

What is problematic is when lay persons disregard overwhelming scientific consensus for data misrepresented to them by a fringe minority--especially when these lay persons are in charge of policy decisions. It's much easier to mislead a handful of politicians than thousands of expert researchers...",2010-08-05 01:02:53.0,753.0,
1256,1273,0,"this seems interesting, but sadly I can't find even a portion of the book from google books, will have a look on it, thanks!",2010-08-05 01:14:49.0,588.0,
1257,1228,0,thanks! i submitted this question in a rush and so maybe i missed the grammar....,2010-08-05 01:15:13.0,588.0,
1258,1099,0,"sure, thanks for the comments!",2010-08-05 01:16:54.0,588.0,
1259,1099,1,You should consider up voting the accepted answer. After all you found that answer useful.,2010-08-05 01:33:43.0,,user28
1260,1271,1,Thank you for the lead Brett!  I wonder if someone got to implement it by now in R (I'd guess not).,2010-08-05 01:43:35.0,253.0,
1261,1261,0,"@Srikant Vadali I just did. Replace p^i with p, j with j/i and k with k/i and you get the exact same probabilities as an output with a different set of parameters (unless i=1 - with that constraint the model is identifiable).",2010-08-05 03:08:28.0,279.0,
1262,1278,0,Can you have a control group (ward)?,2010-08-05 03:10:53.0,279.0,
1263,1271,1,"Right. R has lots of routines for assisting with bootstrap and other randomization methods, but I don't know that you'll find anything specific to this problem.",2010-08-05 03:15:46.0,485.0,
1264,1274,0,I'd hope not - the correlation at lag one is very close to 0.,2010-08-05 03:37:28.0,196.0,
1265,1214,1,Try v2.........,2010-08-05 04:04:43.0,159.0,
1266,1278,0,"no, the ward itself will be its control",2010-08-05 04:34:52.0,588.0,
1267,1277,0,I agree. Books mislead again. http://sas-and-r.blogspot.com/2010/07/example-81-digits-of-pi.html,2010-08-05 04:47:04.0,273.0,
1268,1274,0,"Or in Prof Wecker's words: ""The result was a = .01, which implies little or no serial dependence in the data.""",2010-08-05 04:55:59.0,273.0,
1269,1268,1,I like the title you propose !,2010-08-05 05:27:53.0,223.0,
1270,1286,0,Has anyone noticed that the number of questions that should be in StackOverflow R rocketed up here recently?,2010-08-05 05:32:45.0,601.0,
1271,1256,0,"the question in addition to the answers make me think this should be stackexchange... there should be a discussion on meta about questions with ""how to speed up"" ....",2010-08-05 05:33:21.0,223.0,
1273,1286,1,Why not making random sampling?,2010-08-05 07:21:32.0,88.0,
1274,752,3,well... there is a fine line between inane platitude and profound wisdom. I like the quote for it's poetic quality. Any insight is of secondary importance to me.,2010-08-05 07:25:08.0,127.0,
1275,1245,0,I like your idea about using known arbitrage relations. have you tried this at all in your previous job?,2010-08-05 07:27:53.0,127.0,
1276,1256,0,The discussion Robin called for is now on meta: http://meta.stats.stackexchange.com/questions/248/how-to-speed-up-questions,2010-08-05 07:51:58.0,196.0,
1277,1263,0,"The no-multicore was more a practical restriction, my colleague does not have a multicore machine, so parallelism wasn't going to help him.  I'm sure there is an entire other question to be asked regarding how to know a priori whether a parallel approach (with associated overhead) will be faster than a single threaded approach.",2010-08-05 07:58:09.0,196.0,
1278,1286,3,"@John: If you feel that way discuss the issue at http://meta.stats.stackexchange.com/questions/248/how-to-speed-up-questions, no need to be snarky.",2010-08-05 07:59:42.0,196.0,
1279,1286,0,"@mbq:  Random sampling will quickly provide a reasonable approximate, especially with well behaved data.  However, I did specify that my goal was an exact test.",2010-08-05 08:04:02.0,196.0,
1280,1286,0,@drknexus That's why it was a comment not an answer.,2010-08-05 09:19:40.0,88.0,
1281,1289,2,I think you mean bar charts rather than histograms,2010-08-05 10:09:06.0,159.0,
1283,1261,0,"@Aniko Perhaps, I am missing something here. But, when I substituted the two sets of parameters into the likelihood function that I suggested they are not identical (assuming that I did not make any errors which is always possible).",2010-08-05 10:26:27.0,,user28
1284,1289,0,@Rob: Isn't histogram a special type of bar chart that represents a frequency distribution? I am trying to visualize category frequencies for many bookstores.,2010-08-05 10:28:07.0,760.0,
1285,1290,0,But will this cope with very large combinations?,2010-08-05 10:40:03.0,8.0,
1286,1289,0,"@nimcap No, because histogram is over a continuous variable, and book category is a categorical variable.",2010-08-05 10:46:38.0,88.0,
1287,1291,0,"Thank you for your valuable answer. Situation is hard to describe even in my native language :) Let me  try. I am not interested if bookstores are favoring particular categories but I want to see if they favor categories. Actually this is what I am expecting. Let's say I have 3 bookstores (B1, B2, B3) and 4 categories (C1, C2, C3, C4). These are their sales data: B1(1, 1, 20, 20) B2(90, 1, 1, 1), B3(1, 1, 1, 30). Looking at this data I can tell they favor some categories to others. But if data was like B1(20, 30, 20, 20) B2(90, 100, 100, 100), B3(30, 30, 40, 40) I cant say that.",2010-08-05 10:53:24.0,760.0,
1288,1289,0,"@mbq Let's say a book store has 3 books, and their categories are: B1:[c1, c2, c3] B2:[c1, c3] B3:[c1, c4]. When we aggregate the category counts we get [c1 x 3, c2 x 1, c3 x 2, c4 x 1]. Isn't this enough to generate a histogram?",2010-08-05 10:56:34.0,760.0,
1289,1290,0,"@csgillespie Well, I believe so -- it works *in situ*,  so only one combination is stored in memory at a time, and the results of simulation can be also aggregated to eliminate the need of storing them. This will of course work terribly long, but exhaustive searches usually do. For speed it could be written in C, but then along with the simulation part, which probably is way slower than a generator step.",2010-08-05 11:11:21.0,88.0,
1290,1289,1,"@nimcap No, it is enough to generate a bar chart. Histogram can be done for instance for a price of a book.",2010-08-05 11:13:42.0,88.0,
1291,1289,0,"@mbq I really don't get it, wherever I look it says histogram is about frequencies, can you point to any sources mentioning histogram is for continuous variables.",2010-08-05 11:39:43.0,760.0,
1292,1287,0,"I assume you have the book, what's in chapter 17 (it's referenced in the document you provided)",2010-08-05 11:41:58.0,59.0,
1293,1245,0,"No, we never fully formalized that.  But I think we used some simple ones (ie ETF vs underlying index etc).  It's been a few years though.",2010-08-05 11:43:48.0,334.0,
1294,1294,1,"Parallel plots depend heavily on the ""right"" ordering of variables, so for too many categories this will become tedious. And the correct source seems to be A.Inselberg, 1981.",2010-08-05 11:44:55.0,56.0,
1295,1294,3,They're called parallel coordinate plots: http://en.wikipedia.org/wiki/Parallel_coordinates,2010-08-05 12:12:39.0,495.0,
1296,1298,0,Well I already know p. I also know the amount of events detected: k. So the total events is somewhere around k/p. I would like to find out an interval around k/p so I can be say 95% sure that the total number of events is inside it. Does that make more sense?,2010-08-05 12:24:21.0,762.0,
1297,1295,0,"i'm wonder if this is a practical limitation though--for a variable that only weakly influences classification, my intuition is that Tree won't likely split on that variable (i.e., it's not going to be a node) which in turn means it's invisible as far as Decision Tree classification goes.",2010-08-05 12:52:48.0,438.0,
1298,1291,0,"In my example, shops O-Y favour romance books. This is why these shops are in a distinct group in the PC plot.",2010-08-05 13:00:10.0,8.0,
1299,1295,0,"I am talking of weak interactions, not weak effects on classification. An interaction is a relationship between two of the predictor variables.",2010-08-05 13:03:44.0,159.0,
1300,1295,1,"This may be inefficient, but tree structure can handle it.",2010-08-05 13:12:07.0,88.0,
1301,1295,0,"That's why I said inefficient rather than biased or incorrect. If you have loads of data, it doesn't matter much. But if you fit a tree to a few hundred observations than the assumed interactions can greatly reduce the predictive accuracy.",2010-08-05 13:15:49.0,159.0,
1302,1273,0,"Actually, you don't need the book for this (you definitely should read it, but not for your current problem). All you have to do is plot the time intervals between infections and build your control chart based on this variable. Try it, you'll see that it's quite simple.",2010-08-05 13:18:39.0,666.0,
1303,1289,0,"@nimcap It is about frequencies that some continuous variable fits in a particular interval (bin). After making the cut of space into bins, it boils down to a bar chart, still with richer interpretation.",2010-08-05 13:21:43.0,88.0,
1304,1294,0,"@Simon thanks; @honk I agree, this is one reason why I don't use them.",2010-08-05 13:24:00.0,88.0,
1310,1309,0,I am not sure if the multiple types solution will work here.,2010-08-05 14:32:32.0,,user28
1311,1293,0,Converted to community wiki.,2010-08-05 14:33:26.0,88.0,
1312,1308,0,I presume that it's bioinformatics problem,2010-08-05 14:41:27.0,8.0,
1313,1309,0,"@Srikant you're right, thanks.",2010-08-05 14:42:33.0,5.0,
1314,1309,0,I think that generalisation still only works for 2 or more people sharing a birthday - just that you can have different sub-classes of people.,2010-08-05 14:45:02.0,765.0,
1315,1308,0,"It is actually a bioinformatics problem, but since it boils down to the same concept as the birthday paradox I thought I'd save the irrelevant specifics!",2010-08-05 14:50:57.0,765.0,
1316,1308,2,"Normally I would agree with you, but in this case the specifics might matter since there could already be a bioconductor package that does what you ask.",2010-08-05 14:53:30.0,8.0,
1317,1291,2,"I voted this up as a good general answer but as a practical answer, dealing with that many data points is going to be brutal.",2010-08-05 15:16:13.0,601.0,
1318,1308,0,"If you really want to know, it's a pattern finding problem where I'm trying to accurately estimate the probability of a given level of enrichment of a subsequence within a set of larger sequences.  I therefore have a set of subsequences with associated counts and I know how many subsequences I observed and how many theoretically observable sequences are available.  If I saw a particular sequence 10 times out of 10,000 observations I need to know how likely that was to have occurred by chance.",2010-08-05 15:21:10.0,765.0,
1319,1290,2,"That looks almost identical to how R's combn function is already doing things.  I wrote up a version of combn that does take combinations off the stack one at a time, and as mbq says because it is only storing one combination in memory at a time it can handle very large combinations.  The problem with doing it in R is that doing a step-by-step approach in a function typically involves reading the state variables into the function, manipulating them, then storing them back out to global - which seems to just slow everything /way/ down.",2010-08-05 15:34:43.0,196.0,
1320,1311,0,"Will this solution suffer from the curse of dimensionality? If instead of n=365, n=10^6 is this solution still feasible?",2010-08-05 15:38:04.0,8.0,
1322,1311,0,"Some approximations may have to be used to deal with high dimensions. Perhaps, use Stirling's approximation for factorials in the binomial coefficient. To deal with the product terms you could take logs and compute the sums instead of the products and then take the anti-log of the sum.",2010-08-05 15:45:53.0,,user28
1324,1312,0,"There may be notational ambiguities in what I wrote but $X_3$ is a random variable as it is a function of two random variables $Y_1$ and $Y_2$. In fact, you can compute $P(X_3=X_{31}) = P(Y_1 \\  Y_2 > 0)$.",2010-08-05 16:11:28.0,,user28
1325,1311,0,There are also several other forms of approximations possible using for example the Taylor series expansion for the exponential function. See the wiki page for these approximations: http://en.wikipedia.org/wiki/Birthday_problem#Approximations,2010-08-05 16:15:05.0,,user28
1326,1214,0,"Thank you. Unfortunately, this still doesn't work as I would expect. For the same time series of the previous comment it returns 166, which is only partially right (from my point of view, the evident weekly period is more interesting). And using a very noisy time series, like this one http://dl.dropbox.com/u/540394/chart2.png (a TCP receiver window analysis), the function returns 10, while I would expect 1 (I can't see any obvious periodicity). BTW I know that it will be really difficult to find what I'm looking for, since I'm dealing with too different signals.",2010-08-05 16:17:41.0,667.0,
1327,1312,0,"$X_3$ is random, but a deterministic function of other random quantities in your model. It therefore should not occur explicitly in the likelihood. In the context of graphical modelling, you would refer to $X_3$ as a deterministic node.",2010-08-05 16:30:06.0,643.0,
1328,1291,1,"+1 This is certainly not what OP wants, still it is certainly what she/he should want.",2010-08-05 16:39:07.0,88.0,
1329,1297,2,"From a ML point of view trees can be tested in a same way as any other classifier (CV for instance). Still it rather shows that heavy overfit happened ;-) Also RF escapes multicollinearity not because it is ensemble, but because its trees are suboptimal.",2010-08-05 16:48:36.0,88.0,
1330,1295,1,"Agree; I just wanted to highlight it. Still I think that reduction of predictive accuracy can be removed by using proper training; in phylogenetics the similar problem (greedyness) is reduced by Monte Carlo scanning of the possible tree space to find maximum likelihood ones -- I don't know is there a similar approach in stats, probably no-one was bothered by this problem to such extent.",2010-08-05 16:55:49.0,88.0,
1331,1284,0,Thank you for the link. I'll try to contact the author sometime in the future.,2010-08-05 17:52:15.0,253.0,
1333,1153,0,"I finally had the time to play a bit with this method, and this seems  a good compromise (even if I still have to evaluate how complex is the stl() routine from a memory/cpu point of view), which works pretty well if we known the frequency of the time series. BTW, I noticed that the results of the detection heavily depend on the kind of signal I'm analyzing, and in order to obtain the optimum results I have to manually tune the probabilities for the quantiles or the multiplier for the IQR.
Since you're an expert of this topic, is there a ""smart"" and ""de facto"" solution to deal with this issue?",2010-08-05 18:33:56.0,667.0,
1337,1316,0,"i was actually going to efforts to *avoid* mentioning the Gamma distribution. i saw it on Wikipedia, i cannot actually find the formula for the distribution, or the formulas to estimate the parameters in that formula. And then i got really nervous when i saw *""There is no closed-form solution for k.""*  And i tried it anyway with some formulas - but when you get a packet that comes back in 0ms, the ln(0) blows up.",2010-08-05 19:40:33.0,775.0,
1338,1316,0,"Because while i have good understanding of the normal distribution, from my university days, i am over my head when we get to things like *""Kullback–Leibler divergence""*.",2010-08-05 19:42:41.0,775.0,
1339,1315,1,"Just looking at it, it looks like a skewed normal distribution. Are you sure the outliers are necessary for your analysis?",2010-08-05 19:43:35.0,776.0,
1340,1316,0,"If you have a packet that comes back in 0ms it is not 'really' zero, right? Perhaps, you can set it to a small value and in any case you need not worry about ln(0) as the parameter $k$ does not refer to the data you have. I will update my answer with some details about the estimation process.",2010-08-05 19:45:12.0,,user28
1341,1315,1,My analysis will consist solely of drawing a pretty graph over-top the bars :)   But it would be cheating to pretend there was no top tail...,2010-08-05 19:53:13.0,775.0,
1342,1316,0,"Yes, technically it should be referred to as `<1ms`. And this plot doesn't include zero, because it's going over a higher latency link  (modem). But i can run the program just as well over a faster link (i.e. ping another machine on the LAN), and routinely get `<1ms` and `1ms`, with much less occurrences of `2ms`. Unfortunately Windows only provides resolution of `1ms`. i could manually time it using a high-performance counter, getting µs; but i was still hoping to be able to put them into buckets (to save memory). Perhaps i should add 1ms to everything... `1ms ==> (0..1]`",2010-08-05 19:55:28.0,775.0,
1343,1318,0,Really i want to draw the mathematical curve that follows the distribution. Granted it might not be a known distribution; but i can't imagine that this hasn't been investigated before.,2010-08-05 19:57:17.0,775.0,
1344,1315,0,@Ian: Since your histogram looks like you have 33 bins I would guess a 33rd order polynomial would fit nicely ;),2010-08-05 20:12:16.0,56.0,
1345,138,1,http://stackoverflow.com/questions/3375808/learning-r-where-does-one-start on SO,2010-08-05 21:05:06.0,776.0,
1346,1321,1,"You mean like any one of the numerous ggplot2 example charts such as
[this](http://learnr.wordpress.com/2009/05/18/ggplot2-three-variable-time-series-panel-chart/) or did you have something more specific in mind?",2010-08-05 21:46:47.0,334.0,
1347,1321,1,"For what it is worth, I like plotting with `xts` and `zoo` objects where different series can be merged easily.",2010-08-05 21:52:29.0,334.0,
1348,1315,0,@honk Please no! Even imaging it hurts.,2010-08-05 22:04:39.0,88.0,
1349,1315,0,@Brandon Please no! Even imag(in)ing it hurts.,2010-08-05 22:07:20.0,174.0,
1352,1326,0,"I doubt that anyone is going to top this one. Thanks Matt, great answer!",2010-08-05 23:48:22.0,1356.0,
1354,1322,3,"Here's an example of your suggestion from the NYTimes

http://www.nytimes.com/imagepages/2010/05/02/business/02metrics.html",2010-08-06 00:57:28.0,287.0,
1355,1297,2,"For a probabilistic framework of decision trees, see DTREE (url: http://www.datamining.monash.edu.au/software/dtree/index.shtml) which is based on the paper ""Wallace C.S. & Patrick J.D., `Coding Decision Trees', Machine Learning, 11, 1993, pp7-22"".",2010-08-06 01:17:00.0,530.0,
1357,1322,0,"That's an interesting visualization. JoFrhwld, thanks for the example.",2010-08-06 02:14:34.0,776.0,
1359,1337,4,I made this community wiki as there is no correct answer.,2010-08-06 02:44:34.0,159.0,
1360,1214,0,"166 is not a bad estimate of 168. If you know the data are observed hourly with a weekly pattern, then why estimate the frequency at all?",2010-08-06 03:01:01.0,159.0,
1361,1340,0,"http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1043351251

Freely available document written by Peter Huber on John Tukey's contributions to robust statistics. Reasonably easy read, light on the formulae.",2010-08-06 03:08:39.0,781.0,
1363,1340,0,"It is best to avoid referring to other answers ""above"" and ""below"" as they can move depending on votes and the ordering tab at the top of the answers section.",2010-08-06 03:42:37.0,159.0,
1364,1214,0,"Because I have to analyze a lot of time series (suppose 100 network metrics), and only some of them have a weekly periodicity. In any case, I guess in my implementation I will use an algorithm similar to your function, and I'll manually distinguish the weekly periodicity. Really thanks for your support, I really appreciate (and keep up the good work with the forecast library :-))",2010-08-06 03:55:59.0,667.0,
1365,1339,0,"Thank you for your precious informations, I'll look at that book for sure.",2010-08-06 03:57:38.0,667.0,
1367,1304,0,"Thanks, that looks great. I think this is the answer I was looking for.",2010-08-06 06:18:16.0,762.0,
1368,1327,0,"+1 i thought binomial as well, when i first saw the histogram. (Not sure why this got downvoted).",2010-08-06 06:35:35.0,438.0,
1369,1346,30,"You should also quote the title attribute of the image: ""Correlation doesn't imply causation, but it does waggle its eyebrows suggestively and gesture furtively while mouthing 'look over there'.""",2010-08-06 07:15:51.0,582.0,
1370,1349,1,A simple help through the rseek.org or r-help mailing list will get you on your way with dealing with most (all?) of the methods in R (which is the program package I would suggest to anyone). Good link.,2010-08-06 07:47:25.0,144.0,
1372,1355,0,"I wanted to add the tag ga,es but couldn't.  I will later when I pass the rep threshold.",2010-08-06 09:48:44.0,114.0,
1373,1356,1,I think originaly taken from Yihui XIE's Statistics Jokes Slides (http://www.yihui.name/en/attachment.php?f=attachment/jokes_yihui.pdf),2010-08-06 09:57:54.0,114.0,
1374,1164,1,"Excellent recommendation, thanks.",2010-08-06 10:36:18.0,438.0,
1375,1355,2,"It'd be fun to see your code once you write it :)  Sorry, I don't remember coming across such a thing...",2010-08-06 11:04:58.0,253.0,
1376,1318,2,Look up 'density estimation'.,2010-08-06 11:21:01.0,247.0,
1377,1355,0,I did something similar a while back to analyze the most-frequently-landed-on spaces in Monopoly (wrote a small simulator)... not sure if there's another way of doing it.,2010-08-06 11:47:54.0,292.0,
1378,1355,0,Seems off-topic. I am not sure I see the link to statistical analysis in the question. This seems appropriate for http://stackoverflow.com.,2010-08-06 12:21:39.0,,user28
1379,1355,0,"Strikant - since it can be solved with simulation, I think keeping the question will be o.k.",2010-08-06 12:45:52.0,253.0,
1380,1365,0,"Thank you, this is really helpful. (Shame on me for not recognizing a Fourier series even when hurtling towards it...)",2010-08-06 12:55:07.0,650.0,
1381,1355,0,I agree with it Tal. It's a statistical computing problem.,2010-08-06 12:58:32.0,8.0,
1382,752,0,"thanks a lot, I also like the poetic quality.",2010-08-06 14:19:16.0,223.0,
1383,1275,0,extrapolation would include smoothness in the filled part that does not exists in the existing part. You have to add randomness... hence resampling (and resmapling on the extrapolation seems to be a good idea),2010-08-06 14:28:13.0,223.0,
1384,1369,3,I don't really understand what you are asking. Could you try and reword your question?,2010-08-06 14:28:50.0,8.0,
1385,1275,0,Extrapolating the model would require sampling the error term which would induce the desired randomness.,2010-08-06 14:53:04.0,,user28
1386,1264,0,thanks for this explanation,2010-08-06 15:02:18.0,684.0,
1387,1350,2,"A 'correct' answer is dependent on the model and the code/commands you are using. So, a brief description of both would be useful.",2010-08-06 15:08:53.0,,user28
1388,1355,0,Definitely in topic,2010-08-06 15:10:40.0,582.0,
1389,1359,1,"One of the problems that I have found with several of the DOE books that I am familiar with is that they heavily emphasize analysis.  Is there are good book that really does the design elements well--e.g. blocking, replication, randomization, choosing factor levels, using repeated measurements, split-plots, etc.  What about the (even more often neglected) design of non-experimental analytic studies, like case-control designs or surveys for analytical purposes (a very different prospect than simply estimating a population parameter in terms of how you might structuring that research).",2010-08-06 15:21:35.0,485.0,
1390,1359,0,"I would look for books in epidemiology and economics for what you are describing.  But sadly, I don't have a recommendation to give.",2010-08-06 15:46:26.0,253.0,
1391,1340,0,I realized that after I saw my statement float around. I'll edit it to refer to the specific comment.,2010-08-06 15:49:03.0,781.0,
1392,1282,0,"I'm giving you the tick, because I'll probably implement this.",2010-08-06 15:59:51.0,287.0,
1393,1340,0,thanks for the princeton study: i did not know of it.,2010-08-06 16:16:17.0,603.0,
1395,1234,0,"I do not understand this 
""This leads to underfiltering in quiet periods and overfiltering during more busy times""
care to explain ?",2010-08-06 16:25:05.0,603.0,
1396,1374,0,minus five minus seven? so minus twelve? that doesn't make sense,2010-08-06 17:24:55.0,74.0,
1397,1365,0,Does this mean that the moments of a circular distribution should be compared to the characteristic function of a linear distribution rather than to its moments?,2010-08-06 17:53:04.0,650.0,
1398,1374,1,Five to seven -- I'll try to convert it to en-dash.,2010-08-06 17:53:36.0,88.0,
1399,1338,7,That is certainly true!,2010-08-06 18:30:34.0,253.0,
1400,1374,0,"That made me laugh, my god!",2010-08-06 18:31:09.0,253.0,
1401,1338,3,"My mom in law said to me that this is based on the line ""being in love means you never need to say you are sorry"".  From a book called ""love story"" by arik sigall (I think).",2010-08-06 18:33:19.0,253.0,
1402,1372,0,Thanks that got me on the right way and i was able to solve it!,2010-08-06 18:46:05.0,791.0,
1403,1365,0,"@Rasmus: I guess that depends on exactly what you want to do with the information, but in general I'd say yes.",2010-08-06 19:13:44.0,89.0,
1404,1368,26,"This joke falls flat for me because I can't imagine a statistic professor saying this - a student who failed the course, sure.",2010-08-06 19:25:07.0,196.0,
1405,1316,0,simply fitting gammas with R: http://docs.google.com/viewer?a=v&q=cache:bl4TbieigsEJ:cran.r-project.org/doc/contrib/Ricci-distributions-en.pdf+fitting+gamma+distribution+R&hl=en&gl=us&pid=bl&srcid=ADGEESjkOC5S7YMbhWRHBDF9cjFaKv4F33c19EPChCnV9rgtdRyO28LCncXNgq9NYJHYe1DeqHPoW3Pl5uM9m0uLeofqruGfCfH6vRhZ6ioinju5ukyWdUYm6D4CVRYOF5xVymC7dTHs&sig=AHIEtbSdTddbrI7ADPWiM_N7gLaLeara0w,2010-08-06 20:15:00.0,291.0,
1406,1376,0,thanks for editing to have proper math. I couldn't find info on how to render math in 'markdown' language. looks just like raw \\LaTeX does it?,2010-08-06 21:33:34.0,795.0,
1407,1376,0,"Yes, it is Latex. See this meta thread: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats",2010-08-06 21:46:02.0,,user28
1408,1275,0,IMO both suggestions boil down to predicting future values from existing ones (AR/ARMA models perhaps?). I guess I'm still hoping for a solution that doesn't involve sampling values (thus the possibility of introducing error).. Besides estimating such models is in itself a form of dimensionality reduction :),2010-08-06 22:19:00.0,170.0,
1409,1335,0,"Here is a post with the code and some illustrations:

http://www.markmfredrickson.com/thoughts/2010-08-06-combinadics-in-r.html",2010-08-06 23:50:07.0,729.0,
1410,786,7,According to Wikiquote it is misattributed to Joseph Stalin; the origin is Kurt Tucholsky: http://en.wikiquote.org/wiki/Joseph_Stalin#Misattributed,2010-08-07 00:36:13.0,509.0,
1411,485,0,Just noticed this related question: http://stackoverflow.com/questions/570029/learning-applied-statistics-with-a-focus-on-r,2010-08-07 08:36:28.0,183.0,
1412,1371,27,"I remember seeing George Burns on TV being interviewed on his 100th birthday. He was puffing on a cigar. The interviewer made some comment about the incongruity of longevity and smoking.


George Burns: ""Twenty years ago my doctor told me that these cigars were going to kill me""
 
Interviewer: ""What does he say now?""
 
George Burns: ""I don't know. He's dead""",2010-08-07 09:17:31.0,521.0,
1413,1383,0,"you should include a better description of what data you want to compress... otherwise we will have 10 different answers trying to summarize different part of the huge theory of compression.  

In addition, the two point you give to be more specific are not detailed enough to be understood.",2010-08-07 09:32:00.0,223.0,
1414,1378,0,+1 for interesting real case ! can you help us to understand what is blood test ? (how do you calculate it),2010-08-07 09:35:06.0,223.0,
1415,165,1,"The link doesn't work:
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.13.7133&rep=rep1&type=pdf",2010-08-07 09:47:53.0,521.0,
1416,1391,2,"+1 nice and clear answer. Jeromy, can I ask a question about point 3? I do understand the reasoning behind transforming the data, but something always bothered me about doing that. What is the validity of reporting the results of the t-test on the transformed data to the untransformed data (where you're not ""allowed"" to do a t-test)? In other words, if two groups are different when data is, for instance, log transformed, on what bases can you say the raw data is different too? Bare in mind, I'm not a statistician, so maybe I just said something absolutely stupid :)",2010-08-07 09:50:59.0,582.0,
1417,618,0,"Obviously an unforseen side-effect of the profusion of lemon laws in the US. For example see:
http://en.wikipedia.org/wiki/Lemon_law",2010-08-07 10:03:20.0,521.0,
1418,823,26,"-1 This is an intellectually dishonest quote from a bad novelist, playing up a popular romantic myth of scientists as Rand-esque revolutionary loners in order to pander to anti-science crankery. Moreover, it has nothing whatsoever to do with statistics. What is it even doing in this list?",2010-08-07 14:34:40.0,174.0,
1419,1391,2,"@nico I'm not sure about how to report or think about the results, but if all you want to show is that for some X and Y, mu_X != mu_Y, it should be true that for all X_i < X_j, log(X_i) < log(X_j) and for all all X_i > X_j, log(X_i) > log(X_j). That's why for non-parametric tests which operate with ranks, transformations of the data don't affect the result. I think from this, you can assume that if some test shows that mu_log(X) != mu_log(Y), then  mu_X != mu_Y.",2010-08-07 15:57:56.0,287.0,
1420,1391,0,"thanks for the answer(s). indeed, the t-test appears to maintain nominal type I rate under mildly skewed/kurtotic input. however, I was hoping for something with more power. re: 2, I have implemented Wilcox' `trimpb` and `trimcibt`, but they are a bit too slow to do my power tests, at least for my taste. re: 3, I had thought of this method, but I am interested in the mean of the un-transformed data (i.e., I am not comparing 2 R.V.s with a t-test, in which case, a monotonic transform would be fine for a rank-based comparison, as noted by @JoFrhwld.)",2010-08-07 16:11:08.0,795.0,
1421,1390,0,"the data is most definitely not normal. the excess kurtosis is on the order of 10-20, the skew is on the order of -0.2 to 0.2. I am doing a 1-sample t-test, so I'm not sure I follow you regarding 'unequal variances', or the U-test.",2010-08-07 16:14:18.0,795.0,
1422,1378,0,"Sure - briefly, it's a test for latent tuberculosis infection.  Blood is drawn from the patient into three tubes: one that has no antigens, one that has selected TB antigens, and one that has mitogen.  We then compare amount of immunologic response in the tube with no antigens to the one with TB - if they have a strong boost of response in the TB tube, they may have latent TB.  (The mitogen tube serves as a check to make sure that the person is capable of producing an immunologic response at all - most people have very strong reactions to it)",2010-08-07 17:25:37.0,71.0,
1423,1396,0,It won't help if just the numerical algorithm is diverging for certain parameters.,2010-08-07 18:43:22.0,88.0,
1424,1393,0,"interesting idea, thanks for sharing",2010-08-07 22:20:11.0,170.0,
1425,1405,1,what's the raw information you have?... i.e. how did you come to two DORs?,2010-08-08 01:37:27.0,601.0,
1426,1391,2,"@nico If the population distribution of residuals is the same in two groups, then I imagine any time there is a difference in raw population group means there would also be differences in group means of an order-preserving transformation. That said, p-values and confidence intervals will tend to change slightly based on whether you are using raw data or transformed data. In general I prefer to use transformations when they seem like a meaningful metric for understanding the variable (e.g., Richter scale, decibels, logs of counts, etc.).",2010-08-08 03:49:01.0,183.0,
1427,1337,2,It probably makes sense to leave cartoons in this question: http://stats.stackexchange.com/questions/423/what-is-your-favorite-data-analysis-cartoon,2010-08-08 11:49:27.0,183.0,
1428,1408,0,"The first point you mention is false in general I suggest that you read the first sentence of the chapter http://books.google.fr/books?id=Nxnh48rS9jQC&pg=PA167&lpg=PA167&dq=Orthonormal+bases+of+compactly+supported+wavelets,&source=bl&ots=tbUc1teQFB&sig=gmzBJyET8S5SXd-oPmzyh8BWDKg&hl=fr&ei=DPNeTOeBC8nT4gbsuazaBw&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDAQ6AEwAw#v=onepage&q=Orthonormal%20bases%20of%20compactly%20supported%20wavelets%2C&f=false in the book of Daubechie. In addition, If you had read my answer I already mentionned the nice property of the DWT in the 2nd part of my answer...",2010-08-08 18:13:13.0,223.0,
1429,1405,0,"If you have the raw information available that you used to calculate the odds ratios a statistical test will be possible.  Alternatively there might be some reasonable simulation approaches if you know how many observations went into each odds ratio.

P.S. For those of us who only have a passing familiarity with DORs would you please provide the formula used for calculating one?",2010-08-08 18:17:59.0,196.0,
1430,1414,0,"I'm not sure I understand your answer.  I think you are saying I do not need to specify a link function because the intercept of my model will adjust based on the data and effectively account for the fact that at high difficulties people will not be able to perform the task.  Is that correct?  If so, how does this prevent a flattening of the slope estimate due to the same values at difficulties 4 and 5?  I may need a less technical explanation (if possible).",2010-08-08 18:40:26.0,196.0,
1431,1408,0,"To the first point, you're right.  I should have said ""Most commonly used/implemented discrete wavelet basis functions""; I'll edit to reflect that.

To the second point, you gave a good answer for how the some CWTs (most often a DOG wavelet or the related Ricker wavelet; something like e.g. the Gabor wavelet would not provide the behavior your describe) can detect anomalies of the singularity kind.  I was trying to give an analogous description of how DWT can be used for detecting other kinds of anomalies.",2010-08-08 19:21:26.0,61.0,
1432,242,0,"sorry if I wasn't clear, I mean't that I prefer (personal subjective opinion) questions like yours than the question that asks ""what is a random variable""... but I guess my pleasure is not that of everyone :)",2010-08-08 19:27:36.0,223.0,
1433,1414,0,"*I'm not sure I understand your answer...Is that correct?*

yes

The limitation with the logit is one of symmetry; going back to your example, you force the slope between 4 and 5 to be the same as the slope between 1 and 2.

If symmetry is a problem, you might want to try a complementary log-log link (not implemented in LMER).",2010-08-08 20:09:07.0,603.0,
1434,1364,0,That's perfect ! I was hoping for a pointer but that's a really good answer.  Thanks.,2010-08-08 20:11:57.0,114.0,
1435,1414,0,"I think your example is somewhat boggus in that the variable ""difficulty of the discrimination"" is really a ordinal scale and should be introduced in your regression as such. The problem you would have in this case would be due to you wrongly introducing ""difficulty of the discrimination"" as a continuous predictor when  it is not, not to the logit link per see.",2010-08-08 20:17:42.0,603.0,
1436,1414,0,"@kwak: I introduced it as an ordinal because it is easier to describe that way.  Difficulty of discrimination in many of these experiments is a quantifiable ratio scale variable, e.g. contrast or noise introduced to the signal.",2010-08-08 21:29:05.0,196.0,
1437,1414,0,@kwak: Does your suggestion to use a complementary log-log link still apply given that the IV is an interval scale variable?,2010-08-08 21:36:55.0,196.0,
1438,1414,0,"@drknexus#1:> understood, but the mishandled ordinal scale was what was causing the expected behavior of the logit seem inappropriate, so i pointed it out.
@drknexus#2:> what you mean by ""IV""?",2010-08-08 21:46:14.0,603.0,
1439,1421,0,"The example is a case where a single predictor linear equation, though rationally sound given the dataset, will provide a suboptimal solution to the problem.  If in the example case you fit a quadratic as well you'd likely get an improved model fit would you not?  The improved model fit wouldn't be a consequence of some underlying real quadratic effect of the IV, just that you'd hit a measurement floor.",2010-08-08 22:06:43.0,196.0,
1440,1421,0,"I'll take a look at Dixon - thanks for the reference - it looks right up my alley.  The alternative link I was imagining was an explicitly 2AFC link, e.g. mafc.logit(2) in the psyphy package of R.",2010-08-08 22:14:26.0,196.0,
1441,1414,0,"I'm not sure what you mean by ""mishandled ordinal scale"".  Is the problem that the data itself is not spread out in a continuous fashion?  I'd like to simulate this with continuous data, but I can't think of a way of generating values that don't beg the question.  Any ideas?

By IV I meant ""independent variable""/""predictor"", sorry for using shorthand.",2010-08-08 23:19:29.0,196.0,
1443,608,0,mbq; Is the question in its current form unanswerable or just difficult?,2010-08-08 23:40:54.0,196.0,
1444,1424,0,"By 'such event' do you mean exactly 3 aces, or at least 3 aces?",2010-08-09 00:20:36.0,455.0,
1445,219,2,"Thomas, Howes, Luk - 2009 - A comparison of CPUs, GPUs, FPGAs, and massively parallel processor arrays for random number generation. http://doi.acm.org/10.1145/1508128.1508139.

Discussion + benchmarks of a set of PNGs executing on CPU, GPU, FPGA and Massively Parallel Processor Arrays.",2010-08-09 01:12:09.0,154.0,
1446,1424,0,"That's right, exactly 3 aces, because you roll exactly 3 dices.",2010-08-09 01:39:46.0,1356.0,
1447,1425,0,"Could you, please, point us to a correct calculation? ... in ""as if"" case?",2010-08-09 01:59:02.0,1356.0,
1448,1425,0,"It coccured to me to do it a few minutes ago, so it's already there. Did you want more detail?",2010-08-09 02:00:12.0,805.0,
1450,1419,0,"Yes, I now remember looking at one or two of these videos a few years back. They are quite mathematical.",2010-08-09 06:33:34.0,183.0,
1451,1429,0,"I am not trying to compute a reduced-rank approximation of X, rather a transformed X. You see my goal is not to filter noisy sequences, but to find a representation with a reduced dimensionality (to be used for classification/clustering of time series)... Could you elaborate a bit on the EM-PCA approach?",2010-08-09 06:38:02.0,170.0,
1452,1430,0,"I would like to add a stochastic processes tag, but don't have the reputation, perhaps somebody could edit this for me?",2010-08-09 06:51:17.0,352.0,
1453,1430,0,retag done !,2010-08-09 06:59:16.0,223.0,
1454,1421,0,"Doesn't the AIC adequately assess model fit for these purposes? I only provided the curves to demonstrate/visualize what is going wrong.  The problem isn't simply because the floor has been hit.  To convince yourself of this, try difficulty 4 at 60% and 5 at 55%.  The logistic curve still is dropping down into ""impossible"" territory.  Yes, it might be nice to not assess difficulty levels where the task is ""impossible"" but you don't always know where those levels are going to be in advance of preliminary data collection.",2010-08-09 07:09:45.0,196.0,
1455,1421,0,"My take on it is this: the problem we are observing is analogous to why using percentages as linear predictors is fine until you start reaching the extremes of the percentages - the logistic function and a linear function are reasonably matched until you get upwards of 80%. Likewise, in the low end here the basic logistic function is a reasonable match until the probability of correct answers starts dipping below 60% or so. Then you find yourself in part of the curve where the logistic function is predicting a continued drop off in accuracy whereas the 2AFC function predicts a leveling.",2010-08-09 07:15:08.0,196.0,
1456,430,0,"This is the way I explain things, fully agree !",2010-08-09 07:24:50.0,223.0,
1457,1408,0,The second point you mention is also likely to be false: wavelet support (if it is compact) is giving information about the temporal localization of the wavelet not the frequency localization.,2010-08-09 07:30:30.0,223.0,
1458,1431,0,"Perhaps I am a little confused, or we are using different definitions?  Am I correct in thinking that a process is 2nd order stationary if the joint marginal cdfs
$F_{X(t),X(t+τ)}$ are equal for all $\\tau$?

Similarly for a process to be 1st-order stationary the marginal cdfs $F_{X(t)}$ need to be the same for every $t$. So all the moments of the $X(t)$ must be equal. 

2nd-order stationary implies 1st-order stationary, correct?",2010-08-09 07:44:33.0,352.0,
1459,1431,0,"Extending this, a process is $N$th order stationary if for every $t_1, t_2, \\dots, t_N$ the marginal cdfs $F_{X(t_1 + \\tau), X(t_2 + \\tau)\\dots,X(t_N+\\tau)}$ are the same for all $\\tau$.

Strictly stationary is Nth order stationary for all N.",2010-08-09 07:45:28.0,352.0,
1460,1431,0,"see http://www.statistik.tuwien.ac.at/public/dutt/vorles/geost_03/node49.html for order 2 stationnarity. Anyway, I have tryed to clarify my answer, hope it is better now...",2010-08-09 08:05:01.0,223.0,
1461,833,0,thanks for the update simon !,2010-08-09 08:06:15.0,223.0,
1462,76,0,why is this answer not community wiki ?,2010-08-09 08:18:24.0,223.0,
1463,1431,0,I believe that article describes stationary in the weak sense and this is not what I meant. I should clarify this in the question. See http://en.wikipedia.org/wiki/Stationary_process for a description of the various types of stationarity.,2010-08-09 08:24:15.0,352.0,
1464,1431,0,"@robby OK... I didn't know this ""second order stationnarity"" I think you should not say second order stationnarity in the question and give the definintion instead. For more clarity you should ask another question. do you have a paper that refers to this second order stationnarity ?",2010-08-09 08:32:42.0,223.0,
1465,1425,0,"No, this should suffice. Thanks!",2010-08-09 08:41:22.0,1356.0,
1466,501,0,"Can you define how you ""check the error increase"" please?",2010-08-09 09:03:43.0,521.0,
1467,497,0,"How do you ""train"" your classifier? Presumably this is done on the training set. If it is a Support vector Machine (SVM) there are several parameters to try during training. Is each tested against the validation (test) set? Or are you using k-fold cross validation? How many times are you using the validation (test) set to check your performance - presumably this is accuracy. Sorry to be pedantic, but this is a poorly defined answer and risks over-fitting.",2010-08-09 09:08:36.0,521.0,
1468,1431,0,"I'll put a second question together that better describes what I mean.  In the mean time, your answer is correct for the question I stated!",2010-08-09 09:11:45.0,352.0,
1469,1291,1,"+1 Nice example of a ""down-to-Earth"" application of PCA.",2010-08-09 10:49:45.0,582.0,
1470,1438,0,"not long at all, I am comparing different methods, although I was not sure whether correlating with main components does make sense in terms of time series data. On the other hand the major trends are indeed orthogonal hence PCA should give me solid results",2010-08-09 10:56:26.0,,CLOCK
1471,1203,0,note that this will be your accepted answer for the bounty (in case you do not choose an answer the bounty automatically select the answer with the most points),2010-08-09 11:21:10.0,223.0,
1472,6,3,"It is maybe good to restate what is written in the faq: ""Avoid asking questions that are subjective, argumentative, or require extended discussion""",2010-08-09 11:34:49.0,223.0,
1473,1421,0,"As to your first comment, I see that you have a good answer about how to look at the residuals from another posted question but you still don't have why.  AIC, log-likelihood, etc. all tell you how good you fit is but they don't tell you the nature of the fit.. it's like comparing SD and histograms for looking at variability.  They both tell you about variability.",2010-08-09 12:22:15.0,601.0,
1474,1421,0,"As to your second problem, no.  Of course the chance end of the scale can be variable but pretty much anyone who's extensively studied SAT curves can attest (Ratcliff, Lappin, Pachella) that there isn't much useful variability in the chance end of the scale. It just ditches catastrophically.  All of the above names work with models other than just logistic to come to this.  It's also a simple measurement issue.  At the low end of the scale you have a good measure of error.  At the high end you don't.  So, you can trust your low accuracy measures better.",2010-08-09 12:22:37.0,601.0,
1475,1421,0,And your comment about imagining that accuracy started going up... typically in one curve you want to model a single psychological process.  I doubt accuracy increasing after difficulty exceeded some point does that.  It would be tapping a different process... or just be some chance variation.,2010-08-09 12:24:55.0,601.0,
1476,1421,0,"BTW, to really understand modelling accuracy over a change in difficulty you might want to look at the SAT literature since it does exactly that.",2010-08-09 12:25:27.0,601.0,
1477,1406,0,I think it is really popular.... but not it statistic (we don't want to loose our job because physisics are improving their experiment :) ). Anyway I think Rutherford belongs to this class of spiritual scientist... +1,2010-08-09 12:32:10.0,223.0,
1478,1393,0,"+1 for the web site VideoLecture, I did not know, did you mention it in the question about video lectures ?",2010-08-09 12:34:34.0,223.0,
1479,1393,0,I've only being reading about this stuff recently. I really like Candes and Tao's recent paper on the topic http://arxiv.org/abs/0903.1476,2010-08-09 12:55:23.0,352.0,
1480,1386,0,Is there a reason that prohibit the use of the Welch t-test? Have a look at my answer to this question (http://stats.stackexchange.com/questions/305/when-conducting-a-t-test-why-would-one-prefer-to-assume-or-test-for-equal-varia) where i refer to a paper advocating the use of Welch in case of non-normality and heteroscedasticity.,2010-08-09 13:09:08.0,442.0,
1481,338,3,"Also the paper ""Information Theoretic Inequalities"" by Dembo Cover and Thomas reveals a lot of deep aspects",2010-08-09 13:36:23.0,223.0,
1482,6,10,"Thanks @robin; made CW.  Although I don't entirely see this as ""argumentative""; there are two fields which have informed each other (this is a fact), and the question is how much they have evolved together over the last decade.",2010-08-09 14:17:51.0,5.0,
1483,76,0,@robin: must be a bug; since my answer (and several of the others) were given before the question was made CW.,2010-08-09 14:37:42.0,5.0,
1484,1441,0,I see round 2 is more difficult :) Mathoverflow may give you a faster answer than stat.overflow...,2010-08-09 14:41:14.0,223.0,
1485,1447,0,your question is not about R vs R-square (you understand that $0.8^2=0.64$) it is about interpretation of $r^2$. Please reformulate the title.,2010-08-09 15:03:53.0,223.0,
1486,1408,0,"Discrete wavelets -- or at least the vast majority of ones that are implemented and commonly used -- are typically designed to have useful frequency-based properties under the compact support constraint.  The vanishing moment condition of Daubechies, for instance, is more or less equivalent to flatness in the pass-band.  The frequency localization properties of wavelets are usually what lead the coefficients to be sparse representations and allow estimation of the noise variance under the ""signal + additive zero-mean noise"" assumption.",2010-08-09 15:05:47.0,61.0,
1489,1410,0,Thanks Jeromy.  Just read the Wikipedia entry for MDS -- seems like it could lead somewhere.,2010-08-09 16:24:53.0,251.0,
1490,1452,0,"hmm, can't get the latex ""begin cases"" to accept new lines.  :-/",2010-08-09 16:44:36.0,251.0,
1492,1452,0,@ars I fixed the eqns to use begin cases. I hope I did not mangle the eqns in the process.,2010-08-09 16:55:57.0,,user28
1493,1386,1,"well, the problem is that I want a 1-sample test, not a 2-sample test! I am testing the null $E[X] = \\mu$, and not $E[X_1] = E[X_2]$. I will look up the Kubinger et. al., paper (Ich kann schlecht Deutsche).",2010-08-09 17:02:43.0,795.0,
1494,1452,0,"@Srikant Looks good, thanks!",2010-08-09 17:13:21.0,251.0,
1495,1421,0,I understand now why looking at the residuals directly might be useful.  In this case it would indicate where the model was beginning to fail.  I don't think I said that accuracy should go up at increased levels of difficulty (although that is what the quadradic model would imply).  I just said that accuracy in 2AFC will hit a floor after which it will not continue to go down as a function of increased difficulty.  This is a result of a fairly simple psychological process interacting with the 2AFC design.,2010-08-09 17:16:52.0,196.0,
1496,1421,0,"I'll grant that in the current model, it may make sense simply to drop the high difficulty levels that are giving the logistic model problems. Though if, as you say, you can trust your low accuracy measures better, then aren't you are advocating I throw away the very observations that should be most trusted?  I'll shelve the question for now and may come back to it in a couple months after I've had time to look further at the literature you've cited.",2010-08-09 17:25:19.0,196.0,
1497,1428,0,"Hi Rob,

Thanks for the tip on BFAST - it looks very promising for my purpose! As I briefly mentioned, in my case the rolling aggregation doesn't actually remove the seasonality (there are slacks and surges in effort on a more-or-less predictable schedule, coupled with turnover in the measure denominator), and I have noticed that even with a lag(12) in my breakpoints() model specification, strucchange is a little more sensitive to these seasonal spikes than I'd prefer.",2010-08-09 17:46:42.0,394.0,
1498,1428,0,"Follow up q: Given this, is it fair to say that the pre-aggregation of the data may somewhat dilute (but not completely eliminate) the sensitivity of the BFAST approach to detect breaks in the seasonal component?",2010-08-09 17:47:52.0,394.0,
1499,1457,0,Have no idea why latex looks so bad. Can someone please help and fix this?,2010-08-09 18:05:55.0,,user28
1500,847,0,"[The question has been asked here](http://www.google.com/support/forum/p/websiteoptimizer/thread?tid=449ac5085f82912e&hl=en), where Google employees patrol, and there has been no answer. Also I asked someone that was able to talk to the engineers to ask this question for me, and he said that they would not say; so I wonder..",2010-08-09 18:08:39.0,500.0,
1501,1456,0,I had the feeling it would be something of this sort - many thanks!,2010-08-09 18:13:10.0,253.0,
1502,497,0,"@Thylacoleo This is a very crude basic and greedy method. Often you keep your validation set the same over runs, but whatever you like is ok.",2010-08-09 18:20:12.0,190.0,
1503,1463,0,"Thanks, it's been very helpful.  I'll vote it up when I have enough rep.",2010-08-09 19:23:41.0,148.0,
1504,1458,5,"not really an answer to your question, but have you encountered False Discovery Rates (FDR)? ""Beyond Bonferroni"" by Narum: http://www.springerlink.com/content/c5047h0084528056/",2010-08-09 20:21:27.0,291.0,
1505,1203,0,well - awarding the bounty to myself seems silly - but nobody else has submitted an answer.,2010-08-09 21:01:31.0,196.0,
1506,1468,0,I think you have a typo in your hypotheses -- they both seem to be the same...,2010-08-09 21:50:44.0,174.0,
1507,1428,0,Pre-aggregation will remove or greatly reduce the seasonality. But I don't think that would reduce the ability of BFAST to detect breaks in the seasonal component. It is rare to have breaks in seasonality (at least in all the data I've looked at). Slow changes in seasonality are more common.,2010-08-09 22:12:37.0,159.0,
1509,1474,1,"See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions the current consensus is that this is an appropriate question, if you disagree chime in at meta, don't torment the new question asker.",2010-08-09 22:53:26.0,196.0,
1510,1474,1,"ok, but you'll probably have a bigger audience to get responses for a pure R question (at least for now) -- hence, the ""probably"". ""torment"" is a strong word eh? :)",2010-08-09 23:51:42.0,291.0,
1511,1474,0,"Sure, torment is a bit strong; I'm sorry about that.  I was/am just frustrated with repeat offender users (of which you are not one) who persist in redirecting question askers to StackOverflow without having made any comment on meta (where the consensus seems pretty clear).",2010-08-10 00:15:39.0,196.0,
1512,1397,0,"yes, that is the paper I am looking for, but I cannot find a version of it online. The R-code might be useful, but I am working in Matlab, and don't have time to do the translation (the R code looks nontrivial). I would even be willing to settle for the bibliography of this paper at the moment.",2010-08-10 00:18:02.0,795.0,
1513,1390,0,"I am accepting the 'use a parametric test' advice. it doesn't exactly solve my question, but my question was probably too open-ended.",2010-08-10 00:35:20.0,795.0,
1514,1466,0,"Good point.  If it's not a normal distribution, how do I go about finding  a different ""fit"" ?   (What terminology should I research?)",2010-08-10 00:41:22.0,6967.0,
1515,1465,0,Thank you so much. This is very helpful!,2010-08-10 00:54:57.0,834.0,
1516,1461,0,Thank you so much. It seems that the log transformation is widely practiced in economics. It is not part of economic theory but is widely accepted. I appreciate your very useful response.,2010-08-10 00:56:28.0,834.0,
1518,1435,0,"Reg images: Use one of the free image hosting sites (search google) , upload the plot to that site and link it here.",2010-08-10 01:05:50.0,,user28
1519,1435,0,"I've corrected an error in my original answer. I first wrote p=logit(X*beta). In fact the predicted probability is the inverse logit of the linear combination, p=inv-logit(X*beta). In R this is calculated as p<-plogit(X*beta), which is p=exp(X*beta)/(1+exp(X*beta)).",2010-08-10 01:07:25.0,521.0,
1520,1471,3,We're happy to take R questions here. See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions,2010-08-10 01:22:32.0,159.0,
1521,1385,2,"There's also ""reduced-model"" approach where you train a classifier for every pattern of missing values encountered during testing. IE, to make prediction for x where i'th attribute is missing, remove i'th attribute from all instances of training data and train on that. http://jmlr.csail.mit.edu/papers/v8/saar-tsechansky07a.html",2010-08-10 01:27:11.0,511.0,
1522,1452,0,@ars. I think the `boxcox` function in MASS only estimates $\\lambda_1$ and assumes $\\lambda_2=0$.,2010-08-10 01:27:33.0,159.0,
1523,1448,0,"I do appreciate your attempt at helping, but unfortunately, this just made things 10x worse.  Are you really introducing trigonometry to explain r^2?  You're way too smart to be a good teacher!",2010-08-10 01:49:49.0,6967.0,
1524,1450,0,"> The ratio between the variation explained and the original variation is your R^2

Let's see if I got this.  



If the original variation from mean totals 100, and the regression variation totals 20, then the ratio = 20/100 = .2   You're saying R^2 = .2 b/c 20% of the mean variation (red) is accounted for by the explained variation (green)  



(In the case of r=1) If the original variation totals 50, and the regression variation totals 0, then the ratio = 0/50 = 0 = 0% of the variation from the mean (red) is accounted for by the explained variation (green)  I'd expect R^2 to be 1, not 0.",2010-08-10 01:51:53.0,6967.0,
1525,1452,1,"@Rob: Oh, sorry.  Diggle's geoR is the way to go -- but specify `lambda2=TRUE` in the arguments to `boxcox.fit`.  (Also updated the answer.)",2010-08-10 02:01:31.0,251.0,
1530,1450,0,"R^2 = 1-(SSR/SST) or (SST-SSR)/SST. So, in your examples, R^2=.80 and 1.00. The difference between the regression line and each point is that left UNexplained by the fit. The rest is the proportion explained. Otherwise, that's exactly right.",2010-08-10 04:17:49.0,485.0,
1531,1450,0,"I edited that last paragraph to try to make it a bit clearer. Conceptually (and computationally) all you need is there. It might be clearer to actually add the formula and refer to the SST SSE and SSR, but then I was trying to get at it conceptually",2010-08-10 04:18:19.0,485.0,
1532,1468,0,"@walkytalky: whooops, thanks !",2010-08-10 05:11:21.0,223.0,
1533,498,0,SAS questions (and questions related to other statistical software) are now allowed. See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions for discussion.,2010-08-10 05:36:45.0,159.0,
1534,1469,1,You need to compute the Fisher matrix ? what is your model ? I am sure you can make your question more precise.,2010-08-10 05:50:21.0,223.0,
1535,1466,0,@stats: See my edit.,2010-08-10 06:53:52.0,56.0,
1537,1489,0,Would you please give a link or two to explanation of data standardization?,2010-08-10 07:05:58.0,213.0,
1538,1448,0,"I thought that you wanted to know why correlation^2 = R^2. In any case, different ways of understanding the same concept helps or at least that is my perspective.",2010-08-10 08:47:06.0,,user28
1539,1493,3,your gaussian variables are iid ?,2010-08-10 08:56:47.0,223.0,
1540,1478,0,Could you give a couple of examples of what your code does?,2010-08-10 09:42:11.0,8.0,
1541,1497,0,Does this book provide the code to reproduce the graphics that it shows? (I think it's S that is used),2010-08-10 11:08:22.0,339.0,
1542,1483,0,Raw data is from a 2 x 2 diagnostic contingency table. DOR equation = (TP*TN)/(FP*FN),2010-08-10 11:10:51.0,,Jay
1543,1386,0,Thanks for clarifying. In this case the Kubinger paper will not be very helpful to you. I am Sorry.,2010-08-10 11:18:25.0,442.0,
1544,1472,0,Are you working on the automated procedure?  :),2010-08-10 13:31:06.0,5.0,
1545,1501,3,possible duplicate of [Locating freely available data samples](http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples),2010-08-10 13:55:30.0,,user28
1546,1450,0,"ie: R^2 is the proportion of the total variation from mean (SST) that is the difference b/w the expected regression value and mean value (SSE).  In my example of hours vs. score, the regression value would be the expected test score based on correlation with hours studied.  Any additional variation from that is attributed to SSR.  For a given point, hours studied variable/regression explained x% of the total variation from the mean (SST).  With a high r-value, ""explained"" is big percentage of SST compared to SSR.  With a low r-value, ""explained"" is a lower percentage of SST compared to SSR.",2010-08-10 13:56:30.0,6967.0,
1547,1467,0,I don't know R syntax.  I almost need pseudo-code to get the gist.   I will google some of the concepts you used.  Thanks!,2010-08-10 13:58:05.0,6967.0,
1548,1501,0,Maybe that going into a more fused and detailed catalogue such as Free data set for very high dimensional classification: http://stats.stackexchange.com/questions/973/free-data-set-for-very-high-dimensional-classification could be an idea but otherwise I also feel this is a duplicate.,2010-08-10 14:17:26.0,223.0,
1549,1498,0,"Thank you, that's exactly what I was trying to avoid (having a lot of samples as baseline), because I would like a really reactive approach (e.g. online detection, maybe ""dirty"", after 1-2 weeks of baseline)",2010-08-10 14:49:29.0,667.0,
1550,1470,0,This is also definitely worth citing: http://prefrontal.org/files/posters/Bennett-Salmon-2009.jpg,2010-08-10 14:59:40.0,582.0,
1551,1482,0,"Thanks for the suggestion, but the ability to choose my own measure of distance is critical, so this won't work to me. Someone else may find it useful, though.",2010-08-10 15:06:57.0,,anonymous
1552,1484,0,"Thanks, but is there any benefit to using clusterfly rather than calling ggobi directly? The website only mentions clustering methods, which are interesting, but not my primary goal just yet. gcexplorer has less informative website, but looks like it is for visualizing data after it has already been split to clusters. I will give them a try once I get to that point, but not what I need right now.",2010-08-10 15:12:13.0,,anonymous
1553,1491,0,"Thanks for the suggestion, @Shane. ggobi looks promising, I am installing it right now and will give it a try :)",2010-08-10 15:12:59.0,,anonymous
1554,1502,0,"I thought of this but there doesn't seem to be a reasonable cut point, and domain experts can't justify one either.",2010-08-10 15:15:48.0,,anonymous
1555,1500,0,"Looks like a useful program, but their webpage does not do a good job of convincing me it will solve this exact problem. It looks like it may be too broad, too many features I don't care about, making it hard to do the simple things. I will give it another look if the other choices don't work out.",2010-08-10 15:19:55.0,,anonymous
1556,1499,0,"Here's some criticism of that paper in 4 from the guy who runs eager eyes:
http://junkcharts.typepad.com/junk_charts/2010/05/8-red-flags-about-the-useful-chartjunk-paper.html

http://junkcharts.typepad.com/junk_charts/2010/05/8-red-flags-about-the-useful-chartjunk-paper.html",2010-08-10 15:21:27.0,287.0,
1557,1488,0,"Orange's features page says 'under construction' and they don't list screenshots like what I am doing. weka has no features list at all. They may be able to do what I want, but if they don't promote the feature, how can I tell. I am more convinced by the other choices.",2010-08-10 15:24:40.0,,anonymous
1558,1501,1,I voted to close as duplicate.,2010-08-10 15:36:09.0,5.0,
1559,1501,0,@Shane if I could I would do the same,2010-08-10 16:01:36.0,223.0,
1560,1501,0,"I already voted to close. Perhaps, one of the of the moderators could close this question.",2010-08-10 16:16:12.0,,user28
1561,1507,0,"Why would you calculate the probability of exactly 39 in case 1, but 39 or higher in case 2?",2010-08-10 16:20:35.0,196.0,
1562,1507,0,"@drknexus Case 2 is a continuous approximation of case 1. Thus, it does not make sense to talk about P(Z=10.3) in case 2.",2010-08-10 16:24:29.0,,user28
1563,1478,0,"briefly `pv = AD_test_uni(xs)` takes a sample vector `xs`, which are restricted to the range $[0,1]$ and returns a p-value, `pv` under the null that `xs` are drawn from the Uniform distribution. I can then use this elsewhere to test other tests which spit out a p-value.",2010-08-10 16:28:49.0,795.0,
1564,1479,0,"There is indeed an infinite regress problem here. However, it is certainly not the case that all external libraries are error-free (I have found such errors in the past). Because all of my tests ultimately rest on this A-D test, I thought it might be worthy of extra scrutiny.",2010-08-10 16:44:51.0,795.0,
1565,1502,0,"I would think this could be fairly arbitrary for your stated purpose - honestly, you might not even need to actually cut into binary, just recode a tie value label on a scale of 1 to some manageable number, then progressively hide/show the ties at various levels (optionally also hiding/eliminating any pendants & orphans along the way).

Not directly responding to your request as written, but why not take a more typical approach and use a hybrid clustering method that doesn't use initial centroids to identify preliminary clusters, then feed the centroids from that result into your new analysis?",2010-08-10 16:45:55.0,394.0,
1566,1479,0,"Perhaps, you can use multiple external libraries whose code base does depend on each other. It is unlikely that independent libraries have the same errors which should eliminate some of your concerns.",2010-08-10 16:49:52.0,,user28
1567,1491,1,"Works fine on other platforms, but gtk does not play nice with OSX.",2010-08-10 17:02:53.0,,anonymous
1568,1502,0,"I am guessing you mean to try for many different cutoffs until I see some nice results? I wish to avoid that for standard multiple comparisons reasons. re: your second suggestion I guess I just trust myself better than those algorithms. I use the computer to process large amounts of data too tedious to do by hand, not to replace my thinking.",2010-08-10 17:06:47.0,,anonymous
1569,1493,0,What kind of answer are you looking for?  A name?  A simple formula for the density?,2010-08-10 17:23:56.0,89.0,
1570,1499,0,very interesting. thx for pointing this one out!,2010-08-10 17:34:56.0,22.0,
1571,1501,0,"It's just my two cents, but I think if we limit stats.se to 1 datasets question it makes it harder to share that knowledge.  I'm sorry if I didn't word my question/explanation better to differentiate this question, but I foresee there being lots of 'what open dataset exists to do x?' where x is what stops them from all being dupes.",2010-08-10 17:55:45.0,114.0,
1572,1503,0,"I have seen that question, and your answer was good (and I do rely on a lot of the datasets that ship with R) but not many of them include defined (or should I say well characterized) answers to particular statistical questions. (although I believe that if the dataset has been used in an example to a function in the helpfile, then the package will include the results of running that example, so you could use that)",2010-08-10 17:58:26.0,114.0,
1573,1507,0,@srikant-vadali By the common definition of p-value it should include more extreme cases in addition to the one observed. So the first calculation should be P(39 or 40). Still very small though.,2010-08-10 18:05:53.0,279.0,
1574,1507,0,"@Aniko I think we need to calculate p-values if the question involves inference about a parameter. But, the OP's qn is about the probability of observing a particular event. So, I think case 1 calculation is fine. If we want to compute the probability that we would see at least 39 such events then you would compute P(39 or 40) but perhaps I misinterpreted the OP's goals here.",2010-08-10 18:19:19.0,,user28
1575,1501,0,I think it makes a lot of sense for this question to remain open if you could edit the question to indicate how your request is different from the one that exists and why the answers to the existing question do not satisfy your requirements. See Robin's comment as an example of what I am suggesting.,2010-08-10 18:24:15.0,,user28
1576,1469,0,"I could give more details if that's of interest...as an overview, I'm trying to compare efficiency/bias of n-iteration Contrastive Divergence estimator for fitting Boltzmann Machines with an estimator that uses n iterations of Belief Propagation",2010-08-10 18:31:36.0,511.0,
1577,1312,0,"I thought a bit more about what you said and it makes sense. I suppose one would write $f(Y_3|Y_1,Y_2)$ as $f(Y_3|X_{31},-) I(Y_1 Y_2>0)$",2010-08-10 18:55:01.0,,user28
1578,1511,0,"I do not understand your answer. Suppose, you do not know P(H) and you have to estimate it. You have to write Prob(4H, 1T 39 times out of 40) in terms of P(H) and estimate P(H). Your answer suggests that the probability of observing the event in question is always 1 and hence independent of P(H). I am not sure if that makes sense. Did I miss something?",2010-08-10 19:13:43.0,,user28
1579,1507,0,"@srikant-vadali Then per drknexus the second calculation should also do the same, something like P(38.5<X<39.5).",2010-08-10 19:27:03.0,279.0,
1580,1511,2,"The student observed a result that appears odd.  But once it's observed it's no longer odd, it simply is random chance.  The probability of observed data occurring after you've already observed it is always 1.0.  I put the smiley in because I recognize that you want to know if it's plausible.  But that would involve not only working out the probability of it occurring once but the probability of it occurring given all the students who've ever attempted it and the number of attempts they made.  (also, consider that 4 heads and 1 tail in 39 of 40 attempts has the same probability)",2010-08-10 19:36:42.0,601.0,
1581,1507,0,"@Aniko Perhaps, that would be a better choice.",2010-08-10 19:40:51.0,,user28
1582,1511,0,How does what you say reconcile with the estimation problem I posed in my comment?,2010-08-10 19:42:03.0,,user28
1583,1511,2,"First, I was making a stats joke. But there is a serious point. If you believe the student then the probability is 1.0 because the data have already been observed.",2010-08-10 19:52:38.0,601.0,
1584,1507,0,"Is this a statistics assignment?  If so, then if you are testing the student's plausibility you need to calculate the number of students you've ever assigned this task to and multiply the result by that.",2010-08-10 19:54:25.0,601.0,
1585,1511,1,"But, that is exactly the point isn't it? The OP does not trust the student and wants to make an assessment of how likely the event is under reasonable assumptions (P(H)=0.5, independence etc).",2010-08-10 20:00:30.0,,user28
1586,1507,0,"Or do I?  I'd think I can just base the calc on theoretical probability of the fair coin, and ignore Experimental Probability of what students got?",2010-08-10 20:34:15.0,6967.0,
1587,1507,0,"Yes, in case 1, I could have done P(30 or 40), but I would have just added another 0%, so I skipped it.  I used the inequality in case 2 b/c I was interested in the odds of 39 or anything more extreme.",2010-08-10 20:35:37.0,6967.0,
1588,1511,0,"John, I got the joke.  LOL",2010-08-10 20:36:13.0,6967.0,
1590,538,3,They're not equivalent in the face of interventions.,2010-08-10 22:35:51.0,858.0,
1591,571,2,"Might as well use Pearl's word for ""directly setting [a variable's] value"": an intervention.",2010-08-10 22:36:53.0,858.0,
1592,1470,0,Awesome poster - thanks for linking to it!,2010-08-10 22:48:58.0,561.0,
1593,1517,0,You could learn lot of things but for useful suggestions you should ideally let us know what kinds of questions you like to ask and what kind of answers do you want to those questions. Some exemplars would really help as otherwise your question is too broad.,2010-08-10 22:57:41.0,,user28
1594,1517,0,My thought was that the tools I'm seeking are generic.  Standard Deviation is a generic tool.  What's another one like it?  The basic goal is to remove noise from data.  A bit later I will provide more about why Inventory Turn Rate is difficult for us and why I think more stats knowledge would help.,2010-08-10 23:15:43.0,857.0,
1595,1507,0,"No, you can't ignore how many students do the task.  If you have a 1000 students doing the same assignment and one of them reports a 1:1000 event then that's not terribly unusual.  If you have 200 students a year for the last 10 years and one reports a 1:2000 event it's not unusual... even a higher on like 1:20000, which is now 1:10 is not unusual.  This is similar to the Bulgarian lottery problem   
(of course this is just a theoretical argument... the probability here is far too low for it to matter)",2010-08-10 23:47:55.0,601.0,
1596,1518,0,"Linear Regression is precisely what I'm looking for to deal with one problem (forecasting sales from current customer inquiries).  It looks difficult, but I will read more.  I can't tell about Comparing Means.  Thank you.",2010-08-10 23:52:10.0,857.0,
1597,1518,1,"""Forecasting sales from customer inquiries"" is the type of example that you should have mentioned in the question as that enables a better response. Another tool that helps in this context is correlation coefficient (See: http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient). This concept is closely related to linear regression.",2010-08-11 00:37:42.0,,user28
1598,1518,1,"Likewise, you should indicate whether you are interested in individual sales--will this customer buy--or aggregate sales--how much will we sell this quarter--as those will require different models.",2010-08-11 03:22:30.0,485.0,
1599,1501,0,"I like your question (presumably after being edited).
It sounds like your question is not especially about datasets. Rather its about the resources and infrastructure for comparing reference results to a new implementation.",2010-08-11 04:53:40.0,183.0,
1600,847,0,How did I not answer your question? Read the JMP book. It's free.,2010-08-11 06:33:44.0,74.0,
1601,1523,1,What's the duplicate? I can't see anything obvious.,2010-08-11 06:43:03.0,159.0,
1602,1523,1,Pretty similar to this one I thought: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning,2010-08-11 07:25:58.0,74.0,
1603,1521,1,duplicate? http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning,2010-08-11 07:26:33.0,74.0,
1604,1523,1,"ok. I was searching for data mining, not machine learning. Please vote to close if you think it is a duplicate.",2010-08-11 07:36:37.0,159.0,
1605,1525,0,I'm just wondering whether the edited version of this question should retain the aspect of the original question regarding how the distinction between probabilities and proportions could be described in lay terms.,2010-08-11 08:26:37.0,183.0,
1606,1527,8,"+1 for proportion is empirical, and it is often a good estimate of a probability which is theoretical !",2010-08-11 08:38:14.0,223.0,
1607,944,1,Following on from http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions I've reopened this question.,2010-08-11 10:42:09.0,8.0,
1608,1160,0,Following on from http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions I've reopened this question.,2010-08-11 10:43:21.0,8.0,
1609,1523,0,"Hmm, so Data Mining == Machine Learning?",2010-08-11 10:47:42.0,251.0,
1610,1531,8,You need to change your question as many people won't (and shouldn't) run a random excel file on their computer. Pull out the formula from the file and ask a question about that.,2010-08-11 11:06:49.0,8.0,
1611,1502,1,"You're using hypothesis testing language but yet talking about a very exploratory, know-it-when-you-see-it approach @ the same time - so it's not clear what your goal really is for this part of your analysis. If you have hypotheses you're testing later (e.g. predicting cluster membership or using clust membership as predictor) then you can choose not to do things that will tempt bias there. But ""multiple comparison"" issues don't really figure into the exploratory problem you're describing. The viz cutoffs are just to help you see what's there - but your trust may still be misplaced.",2010-08-11 11:48:50.0,394.0,
1612,1517,0,"After learning the basics, it would be quite helpful if you use some tool i.e. Matlab, R, Excel to view your results in a more human understandable format.",2010-08-11 12:01:36.0,734.0,
1613,303,0,I guess you can put $$ now ?,2010-08-11 12:51:26.0,223.0,
1616,1521,2,If it's duplicate I can guess that data mining and machine learning are the same thing!,2010-08-11 14:16:22.0,339.0,
1617,1532,0,"This is a great paper and is consistent with my perspective on what data mining is and how it differs from statistics.  The catch is, it's from 1997!  Note an indictment of the paper or your recommendation, but the degree to which I have kept up with data mining.  It sounds like I need to grab a current book on data mining to catch up.",2010-08-11 14:19:58.0,485.0,
1619,1522,0,"I think points 2 and 3 are useful comments and consistent with what I see as the distinction between the two SA and DM.  I'm not so sure about your first point.  I've done statistical work where I was interested in improving understanding about causal relationships.  However, I've also done statistical work where the task was to take known relationships and develop models with the sole purpose of prediction but which shared non of the other features of ""data mining"".",2010-08-11 14:39:04.0,485.0,
1620,1526,0,"I can buy that.  Data mining is more exploratory application of statistical techniques.  Though, I don't think that distinction is enough.  When I'm doing EDA on my set of 100 observations from a designed experiment, I don't think anyone would call that data mining, would they?",2010-08-11 14:42:14.0,485.0,
1621,1523,1,"1) I'm not seeing the comp stat distinction. There's not much that statisticians do that doesn't require a computer. I suppose you mean computationally intensive procedures such as iterative solutions, etc? But then, these are also common in modern statistical work that is not data mining. 2) In my own (stats) work, I've been interested in model building for explanation and for prediction, depending on the problem-I wouldn't have considered that data mining. 3) I'm left with the conclusion that modern DM is a particular application of statistics, which I think is a fine conclusion.",2010-08-11 14:44:15.0,485.0,
1623,1509,0,"A really helpful link, thank you.",2010-08-11 14:59:37.0,667.0,
1625,1539,0,Shouldn't we apply the CLT to all finite-dimensional marginal distributions?,2010-08-11 15:26:01.0,650.0,
1626,1470,0,I'm sure they had lots of fun asking a dead salmon about its emotions!!!,2010-08-11 15:48:04.0,582.0,
1627,1551,0,"About N.B.2 - I think you're exactly right regarding the connotation of data mining and I had not made the connection to machine learning.  My training always emphasized the problems of over-fitting, spuriousness, and capitalizing on chance and as such I have been skeptical on DM--and still am, perhaps until someone actually tells me WHAT they are doing and HOW.  Thanks.",2010-08-11 16:05:00.0,485.0,
1628,1539,0,you asked for an intuitive answer :) also I choose not to bother you with the tricky mathematical part which is to show that the convergence for all t implies the convergence (in law) of the supremum... do you want me to complete the answer ?,2010-08-11 16:11:46.0,223.0,
1629,1551,1,"My only quibble on the ML/DM distinction would be that I think DM is broader.  For example, OLAP and related tools include mining technologies.  But these come from the database side of computer science rather than machine learning.  The role of commerce in shaping the ""meaning"" of data mining is hard to ignore -- it brings in elements of management sciences, operations research, machine learning and statistics as required.  It also gives the impression of something flimsy, but that's usually a problem for purists not practitioners.",2010-08-11 16:47:59.0,251.0,
1630,1539,0,"Dear robin girard, I think your answer is fine as it stands. Thank you!",2010-08-11 17:16:09.0,650.0,
1631,1532,0,"Heh, I kept the date out on purpose because I thought it would be amusing to notice the time span.  :)  The books by Michael Berry and Gordon Linoff are pretty good and will appeal to statisticians (for the broader exposure rather than learning statistical techniques).  If you want a sense of the fuzzy, ""enterprise"" side of this field, skimming through one of the books on a vendor product, like SAS's Enterprise Miner or SPSS's Clementine, may help.  I wouldn't recommend buying them unless you're going to work with the product itself.",2010-08-11 17:50:21.0,251.0,
1633,1338,1,"@Tal Galili: http://en.wikipedia.org/wiki/Love_means_never_having_to_say_you%27re_sorry. (My favorite is the line from *What's Up, Doc?*)",2010-08-11 19:23:58.0,877.0,
1634,1551,0,"@ars: I agree.  I was trying to say that a little by saying ""machine learning techniques are used in data mining"" (i.e. data mining is a super-set).  Your point about the commercial applications is also spot on.  Although someone in a commercial application now-a-days might refer to their work as something else (e.g. ""data science"").",2010-08-11 19:36:59.0,5.0,
1635,1551,0,"Right, I should have said I was trying to flesh out the differences, rather than actually quibble with what you wrote.  Apologies for the misdirection.  Good point on changing times and terms like the adoption of ""data science"".  Doesn't one of Gelman's books start with something like ""statistics is the science of data""?  So ""they're"" stealing from statisticians.  Again. :)",2010-08-11 20:39:16.0,251.0,
1636,1555,2,"You need to explain the data structure more, otherwise it is not clear how would a chi-square test apply. What do the values represent? Are the rows meaningful?",2010-08-11 21:45:06.0,279.0,
1639,1282,0,Just implemented this. Worked like a dream. Thanks so much!,2010-08-11 23:31:13.0,287.0,
1640,1031,0,"This seems to be a characterization of directional derivative of KL rather than of KL itself, and it doesn't seem possible to get KL divergence out of it because unlike the derivative, KL-divergence doesn't depend on the geometry of the manifold",2010-08-11 23:37:49.0,511.0,
1641,1304,0,+1 thanks for the link to the Feldman/Cousins paper.,2010-08-12 00:08:51.0,251.0,
1642,1572,0,"the priors are distributed normal N(b0, B0^-1). note above, that I use a precision (B0) of .00001, which is pretty diffuse.",2010-08-12 00:59:36.0,291.0,
1643,1572,1,"Actually, I think one other factor that matters is sample size. I am not sure if you can expect identical values with a sample size of just 15 odd points. With only 15 points there is not enough information in the likelihood to outweigh the prior. In other words, the likelihood is not 'concentrated enough' which suggests that when you integrate out the parameters you may get a different mcmc marginal likelihood. Does that make any sense? You can do a small simulation to check the above.",2010-08-12 01:13:30.0,,user28
1644,1573,0,"your paper will take time to digest. it looks like the -logLik changes fairly significantly if I changed the number of digits in the prior precision. MCMCpack doesn't let me specify other priors (aside from uniform), so I'm going to try another package. btw, what is a ""disjoint support?""",2010-08-12 01:55:20.0,291.0,
1645,1573,0,"On second though, my paper may not be so important here. But the point about the priors is important.  Look at the differences in the priors at the empirical mean of the data.  If that difference is large, that could explain your problem.",2010-08-12 02:10:24.0,319.0,
1646,1573,0,"As for the paper, disjoint support means there is no region to which both hypotheses assign positive probability.  i.e. the alternative really is an alternative.",2010-08-12 02:12:21.0,319.0,
1647,1573,0,"as I changed the precision of the prior from .01, .001, .0001, .00001, .000001, the -logLik were: -0.85, -2.00, -3.15, -4.31, -5.46 respectively. All the estimates (mean of mean, sd; sd of mean, sd) were the same at least to the 8th digit. (i used mean of 0.38 ~ the sample avg)",2010-08-12 02:23:19.0,291.0,
1648,1412,0,BTW Brian D. Ripley thinks it is a problem too: https://stat.ethz.ch/pipermail/r-help/2006-December/122353.html,2010-08-12 02:27:04.0,196.0,
1649,1573,0,"@John Why would the priors matter as sample size goes to infinity? Asymptotically, the likelihood function 'collapses' (technically, a dirac delta function) at the true parameter vector. Thus, it seems to me that the effect of the prior on marginal likelihood should weaken as sample size increases and should vanish asymptotically. Is the above intuition not correct?",2010-08-12 03:12:07.0,,user28
1650,1525,10,"If you eat Hamburgers _every_ Tuesday, the probability of you eating a hamburger in any given week is 1.",2010-08-12 03:28:16.0,776.0,
1651,1573,2,"it looks this problem has been widely known since 1939, called Jeffrey-Lindley paradox. Gelman's book (p185 & 250) also says uninformative priors w/ Bayes Factors are a no-no. Basically, they say that formulating point null hypotheses are stupid, and tests should be constructed from the posteriors. http://www.artsci.uc.edu/collegedepts/economics/research/docs/Wppdf/2009-01.pdf",2010-08-12 04:50:06.0,291.0,
1652,1581,0,"Thanks for the answer. I had these thoughts.
(a) Multicollinearity: I agree. Wihtout it, the coefficients should not change. (b) Is it interesting? I actually think that the sign flipping can have interesting theoretical interpretations in some instances; but perhaps not from a pure prediction perspective. (c) Residualisation: I'd be keen to hear what other people think of this approach.",2010-08-12 06:05:59.0,183.0,
1653,1581,0,"I'm not sure if multicollinearity could be interesting. Say you had some outcome `O`, and your predictors are `Income` and `Father's Income`. The fact that `Income` is correlated with `Father's Income` is intrinsically interesting, but that fact would be true no matter the value of `O`. That is, you could establish that `O`'s predictors are all collinear without ever collecting your outcome data, or even knowing what the outcome is! Those facts shouldn't get especially more interesting once you know that `O` is really `Education`.",2010-08-12 06:25:19.0,287.0,
1654,1581,0,"I'm suggesting that the suppressor effect can be theoretically interesting, of which presumably multicollinearity provides a starting point for an explanation.",2010-08-12 06:35:33.0,183.0,
1655,1559,1,"Hmmmm... maybe you should have a look at http://en.wikipedia.org/wiki/Percentage 1 and 100% ARE the same, as are 0.35 and 35% or 2.24 and 224%.",2010-08-12 06:42:33.0,582.0,
1657,1413,0,"It is unfortunate that nobody could chime in with a package where this is easy to do.  However, it turned out that adding the appropriate functions to lme4 wasn't as difficult as I had feared.  Now the only challenge that remains is to find the ""correct"" link function (see: http://stats.stackexchange.com/questions/1583/appropriate-link-function-for-2afc-data).  So I'll accept ars' answer, but someone may want to ask this again in the future since next time the answer may be different.",2010-08-12 07:09:59.0,196.0,
1658,1582,0,"Thanks for the suggestion to explore ridge or PCA regression.
Just a side point regarding your comment ""if your variables are positively correlated, then the coefficients will be negatively correlated leading to sign reversal."": positively correlated predictors do not typically lead to sign-reversal.",2010-08-12 07:11:15.0,183.0,
1659,1432,0,"There are elements to this question that remain unanswered, e.g. the nature of the ""pearson"", ""working"",""response"", and ""partial"" residuals, but for now I will accept Thylacoleo's answer.",2010-08-12 07:12:16.0,196.0,
1660,1335,0,"I'm accepting this answer because it solves (what I think) is the harder of the problems I was looking for a solution to - picking a particular combination out without calculating the preceding values.  Unfortunately, it is still very slow.  Perhaps as mentioned here and elsewhere a binary search would help speed things up.  Perhaps the best approach is to have one thread generating the combinations stepwise as in mbq's answer and another thread reading them off and testing them.",2010-08-12 07:15:40.0,196.0,
1661,1582,0,"Sorry, that's a botched one line explanation written in haste.  Fixed now, thanks.",2010-08-12 07:19:53.0,251.0,
1662,1586,0,Good point. All the examples of Simpson's Paradox apply to categorical variables. Is the concept of a supressor variable the numeric equivalent?,2010-08-12 07:55:46.0,183.0,
1663,967,0,I guess Val Harian is not a statistician if he is not kidding... what is a sexy job ? for me it is like the sitation with the sword of the century... fun but a bit trivial :),2010-08-12 07:58:10.0,223.0,
1664,1585,0,+1 for this paper ! my reference for the link between thresholding and penalized estimation was http://www.google.fr/url?sa=t&source=web&cd=1&ved=0CB0QFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.8.6694%26rep%3Drep1%26type%3Dpdf&ei=X6xjTILlHsiU4gashYThCg&usg=AFQjCNE2kNFinNcuvG4qkBBPQAi2omY-9A&sig2=D0NgkuN_KfgV8ZdzADKcxg,2010-08-12 08:11:28.0,223.0,
1665,435,7,"This one reminds me of the recent bailout in the States, where they just made up 700 billion number - they said they just wanted a really large number. :)",2010-08-12 08:53:40.0,144.0,
1668,1493,0,"yes , they are iid",2010-08-12 09:49:25.0,852.0,
1672,1595,0,"@csgillespie: Thanks for adding the python tag. :) I wanted to, but as a new user I couldn't.",2010-08-12 10:48:58.0,890.0,
1673,1595,0,You're welcome!,2010-08-12 10:50:18.0,8.0,
1674,1569,0,"+1 I like this interpretation! could you clarify "" p below e ""? why do you take small e ? you say ""the probability of making the opposite mistake is"" it is an upper bound  or exact probability? If I remember, this type of approach is due to Chernoff, do you have the references (I find your first reference is not clarifying the point :) ) ?",2010-08-12 11:08:21.0,223.0,
1675,1383,0,"Thank you for updating! you said: ""I'm talking about universal compression obviously"" even if things are obvious within your community you maximise the chance of having original instructive answer by recalling things we few words or a link.",2010-08-12 11:31:11.0,223.0,
1676,1383,0,"My personal view is that we should not mix comments and question. Comment are made to give feedback or to ask questions about the question, most often it is a dialogue between two people. The question should be something clear, easy to read and understandable by as much people as possible... it's you explaining your problem to get an answer.",2010-08-12 11:36:18.0,223.0,
1678,1249,0,"@srikant an upper bound is needed here. The last sentence just states that an upper bound on the desired quantity will be used for proving the minimum error in the testing problem, as @robin explained.",2010-08-12 13:13:54.0,168.0,
1679,1573,1,"Srikant: The problem is that you have *two* posteriors becoming more concentrated and you're looking at their relative fit.  Either prior alone would become irrelevant in the limit, but the priors continue to impact the ratio of posterior model probabilities.",2010-08-12 13:56:30.0,319.0,
1680,1555,0,"@Aniko, the data structure is a frequency structure where each column represent a specific treatment and the rows represents result broken up into bins.",2010-08-12 14:31:34.0,559.0,
1681,1598,16,Inserting shameless plug for Rcpp :),2010-08-12 14:37:32.0,334.0,
1682,1546,0,"i am trying to measure whether or not i am getting more visitors after i turn on SEO, and if possible to quantify that ""lift"".  it's important to compare before and after on a per-account basis, and then say something about the comparison in aggregate.

it is possible that there is a ""ramp up"" period after turning on SEO, so i'm not sure that just looking at the days where SEO is on and the days where it is off is sufficient (maybe an average within a window before and after turning it on?)",2010-08-12 14:44:45.0,125.0,
1683,1589,0,"It is an interesting idea to fit the previous dataset here, it would tell me the extent to which the ""problem"" I'm worried about is solved by using these functions.  Also, I may fit existing psychometric data individually by subject using glm and produce the output curves on the probability scale.  Perhaps one will pass the eyeball test and the other won't.  I'll update the question when I know more.",2010-08-12 14:49:59.0,196.0,
1684,1589,0,"ASIDE: I'll grant that the model should match the process.  In the case of modeling successes and failures of the robot arm you propose, values past its maximum movement are non-nonsensical, consequently they are past the point of observed values; so if I did that I'd be committing another error, attempting to extrapolate beyond the range of observed values.",2010-08-12 14:57:40.0,196.0,
1685,1589,0,"ASIDE continued: However, in psychophysics we have a system that starts off not working at all, gradually starts being able to process the information provided to it, and then asymptotes near 100% when the information is clearly available.  Here I may have presented stimuli that are too hard to discriminate (beyond its range on the lower bound) or presented stimuli that are too easy to discriminate (beyond its range on the upper bound).  Here it is reasonable ask what the system will do at those extreme values.",2010-08-12 15:03:25.0,196.0,
1686,956,0,"We sort the test results in increasing order by their un-corrected original p-value, then, iterating over the list, calculate the FDR expected if we were to reject the null hypothesis for this and all tests prior in the list, using the B-H correction using an alpha equal to the observed, un-corrected p-value. We then take, as what we've been calling our ""q-value"", the maximum of the previously corrected value (FDR at iteration `i - 1`) or the current value (at `i`), to preserve monotonicity. Does this sound like the procedure you described in your second paragraph?",2010-08-12 15:46:43.0,520.0,
1687,1603,0,"Given that $z^2$ can be quite large, ignoring higher order terms in the Taylor expansion is pretty fishy.",2010-08-12 15:48:20.0,89.0,
1688,1603,0,hmm on second thoughts I may have my inequality reversed. The bound I computed is a lower bound and not a upper bound. Please vote it down to oblivion. :-),2010-08-12 15:53:35.0,,user28
1689,1605,1,"I believe that the sums need to be normalized (e.g., use the mean score) for the distribution to tend to normality.",2010-08-12 16:05:45.0,,user28
1690,870,0,"@robin Many thanks for your comments. I apologize for my confusion of terminology. I have updated the question to include a more complete description of our correction procedure, in the hopes that it provides clarification. I have also updated the q-value link; thanks for pointing me to that.",2010-08-12 16:09:26.0,520.0,
1691,1588,0,"I like his 2008 paper on FDR. I don't use R, though :(",2010-08-12 16:21:50.0,795.0,
1692,1560,0,"I guess I'm wondering *why* the James-Stein estimator isn't widely used. Is it subsumed by these other techniques, or are the conditions of the theorem not met in practice?",2010-08-12 16:23:45.0,795.0,
1695,1160,0,Now you tell me :D  (thanks anyway),2010-08-12 16:56:01.0,253.0,
1696,1587,0,"I think you have the interpretation of the R code correct (to be sure, hadcru.mcmc.zero has no slope parameter, it is just a mean model). I don't understand why the prior should be constrained just to the positive values though (I also wouldn't know how to code or write it down mathematically either; I want a diffuse prior but ignore negative values?). Shouldn't the H1 model include all values of the slope? Maybe I need to test whether H0: \\theta <= 0 and H1: \\theta > 0, in that case, I wouldn't know how to code it either :)",2010-08-12 17:07:31.0,291.0,
1698,1605,1,"Yes, that is correct. In my example I assumed the classes would have the same number of students, which is not realistic. Thank you.",2010-08-12 17:22:06.0,666.0,
1699,1249,0,Link to the paper http://math.unice.fr/~baraud/publications/min-rev1.pdf,2010-08-12 17:30:09.0,223.0,
1700,1285,0,"Thanks, that is what I needed!",2010-08-12 18:12:39.0,,MarcO
1702,1610,0,"Terminology is a bit vague. I changed error to typeI-errors and typeII-errors. Hope that is fine. Also, your question should be community wiki as there is no correct answer to your question.",2010-08-12 20:00:22.0,,user28
1703,1610,0,"@Srikant: in that case, we should make questions like this cw as well: http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english.",2010-08-12 20:01:51.0,5.0,
1704,1610,0,"Older literature calls H2 the null hypothesis, H1 the alternate hypothesis, then it's natural to call type i error as the error of mistakenly accepting Hi hypothesis",2010-08-12 20:03:10.0,511.0,
1705,1610,0,@Shane: I will abstain from commenting on your point as the present top voted answer is mine (conflict of interest). I will go with what the community feels is appropriate.,2010-08-12 20:04:53.0,,user28
1706,1610,0,@Shane you're totally right I think the question you mentionned should be community too.,2010-08-12 20:06:40.0,223.0,
1707,1610,0,"The tags in this case should be type-i-errors and type-ii-errors as the dashes replace spaces. A terminology tag would help people to find questions that ask about definitions of words, like this one. Finally, I'm not making this community wiki as there is a correct answer that I will be accepting - whatever technique I feel best helps me to remember the difference.",2010-08-12 20:07:02.0,110.0,
1708,1569,0,"Why do I take small e...hmm...that's what Balasubramanian's paper did, but now, going back to Kullback, it seems his bound holds for any e, and he also gives bound for finite n, let me update the answer",2010-08-12 20:07:21.0,511.0,
1709,1249,0,Can you give us the hypothesis you want to test ?,2010-08-12 20:08:37.0,223.0,
1710,1610,0,ok I will edit as per your request. Reg CW- fine with me as long as that makes sense to everyone else.,2010-08-12 20:10:04.0,,user28
1711,1610,4,"Honestly, perhaps the community wikiness of this question should be discussed on meta. I personally feel that there is a singular right answer to this question - the answer that helps me. However, that singular right answer won't apply to everyone (some people might find an alternative answer to be better). Personally, I want to give reputation to the person or people who help me with my problem, but if the community wants this to be community wiki, I can make it happen (but not without a discussion on meta first).",2010-08-12 20:11:46.0,110.0,
1712,1610,0,I started a thread here: http://meta.stats.stackexchange.com/questions/290/what-is-community-wiki.  I think that I agree with @Thomas; we should reward good answers on questions like these.,2010-08-12 20:16:14.0,5.0,
1715,1612,0,"I've never even thought of it pictorially before. Normally, thinking in pictures doesn't work for me, but I'll read that article and maybe this is a special case where it will help me.",2010-08-12 20:27:30.0,110.0,
1716,1614,0,Isn't that agism?  :),2010-08-12 20:33:46.0,5.0,
1717,1608,0,"The binning is unavoidable. Each bin represents a geometric region. I also use the Kruskal-Wallis test for continuous variables for the entire region, which was worked out fine. 

The rows are not ordered.",2010-08-12 20:42:55.0,559.0,
1718,1614,9,"How about ""once bitten, twice shy""? No funnier, but commonplace enough to remember. And no ageism required!",2010-08-12 20:54:07.0,174.0,
1720,1616,0,I kind of like that. I'm thinking this might work for me.,2010-08-12 21:42:13.0,110.0,
1721,1616,2,"it's sort of like how in elementary school kids would ask ""are you not not cool?""",2010-08-12 22:10:11.0,900.0,
1722,1569,0,"ok, we don't need small e (now called b, Type II error) to be small for bound to hold, but b=0 is the value for which the simplified (exp(-n KL(p,q)) bound matches the more complicated bound above. Curiously enough, lower bound for Type I error given 0 Type II error is <1, I wonder if <1 Type II error rate is actually achievable",2010-08-12 22:54:33.0,511.0,
1723,1619,0,"But you still have to associate type I with an innocent man going to jail and type II with a guilty man walking free. So in the end, it really doesn't get me anywhere.",2010-08-12 23:07:53.0,110.0,
1724,1616,0,"yes, now you just need to remember *which* hypothesis (null or alternate) :P",2010-08-12 23:17:20.0,511.0,
1725,1616,0,It's easier to remember that both types of errors deal with the null hypothesis. I'm seriously considering accepting this answer - I think this one helps me the most.,2010-08-12 23:33:33.0,110.0,
1726,1609,0,"Good references, but I don't understand your last comment about a parametric distribution. Splines are usually used for modelling the conditional mean of Y|X and are not directly to do with the distribution of either Y or X. I know you can fit spline-based density estimates, but I doubt that is what Henry was asking about.",2010-08-12 23:45:04.0,159.0,
1727,1616,6,Actually it's the alternate hypothesis in both cases,2010-08-13 00:10:23.0,511.0,
1728,134,1,Re-opened following discussion at http://meta.stats.stackexchange.com/questions/276/should-we-unclose-computing-questions,2010-08-13 00:31:38.0,159.0,
1729,1603,0,"by pigeonhole principle, $\\bar{P} >= 1/(n+1)$, thus I think your proposed upper bound based on the mode is going to be, asymptotically, the same as the trivial upper bound.",2010-08-13 01:52:33.0,795.0,
1731,1582,0,Great point about the importance of causal mechanisms.,2010-08-13 02:24:18.0,183.0,
1732,1446,4,Every answer to my question has provided useful information and I've up-voted them all. But I can only select one answer and Srikant's provides the best overview IMO.,2010-08-13 02:25:01.0,159.0,
1734,1609,0,"Yes, the latter, but due to meandering and unrelated thoughts, not meant for ""publication"".  I was thinking about density estimation because of some other question and, in light of this question, was working through explaining spline methods to *myself*.  Strangely, I remember thinking what I typed was cogent -- because of what was in my *head*.  Ridiculous.  :-/  Thanks for the catch; impressed that it occurred to you, though I shouldn't be based on your quality answers.",2010-08-13 02:44:42.0,251.0,
1735,1616,0,"Yaroslav - I thought the same.  But well, the researcher doesn't usually want the H0 (H NULL) to be true...",2010-08-13 03:33:10.0,253.0,
1736,1610,0,I just wanted to say I loved your question - and I love the answers even more!,2010-08-13 03:35:25.0,253.0,
1738,1579,2,The rule of thumb there is highly useful. Thanks for that.,2010-08-13 03:38:38.0,776.0,
1739,1619,5,"+1, I like.  @Thomas: Given an ""innocent until proven guilty"" system, you could think of type I as a primary error to avoid (imprisoning the innocent) and type II as a secondary error (guilty go free).",2010-08-13 03:42:03.0,251.0,
1740,1444,7,I've summarized some of the answers plus some other material at http://robjhyndman.com/researchtips/transformations/,2010-08-13 04:28:23.0,159.0,
1741,151,7,"Much of the field of robust statistics is an attempt to deal with the excessive sensitivity to outliers that that is a consequence of choosing the variance as a measure of data spread (technically scale or dispersion).
http://en.wikipedia.org/wiki/Robust_statistics",2010-08-13 05:15:29.0,521.0,
1742,1619,0,"That's actually quite deep ars, thanks! My way of remembering was admittedly more pedestrian: ""innocent"" starts with ""I"".",2010-08-13 05:32:10.0,830.0,
1743,1622,0,"When you say that the Sharpe ratio is the mean divided by the sample standard deviation... ""Up to a constant factor (sqrt(n), where n is the number of observations)""... what does this last part mean?  Is it really the mean divided by the standard error of the mean?",2010-08-13 05:44:29.0,196.0,
1744,1590,1,Consider selecting a more informative question title; someone asking a similar question in the future is unlikely to check the answer under a vague title than under a specific one.,2010-08-13 05:58:54.0,196.0,
1746,1570,0,You don't need assumption about 2x2 table but rather kxm table.,2010-08-13 06:06:22.0,419.0,
1748,1570,0,Of course. Now fixed.,2010-08-13 06:32:31.0,159.0,
1749,1456,0,Some correction should be made to formula above. It is var(log(OR)) not var(OR).,2010-08-13 06:34:43.0,419.0,
1750,1456,0,"@Wojtek: fixed, thanks.",2010-08-13 06:44:41.0,251.0,
1751,1587,2,"The Bayes factor is the ratio of the marginal likelihood assuming H1 to the marginal likelihood assuming H0.  In this case, the alternative hypothesis H1 is that the slope is positive, so your prior fo H1 must exclude negative values for the slope parameter as H1 says they are implausible a-priori.  One way of achieving this might be to use an exponential prior, which is strictly positive, for the slope and a Gaussian prior for the intercept.  You could then implement a ""negative slope"" null hypothesis by flipping the data upside-down and using the same model.

HTH",2010-08-13 07:23:13.0,887.0,
1752,1598,0,curious if you've tried PyMC and how the performance compares (relative to python/C) for your models.,2010-08-13 07:31:00.0,251.0,
1753,1607,0,"If we apply t-tools to Box-Cox transformed data we will get inferences about the difference in means of the transformed data. How can we interpret those on the original scale of measurement? (The mean of the transformed values is not the transformed mean). In other words (if I'm correct), taking the inverse transform of the estimate of the mean, on the transformed scale, does not give an estimate of the mean on the original scale.",2010-08-13 07:39:21.0,339.0,
1754,1452,0,Is there a page where I can find all those latex symbols (including begin{cases}) ?,2010-08-13 08:45:33.0,339.0,
1755,1621,0,"Fisher's information gives lower bound in parameter estimation. It is a natural metric because it roughly says something like ""in this direction the difficulty of my problem cannot decrease more than that"".  What you call generalization bounds are upper bounds ? do you want to know the performence of the method that use Fisher metric (the large body you mention is a good list)? sorry but I don't really get the question :) can you reformulate that point ?",2010-08-13 08:45:45.0,223.0,
1756,1637,9,One point: t-tests use the t distribution not the Z distribution,2010-08-13 10:12:21.0,183.0,
1758,1444,2,excellent way to transform and promote stat.stackoverflow !,2010-08-13 11:49:16.0,223.0,
1759,1598,0,"@ars: In the case above, each iteration (of the 10^8 iterations) involved solving 5 ODEs. This really had to be done in C. The rest of the code was fairly simple and so the C code was straightforward. My application was non-standard and so PyMC wasn't applicable - also it was ~2 years ago.",2010-08-13 12:33:10.0,8.0,
1760,452,0,Would you put a link to Angrist and Pischke.,2010-08-13 13:37:18.0,8.0,
1761,1608,0,"@Elpezmuerto If each row represents a region and they are not ordered, then how are you doing a Kruskall-Wallis test? That test only applies to ordered outcomes. Please, please, please add a compelete description of your problem by editing your original question.",2010-08-13 13:50:46.0,279.0,
1762,1651,1,"I hope I did not mangle the eqns while converting them to latex. You can use latex on this site. See: http://meta.stats.stackexchange.com/questions/218/tex-processing-for-stats Could you also please clean up the question a bit. For example, the first line is a bit odd, what do you mean by "" I know the value of $y_{ij}$, but it could be more than that value."" etc.",2010-08-13 14:32:35.0,,user28
1763,1651,0,ooh nice! Thank you very much! :),2010-08-13 14:34:58.0,,Kim
1764,1639,2,"I can look up a citation, but this is easy enough to to test empirically.  F from an ANOVA with two groups is exactly equaly to t^2 and the p-values will be exactly the same.  The only reason it wouldn't be equivalent in the case of unequal variances is if you apply a correction.  Otherwise, they are the same.",2010-08-13 14:54:42.0,485.0,
1765,1621,0,Let's say that Fisher's Information Matrix gives our Riemannian metric tensor. It lets us find arclength of any curve by integrating. Then you define the distance between p and q as the smallest arclength over all curves connecting p and q. This is the distance measure I'm asking about. Same with volume.,2010-08-13 15:19:41.0,511.0,
1766,1653,0,Would be interested to know how to fit repeated measures ANOVA models using lm.,2010-08-13 15:51:49.0,702.0,
1767,1653,0,"The issues of coding categorical variables, the equivalence of regression and ANOVA models, and regression coding for repeated measures are described in this article. http://dionysus.psych.wisc.edu/Lit/Topics/Statistics/Contrasts/Wendorf2004a.pdf Here's the citation... Wendorf, C. A. (2004). Primer on multiple regression coding: Common forms and the additional case of repeated contrasts. Understanding Statistics 3, 47-57.",2010-08-13 16:11:17.0,485.0,
1768,1648,0,"Thanks Shane, great answer! Well, ""the others"" from my field often use SPSS, so they use Kolmogorov-Smirnov (if they check normality at all), though IMHO the Lilliefors' test is a better choice when the data is gathered from a sample (when parameters are unknown). I was taught that Shapiro-Wilk's is appropriate for small samples, and just wanted to get more info about ""small samples normality tests""...  BTW, I use nortest in R! =)",2010-08-13 16:11:51.0,1356.0,
1769,1646,0,"Not to start a sub-question, but I've never understood the utility of intra-class correlation; it strikes me that the scenario you describe can be adequately captured by talking about (1) the variance of the team means and (2) the mean within-team variance.",2010-08-13 16:15:00.0,364.0,
1770,1658,0,Do we vote for the first one or the second one :) ?,2010-08-13 16:35:07.0,223.0,
1771,1635,0,"yes, my question was poorly worded and ambiguous. to your last point: there are well known techniques for comparing Sharpe given paired observations. My Q is for independent observations, applicable for:
1. non-overlapping times, or
2. the case where summary statistics are provided by a third party (e.g. in a fund prospectus).",2010-08-13 16:46:03.0,795.0,
1772,1647,0,"I agree. I performed a quick test of the A-D test, Jarque-Bera, and Spiegelhalter's test (1983), under the null, with sample size 8, repeating 10,000 times. The A-D test maintains nominal rejection rate, and gives uniform pvals, while J-B test is terrible, Spiegelhalter is middling.",2010-08-13 17:18:29.0,795.0,
1773,1607,0,"@gd047, some tests assume normality of the distribution of the mean, not the data.  t-test tends to be pretty robust w.r.t to underlying data.  You're right though -- with post-transformation tests, results are reported after inverse-transforming, and interpretation can be very problematic.  It comes down to how ""un-normal"" your data is, can you get away without transforming or applying, say, a log transform which is easier to interpret.  Otherwise, it's contextual on the actual transformation and domain and I don't really have a good answer.  Might be worth asking to see what others say?",2010-08-13 17:42:32.0,251.0,
1775,1452,2,@gd047: here's a nice reference: http://elevatorlady.ca/doc/refcard/expressions.html,2010-08-13 17:45:12.0,251.0,
1776,1598,0,"@cgillespie: curiosity sated.  sounds interesting, thanks. :)",2010-08-13 17:46:37.0,251.0,
1777,1661,0,"What is the question you want to answer? Do you want to compare males to females, or wing length to tail length, or something else? And why are you restricted to a univariate test?",2010-08-13 17:50:24.0,279.0,
1778,1662,0,I'm testing individual component means,2010-08-13 18:04:05.0,,Matt
1779,1662,0,I'm restricted to unvariate t-tests.,2010-08-13 18:05:27.0,,Matt
1780,1573,0,"i'm going to make this the best answer, since it provided a good starting point of study. Thanks for the discussion guys!",2010-08-13 18:06:57.0,291.0,
1781,328,0,"@James: Alternatively, this might argue for more education in the field, not less...",2010-08-13 18:11:14.0,5.0,
1782,1662,0,"What do you mean by individual components? What are these 'components'? Ideally, you should edit the question to clarify so that others can also post appropriate answers. See also Aniko's comments to your question.",2010-08-13 18:13:08.0,,user28
1783,1651,0,"I don't know of an R procedure that would do it, however there is a Stata module CENSORNB that does exactly this. Perhaps looking at the code might give you ideas.",2010-08-13 19:23:40.0,279.0,
1784,1642,0,"Interesting idea and it makes sense. However, the exam that I'm studying for uses Type I and Type II errors.",2010-08-13 20:07:47.0,110.0,
1785,1664,0,"i need to compare tail length male vs tail length female and in a separate test compare wing length male vs wing length female.

Ho: X1-male = X1-female
H1: not equal

and

Ho: X2-male = X2-female
H1: not equal

I thinking 2 separate univariate t-tests?",2010-08-13 20:45:05.0,,Matt
1786,1664,0,That's what my answer describes.,2010-08-13 20:58:11.0,8.0,
1787,1569,0,"Actually a much easier to understand reference for this is Cover's ""Elements of Information Theory"", page 309, 12.8 ""Stein's Lemma""",2010-08-13 21:25:20.0,511.0,
1788,1664,0,"In R:

##X1 TEST

t.test(T6.11.dat[1],T5.12.dat[1])

RIGHT?",2010-08-13 21:28:04.0,,Matt
1789,1669,0,"Bookmarked. I wish they offered PDFs, but you can't go wrong with NIST's documents.",2010-08-14 00:47:50.0,110.0,
1790,1676,0,"Debating making this Community Wiki - I really want to see what the first couple of answers are, first, though. If someone can provide a list of ""must have"" R packages, I'll probably accept that. If it begins to turn into a list of suggestions (or someone has an otherwise good reason), I'll CW this thing up.",2010-08-14 01:40:13.0,110.0,
1791,1677,0,Good call. I use SO and I was unaware that question even existed. It's good to have a cross-linking for times when questions might appear on multiple exchanges.,2010-08-14 01:46:17.0,110.0,
1792,1676,2,I would say that this should be community wiki.,2010-08-14 02:13:08.0,5.0,
1793,1676,7,Possible duplicate: http://stats.stackexchange.com/questions/73/what-r-packages-do-you-find-most-useful-in-your-daily-work/,2010-08-14 02:14:21.0,5.0,
1794,452,0,"Angrist, Joshua D. and Jorn-Steffen Pischke. 2009. Mostly Harmless Econometrics: An Empiricist's Companion. Princeton University Press: Princeton, NJ.",2010-08-14 02:40:04.0,401.0,
1795,1676,0,"I agree it should be CW as the answer depends entirely on what you want to do. I load a lot of forecasting and time series packages, but others may not need them at all.",2010-08-14 05:13:41.0,159.0,
1796,1594,0,"Thanks, your post looks really nice, and its what I'd expect from
a statistician too. I'd appreciate the explanation of the choice
of logistic regression though :)
But unfortunately it doesn't help me at all. Like, your function
fitting is LMS, right? Even with logit, does it really minimize
the (abs) entropy? Also all the common models in data compression are discrete -
eg. a bit run model.
Anyway, I guess I'd really have to ask a few more specific questions
instead.",2010-08-14 09:15:01.0,799.0,
1797,1676,0,"Making community wiki, and this is a really close duplicate of the other. Is the searching acting up? I searched for ""r packages"" with and without quotes and didn't see that...I wonder if it had something to do with the downtime last night.",2010-08-14 09:41:38.0,110.0,
1798,1594,0,"It's usually fitted using maximum likelihood estimation.  You really need to say some more about what might have generated the binary data to progress any further.  Also is there a specific data set?  I see you linked to one, but was that just randomly chosen or do you have another specific one in mind?",2010-08-14 10:51:58.0,702.0,
1799,154,0,Looks very interesting indeed!,2010-08-14 11:29:43.0,702.0,
1800,1594,0,"Its good then if its maximum likelihood, but it turns into bruteforce in real cases which is not very practical. As to ""specific data"", I can post some (eg from a text compressor), but the whole point is that we don't know a good model for it. So instead we have to ""mix"" predictions of a few not-so-good models.",2010-08-14 11:46:12.0,799.0,
1801,1676,0,This one's _definitely_ for SO!,2010-08-14 11:58:19.0,1356.0,
1802,1594,0,posted an example,2010-08-14 12:11:31.0,799.0,
1803,1676,0,"I disagree that it's for SO, especially considering the discussions on meta that supporting tools for statistical analysis (including software) is on-topic....",2010-08-14 12:25:54.0,110.0,
1804,1684,1,"No disrespect, but that strikes me as entirely subjective. I have used latex for since the late 80s and R since the 90s yet I see no good reason to move away from embedding eps files. Everybody's mileage will differ here, and Thomas should maybe look at Task Views for particular problem domains.",2010-08-14 13:04:26.0,334.0,
1805,63,2,"Judgement is a good word, since all we can ever observe is correlation. All that experiments and/or clever statistics can do is allow us to exclude some alternative explanations for what could have caused an effect.",2010-08-14 18:18:08.0,198.0,
1806,1600,4,"I have done a lot of number crunching, one website and few administrative scripts in R and they are working quite nice.",2010-08-14 19:40:50.0,88.0,
1807,501,0,"@Thylacoleo By comparing the error of a full set and the set with some feature knock-outed, for instance by shuffling its values across objects. Read about RF importance to learn more about the concept.",2010-08-14 19:49:45.0,88.0,
1808,1676,0,"While not being moderator, I would vote to close as a duplicate; we don't need more than one R package feed.",2010-08-14 19:56:33.0,88.0,
1809,1664,0,@Matt: that's correct,2010-08-14 21:42:37.0,8.0,
1810,1691,0,On second thoughts my answer may not make much sense. I will leave it up in case it gives you some ideas.,2010-08-14 22:15:33.0,,user28
1811,1586,1,"At the page most are categorical but there is an figure at the top of the page one could imagine a variety of derivatives of... Specifically, if you have a categorical and continuous predictor then the continuous predictor could easily flip sign if the categorical one is added and within each category the sign is different than for the overall score.",2010-08-14 23:07:11.0,601.0,
1812,538,0,"Those Bayesian networks are equivalent in a sense that given data sampled from one of them, you can't tell which one it was",2010-08-14 23:52:21.0,511.0,
1813,1678,0,+1 for lattice.,2010-08-15 00:30:18.0,389.0,
1814,596,0,"EM is simply one way of maximizing likelihood when your log-likelihood function involves logs of sums. This can happen when you have latent variables either in unsupervised or unsupervised models. So for instance, if you fit a CRF and assume some of the outputs are unobserved, it would be suitable to use EM. But you could also maximize that (conditional) likelihood by gradient descent.",2010-08-15 06:48:34.0,511.0,
1815,1699,1,The link you gave points to your own machine - 127.0.0.1,2010-08-15 07:00:41.0,8.0,
1816,1699,0,@csgillespie Fixed.,2010-08-15 08:41:40.0,88.0,
1820,1709,0,It is not clear what you want. Do you want a visualization of the 5 points you collected or a plot of the theoretical distribution that you feel generated the data? Please clarify the question so that an appropriate answer can be provided.,2010-08-15 15:08:06.0,,user28
1822,1709,0,"I'm not sure I understand. If you know they're distributed normally then... well they'll be distributed normally! If you don't know how they are distributed then plot your data and think of which distribution seems to be the more likely to fit them. Also, do you have any knowledge at all about the process you are analysing? Could you infer the distribution from that?",2010-08-15 15:24:36.0,582.0,
1823,1709,0,@Srikant: I tried to clarify the question. Better?,2010-08-15 15:27:11.0,198.0,
1824,1709,0,"@nico: I don't know how the data are distributed. I mentioned the normal distribution only to help you understand the problem. Since we are trying to find out what process generates the distribution, we unfortunately don't know anything a priori about the distribution. I would like to visualize the data this way so that we can better identify patterns in the distribution and how the distribution changes with experimental perturbations.",2010-08-15 15:31:45.0,198.0,
1826,1708,2,"Can you write some crash-introduction of what you want to do (including Eigenstrat step)? People are rather lazy, and are much more prone to answer questions that don't require too much googling just to be understood.",2010-08-15 16:12:09.0,88.0,
1827,1706,0,That is not finding the tangency portfolio. It is just discretizing the returns space and plotting points of min variance on the graph.,2010-08-15 18:47:03.0,862.0,
1828,1708,0,"For example, are you working with SNP arrays?",2010-08-15 18:57:02.0,8.0,
1829,1716,0,"Thank you for the suggestions. These are what I'd normally do with this kind of data. However, as I hope to explain in my edit, I really want to try something different.",2010-08-15 20:07:55.0,198.0,
1830,1717,0,"Thank you very much. This looks very promising! What if the data is not normally distributed? How bad is it to still use $\\kappa=3-L$ ? Also, I guess if I wanted to have points with equal weight, I'd just disregard the first one, right?",2010-08-15 20:09:39.0,198.0,
1832,1621,0,"So, just as an example, Rodriguez gets a significant improvement by using ""information volume"" as measure of model complexity, but surprisingly I can't see anyone else trying this out",2010-08-16 00:54:39.0,511.0,
1834,1720,0,Inference is conditional on the BC parameter $\\lambda$ -- does this have an easy interpretation on the original scale?  I think the usual course is simply to report it that way and leave it at that (usually resting on some result about asymptotic equivalence which may not apply as generally).,2010-08-16 01:06:16.0,251.0,
1836,1702,0,+1 the Marsland book is quite good and filled a big gap in the existing selection of ML books.,2010-08-16 03:11:15.0,251.0,
1837,1714,0,I can only second that... from what I've seen can you select data with the mouse in one projection while looking how the selected subset looks like in another projection.,2010-08-16 06:22:04.0,961.0,
1838,1681,0,deviation from the mean of what? what are the assumption on your random variable or what type of bound do you need (exponential?). I think your question would be easier to answer if you could formalize it a bit more and if you add some details. take a look at http://en.wikipedia.org/wiki/Large_deviations_theory,2010-08-16 06:37:35.0,223.0,
1839,1717,0,"The choice of $\\kappa$ has to do with the properties of the *transformed* points (if you want to non-lineraly transform your random variables). A proper choice of $\\kappa$ can also scale the points towards or away from the mean. You can safely select $\\kappa$ so that $0\\lt L+\\kappa \\lt k $ where $k$ the kurtosis of your distribution. Set $\\kappa=0$ to disregard the point on the mean, without even changing the formulas.",2010-08-16 06:43:50.0,339.0,
1840,1720,0,Thank you. Maybe because the sample (from a population that I think it should follow an approximately symmetric distribution) might just happened to be skewed by chance.,2010-08-16 07:10:32.0,339.0,
1841,1729,0,"Thanks for the tags Rob, I was unable to create the latex tag myself.",2010-08-16 08:42:16.0,913.0,
1842,1731,0,Thank you for the tips on writing a custom export and the useful functions.,2010-08-16 08:47:58.0,913.0,
1843,1730,0,"This works perfectly, an easy method of achieving exactly what I wanted.",2010-08-16 08:48:27.0,913.0,
1844,1719,0,Should be a community wiki,2010-08-16 09:32:58.0,8.0,
1846,63,0,Very good comment about the symmetric/asymmetric relations. One also might claim that global warming causes piracy to increase.,2010-08-16 11:02:28.0,961.0,
1847,1725,0,"I've tried that series before, and that guy although he's awesome; he umms and umms and umms and umms a lot. It is really distracting",2010-08-16 11:21:58.0,59.0,
1848,1735,3,"A few suggestions:
1. Replace acronyms with complete words,
2. Elaborate a little more on what you are trying to achieve statistically.
Are you simply trying to determine which regression coefficients are larger or smaller?",2010-08-16 11:45:12.0,183.0,
1849,1710,2,"It's also a good example of the ecological fallacy (i.e., making inferences about the individual-level from group-level data).",2010-08-16 11:50:18.0,183.0,
1850,1736,7,Use Rpy to call R from Python ;-),2010-08-16 12:35:36.0,88.0,
1852,1709,0,Picking one of your actual experimental results randomly is a good way to illustrate a typical result and can be used for simulation as well (a'la bootstrap).,2010-08-16 13:44:05.0,279.0,
1853,1739,0,Thank You! currently looking into this.,2010-08-16 14:00:42.0,,humble Student
1854,1740,0,"Thank You! This worked, although I had to drop a comma in the call to data.frame.

    stats = ddply(
    .data = ords
    , .variables = .(Symbol,SysID,Hour) 
    , .fun = function(x){ 
        to_return = data.frame(
             s = sum(x$Profit)
            , m = mean(x$Profit)
        )
        return(to_return)
    }
    , .progress = 'text'
)",2010-08-16 16:06:37.0,,humble Student
1855,1739,0,fastest running correct answer,2010-08-16 16:37:33.0,601.0,
1856,1181,1,"This line: freq.ex<-(dpois(0:max(x.poi),lambda=lambda.est)*200) produces errors with some real world data, because the length of freq.ex won't match freq.obs on this line acc <- mean(abs(freq.os-trunc(freq.ex))).  I adapted this line to freq.ex<-(dpois(seq(0,max(x.poi))[seq(0:max(x.poi)) %in% x.poi],lambda=lambda.est)*200) but something still isn't quite right because goodfit produces warnings.",2010-08-16 17:15:34.0,196.0,
1857,1747,0,PS -- the real reason I included the code is to prompt responses from anyone in the programming realm.  This function is probably found (and more elegantly) in some 'VBA stats snippets' resource ... I should search on that.,2010-08-16 17:18:20.0,857.0,
1858,1527,0,"You change the viewpoint here. You could just as easily say, ""the proportion of heads on any one flips is .50"". 

I contend that probabilities and proportions are essentially the same.",2010-08-16 18:13:04.0,74.0,
1859,1751,0,"Using the information that what I was looking for was the negative binomial distribution I was able to find the correct function in R: qnbinom(c(.025,.975),1,1/104) - Thanks a bunch!",2010-08-16 18:27:32.0,196.0,
1860,1753,0,"What was on the x, what was on the y, and what was the point of the graph?",2010-08-16 18:46:40.0,702.0,
1861,1753,0,"@Andy: It doesn't matter. This is a software package which customers will use to their own needs with figures that can represent literally anything. In my test case I have a set of six points of data which are all positive and sit in the upper, right cartesian quadrant.",2010-08-16 18:55:11.0,968.0,
1862,1753,0,"Perhaps, the line is drawn such that it is consistent with your points 1, 2 and 3. In any case, your question is so vague that I am not sure a reasonable answer can be given.",2010-08-16 19:11:44.0,,user28
1863,1644,9,To give an example: Often one wants to know the P(model | data) ). Frequentist analysis gives you P(data | model) however (which then people often read as P (model |data). By assuming a prior probability P(model) you can get P(model | data) in Bayesian statsitics. But then you can debate what P(model) should be.,2010-08-16 19:20:24.0,961.0,
1864,1753,0,Added a rephrase of the question to hopefully clarify via an example.,2010-08-16 19:37:30.0,968.0,
1866,1753,3,"No, what was on the axes definitely does matter.  Based on what you've written, I can imagine two different plots: one that is illustrating change in the variability of Y as X increases (in which case a simple line might be valid), and one that's illustrating the actual relationship of X and Y, which might include the mean and and SD lines... or might not.  Why 1 SD?  Why not 2?  We can only give plotting advice with some kind of context.  If people will be plotting whatever they want, all you can do is make it as flexible as possible.",2010-08-16 19:50:46.0,71.0,
1867,1754,0,"Thanks. That's exactly what i'm wrestling with. I have a deprecated app from a defunct company that i'm supposed to resurrect while at the same time troubleshooting their original intent. For example, the only clue (and the basis of my question) is a check box on a graph titled ""show standard dev."" Now, i'm supposed to recreate the functionality. Thanks for your patience btw. Any further insight is greatly appreciated.",2010-08-16 20:04:55.0,968.0,
1869,1681,0,"Hm...I thought it was unambiguous given that I'm talking about Chernoff bound for absolute error...added a link in question (it seems to break in comment)...in that notation, I'm looking for epsilon as a function of m",2010-08-16 21:55:47.0,511.0,
1870,1754,0,"Wow.  I don't even you that task.  In that context, I think what I'd do is have use the mean line + 2 SD for continuous X, and then use bars like csgillespie's for categorical data.  Not sure how you'd be able to differentiate reliably between cont. and cat., but I'm sure many of attempted that before.",2010-08-16 22:29:48.0,71.0,
1871,1525,2,"Personally I liked the first title: ""Your blonde girlfriend asks, ""Hey, how's a probability different than a plain old proportion?"" Answer her in plain English.""",2010-08-17 01:58:33.0,776.0,
1872,1559,0,They are not the same if one represents a probability and the other a proportion.,2010-08-17 02:00:28.0,776.0,
1873,1176,0,"Just for the protocol, Frank's talk (in video) is now online: http://www.r-bloggers.com/RUG/2010/08/user-2010-conference-videos/",2010-08-17 02:43:58.0,253.0,
1874,1758,0,"Sorry, but I don't understand the point of derivations",2010-08-17 03:30:40.0,511.0,
1875,1674,0,"It seems from the question, the entropy/volume connection is established, similarly we know of the volume/surface area connection. So the point here is to connect the trace of the FI to entropy. Or am I missing the point of your question?",2010-08-17 04:26:15.0,251.0,
1876,1763,0,"Great list Jeromy, thank you :)",2010-08-17 04:47:44.0,253.0,
1877,1708,0,"Yes, I am working with SNP data from SNP arrays.
But the problem is much simpler. I am looking for a statistical test that will give an estimate of max. variation along 3 eigenvalues.",2010-08-17 05:16:29.0,952.0,
1878,1755,0,I don't know... Paul provided xy coordinates but not SD for all the points. We should see a graph with 5 points with perhaps some error bars (assuming we know the SD).,2010-08-17 05:27:37.0,144.0,
1879,1764,1,Those Excel guides look pretty spooky...,2010-08-17 05:41:55.0,88.0,
1880,1737,1,Question is misleading as you ask about unique combinations of factors and then in details you ask about summary by unique combinations.,2010-08-17 05:59:00.0,419.0,
1881,1665,0,It rather does not change your knowledge about distribution of other variable. Definition of independence: P(Y|X)=P(Y). We know value of other variable only in rare cases of perfect prediction (correlation equals 1 or -1).,2010-08-17 06:10:20.0,419.0,
1882,1764,4,Can you imagine how a tutorial on R looks to a person who has never seen a line of code in his or her life? :),2010-08-17 06:50:39.0,144.0,
1883,262,0,For my part graphs that are black and white plots without shaded backgrounds that use different icons or line types are important.  Also an easy way to do confidence intervals (where appropriate) would be nice too.,2010-08-17 07:10:38.0,196.0,
1884,1676,2,I would close as a duplicate; also this question is within the scope of the site.,2010-08-17 07:13:50.0,196.0,
1885,1483,0,Where TP is Test Positive and TN is Test Negative and FP is False Positive and FN is False Negative?,2010-08-17 07:20:04.0,196.0,
1886,1762,0,"About the chronographs' +/- 4%: A friend and I setup our two chronographs in tandem so that a single bullet fired would be measured by both. In the shade, his would measure a consistent 2% lower than mine: 940.0 fps on mine would record about 921.2 fps on his. When we moved the chronos to brighter lighting conditions, his would consistently record about 1.5% faster than mine, but both of our chronos were also recording about 1% higher than in the shade.",2010-08-17 07:40:01.0,937.0,
1887,1483,0,How would the question asker obtain SE^2_1 and SE^2_2?,2010-08-17 07:52:09.0,196.0,
1888,1758,0,"Perhaps, I misinterpreted what you wanted. Did you not want to express $\\epsilon$ as a function of $m$? The above does not exactly get you there but I thought it is a start towards that goal.",2010-08-17 08:19:50.0,,user28
1889,1764,3,"Ok, but I can also imagine all those people drawing manually dozens of bar breaks in Excel and believing that it is an only (and thus easiest and fastest) way to do this. Or people spending hours trying to unify the formatting in a large Word document.",2010-08-17 08:54:25.0,88.0,
1891,1774,0,I really do not know the rate. To be honest all I know is that my program crashed with a division-by-zero and that I need to handle that case somehow.,2010-08-17 09:54:40.0,977.0,
1892,1773,0,I don't think we need limit tag.,2010-08-17 11:08:27.0,88.0,
1893,1773,0,"Presumably you're attempting to quantify performance of some diagnostic procedure; is there any reason you're not using a proper signal detection theory metric like d', A', or area under the ROC curve?",2010-08-17 11:19:07.0,364.0,
1894,1775,3,"Extending your answer: If TP=0 (as in both cases), recall is 1, since the method has discovered all of none true positives; precision is 0 if there is any FP and 1 otherwise.",2010-08-17 11:44:27.0,88.0,
1895,1658,0,"first one, I'm not famous... yet ;o)

BTW, the second one is intended as a complement, just in case there was any doubt.",2010-08-17 11:50:40.0,887.0,
1896,1778,0,+1 for pointing R^2.,2010-08-17 11:59:01.0,88.0,
1897,1334,1,shame they often only look better... :-(,2010-08-17 12:05:19.0,887.0,
1898,1764,0,"""Father, forgive them, for they do not know what they are doing."" comes to mind. :)",2010-08-17 12:19:12.0,144.0,
1899,1776,0,"Cool, butterfly data. If you would be willing to discuss about your work, drop me an email. I'm registered at GMail.",2010-08-17 12:30:12.0,144.0,
1901,485,0,"for more stats 101 videos, see: http://stats.stackexchange.com/questions/1761/statistics-probability-videos-for-beginners",2010-08-17 13:54:19.0,183.0,
1902,1776,1,"@downvoters: Could you please leave a comment as to how the question can be improved? I am not sure if downvoting without leaving a comment is helping the OP.

@Elaine I think you should fix the qn to give the correct value for R^2 and any other edits that you feel would help others understand your question.",2010-08-17 14:07:54.0,,user28
1903,1781,0,Just so I understand correctly the censoring issue: When you dilute a sample the concentration of a compound falls so low that the test instrument can fail to detect its presence. Is that an accurate re-phrasing of the censoring problem?,2010-08-17 14:34:17.0,,user28
1904,1771,0,"Thanks for the help.  This is where show my novice.  I , simply have been working through the basic examples in the r packages and aggregated the data to a point where I created my own graph.  Up to this point, it's been an exercise to get started in network analysis and data visualization.  This is a huge step forward though, thanks.",2010-08-17 14:34:31.0,569.0,
1905,1754,0,"Er... don't envy, I meant.",2010-08-17 14:43:19.0,71.0,
1906,1758,0,"Yes, as function of $m$ and $P$. Since $P$ is given, it doesn't make sense to rewrite it",2010-08-17 16:41:00.0,511.0,
1907,1723,0,"Thanks for a great answer, and a great question afterwards. It's crucial to get an insight about the background of the problem. Well, so many times I've seen people doing t-test, Pearson's r or ANOVA without getting any idea about the shape of distribution (which is often heavy-skewed) - parametric techniques ""need"" satisfied normality assumption. In psychology (which is my field of interest), we often deal with small samples, therefore I need appropriate normality test.",2010-08-17 16:41:53.0,1356.0,
1908,1791,0,"thanks! however - which one would I use under which circumstance, and how would I deal with >2 methods?",2010-08-17 16:56:14.0,979.0,
1909,1758,0,"Oh, I understand.",2010-08-17 17:18:02.0,,user28
1910,1787,3,Any reason not to like the the standard errors produced by glm?,2010-08-17 17:21:16.0,279.0,
1911,1791,0,"Good question; test will only handle two methods, because it can only say if they are not equal. Still, you can convert scores for each sample to ranks and there take mean rank per method and select this with smallest mean. Then you can use test to justify if the best one is significantly better than the second.",2010-08-17 17:32:00.0,88.0,
1912,1782,0,Didn't know pscal handled classical NB models (besides zero inflated and hurdle).  Thanks.,2010-08-17 17:45:03.0,251.0,
1913,1773,3,"@Mike, precision and recall are common evaluation metrics in, e.g., information retrieval where ROC, or in particular specificity is awkward to use because you already expect a high number of false positives.",2010-08-17 18:02:36.0,979.0,
1914,1791,1,You may also look at http://en.wikipedia.org/wiki/MaxDiff,2010-08-17 18:09:25.0,88.0,
1915,878,43,"Good quote, but it's not true!  Absence of evidence is not *proof* of absence, but it certainly is *evidence*.  Why do we think magnetic monopoles (or unicorns, for that matter) don't exist?  Because we've looked and haven't found any.",2010-08-17 18:15:29.0,319.0,
1916,1781,0,"Yes, that is correct: dilution by a factor of D increases all detection limits by a factor of D as well.  (The matrix interference issue is more difficult to quantify and the general situation is extremely complex.  To simplify this, the conventional model is that a suite of tests on one sample yields a vector (x[1], ..., x[k]) where the x[i] are either real numbers or are intervals of reals, typically with left endpoint at -infinity; an interval identifies a set in which the true value is assumed to lie.)",2010-08-17 18:20:52.0,919.0,
1917,1632,8,"All answers were both helpful and useful, and would all deserve to be accepted. This one, however, does a very good job at answering the question: with Python, you have to put together lots of pieces to do what you want. These pointers will no doubt be very useful for anyone wanting to do statistics/modeling/etc. with Python. Thanks to everyone!",2010-08-17 18:42:30.0,890.0,
1918,1787,4,Standard error is a measure of the expected quality of your estimate while a standard deviation is a measure of variability.  The former decreases with n while the latter does not.,2010-08-17 18:53:33.0,601.0,
1919,1483,0,"@Jay: didn't see your comment sooner, but the above is applicable if the DOR are independent.  @drknexus: updated the answer for how to obtain SE.",2010-08-17 18:59:43.0,251.0,
1920,1764,3,Official band of stats.stackexchange.com: The Broken Axes.,2010-08-17 19:09:18.0,71.0,
1921,1762,0,Sorry if this is a dumb question but what do the lighting conditions have to do with chronograph accuracy?,2010-08-17 19:40:22.0,666.0,
1922,1762,0,"Old chronos worked by shooting metal screens at start and stop gates which would break a circuit to mark start and stop times. Modern chronos use a light/IR sensor to detect the bullet passing over the gates. These sensors are affected by different ambient light levels. At major matches, the staff will typically build a box with lights inside around a pair of tandem chronos to provide consistent lighting to eliminate this factor.",2010-08-17 20:21:07.0,937.0,
1923,1796,0,"But the variability of results among one method is dominated by the variability of tests, so I am afraid the spread may be negligibly meaningful.",2010-08-17 21:22:41.0,88.0,
1924,1781,0,Why would the detection limits go up? Are they not a feature of the test instrument rather than that of the sample being tested?,2010-08-17 21:33:50.0,,user28
1925,1796,0,"But, that is the point. If a method's performance is not robust to diversity of test situations then I would tend to prefer it less than a method that is more robust assuming of course that on average they are comparable. Of course, this is all abstract as we do not know if the OP really cares about reliability/robust methods.",2010-08-17 21:39:00.0,,user28
1926,1804,0,"this procedure results in an estimate of $M^T M$ that may not be positive semidefinite, which would be bad.",2010-08-18 00:40:16.0,795.0,
1927,1723,2,"But normality is *never* satisfied. It's sometimes a reasonable description of the data, but they're not actually normal.While it's sensible to check for non-normality when you assume it, it's not particularly useful to test it (for the reasons I described above). I do a qq-plot, for example, but a hypothesis test answers the wrong question in this situation. t-tests and anova usually work reasonably well if the distributions aren't heavily skew. A better approach might be to use procedures that don't assume normality - perhaps resampling techniques.",2010-08-18 00:50:33.0,805.0,
1929,1723,0,"Or you can use non-parametric tests, at cost of having less power. And nothing is absolutely satisfied in statistics, it's not solely a normality issue. However, bootstrapping or jackknifing are not a solution when introducing someone to t-test and/or ANOVA assumptions. I doubt that resampling techniques solve normality issues at all. One should check normality both graphically (density plot, boxplot, QQplot, histogram) and ""numerically"" (normality tests, skewness, kurtosis, etc.). What do you suggest? This is completely off topic, but how would you check, say, ANOVA normality assumptions?",2010-08-18 01:55:18.0,1356.0,
1932,1815,2,see this similar existing question: http://stats.stackexchange.com/questions/1352/references-for-how-to-plan-a-study,2010-08-18 09:27:20.0,183.0,
1933,1796,0,"Hi, thanks for the input. You're right about mean/variance tradeoff one has to look at. However, in the given situation, mbq's right, that's not so much the issue. The point is that results almost never looks like your [0.80, 0.60] / [0.71, 0.69] example: The first one would mostly stay better, just on a different scale. Still, this is a helpful reference, since I might use this at another point (I also have different families of sample generators which might need to be compared), and also I'll update my question.",2010-08-18 10:15:24.0,979.0,
1934,1816,0,How would I go about using the information from all generations?,2010-08-18 10:37:23.0,986.0,
1935,1816,0,"The easiest way is to do multiple tests, i.e. test at every generation, then use a Bonferroni or fdr correction.",2010-08-18 10:43:58.0,8.0,
1936,1332,0,"The best antidote to ""...lies, damned lies, and statistics.""",2010-08-18 10:44:57.0,521.0,
1937,328,0,"@Shane: Yes, absolutely. I was being slightly facetious in my comment, but I do believe important insights are being withheld by the private-sector community as competetive advantages, which is ultimately to the detriment of an efficient global economy.",2010-08-18 11:18:07.0,229.0,
1938,1816,0,"When comparing at every generation, I would have to test at a significance level of 1/1000 * 0.05 ? Isn't that a bit harsh?",2010-08-18 11:18:28.0,986.0,
1939,1821,1,It is the variance which is an unbiased estimator. S is not an unbiased estimate of the population standard deviation.,2010-08-18 12:13:50.0,159.0,
1940,1816,0,"True, but you're also doing lots of testing - can't have everything ;) You could rank the p-values, use them as a guide to see where possible errors may occur.",2010-08-18 12:26:57.0,8.0,
1941,1821,0,"Thanks Rob, I stand corrected! This is a subtlety that I was previousy unaware of. Others may wish to see:

http://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation",2010-08-18 12:41:14.0,521.0,
1942,1824,0,"sounds reasonable, any more details?",2010-08-18 12:59:49.0,986.0,
1943,1823,0,Thank you for the answer Jeromy.  Did you happen to find something similar in R ?,2010-08-18 13:01:36.0,253.0,
1945,1815,0,"@Jeromy Fair enough, but all the answers there so far have managed to avoid recommending even a single book!",2010-08-18 13:14:37.0,174.0,
1946,1826,0,This question is to help with any confusion over the site name proposal: http://meta.stats.stackexchange.com/questions/21/what-should-our-site-be-called-what-should-our-domain-name-be/53#53.,2010-08-18 13:23:01.0,5.0,
1947,1827,0,@srikant thanks! i'll take a look at it.,2010-08-18 13:30:04.0,283.0,
1948,1830,1,"Technically, this is holdout validation but one can imagine extending the subway example to a cross-validation context. If it helps I will re-write the example and the rest of the text to be specific to cross-validation.",2010-08-18 13:42:31.0,,user28
1949,1831,0,"that book looks pretty nice! i just installed the 'arm' package, too. thanks shane!",2010-08-18 13:52:25.0,283.0,
1950,1815,1,I agree more discussion from a different angle would be great. I was just posting the other question as related supplementary reading.,2010-08-18 13:53:58.0,183.0,
1951,1831,0,@mropa: That is my favorite statistics text: enjoy it!,2010-08-18 13:54:29.0,5.0,
1952,1781,0,"As an example, suppose an instrument's detection limit is 1 microgram per Liter (ug/L).  A sample is diluted 10:1 (with great precision, so we don't worry about error here) and the instrument reads ""<1""; that is, nondetectable, for the diluted sample.  The laboratory infers that the concentration in the sample is less than 10*1 = 10 ug/L and reports it as such; that is, as ""<10"".",2010-08-18 14:06:17.0,919.0,
1953,1804,0,"That's a good point, but the result might not be so bad.  What one hopes is that the estimate of M*M is close enough to the true value that the perturbation of eigenvalues is reasonably small.  Thus, by projecting to the eigenspace corresponding to the largest eigenvalues, you achieve only a slight perturbation of the correct solution, still achieving the sought-after dimension reduction.  Perhaps the biggest problem may be algorithmic: since you can no longer assume semidefiniteness, you might need to use a more general-purpose algorithm to find the eigensystem.",2010-08-18 14:09:39.0,919.0,
1954,1828,2,It seems given discussions elsewhere on this site that k-fold cross validation is just one type of cross validation and describing it does not do the general job of describing what cross validation is.,2010-08-18 14:31:12.0,196.0,
1955,1836,0,That is counter-intuitive!,2010-08-18 14:41:53.0,,user28
1956,1836,2,"Yes it is!  That's why it's well worth studying: challenges to our intuition are exceptionally educational.  I first learned of this from a clear paper on Carlos Rodriguez' (SUNY Albany) Web page but I couldn't find it this morning: it appears the server is down.  Try Googling ""carlos rogriguez statistics"" later.  (His paper is supposed to be at http://omega.albany.edu/8008/confint.html , but this might be an old URL.)",2010-08-18 14:51:48.0,919.0,
1957,1797,0,"Since in essence the nesting in a lmer makes it a repeated measures design is there a way in which your question about the appropriate confidence interval around the effect size is related to the question in repeated-measures ANOVA about which measure of effect size to report?  Specifically, it is unclear whether the error term should include subject variance or not (etc)?",2010-08-18 14:55:54.0,196.0,
1958,1813,0,"Just as a clarification: isn't it the case that a genetic algorithm searches randomly for a solution, so that the initial segment of any run is unlikely to produce any worthwhile solution?  Also, what exactly do you mean by ""the minimum error in the population""?  If you mean the minimum difference between a known true value and any solution out of the 1000 values in a run, then isn't that a biased indication of the run's result?  After all, in practice you would accept the final solution in each run and reject everything that precedes it, right?",2010-08-18 14:57:26.0,919.0,
1959,1823,2,"@Tal Quick-R lists a few power analysis procedures in R: http://www.statmethods.net/stats/power.html ; or you can use R to run a simulation for custom power analysis. 
The lme4 package is good for multilevel modelling.",2010-08-18 15:01:28.0,183.0,
1960,1311,0,"Suppose y=2, n=4, and there are just two birthdays.  Your formula, adapted by replacing 365 by 2, seems to say the probability that exactly 2 people share a birthday is Comb(4,2)*(2/2)^2*(1-1/2)*(1-2/2) = 0.

(In fact, it's easy to see--by brute force enumeration if you like--that the probabilities that 2, 3, or 4 people share a ""birthday"" are 6/16, 8/16, and 2/16, respectively.)

Indeed, whenever n-y >= 365, your formula yields 0, whereas as n gets large and y is fixed the probability should increase to a non-zero maximum before n reaches 365*y and then decrease, but never down to 0.",2010-08-18 15:05:43.0,919.0,
1961,1797,0,Nevermind - I didn't think that all the way through.,2010-08-18 15:06:22.0,196.0,
1963,1813,0,"By error I basically mean 1/fitness, so I'm talking about the value of the best individual in a generation.

I've recorded the fitness value of the best individual for every generation.

So I have 1000*20*2 numbers, each corresponding to the ""fitness"" of the best individual in a particular generation of a particular run.",2010-08-18 15:12:47.0,986.0,
1964,1828,2,"@drknexus: That's fair, but I mention that it's k-fold and I wanted to provide a visualization of the process to help explain it.",2010-08-18 15:16:04.0,5.0,
1965,1839,0,"By algorithm, I mean roughly the software implementation used for fitting a line to model the mean of a distribution.",2010-08-18 15:18:39.0,988.0,
1967,1834,0,"Right, that's why I consider OLS to be an ""algorithm"" used in linear regression...",2010-08-18 15:22:58.0,988.0,
1968,1813,0,"I guess the initial question was ill-posed, I've added some clarifications ..",2010-08-18 15:23:11.0,986.0,
1969,1834,3,"Ordinary least squares is an estimator. There are a variety of  algorithms for computing the estimate: usually some sort of orthogonal matrix decomposition, such as QR, is used. See http://en.wikipedia.org/wiki/Numerical_methods_for_linear_least_squares",2010-08-18 15:34:31.0,495.0,
1970,1842,0,"Uh, I tried setting up df = data.frame(t=days, values=c(data2,cum), type=rep(c(""Bytes"", ""Changes""), each=1001)), but it gives an Error in rbind.zoo(...) : indexes overlap",2010-08-18 15:36:42.0,990.0,
1971,1311,0,"Why you are replacing 365 by $n$? The probability that 2 people share a birthday is computed as: 1 - Prob(they have unique birthday). Prob(that they have unique birthday) = (364/365). The logic is as follows: Pick a person. This person can have any day of the 365 days as a birthday. The second person can then only have a birthday on one of the remaining 364 days. Thus, the prob that they have a unique birthday is 364/365. I am not sure how you are calculating 6/16.",2010-08-18 15:39:58.0,,user28
1972,1842,0,"That's because data2 and cum are zoo objects. Use as.vector(data2) to get the raw values. Also, I used 1001 because I had 1001 observations. You will need something different.",2010-08-18 15:42:29.0,8.0,
1973,1843,0,"That's interesting, but how do we tell the reader which scale corresponds to which line?",2010-08-18 15:52:35.0,990.0,
1974,1842,0,"Noob R user here: Error in data.frame(t = days, values = c(as.vector(data2), as.vector(cum)), : arguments imply differing number of rows: 1063, 1300, 2",2010-08-18 16:01:35.0,990.0,
1975,1842,0,"Type ""days"", ""data2"" and ""cum"" to look at your data. Then look at ""length(days)"", etc. You need to match up the time points with the values.",2010-08-18 16:05:46.0,8.0,
1976,1843,0,"Have a look at this graph: http://imgur.com/K8BCr.png
There, we present y-axis labels and ticks only where they apply to the data (i.e., for the left axis on the top of the graph, as the corresponding data, and for the right axis on the bottom of the graph, as the correspoding data).
Additionally we used different colors (as in the example above) and line types and explained it in the caption.
You could also use a line chart on the left and a bar chart on the right axis to make the distinction clearer.",2010-08-18 16:11:38.0,442.0,
1977,1816,1,"Instead of bonferroni correction, you could always use the more powerful bonferroni holm. See my anyswer here: http://stats.stackexchange.com/questions/575/post-hocs-for-within-subjects-tests/760#760",2010-08-18 16:19:04.0,442.0,
1979,1832,1,+1 for Maxwell & Delaney!,2010-08-18 16:40:08.0,442.0,
1980,1844,0,How do these estimates relate to the one given by Cox regression? That's gotta be the gold standard for estimating HR.,2010-08-18 16:41:19.0,279.0,
1983,1636,0,"A similar technique proceeds by computing the $\\phi$ statistic of the contingency table. Since $\\phi$ is actually the correlation coefficient of two 0-1 vectors (one based on prediction, the other on reality), one can use the Fisher R-Z transform (just $\\atanh$) to get an approximately normal statistic. Comparing $\\atanh{(\\phi_1)}$ to $\\atanh{(\\phi_2)}$ becomes an exercise in comparing independent normals with known variances. A similar technique for tetrachoric coefficient probably exists as well. Regrettably, these techniques are fairly low power...",2010-08-18 17:05:15.0,795.0,
1985,1720,0,"A nice example of a need to make inferences about the means, no matter what, is afforded by some environmental risk assessments. To simplify greatly, imagine you are planning to develop land into a park. You test the soils for some compound of concern and, as is often the case, find its concentration is approximately lognormally distributed.  Nevertheless, people using the park--who might directly become exposed to these soils--will effectively ""sample"" the soils uniformly at random as they move around. Their exposure over time will be the arithmetic mean concentration, not its geometric mean.",2010-08-18 18:42:14.0,919.0,
1988,1840,2,"I don't think I'd call that ""plain"" English :)",2010-08-18 19:53:40.0,74.0,
1989,1828,0,This is very simple and useful,2010-08-18 19:54:32.0,74.0,
1990,1840,0,"I think it's not so bad, generally I don't plan to change it.",2010-08-18 20:12:03.0,88.0,
1991,1830,0,Could the downvoters explain why they feel the answer does not help?,2010-08-18 20:12:08.0,,user28
1992,1830,0,I agree with Srikant. Would any downvoters please explain what's wrong.,2010-08-18 20:25:41.0,8.0,
1993,1830,1,"By the way, if something can be done to improve the answer, feel free to offer suggestions. I would be more than happy to edit the answer in response to reasonable, constructive feedback.",2010-08-18 20:37:32.0,,user28
1994,1825,0,"Hi Mike, thank you for your feedback. Temperature levels X1....X5 are specific degree values (i.e., heat stimuli = 42C, 44C, 46C, 48C, 50C; cold stimuli = 8C, 6C, 4C, 2C, 0C); however, they can also be conceptualized as stimuli ranging from ""likely to cause minimal discomfort (i.e., 42C or 8C)"" to ""likely to cause maximum discomfort permissible by my research ethics board (i.e., 50C, 0C)"". With this being the case do you think that a series of univariate repeated measures or a multivariate analysis is ok?<br/>
I too am am interested to hear suggestions from any mixed effects/multivariate gurus",2010-08-18 20:39:53.0,835.0,
1995,1856,0,"Just to be clear: your question is about the size of the data, not about the setting, correct?",2010-08-18 20:40:34.0,5.0,
1996,1856,0,"Exactly, I wonder if there are any references about the ""smallest"" n (wrt. to a high number of variables), or more precisely if any cross-validation techniques (or resampling strategy like in RFs) remain valid in such an extreme case.",2010-08-18 20:45:10.0,930.0,
1997,1843,0,The example you've given is very good... How did you managed to vertical offset each axis?,2010-08-18 20:55:13.0,990.0,
1998,1454,0,"You can find more ideas and code in ""Separating Two Populations in a Sample,"" http://stats.stackexchange.com/questions/899/separating-two-populations-from-the-sample.  (Is there perhaps some better way of cross-referencing threads than I've done here?)",2010-08-18 21:49:36.0,919.0,
2000,1326,0,Forgotten bonus: unbalanced data are still useful!,2010-08-18 23:17:53.0,71.0,
2001,1855,0,"That's my second scenario, the confidence interval is too large to have any inferential value within the design of the experiment since differences between conditions are based on effects with between S variability removed.  It seems it always has a compromise meaning and needs it's own special name because you can't use it like a regular CI.",2010-08-18 23:19:21.0,601.0,
2002,1855,0,Blouin & Riopelle (2005) called them narrow and broad inference confidence intervals but given that the general scientific populace outside stats has a hard enough time with regular ones...,2010-08-18 23:25:54.0,601.0,
2003,1836,1,Amazing. I didn't know that. Thanks for the reference.,2010-08-19 00:29:41.0,159.0,
2005,1823,0,"Thanks Jeromy.  I think I'll just ask here on how to do that.  I have wrote code in the past for power analysis, but they tended to get complex - I would be curious to see how betteR coder then me would solve this.",2010-08-19 01:32:30.0,253.0,
2006,1865,0,Apparently this has an easy solution.,2010-08-19 01:38:51.0,994.0,
2007,1847,0,I am guessing you mean this:  http://www.stat.columbia.edu/~gelman/arm/  I just read a good review about it here: http://www.r-bloggers.com/bookshelf-remodelling/,2010-08-19 02:02:15.0,253.0,
2008,1865,1,Want to post it ?,2010-08-19 02:28:26.0,253.0,
2009,1862,0,How about PCA/FA? You would shrink correlated variables into factors and work from there...,2010-08-19 05:46:16.0,144.0,
2010,1560,0,according to the paper I quote both James stein and soft/hard thresholding satisfy oracle inequalities. I guess James Stein is more difficult to manipulate og to understand intuitively than hard thresholding but the why question is a good question!,2010-08-19 06:32:30.0,223.0,
2011,1874,0,you mean a a good 3D image reconstruction algorithm? maybe you only need an algorithm to estimate 3-D connex region? something like level set segmentation?,2010-08-19 06:47:33.0,223.0,
2012,1850,0,In case it's useful for a potential answerer formulas are listed here: http://en.wikipedia.org/wiki/Effect_size,2010-08-19 06:58:32.0,183.0,
2013,1883,0,The only way you can keep this question open is by making it community wiki so please tick the case.,2010-08-19 07:03:02.0,223.0,
2014,1883,2,"However, I have the feeling that this is subjective, argumentative and will require extended discussion please read http://stats.stackexchange.com/faq
I vote to close but encourage you to ask a more specific question (since the idea of the question is good but way too wide).",2010-08-19 07:05:59.0,223.0,
2015,1883,0,one of the extended discussion that could start: are you sure that Prof Rob Hyndman was a reasearcher when parzen and Rozenblatt proposed exponential smoothing :) ?,2010-08-19 07:08:13.0,223.0,
2016,1879,0,"Very helpful answer!. ""Discovering Statistics Using SPSS"" is the one I was also using. Perhaps I'll also have to review SPSS in the year to come. :-(",2010-08-19 07:17:32.0,339.0,
2017,1883,1,"I think with the availability of more powerful computers, different kinds of methods suddenly become practical and important (would one use e.g. boosted decision trees without fast computers ?)",2010-08-19 07:32:31.0,961.0,
2018,1857,0,"Thanks! I have Hastie's book and that of C. Bishop (Pattern Recognition and Machine Learning). I know that such a small n would lead to spurious or unreliable (see Jeromy Anglim's comment) association. However, the RF algorithm as implemented by Breiman allows to cope with a limited number of features each time a tree is grown (in my case, 3 or 4) and although OOB error rate is rather high (but this should be expected), analyzing variable importance lead me to conclude that I would reach similar conclusion using bivariate tests (with permutation test).",2010-08-19 07:35:00.0,930.0,
2019,1883,0,"To add to Robin's comment, your question seems more appropriate for a blog or a discussion forum.",2010-08-19 07:36:07.0,,user28
2020,1858,0,"Thanks! I was attending the IVth EAM-SMABS conference last month, and one of the speaker presented an application of ML in a biomedical study; unfortunately, this was a somewhat ""standard"" study with N~300 subjects and p=10 predictors. He is about to submit a paper to *Statistics in Medicine*. What I am looking for is merely articles/references wrt. standard clinical study with, e.g. outpatients, where generalizability of the results is not so much an issue.",2010-08-19 07:39:58.0,930.0,
2021,1423,11,Prediction about the past can also be surprisingly tricky!,2010-08-19 07:58:29.0,174.0,
2022,1850,0,"A simulation in R with varying n1, n2, s1, s2, and population difference would make a nice exercise. Anyone?",2010-08-19 08:04:16.0,183.0,
2023,1843,2,"Really good example. The only issue with your graph, is that both Y variable names are overlapping. In this case you would want one on the left and the other on the right (possibly even in a vertical position). To upgrade your example from ""really good"" to ""perfect"", you might wanna use the mtext function from R to do the variable names",2010-08-19 09:18:15.0,447.0,
2024,1883,0,To subjective. I would vote close.,2010-08-19 09:25:47.0,8.0,
2025,1877,0,Yes in this case you should go for binomial test instead of using asymptotic chi squared test.,2010-08-19 09:28:02.0,994.0,
2026,813,0,"By God, he was right!",2010-08-19 09:32:41.0,994.0,
2027,729,35,God must bring data too.,2010-08-19 09:33:49.0,994.0,
2029,1843,0,@Hugo @Dave: See my update for an incorporation of both comments.,2010-08-19 10:00:12.0,442.0,
2030,750,4,"Whoever said this had no basic understanding of Statistics, or he was joking.",2010-08-19 10:21:44.0,994.0,
2031,835,0,"I would say data is the bullet, Statistics is the gun.",2010-08-19 10:23:50.0,994.0,
2032,1406,0,All physical experiments I have ever seen have a standard deviation attached to it in some form (most commonly a +/- range). So he must have been joking.,2010-08-19 10:25:27.0,994.0,
2033,1888,0,"The Clason and Dormody article looks good.
Your response distribution comments are interesting to contemplate. I agree that differences in distributions might be of interest. But if you were only interested in whether population group means were different, it would not necessarily matter what distributions gave rise to such equality.",2010-08-19 10:33:16.0,183.0,
2034,1889,0,like so ;-),2010-08-19 10:44:24.0,88.0,
2035,1849,2,"I agree. Also, depending on the domain a weighted composite of categories might be a more appropriate index of an overall rating.",2010-08-19 10:55:16.0,183.0,
2036,1848,2,"Interesting question. Initially, I had some trouble understanding your pseudo-code, and then I read: http://www.thebroth.com/blog/118/bayesian-rating",2010-08-19 10:57:16.0,183.0,
2037,1888,0,"In this case, you are assuming that your Likert scale (in other words, the perceived difference between, e.g. much satisfied and ""just"" satisfied) behaves ideally and is perceived as having the same meaning in both population. Thus you are implicitly making the assumption that this is a numeric scale, but I agree that this is often considered as such in applied research, especially if participants come from the same country. My point was just to emphasize the categorical data analysis perspective, as usually found in the Factor Analysis tradition, like in my reply to Question #10.",2010-08-19 11:01:26.0,930.0,
2038,1889,0,Thanks -- I once knew that trick.,2010-08-19 11:23:24.0,334.0,
2039,1892,0,What about SAS and spss?,2010-08-19 11:29:12.0,5.0,
2040,1883,0,There are answers from three persons that can vote for closing and only csgillepsie and I voted for closing. I guess this means people want it open. It is important to discuss that. I have openned a discussion about that on meta http://meta.stats.stackexchange.com/questions/336/what-is-the-limit-in-subjective-argumentative-questions,2010-08-19 11:40:28.0,223.0,
2041,1890,0,is turing the site into a discussion forum a revolution ;) ?,2010-08-19 11:41:47.0,223.0,
2042,1896,0,"I would go with variance and/or standard deviation (sd is just the square root of the variance) instead of the absolute deviation, because variance and sd punish extreme outliers stronger. This seems desirable in your case.",2010-08-19 12:13:10.0,442.0,
2043,1896,0,"When I go with standard deviation or variance, I come up with the problem of how to combine the results for the subbenchmarks into a whole. Absolute variance (assuming that's what I did) gives me a single number.",2010-08-19 12:17:38.0,1001.0,
2044,1896,0,"Note that I tried to combine my sub-variances using ""the variance of the total is the mean of the variances of the subgroups + variance of the means of the subgroups"", which I got from http://en.wikipedia.org/wiki/Variance.",2010-08-19 12:18:48.0,1001.0,
2045,1892,0,And don't forget Stata.,2010-08-19 12:38:20.0,521.0,
2046,1892,0,Probably deserving of its own question: Which statistical package has made the most revolutionary contribution to the science and practice of data analysis and statistics?,2010-08-19 12:44:34.0,183.0,
2047,1897,0,I'm told that the results of the benchmarks (the times in ms) are not normally distributed. Will the variances be normally distributed? How does that affect the choice of 1.96?,2010-08-19 12:49:45.0,1001.0,
2048,1892,0,That would be overly argumentative.  I think having an answer here that acknowledges all statistical software is on point.,2010-08-19 13:06:15.0,5.0,
2049,1883,0,"I would vote to close, but when moderators vote to close, it's closed immediately.  I think that Colin and I are showing restraint and letting the community decide.",2010-08-19 13:07:50.0,5.0,
2050,1883,1,"Answering the question is not really a clear indication that you would vote to close. People saw my comment, they saw your answer, ... 10 very fast heterogeneous answers in less than an hour ! looks like a chat room ;)",2010-08-19 13:25:07.0,223.0,
2051,1883,0,"That's fair, although so long as the question is open, I think that it should be answered correctly.  :)  If the close votes get to 4, then I will vote to close.",2010-08-19 13:28:31.0,5.0,
2052,1892,0,"@Shane. Fair enough. I used to use SPSS. Now I use R. R revolutionised the way that I think about and conduct data analysis. It made data analysis fun. I can't speak too much about Stata and SAS, so I'll leave it for others to justify why they might be revolutionary.",2010-08-19 13:32:03.0,183.0,
2053,1896,0,"A strong case can be made for robust techniques that do the opposite of ""punishing"" outliers, because outliers in benchmarks often occur unavoidably due to other processes and external perturbations causing a process to wait from time to time.  This could be the reason the benchmarks do not appear to be normally distributed, too.  However, as cgillespie's answer points out, you can go a long way with the standard hypothesis testing machinery applied to SDs.  As a compromise, a lightly Winsorized version of the SD might be a good choice.  (http://en.wikipedia.org/wiki/Winsorising )",2010-08-19 13:47:27.0,919.0,
2054,1895,2,An amusing observation: all your benchmark times are multiples of 2^-12 (=1/4096).,2010-08-19 13:48:59.0,919.0,
2055,1909,1,The www.r-project.org/useR-20xx  URLs are preferable.,2010-08-19 13:56:00.0,334.0,
2056,1912,0,"sorry if it is my english (unfortunatly I'm french) but is it proper english ? what do you mean by ""losing power"" is it about testing? if yes, testing what ?",2010-08-19 14:22:29.0,223.0,
2057,1912,1,"There's nothing unfortunate about being French.  :)  Regretfully, I don't entirely know what Gary was talking about (hence the question).  I might assume that he meant ""loss of information"", but would be happy to get guidance in that respect.  I would say that this is correct english, but questionable statistics.",2010-08-19 14:28:46.0,5.0,
2058,1901,0,"Interesting... I'm a bit busy at the moment, but I'll definitely have a look at that!",2010-08-19 14:35:21.0,582.0,
2059,1912,0,"Perhaps, you can email him and ask? You could use the opportunity to point to this thread seeking his permission to post his answer here. (indirect promotion of the site)",2010-08-19 14:41:44.0,,user28
2060,1912,0,@Srikant: Done; we'll see what he says.,2010-08-19 14:54:49.0,5.0,
2061,1636,0,"Just for those of you not familiar with Phi (\\phi), http://en.wikipedia.org/wiki/Contingency_table#Measures_of_association",2010-08-19 15:14:29.0,196.0,
2062,1915,0,"sorry for the graphical-techniques tag, but my brand new account doesn't let me tag with 'graph' and 'partitioning' as I intended to.",2010-08-19 16:10:09.0,1007.0,
2063,1871,0,"Thanks all.

The behavior that lead to the accuracy and time measures was very controlled. I'm looking at the results from an exploratory perspective, while trying to ask why the inaccuracies occurred.

I'm unsure about ""Each participant provides 6 values, therefore it is not unbalanced""? When subjects are correct, I have a different number of time measures between the levels, e.g. subject A got 23 correct in level 1, 16 in level 2, 10 in level 3. If I take the mean correct per level per subject can I just ignore the number of correct answers & the difference in the number of correct answers?",2010-08-19 16:25:38.0,993.0,
2064,1915,0,@csgillespie: thank you for retagging it :),2010-08-19 16:55:17.0,1007.0,
2065,1852,1,"(1) I think you meant to write that the *sample*, not the *population* mean, is 112 with 95% *probability*, not *confidence.*
(2) Your point is well taken--it could apply to any question--but isn't it stated a little extremely? First, the question doesn't ask for an inference about the population mean: we are told it is $\\$$200. Thus, conditional on the assumptions, we can estimate the population mean with certainty! Second, even if we were asked to estimate the population mean from the sample, there's still some trivial information we could offer (e.g., it doesn't exceed $10^11 per annum).",2010-08-19 16:56:09.0,919.0,
2066,1852,0,"(1) good catch. (2), yes, I can make the problem setup asymptotically perverse for fixed results, _post hoc_. my bad.
however, I am no longer sure what the OP is trying to test. If they know the population mean is 200, why are they trying to test it?",2010-08-19 17:13:03.0,795.0,
2067,1852,0,"BTW, evidently a CEO salary/least paid salary ratio of 400 is not considered extreme in the US. 800 is a bit perverse, though.",2010-08-19 17:15:45.0,795.0,
2068,1919,0,May fit better under: http://stats.stackexchange.com/questions/1906/data-mining-conferences?,2010-08-19 17:16:13.0,5.0,
2069,1920,1,Already noted here: http://stats.stackexchange.com/questions/1906/data-mining-conferences,2010-08-19 17:21:43.0,5.0,
2070,1919,0,"@Shane, I guess so. I hadn't seen the other two conference questions before I answered.",2010-08-19 17:28:27.0,11.0,
2071,1912,0,"I saw this as well, and assumed it was a warning against blindly studentizing data.",2010-08-19 17:47:36.0,795.0,
2072,1916,0,"Thanks. I fitted a no nugget model to the likelihood using proc mixed, I guess I should compare the AICC with that of a model with nugget.",2010-08-19 18:08:06.0,1004.0,
2073,1844,0,"Cox model incorporates covariates. The Kaplan-Meier, Nelson-Aalen, Mantel-Haenszel methods model hazard as a function only of age.",2010-08-19 18:54:53.0,795.0,
2074,1871,0,"those raw answers... 23, 16, etc. cannot go into a repeated measures ANOVA.  You need to get their means and enter one value of each (6 total) for each S.",2010-08-19 19:52:32.0,601.0,
2075,1924,0,Hm...following the derivations under asymptotic normality link I'm a bit confused...what's the difference between I and H?,2010-08-19 20:14:36.0,511.0,
2076,1924,0,"There is no difference. See the [definition of fisher information i.e., I](http://en.wikipedia.org/wiki/Fisher_information#Definition) where it is re-written as H. I guess the variance expression can be simplified on the wiki as $H^{-1} I$ is an identity matrix.",2010-08-19 20:32:39.0,,user28
2077,1924,0,"Hm....it says that H becomes I when the model is correctly specified, this makes me wonder what's the difference when it is not",2010-08-19 20:50:44.0,511.0,
2078,1928,0,Nice reference: thanks.  It comprehensively covers everything mentioned here.,2010-08-19 21:03:18.0,919.0,
2079,1884,0,I am not convinced -- is P(X_n != X_(n+1)) independent from n?,2010-08-19 21:36:15.0,573.0,
2080,1874,0,"Just a good image reconstruction algorithm(The regions involved are often not convex). It's just that there are a lot of different ones, and I'm not familiar which ones are best in the context I'm approaching the problem in. (Easy to parametrize estimated surfaces, easy to compute volume, and easy to find minimum distance from a given point)",2010-08-19 21:55:29.0,996.0,
2081,1924,0,Where does it say that? I am a bit unsure what the model correctness has got to do with the relationship between H and I.,2010-08-19 22:36:29.0,,user28
2082,1791,0,"Well that's a bunch of hints. I'll have a look at those, thx!",2010-08-19 23:02:21.0,979.0,
2083,1924,0,"At the bottom of the asymptotic normality derivation from your link. It says ""Finally, the information equality guarantees that when the model is correctly specified, matrix H will be equal to the Fisher information I, so that the variance expression simplifies to just I^−1""",2010-08-19 23:19:04.0,511.0,
2084,1924,0,I thought about it and I fail to see the connection to the model being the correct one for the data. I do not have a standard textbook to see what the issue is. If I can think of something I will update my answer or add a comment.,2010-08-20 01:03:51.0,,user28
2085,1909,0,Thanks Dirk. I added the link to R conferences on R-project,2010-08-20 01:44:32.0,183.0,
2086,1940,0,Great article - thanks!,2010-08-20 01:48:14.0,561.0,
2087,1925,0,"Would you like to explain your reasons?
I can see how such a model might provide a more precise model of observed responses. However, in the typical practical research situations that I have seen, researchers are interested in whether the two groups differ in terms of the mean (e.g., did the training group report greater performance than the control; was student satisfaction higher one year to the next). The proportional odds ratio model does not test this question exactly as far as I am aware.",2010-08-20 01:53:50.0,183.0,
2088,1888,0,"I assume that the mean of the sample responding to a Likert item is generally a meaningful summary of the group's position on the underlying dimension.
It's interesting to think about when the meaning of a Likert item would vary systematically between groups. Of course, this issue extends beyond just Likert items, probably to any subjective measurement procedure.",2010-08-20 02:03:10.0,183.0,
2089,1836,2,Thanks - any chance this is the Rodriguez paper you're thinking of?  http://arxiv.org/abs/bayes-an/9504001,2010-08-20 06:07:24.0,251.0,
2090,1849,0,Thanks for the thoughts Aniko. I'll take a look at this in the next few days.,2010-08-20 06:18:29.0,991.0,
2091,1844,0,"@shabbychef: with Cox PH, use a single binary covariate, i.e. coded 0/1 for reference/comparison groups, then exp(beta) = HR.",2010-08-20 06:47:38.0,251.0,
2092,1944,2,maybe `standardized` ?,2010-08-20 08:34:49.0,339.0,
2093,1945,0,wtf. you are to fast. i was still typing. nevertheless +1 for the correct answer.,2010-08-20 08:45:05.0,442.0,
2094,1946,0,"no, it is called z-score (small letter), big letter Z-scores refer to mean = 100 and sd = 10.",2010-08-20 08:46:51.0,442.0,
2095,1945,0,Thank you ars! :),2010-08-20 08:49:46.0,582.0,
2096,1946,0,"@Henrik I have never heard of something like this; nevertheless I've made the change, it seems lowercase z is used more often.",2010-08-20 09:25:39.0,88.0,
2097,1946,0,could be that it is a psychology specific distinction. browsing the web i realized that it could even be specific German. At least I only found it in the German wikipedia (not surprisingly i learned it in a German university): http://de.wikipedia.org/wiki/Normskala,2010-08-20 09:50:21.0,442.0,
2099,1853,0,Multivariate or univariate ?(it makes a huge difference in this context),2010-08-20 10:09:52.0,603.0,
2100,1944,0,"Just a comment on the tag. Right now we have one question tagged ""nomenclature"" and another question tagged ""terminology"". I personally prefer ""terminology"" - it's much more common in everyday language than nomenclature, but I see nomenclature potentially being used. Could someone consider a tag synonym or just a rename one way or the other?",2010-08-20 10:18:43.0,110.0,
2101,1944,0,"@Thomas Owens: you are right, terminology is better, I changed it.
I guess ""nomenclature"" sounds better to people in biology :)",2010-08-20 11:05:26.0,582.0,
2102,1922,0,"upvoted for the reference to the paper and the book, I'll read them",2010-08-20 11:54:36.0,840.0,
2103,1862,0,"this might be too much, if management asks 'how did you get the aggregated numbers?' they'll want a simpler technique so they can (feel they) understand it. Alas, the real world :-( Thanks, though.",2010-08-20 11:57:37.0,840.0,
2104,1869,0,I'll mark this as the answer; there are several good suggestions in it so I'll think how to apply them.,2010-08-20 11:58:40.0,840.0,
2105,1943,0,Having had a brief look at that paper i'm not sure the estimates they consider are the same as those in the questioner's equations. I agree with the comments under the question - maybe in 1981 approximate methods were useful but these days there's no obvious reason not to use Cox regression.,2010-08-20 12:42:10.0,449.0,
2106,1844,0,"The log-rank is a more powerful test than Cox PH when the proportionbal Hazards assumption is satisfied. So with a single 2-level covariate, a log-rank or Mantel-Haenszel test is preferable.",2010-08-20 12:54:43.0,521.0,
2107,1844,0,see below for answer...,2010-08-20 13:11:51.0,521.0,
2108,1949,0,Interesting... I guess analytical chemists are very much behind the times! Mind telling me how both of these became discredited? I will look into your reference and see how the algorithms for these look like.,2010-08-20 13:31:16.0,830.0,
2109,1836,0,"@ars: Yes, it is: the URL in the subtitle confirms it.  Thanks for finding it.",2010-08-20 14:16:48.0,919.0,
2110,1941,0,"Thank you very much for this thought.  Indeed, this is a standard and well-documented approach to multiple censoring.  One difficulty lies in its intractability: those integrals are notoriously difficult to compute.  There's a modeling problem lurking here, too: the value of *d* is usually positively correlated with *Y*, as implied by the first paragraph of my description.",2010-08-20 14:22:58.0,919.0,
2111,1872,0,"Thank you Kevin - I went with this option, went a head and ordered his book.",2010-08-20 14:37:13.0,253.0,
2112,1879,0,"Great answer Jeromy, I will make use of the resources you offered. thanks!",2010-08-20 14:38:01.0,253.0,
2113,1912,0,"He says:
""not stat'l power, but scale invariance loses the power that can be extracted from knowledge of the substance""
http://twitter.com/kinggary/status/21624260888",2010-08-20 14:40:01.0,449.0,
2116,1912,0,"Thanks @onestop: now I understand what he meant.  Even though I know the answer, I'll update the question for future searches.",2010-08-20 15:21:20.0,5.0,
2117,1956,1,You can use the HMM package: http://cran.r-project.org/web/packages/HMM/index.html.,2010-08-20 15:26:12.0,5.0,
2118,1943,0,"@onestop: hmm, think definition of O/E == LR with the log forgotten above? I agree with what you say about Cox PH -- that's not the question I was trying to answer, but your advice is better in the broader context.",2010-08-20 15:27:21.0,251.0,
2119,1946,0,Let's not confuse the z-score with the process of standardization.  The former is described in ars' answer: it is a statistic.  Standardization consists of replacing each of the X[i] by (X[i]-m)/s for further analysis.  That's what nico was asking about.,2010-08-20 15:58:49.0,919.0,
2120,1844,0,@Thylacoleo: the log-rank test is equivalent to the Score test under Cox PH. Think you're confusing PH *tests* with HR *estimators* here.,2010-08-20 16:10:02.0,251.0,
2121,1961,1,"In the context of categorical variables, one usually talks of a ""measure of association"" instead of ""correlation"". Searching for this term should lead you to lots of options.",2010-08-20 17:20:03.0,279.0,
2122,1958,0,"Thanks for the suggestions. I'll have to look into them in more detail, but one thing that occurs immediately is that the series do not share a time frame, so cointegration and VAR are probably not relevant. I've edited the question to try to clarify this, apologies for being misleading before. Noise filtering brings up some other issues, but those are for another question and another day!",2010-08-20 17:44:04.0,174.0,
2123,1958,0,"Ok.  Well, looking at the distributions may be the way to go.  You can also try a distribution test such as Anderson–Darling (as in this question: http://stats.stackexchange.com/questions/1645/appropriate-normality-tests-for-small-samples).",2010-08-20 18:01:16.0,5.0,
2124,1961,0,Thanks Aniko. I'll correct my question here (and also look for it),2010-08-20 18:04:46.0,253.0,
2125,1947,0,"Nice answer, Robin!",2010-08-20 18:07:46.0,251.0,
2129,1895,0,@whuber: must be the maximum resolution - a quarter of a microsecond ain't bad! 22-bits resolution-ish?,2010-08-20 19:51:40.0,1001.0,
2130,1963,0,Thank you @shane for creating the meta-analysis tag: I wanted to but couldn't.,2010-08-20 20:38:57.0,919.0,
2133,1965,0,"You're correct for the case beta = 0, but not for positive values of beta: you lost everything by throwing away the -beta*n*log(n) term.",2010-08-20 21:41:12.0,919.0,
2134,498,0,"I'm with Peter and Srikant. While I generally support the policy change of being more broadly accepting of computing questions, this one seems far removed from statistical content and more like a question for tech support. One here and there doesn't bother me, but I think it would be seriously detrimental if the main page became cluttered by such questions.",2010-08-20 22:32:00.0,71.0,
2135,498,0,"Keeping the majority of questions about statistical analysis will encourage experts to check out questions that might lie outside of the immediate domain.  If we get to the point where tags have to be used to sort past all of the questions about interfacing with statistical programs, I suspect we'll lose valuable cross-pollination.",2010-08-20 22:33:43.0,71.0,
2136,498,0,"For example: the GIS Exchange (http://gis.stackexchange.com/) is very permissive about what kinds of questions can be asked there, and as a result, the main page is almost completely dominated by technical questions.  That's fine and clearly what the community there wants, but it makes it tough to find questions that grapple with interesting problems or applications - so I largely ignore most of the questions and overall participate much less than I do here.  Interesting theoretical questions and applications are the majority of what I see here, and I'd like to keep it that way.",2010-08-20 22:47:36.0,71.0,
2137,498,0,(Sorry for the big block o' text),2010-08-20 22:48:20.0,71.0,
2139,1965,0,"uhh, no, not really. I assumed the OP was interested in an asymptotic (in $n$) bound. the worst case for my lower bound is $\\beta = 1$. Then for, say, $n > 35$, we have $\\log{n} < 0.1 n$, and thus $\\log{L} = -n\\log{n} + n^2 > 0.9 n^2$. Using L'Hopital's rule, you can show that $\\log{L} \\ge (1 - \\epsilon) n^2$, asymptotically in $n$, for every $\\epsilon >= 0$.",2010-08-21 00:47:02.0,795.0,
2140,1972,0,Articles directly about this would be hard to find... perhaps look up least significant difference scores... which is equivalent to what you ware trying to do.,2010-08-21 02:09:24.0,601.0,
2141,1961,1,"After a second thought - what I am looking at here is ordered, so looking at three levels as categorical would not fit here...",2010-08-21 03:40:05.0,253.0,
2142,1962,2,Quick note: Phi and Pearson correlation give the same value. Phi is just a simpler calculation procedure.,2010-08-21 05:10:33.0,183.0,
2143,1962,0,@Jeromy: thanks.,2010-08-21 05:50:03.0,251.0,
2144,1982,0,These look like some good examples. Cheers.,2010-08-21 07:02:06.0,183.0,
2145,1844,0,"ars, you are probably correct. I wrote my comment from memory and it doesn't seem correct on reflection.",2010-08-21 09:48:28.0,521.0,
2146,1962,1,And Cramer's V gives the same value as Phi for 2x2 tables.,2010-08-21 11:02:27.0,1356.0,
2147,1984,0,"No, not formally -- LaTeX submission is encourages but if you look at the [instructions page](http://www.jstatsoft.org/instructions) it does not contain the word Sweave.  Authors do use it and/or ship the R code with the paper, but to me this echos Shane's point about package vignettes.",2010-08-21 11:44:07.0,334.0,
2148,1984,0,"Ok, still most submitters do use it (also journal style includes Swave.sty); the main problem is that there are no Rnws published, still papers made by Sweave come with Stangle output.",2010-08-21 13:21:08.0,88.0,
2149,1965,0,"oops, should be $\\epsilon > 0$; letting $\\epsilon = 0$ is right out.",2010-08-21 16:03:09.0,795.0,
2150,1884,7,"You ought to credit the person who *really* answered this question (George Lowther), especially because you copied his answer with only trivial changes.  See http://math.stackexchange.com/questions/2763/what-is-the-expected-number-of-runs-of-same-color-in-a-standard-deck-of-cards .",2010-08-21 17:17:44.0,919.0,
2152,1359,1,"@Brett There are a few book recommendations on my [related question](http://stats.stackexchange.com/questions/1815/recommended-books-on-experiment-design) and some others may accrue in time. Not sure how well they fit your criteria, but might be worth a glance.",2010-08-21 18:18:12.0,174.0,
2154,1986,0,"I know about Tukey's HSD. I think I was unclear in my question. While i'm doing ANCOVA, i'm not trying to generate CIs from post-hoc pairwise comparisons. I'm just generating them by bootstrapping the samples in each level of the factor (also tried using the t-distribution). The CIs are for the mean of each individual group (factor level). I agree with Tal's comment that this does not allow differences between groups to be assessed. It's easy to do a Bonferroni correction on these CIs, is it possible to do the Hochberg (or other) method? (p.adjust requires a vector of p values I don't have).",2010-08-21 23:26:12.0,1029.0,
2155,1868,0,Your blog is chock full of useful nuggets; thanks!,2010-08-22 00:21:34.0,251.0,
2156,1998,0,"The following question on [""intuitive explanation of multicollinearity""](http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-re) may be of some help in the above context.",2010-08-22 01:30:24.0,,user28
2157,1997,2,"I second this answer and would just like to add another great reference on this topic:  Singer's Applied Longitudinal Data Analysis text <http://gseacademic.harvard.edu/alda/>.  Though it is specific to longitudinal analysis, it gives a nice overview of MLM in general.  I also found Snidjers and Bosker's Multilevel Analysis good and readable <http://stat.gamma.rug.nl/multilevel.htm >.  John Fox also provides a nice intro to these models in R here <http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf>.",2010-08-22 03:17:00.0,485.0,
2158,2002,0,Notice that the span measure would mean different window size for different number of observations.,2010-08-22 04:10:17.0,253.0,
2159,2007,1,I advise this to be a community wiki.,2010-08-22 04:13:00.0,253.0,
2160,2007,0,sounds like a homework question,2010-08-22 06:15:00.0,74.0,
2161,2008,6,"If your aim is to judge how much that citing article is ""important"" or ""good"" by itself, you could always try reading it...",2010-08-22 07:47:05.0,449.0,
2163,2007,0,"@Tal Certainly, now converted.",2010-08-22 09:48:01.0,88.0,
2164,1868,0,@ars. Thanks for that.,2010-08-22 09:57:07.0,183.0,
2165,2008,2,"Well, reading takes time and you may not even have the article available to begin with, so trying to estimate the relevance and quality vs time and maybe even money investment required is crucial.",2010-08-22 10:40:55.0,979.0,
2166,1763,0,stat579: Where exactly are the video lectures?,2010-08-22 11:52:22.0,339.0,
2167,1763,0,@gd047 That's sad. stat579 videos appear to have been removed. I've removed the link from my answer.,2010-08-22 12:07:15.0,183.0,
2168,1884,0,"@whuber Yes definitely, it's not my credit. I merely reproduced it here. Thanks for putting up that note.
@Karsten It is shown by a counting argument - once you have fixed $X_n$ in 52 ways, there are 26 ways you can select non-matching $X_{n+1}$. So $P(X_n\\ne X_{n+1})=\\frac{52*26}{52*51}$.",2010-08-22 12:35:10.0,994.0,
2169,1884,0,"@Karsten If you are asking if the events $X_n \\ne X_{n+1}$ are independent of $n$, they are not. But that doesn't matter since expectation of sum = sum of expectations even if the quantities are dependent.",2010-08-22 12:38:02.0,994.0,
2170,2007,0,@el chief - It's a very broad and general question... but I'm afraid it's not a homework question.,2010-08-22 13:16:01.0,1026.0,
2172,1763,0,"Perhaps because this ""newer version"" is available. http://www.public.iastate.edu/~hofmann/stat480/",2010-08-22 14:32:49.0,339.0,
2173,2023,0,"Thanks. I don't have easy access to the full-text, but from the opening page it does look useful regarding point 1 (i.e., combining two correlations).",2010-08-22 14:55:24.0,183.0,
2174,2024,0,I alfred.  What model (on your data) do you use to estimate the mean and SE ?,2010-08-22 14:59:10.0,253.0,
2175,1763,0,The videos appear to still be available online. I added the links.,2010-08-22 15:36:08.0,183.0,
2176,2024,0,Can you post an example graph?,2010-08-22 16:10:03.0,88.0,
2177,1763,0,@Jeromy Anglim Ahhh! You did a great job! (I wonder how). Is there any video from stat480 online also?,2010-08-22 16:20:18.0,339.0,
2179,2013,1,you could add Red-R to the list (kind of a clone of Orange in R): http://www.red-r.org/,2010-08-22 19:10:44.0,170.0,
2180,2027,0,"Thank you John & Tai

I guess that covers any potential avenue of my poorly formulated question.

regards

Alfredo",2010-08-22 19:23:31.0,,Alfred
2181,2032,0,Please make this community wiki.,2010-08-22 22:42:42.0,5.0,
2182,2032,0,Possible duplicate: http://stats.stackexchange.com/questions/652/best-books-for-an-introduction-to-statistical-data-analysis,2010-08-22 22:43:40.0,5.0,
2183,2027,0,"You're welcome Alfred.  Please choose Johns answer so to verify you found your answer :)  Cheers,
Tal",2010-08-22 23:12:32.0,253.0,
2185,949,1,"I assume you mean ""when are the MLEs of the parameters in closed form?""",2010-08-23 00:43:52.0,805.0,
2186,203,1,"A related question: People often use the nonparametric Mann-Whitney test for this kind of data. Since there are only five possible values, there will be lots of tied ranks. The Mann-Whitney test adjusts for tied ranks, but does this adjustment work when there are a huge number of ties?",2010-08-23 01:22:52.0,25.0,
2188,707,0,"@mbq: I say the ""usual"" choice. Does not mean every choice",2010-08-23 02:52:22.0,188.0,
2189,2034,0,"Good point. However, in research settings this may or may not be possible. Means and standard deviations are typically known. However, in some cases a different measurement tool is used in the two studies. E.g., Imagine a study looking at the correlation between intelligence and job performance across jobs and two studies varied in the measurement of job performance or in the particular intelligence test used. Thus, the observed variables are on different metrics, even though the theorised latent variables are the same.",2010-08-23 03:40:13.0,183.0,
2190,1853,0,univariate. but now you've made me curious about the multivariate case. ;),2010-08-23 03:49:11.0,795.0,
2191,2032,0,I collected some links here: http://meta.stats.stackexchange.com/questions/6/what-should-our-faq-contain/361#361,2010-08-23 04:47:35.0,251.0,
2195,2013,0,I've downloaded R and I am playing with it now.,2010-08-23 11:52:45.0,1026.0,
2196,2036,0,"Thanks!, let me check it out!",2010-08-23 12:05:31.0,1043.0,
2197,2032,0,Community-wiki'ed.,2010-08-23 13:39:32.0,1049.0,
2198,2034,0,"A necessary condition for combining these would be some quantitative relationship between the two forms of measurement, Jeromy.  Absent that, it's difficult to see how any combination of the two study results could be adequately defended.",2010-08-23 13:59:41.0,919.0,
2199,2037,0,"I changed the tag from ""truncated-gaussian"" to ""truncation"" because most answers will be potentially useful in situations involving other distributions.",2010-08-23 14:21:10.0,919.0,
2202,1997,0,"Thank you all for your responses :) As a follow up question, couldn't most data be conceptualized as being naturally hierarchical/nested? For example, in most psychological studies the there are a number of dependent variables (questionnaires, stimuli responses, etc...) nested within individuals, which are further nested within two or more groups (randomly or non-randomly assigned). Would you agree that this represents a naturally hierarchical and/or nested data structure?",2010-08-23 16:20:47.0,835.0,
2203,1988,0,"Carlos, thank you for your contribution, but the level of scientific validation that I need should, at least, give me a ""probability"" value of that claim. Thanks, anyway.",2010-08-23 16:20:59.0,990.0,
2204,1999,0,"Thank you for your detailed reply Srikant :) I am not currently familiar with Bayesian analyses, but i is one of the topics that I have been meaning to investigate. Is Hierarchical Bayesian analysis different from the other multilevel/hierarchical analyses discussed on this page? If so do you have a recommended resource for interested parties to learn more?",2010-08-23 16:24:56.0,835.0,
2205,1997,0,"If any of you multilevel/hierarchical gurus could spare a few minutes I would be very grateful if you could weigh in on the analysis questions posed in a different post (http://stats.stackexchange.com/questions/1799/recommendations-or-best-practices-for-analyzing-non-independent-data-specifi). Specifically, do you think that the pain perception data outlined in that post would be better analyzed by hierarchical analyses than non-hierarchical analyses? Or would it not make a difference or even be inappropriate? Thanks :D",2010-08-23 16:31:20.0,835.0,
2206,1999,0,"From an analytical perspective HB analysis = multi-level models. However, the term multi-level models is used when you have different levels that occur naturally (See the example of @ars). The term HB models is used when you do not necessarily have different levels in the situation. For example, if you are modeling a consumer's response to various marketing variables (e.g., price, adv spend etc) then you may have the following structure at the consumer level: $β_i \\sim N(\\bar{\\beta},\\Sigma)$ and $\\bar{\\beta} \\sim N(.,.)$ at the population level. For references: See the other answers.",2010-08-23 16:35:38.0,,user28
2207,1943,0,"Bernstein et. al. show some reasons (small n, ties) that cause the two methods to be inaccurate or different. But all of the discrepancies they showed are small. So I don't think anything in that paper explains the three fold discrepancy I saw that prompted this question. See below for the answer I came up with.",2010-08-23 17:38:22.0,25.0,
2209,1965,0,"The point is that O(n^2) is too crude.  You obtained the next term, -beta*n*log(n), but then threw that away.  So your answer, O(exp(n^2)), is correct but it's not best.",2010-08-23 17:57:47.0,919.0,
2210,2013,0,"@Amro Thanks! However, it is not available on Mac platform, unless I'm mistaking?",2010-08-23 18:37:46.0,930.0,
2213,2036,0,"According to reference, the distribution function is: $g(y) = C\\sum_{k=1}^\\infty\\frac{\\delta_k y^{\\rho+k-1}e^{-\\frac{y}{\\beta_1}}}{\\Gamma(\\rho+k)\\beta_i^{\\rho+k}} $",2010-08-23 20:53:17.0,1043.0,
2216,2036,0,where $C = \\sum_{i=1}^n\\left(\\frac{\\beta_1}{\\beta_i}\\right)^{\\alpha_i}$,2010-08-23 20:59:32.0,1043.0,
2217,2036,0,And $\\gamma_k = \\sum_{i=1}^n\\alpha_i\\frac{(1 - \\frac{\\beta_1}{\\beta_i})^k}{k}$,2010-08-23 21:19:17.0,1043.0,
2218,2036,0,"with
$\\rho = \\sum_{i=1}^n\\alpha_i$",2010-08-23 21:19:36.0,1043.0,
2219,2036,0,"And finally 
$\\delta_k = \\frac{1}{k+1}\\sum_{i=1}^{k+1}i\\gamma_i\\delta_{k+1-i}; \\delta_0=1, k=0,1,2,3,\\ldots$",2010-08-23 21:19:54.0,1043.0,
2220,386,1,"This is a nice idea, but if I have understood it correctly, it seems to enlarge the idea of ""outlier"" to include *any* value in a dataset that is distant from the others.  For example, in the batch {-110[1]-90, 0, 90[1]110} of 43 integers, wouldn't your procedure identify the 0 (which is the *median* of these numbers!) as the unique ""outlier""?",2010-08-23 22:36:55.0,919.0,
2221,1965,0,"It is not clear what kind of bound the OP is looking for. Because the free variable was given as $n$, I assume a bound asymptotic in $n$ is desired. In my answer I note that this is an asymptotic bound. (the big-O notation is another hint that this is the case.) The bound that I showed is an asymptotic *lower bound*. I am not 'throwing away' the log term: $\\Omega{(n^2 - n\\log{n})}$ is $\\Omega{(n^2)}$. Then since this lower bound is the same as the trivial upper bound, the trivial upper bound is optimal.",2010-08-23 22:47:04.0,795.0,
2222,2059,2,"EM is _not_ a heuristic, it is an algortihm for maximizing likelihood with known properties. The error you are looking for probably has nothing to do with EM, but rather the efficiency of the maximum likelihood estimate. This will surely depend on the unknown parameters, and not just the sample size.",2010-08-24 00:00:20.0,279.0,
2223,2059,0,"Yep, my question refers to those ""known properties"". It is used as a heuristic in the sense that I see people apply it on ""good faith"" beyond what is explained by provable bounds. I changed the wording of the question, since this is really not the issue I'm interested in.",2010-08-24 00:59:35.0,1072.0,
2224,2060,0,"@Moritz : In reply to your comment about EM being a heuristic, EM does have strong theoretical foundations and it is known to provably converge to a local maximum.",2010-08-24 01:13:51.0,881.0,
2225,2061,0,Great question!!,2010-08-24 02:54:22.0,253.0,
2226,2062,0,This looks good. Thanks.,2010-08-24 03:14:02.0,183.0,
2229,1762,0,I think I'm going to simply write a simulator and tabulate results.,2010-08-24 06:20:45.0,937.0,
2230,2066,1,Do you want to know the mathematics or simply a code solution in R or some such?,2010-08-24 10:40:50.0,601.0,
2231,2066,0,"I need to implement this in C#, so the mathematics would be good. A code sample would be fine too, if it doesn't rely on some builtin R/Matlab/Mathematica function I can't translate to C#.",2010-08-24 10:55:38.0,956.0,
2232,1873,4,He's clumsy enough to change his name in the middle of a question at least! ;),2010-08-24 11:10:19.0,229.0,
2233,2056,1,"Interesting, I'll have to look that reference up. Are you sure you mean ""fourth"" approach? The first approach I list seems to describe ""simply resampling the subjects"".",2010-08-24 11:17:48.0,364.0,
2234,2067,1,"Can you clarify the predicted variable, ""mean endorsement""?. Is this a 0-100 scale that participants used for response, or is this a measure of the proportion of trials on which participants said ""yes, I endorse"" (vs. ""no, I do not endorse""). If the latter, then it is inappropriate to analyse this data as proportions. Instead, you should be analyzing the raw, trial-by-trial data using a mixed effects model with a binomial link function.",2010-08-24 11:36:06.0,364.0,
2235,2067,0,"Sorry, for omitting this: it is a 0-100 response scale.",2010-08-24 11:39:40.0,442.0,
2236,1898,0,"what is FWE and FDR? I suppose FWE to be family wise error, but the other??",2010-08-24 11:56:34.0,442.0,
2237,2069,0,"I am not quite sure I understand your recommendation in 1. As actual SE [i.e., SD/sqrt(n)] and estimated SE are both model based, you recommend using the model-based. So which one?
Or do you mean: go with the more complicated model (here: ANOVA) cause both models are reasonable.",2010-08-24 12:07:14.0,442.0,
2238,2069,0,agree with point 1 completely,2010-08-24 12:09:29.0,601.0,
2239,2067,0,"Do you have many 0's or 100's? If not, I'd consider dividing by 100 and performing a logit transform to take into account the restriction of range at the extremes. This is essentially what is achieved by the binomial link function when you have binary data, but is useful if you only have proportion-like data as you seem to have here. However, you can't logit transform 1 or 0, so you'd have to toss any responses of 100 or 0.",2010-08-24 12:16:05.0,364.0,
2240,2068,0,"I have good (non-statistical) reasons to plot the graph as it is: You directly see the answer to the research question. Furthermore, I am not looking for a error bars for inferential purposes as I know about the within-between problems. But, thanks to pinpointing me back to Mason & Loftus, I must have forgotten that they had a mixed example. I have to think on whether or not it serves my purpose.",2010-08-24 12:16:29.0,442.0,
2241,2067,0,"Oops, just realized that my first comment was not 100% correct. Each plotted mean represents the mean of two responses on a 0-100 scale. In this data there are a lot values very near to 100, and some directly on 100, but actually very little at 0 and around 0. You have some literature for justifing your recommendation?",2010-08-24 12:28:31.0,442.0,
2242,2066,0,"PDF, CDF or inverse CDF?",2010-08-24 12:29:48.0,830.0,
2243,604,0,"@Srikant Because the RSS would vanish in this case, attached is a toy example: http://gist.github.com/547494 (I did not investigate the DA results though).",2010-08-24 12:59:06.0,930.0,
2244,2013,0,"I'm not a Mac user, but I think the Linux build could work for you (you need to manually install all python dependencies): http://www.red-r.org/forum/topic.php?id=22#post-76",2010-08-24 13:03:57.0,170.0,
2245,2067,0,"As a side note, because the conditions are not varying along continuous dimensions some data visualization people might claim this data is better represented by bar graphs.",2010-08-24 13:25:06.0,196.0,
2246,2070,0,"Mike, in the package languageR the pvals.fnc function does an MCMC to evaluate the hypotheses of the lmer model - however it does not handle designs with random slopes - that lead me to suspect that there was some reason doing MCMC with random slopes was in someway problematic, do you know definitively that there is no such problem?",2010-08-24 13:32:04.0,196.0,
2247,1898,0,"Well, this thread is subjective, so who knows...  Now seriously - FDR stands for false discovery rate (wikipedia it)",2010-08-24 13:45:03.0,253.0,
2248,2070,0,"I have to admit I still haven't figured out how MCMC works, which is one of the reasons I opted for bootstrapping instead. While bootstrapping should be possible with random slopes, as you intimated, it may be that  pvals.fnc doesn't let you do CIs for models with random slopes because this is for some reason invalid, and further it may be that this invalidity extends to bootstrapping such models. I don't intuitively think there would be any problem with bootstrapping, but that may be a function of my limited expertise.",2010-08-24 14:28:02.0,364.0,
2249,2073,0,"When alpha and beta are not too far apart (i.e., alpha/beta are bounded above and below), the SD of Beta[alpha, beta] is proportional to 1/Sqrt(alpha).  E.g., for alpha = beta = 10^6, the SD is very close to 1/Sqrt(8) / 1000.  I think there will be no problem with the representation of l and r even if you're only using single precision floats.",2010-08-24 15:25:46.0,919.0,
2250,2071,0,"Stupid question: How did you do that graphical experiment? I tried to plot the distribution for alpha/beta around 100, but I couldn't see anything due to underflow errors.",2010-08-24 15:57:41.0,956.0,
2251,2073,0,which is to say that $10^6$ is not 'sufficiently large' ;),2010-08-24 16:25:59.0,795.0,
2252,931,0,"No, although there are plug-ins (for Firefox, at least) that help with downloading Flash.  Not easy in all cases, but possible in most.",2010-08-24 16:57:58.0,71.0,
2253,2073,1,"Yeah, it's a crazy number for a beta application.
BTW, those inequalities won't produce good intervals at all, because they are extremes over all distributions (satisfying certain constraints).",2010-08-24 17:19:34.0,919.0,
2254,2071,0,"You don't want to plot the integrand: you want to plot the integral.  However, you can get the integrand in many ways.  One is to enter ""plot D (beta(x, 1000000, 2000000), x) / beta(1, 1000000, 2000000) from 0.3325 to 0.334"" at the Wolfram Alpha site.  The integral itself is seen with ""Plot beta(x, 1000000, 2000000) / beta(1, 1000000, 2000000) from 0.3325 to 0.334"".",2010-08-24 17:27:44.0,919.0,
2255,2074,0,"Perfect! I had the NR book on my desk all the time, but never thought to look there. Thanks a lot.",2010-08-24 17:34:37.0,956.0,
2256,2067,1,Other data visualization people might claim that bar graphs are a crime against humanity :Op,2010-08-24 17:52:21.0,364.0,
2257,2065,0,"I figured out the answer two days ago, and posted it here as a new answer. I also then expanded and updated the web page at graphpad.com that you found. I just edited that page again to include a link to an Excel file with the problem data (http://www.graphpad.com/faq/file/1226.xls). I couldn't do that until I got permission from the guy who generated the data (he wants to be anonymous, and the data is vaguely labeled).",2010-08-24 18:28:08.0,25.0,
2258,1944,0,"btw, this is special case of ""whitening"" which takes data and makes it's empirical covariance matrix identity",2010-08-24 19:30:40.0,511.0,
2259,1931,0,Any measure that coincides with the usual median when restricted to R1 is a candidate generalization. There must be a lot of them.,2010-08-24 19:37:24.0,1011.0,
2260,2073,0,"@whuber: You're right, they are crazy numbers. With my naive algorithm, the ""sane"" numbers were easy and worked well, but I couldn't imagine how to calculate it for ""crazy"" parameters. Hence the question.",2010-08-24 19:52:59.0,956.0,
2261,1963,0,"Thank you @propofol and @Jeromy Anglim for your excellent contributions: after skimming DeCoster and looking over the Amazon reviews of the books, I have passed them all along to my colleague.  It is a real shame one cannot checkmark multiple answers; in this case I literally flipped a coin: tails, it went to propofol.",2010-08-24 22:29:17.0,919.0,
2262,2081,0,"Yes, it looks like it needs to be solved numerically in general. I really just needed a practical way of plotting epsilon as a function of n, and ended up using a version of Chernoff inequality for fair coin from which expression for epsilon can be obtained in closed form",2010-08-24 23:00:01.0,511.0,
2263,2069,0,"Hi Henrik,
Simple example - compare two groups (x1, x2) assumed ND.
Assumptions & models:

1) Independently sampled, different variance. SEs for x1, x2 estimated separately. This is implicitly the assumption in many graphical presentations. The estimated SEs differ.
 
2) Indep., same var. Usual ANOVA assumption. Estimate SEs using pooled RSS. Estimate is more robust IF assumptions correct.

3) Each x1 has an x2 pair. SEs estimated from x1-x2. To effectively plot them you need to plot the difference x1-x2.

Once you mix 1) and 2) you have a real problem plotting meaningful SEs or CIs.",2010-08-25 00:42:31.0,521.0,
2264,2065,0,"Thanks Harvey,
I thought your answer was very good.",2010-08-25 00:47:26.0,521.0,
2265,2069,0,"Henrik, a comment on the plot. How many subjects do you have? I would strongly recommend plotting the data individually and use line segments to link individuals. (Line segments linking means is deceptive.) There is no need to plot SEs. The idea is to visually support your statistical analysis. Provided the plot does not become too cluttered, a reader should see (for example) that the clear majority of scores go up from MP-valid-implaus to AC-inval-plaus for the Inductive group & down for the Deductive group. See:
http://www.jstor.org/stable/2685323?seq=1
Especially Figs 1 & 9 bottom panels.",2010-08-25 02:01:21.0,521.0,
2266,2087,1,"Thanks Tal, I'll try corrplot now. I also wish I knew how to simplify your solution (which I linked to in the question) but I'm just a newbie in R so you know more than me. I'll update the question to clarify the solution looks complicated *to me*",2010-08-25 04:04:52.0,840.0,
2267,2086,2,You sound like R is inferior to propriety software. :),2010-08-25 05:49:51.0,144.0,
2268,2076,0,Consider this posting as a stand-alone question. I would be very interested what gurus have to say about this.,2010-08-25 06:02:31.0,144.0,
2269,2093,0,Do you want to generate normal random variables truncated to range between 0 and 1 or uniform random variables between 0 and 1? The Marsagla polar method will return numbers over the entire real line as it generates standard normal random variables.,2010-08-25 09:46:57.0,,user28
2271,2086,0,"For me it sounds totally reasonable to use the pearson product-moment-correlation (assuming continuous data) in your case (assuming enough points on your scale and not a don't know midpoint). Whole fields within psychology (e.g., personality or social psychology) rest (successfully) on the assumption that answers to a single item on an e.g., five-point (or seven-point) scale ranging from very un-X to very X can be treated as continuous. See also this thread: http://stats.stackexchange.com/questions/539/does-it-ever-make-sense-to-treat-categorical-data-as-continuous",2010-08-25 09:59:53.0,442.0,
2272,2093,0,"I don't know if this solves your problem, but you might wanna take a look at this question where some transformations that squash everything between 0 and 1 are discussed: http://stats.stackexchange.com/questions/1112/how-to-represent-an-unbounded-variable-as-number-between-0-and-1/1113#1113",2010-08-25 10:40:47.0,442.0,
2273,2086,0,@romunov: Not sure how you got the impression that I believe R is inferior to other s/w. But it's not the case at all.,2010-08-25 12:45:32.0,840.0,
2274,2071,0,"I plotted the integrand, i.e. the pdf of the beta distribution, in Stata - it has a builtin function for the pdf. For large alpha and beta you need to restrict the range of the plot to see it's close to normal.


If I was programming it myself i'd compute its logarithm then exponentiate at the end. That should help with the underflow problems. The beta function in the denominator is defined in terms of gamma functions, equivalent to factorials for integer alpha and beta, and many packages/libraries include lngamma() or lnfactorial() instead/as well as gamma() and factorial() functions.",2010-08-25 12:51:07.0,449.0,
2275,2090,0,"Although most of them don't seem relevant for the OP questions, still, this is a nice link - thanks Jeromy!",2010-08-25 13:20:17.0,253.0,
2276,2057,0,"That was a great insight! All your assumptions can be considered, and your rationale is sound! I do have some doubts though, if you don't mind: 

(I) Shouldn't d2[j,i] = h[j,i+2] ... ?

(II) Why do you consider the increment as y[j,i+2] - 2*y[j,i+1] + y[j,i] and not just y[j,i+1]-y[j,i] ? Is it a kind of derivative (i.e., to calculate the ""slope"")?",2010-08-25 13:25:00.0,990.0,
2277,2088,0,"The Sunflower is a fun solution.  Using a jitter is what I tried when first I looked at the topic, but I found it do be not effective enough for the plotting of correlation matrixs...",2010-08-25 13:26:17.0,253.0,
2278,2057,0,"(1) Yes, good catch.  I fixed that typo.

(2) We're really looking at *second* derivatives: the increments d[j,i] = h[j,i+1] - h[j,i] act like slopes (and are slopes when times are equally spaced).  I am proposing to distinguish ""exponential"" from ""sublinear"" behavior in terms of changes in the slopes.  Whence we want to consider the signs of

d2[j,i] := d[j,i+1] - d[j,i] = (h[j,i+2] - h[j,i+1]) - (h[j,i+1] - h[j,i]) etc.",2010-08-25 13:32:12.0,919.0,
2279,2087,0,"The corrplot looks good. It gives a great visual snapshot of size and direction of correlations. In the case of 5-point ordered categorical variables, it might be useful to supply some other measure of association besides Pearson's correlation: e.g., polychoric correlations. The size of standard Pearson's correlations of ordered categorical variables is influenced somewhat by the mean of the two variables.",2010-08-25 13:38:35.0,183.0,
2280,2088,0,"Yeah, jitter could get pretty messy with a scattermatrix with lots of variables. I suppose the benefit of jitter and sunflower is that you get to see the raw data (albeit perturbed in the jitter case).",2010-08-25 13:40:47.0,183.0,
2282,2085,3,"R and S-Plus are very similar, but definitely not the same (I know from painful experience!).",2010-08-25 13:44:10.0,5.0,
2283,30,0,"Retagged ""random-variate"" to the more popular (and more appropriate) ""random-variable"".",2010-08-25 14:13:18.0,919.0,
2284,40,0,"Retagged ""random-variate"" to ""random-variable"" for consistency with similar questions.",2010-08-25 14:14:06.0,919.0,
2285,111,0,"Are you sure about your second point, generating normal random variables by inverting the CDF?  The inverse of the normal CDF is a fairly expensive function to evaluate.  I imagine Box-Muller's method would be faster.  Faster still would be Marsaglia's ziggurat method for generating normals.",2010-08-25 14:53:59.0,319.0,
2286,2073,2,"OK, you're right: once alpha+beta exceeds 10^30 or so, you will have difficulties with doubles :-).  (But if you represent l and r as differences from the mean of alpha/(alpha+beta), you'll be fine until alpha or beta exceed about 10^303.)",2010-08-25 15:34:21.0,919.0,
2287,2106,0,"Thank you for taking the time to answer my question. however, your response doesn't answer my question. I already mentioned in my post that I already came across the ezANOVA function. I tried it without success. I am also already familiar with the aov notation you metnion.",2010-08-25 15:55:28.0,1084.0,
2289,2106,1,"Do you get any error messages when using ezANOVA? Whenever I use it, I get a nice table of sphericity tests and GG/HF corrections.
It does not work for ANCOVA models, for this you will need to use the lme4 package, and there are other things to account for variance.",2010-08-25 16:05:43.0,966.0,
2290,2106,0,"It would probably help if you put the output of ""str(p12bl)"" or ""str(subset(p12bl, exp2==1))"" here.",2010-08-25 16:16:23.0,966.0,
2291,111,0,"I also find this suspicious. Marsaglia's Ziggurat is the default in Matlab, and I can't imagine Matlab being better than R in the field of random number generation.",2010-08-25 16:21:20.0,795.0,
2292,2088,0,"Agreed (I love jitter, simply not for this :) )",2010-08-25 16:32:54.0,253.0,
2293,2103,0,+1 A better answer than mine as it explicitly takes into consideration the range over which the pdf is non-zero.,2010-08-25 16:36:18.0,,user28
2295,2104,1,Do you want to look at sphericity or a factor to correct F-values because of violations in sphericity?,2010-08-25 16:50:57.0,601.0,
2296,2108,0,"This may be a question more appropriately asked on the stackoverflow forum, also you may be able to get a more concise answer if you describe what type of file the url is pointing to.",2010-08-25 17:38:06.0,1036.0,
2297,2103,0,"@Srikant: Thanks; you are generous.  Did you notice the connection with the theory of copulas?  Maybe we should even add a ""copula"" tag.",2010-08-25 17:39:22.0,919.0,
2298,2094,1,Ok this makes it clear. Exponential pdf can be used to model waiting times between any two successive poisson hits while poisson models the probability of number of hits. Poisson is discrete while exponential is continuous distribution. It would be interesting to see a real life example where the two come into play at the same time.,2010-08-25 18:03:50.0,862.0,
2299,2093,0,"@Ants: ""Scaling down"" from the real line to an interval will necessarily make the resulting generator non-Normal.",2010-08-25 18:14:27.0,919.0,
2300,2112,1,I was just about to recommend this.  The ez package relies on car for some analysis.,2010-08-25 18:47:42.0,601.0,
2303,2104,0,@John I need to see if I need the correction,2010-08-25 18:55:22.0,1084.0,
2304,2112,0,"Thank you, the R newsletter is where I picked the mlm direction from. alas, I am still figuring how to choose idata idesign for a nested within subject design. I will try to figure out Anova().",2010-08-25 19:14:54.0,1084.0,
2305,2111,1,"Are you refering to the following paper?
Kuk, A.Y.C. (1984) All subsets regression in a proportional hazards model. Biometrika, 71, 587-592",2010-08-25 19:22:05.0,930.0,
2306,2013,0,@Amro I'll give it a try; in the past I've been testing RAnalyticFlow (http://j.mp/bYF8xs) but did not get convinced: I am basically a CLI user :-),2010-08-25 19:52:42.0,930.0,
2307,2114,2,"BTW, the penalized R package (http://j.mp/bdQ0Rp) includes l1/l2 penalized estimation for Generalized Linear and Cox models.",2010-08-25 20:07:49.0,930.0,
2308,2105,0,I definitely want a normally distributed variable. I'll update the question with the specific problem I'm trying to solve.,2010-08-25 20:08:50.0,937.0,
2309,2098,0,"For rejection sampling, what will I use as my g(x) and M value to simulate the normal distribution? For inverse transform sampling, do I use D Ibbetson's Algorithm 209 (http://old.sigchi.org/~perlman/stat/doc/z.c) to compute the cdf? What do I use as the inverse?",2010-08-25 20:21:40.0,937.0,
2310,2117,0,"Not bad at all. The format is static, but the information is very useful.",2010-08-25 20:32:56.0,840.0,
2311,2102,0,"I want to get a ""normal"" distribution with the appropriate hump at 0.5.",2010-08-25 21:03:41.0,937.0,
2312,219,0,Maybe also L'Ecuyer's RNG with multiple streams (http://j.mp/bzJSlm)?,2010-08-25 21:08:09.0,930.0,
2313,2098,0,"@Ants If I am right, you don't need a cannon like ITS.",2010-08-25 21:25:22.0,88.0,
2314,111,0,"@John Indeed, the polar method is available in R, see the setRNG package.",2010-08-25 21:28:18.0,930.0,
2315,2098,0,"For rejection sampling set f(x) ~ standard normal pdf I(0<x<1), g(x) ~ standard normal and and M=1. Your condition will then become select x if u < I(0<x<1) which is basically what I stated in my one line sentence above under rejection sampling.",2010-08-25 21:34:16.0,,user28
2316,45,0,"e.g., Python (http://j.mp/9TVyhv), Perl (http://j.mp/by85El), Octave (http://j.mp/aVQ5Xz), Clojure (http://j.mp/dkj9Z9, not the default one), Haskell (http://j.mp/aWK7kK), lua (http://j.mp/bSD2vO), or even SQL (http://j.mp/aOPJMW, for an overview) :-)",2010-08-25 21:41:15.0,930.0,
2317,2103,0,"@whuber My familiarity with copulas does not go very far. In any case, I am not sure how many will see the connection. So, a copula tag may just confuse some.",2010-08-25 21:56:16.0,,user28
2318,2111,0,"yes indeed. I guess I'll have to dig up that paper somehow. It seems old, however.",2010-08-25 22:56:20.0,795.0,
2319,2114,0,"stuck in matlab land, implementing it myself...",2010-08-25 23:23:39.0,795.0,
2321,2098,0,@mbq: I don't get the ITS reference. I've been missing out on a lot of pop culture and current events references the past few months.,2010-08-26 00:37:51.0,937.0,
2322,2098,0,"@Srikant: Oh, okay. Thanks!",2010-08-26 00:39:19.0,937.0,
2323,2093,0,@Henrik: Thanks. I'll have to remember some of those tricks for future reference.,2010-08-26 00:41:23.0,937.0,
2324,2097,0,"Thanks for the pointer. LOL! My new implementation for NormalDistribution looks a lot like I'd taken a slice of that code project and focused on just normal random numbers. I've got the same abstract classes for RNG's and Distribution's. I've got just one concrete RNG that uses the .NET Random class, and a concrete NormalDistribution class.",2010-08-26 00:48:57.0,937.0,
2325,2103,0,@Srikant: Granted.  But isn't the point of tags to facilitate searches and create semantic ties among threads?  I'm sure plenty of existing tags also have the potential to confuse the uninitiated already.  Maybe this is a discussion for the meta site?,2010-08-26 01:05:29.0,919.0,
2326,2102,0,"Then you won't be able to restrict it to the interval [0, 1).  As a practical matter, normal probabilities further out than about seven standard deviations are too small to be worth computing, anyway, so you could set the standard deviation to 1/2 / 7 = 1/14 = about 0.07.  This isn't too far from the value you found empirically.  With your value and enough draws, you are likely to get a number outside [0, 1); with 1/14, it's extremely unlikely.  Thus, my recommendation is to take a value from the Marsaglia algorithm, divide by 14, and add 1/2.  Adjust the 1/14 to suit your application.",2010-08-26 01:08:43.0,919.0,
2327,1991,0,"But what does this have to do with statistical ""power"" in the King quotation?  (I would ask the same question of stormygeorge, too.)",2010-08-26 01:15:10.0,919.0,
2328,2125,3,I have no idea what this question is about.,2010-08-26 01:35:53.0,352.0,
2330,2125,18,It sounds like it is about the difference between correlation and simple linear regression.,2010-08-26 02:19:02.0,485.0,
2331,2098,0,@Ants ITS=Inverse Transform Sampling,2010-08-26 06:53:50.0,88.0,
2332,2097,0,"But as I can see they are not overloading Microsoft's original, so they can break the [0,1) limit.",2010-08-26 06:55:11.0,88.0,
2333,2131,0,This asymmetry does not look *so* bad; are you sure you don't have an error somewhere else? Also can you give some more details about the data? There is not a general solution to this.,2010-08-26 07:05:53.0,88.0,
2334,2089,0,"Another great answer! PyIMSL Studio sounds interesting, too bad it isn't open source. There's some overlap with NumPy/SciPy, though. In any case, I think these were good tips for anyone wanting to assemble their own Python statistics workbench!",2010-08-26 07:29:27.0,890.0,
2335,2097,0,"Yes, that is correct.",2010-08-26 08:30:42.0,937.0,
2337,2110,0,"Thanks Shane, it's actually an XML web service. I had a quick look at the XML package documentation, am I right in thinking that it just does the parsing? I'm still unsure of how to actually request the data, from the web service. Thanks",2010-08-26 10:07:01.0,428.0,
2338,2136,1,"Actually, ""conditional logit"" is a very ambiguous term. It some contexts (mainly when dealing with panel data), it is equivalent to Chamberlain's estimator, but it is very unfrequent. Most of the times, it refers to a cross-sectional model where the outcome variable can take more than 2 values. All your proposals actually refers to packages that consider this last possibility. The same with mixed-logit: it is not a fixed-effect logit. I've already taken a look at Farnsworth's overview, but it is not exhaustive enough to speak about this estimator. Anyway, thank you for your answer !",2010-08-26 10:11:31.0,1091.0,
2339,2110,0,"@Matt As Shane wrote; connect with RCurl, get the output and parse it with XML. We would be able to give you the code if you'd give more details about this web service.",2010-08-26 10:21:34.0,88.0,
2340,2135,2,+1; still ksvm from kernlab seems to be the preferred R SVM implementation. For instance it scales variables on its own and has a nice (=working very well) heuristic procedure for picking good $\\gamma$ for RBF.,2010-08-26 10:49:51.0,88.0,
2341,2123,0,"Thanks, that's all I've wanted to hear. Now I can permalink to your answer! =)",2010-08-26 12:27:31.0,1356.0,
2342,2138,0,"Excellent Henrik. I also prefer the ""points with means dislocated"". Linking subjects with line segments may look too cluttered. Pity. As to homogeneity of variance I am a little more sanguine. The variance problem may not be as bad as it looks in the raw data. For the most part I suspect you will be comparing contrasts - within group differences. Contrast variances will be more homogeneous that the variances of the raw data. If raw measures with different variances are  compared (eg. Inductive vs Deductive in the MP-valiad & plausible group) a non-parametric test could be used as a back-up.",2010-08-26 12:58:52.0,521.0,
2343,2093,0,"Your clarification completely confused me. You plan to generate a random normal variable `x1` truncated to the 0-1 range, then plug it into the `x1 * w * stdDev + mean` formula (what is `w`?) to get a normal with mean `mean` and standard deviation `stdDev`? If so, that's totally wrong. Just take a standard normal `z`, and calculate `mean + z*stdDev` to get a non-standard normal variable.",2010-08-26 13:01:13.0,279.0,
2344,2136,0,"""Conditional logit"" does *not* refer to having more than two outcome levels. Some functions might extend it to that situation, but that is not the point.",2010-08-26 13:09:34.0,279.0,
2345,2112,0,"@ John Anova() is supposed to take my aov() object as a model, but for some reason I am having problems making it work.",2010-08-26 13:21:49.0,1084.0,
2346,2141,0,"Thanks, that looks like exactly what I'm after.",2010-08-26 13:26:28.0,1094.0,
2347,733,1,"This remind me of a poster I saw at the Human Brain Mapping 2010 conference. Tom Nichols used parametric bootstrap to assess the stability of the cophenetic correlation and silhouette in hierarchical clustering, but see his poster: http://j.mp/9yXObA.",2010-08-26 13:27:22.0,930.0,
2348,2112,1,"@Adam In the on-line help for Anova(), you will find a working example with within- and between-subjects factors; the main caveats of Anova() is that you need to pass idata/idesign parameters in addition to your lm object. Also see this thread on the r-help mailing list, http://j.mp/9lphud.",2010-08-26 13:39:37.0,930.0,
2349,2103,0,"@whuber True. But, right now the connection to copulas is a bit obscure for those who do not know what they are. Perhaps, you can add a line or two to make the connection in which case the tag 'copula' may make sense.",2010-08-26 13:54:52.0,,user28
2350,2141,0,"In R that's var.test(q1, q2)",2010-08-26 14:08:55.0,601.0,
2352,1991,1,"@whuber Please see in the question where Gary King explicitly says that he is not talking about statistical power, but about ""the power that can be extracted from knowledge of the substance"".",2010-08-26 14:23:36.0,666.0,
2353,2100,0,"Thanks, Jeromy. Actually this is even better than what I asked.",2010-08-26 14:25:10.0,666.0,
2354,2093,0,"@Aniko: I was just using 'w' because that's what code that I grabbed had. I should have renamed it 'r2' to represent r-squared during the looping, and then added a new variable 'transform' for the 'w' in the second to the last line in the original code. When I gave up trying to get a return value from 0-1, I resorted to <code>x1 * w * stdDev + mean<code> but the code is not used in the context of overriding Random.Sample() anymore. How do I get <code>z<code> given the Marsaglia polar method?",2010-08-26 14:39:12.0,937.0,
2355,2145,0,"Does that mean we can always get 
$$\\frac{d^kf(a)}{dx^k} = \\gamma_k$$?",2010-08-26 14:57:54.0,1043.0,
2357,2093,0,"Sorry, I did not scroll up to the code. Now I see what `x1` and `w` are. Mea culpa. Then using `x1 * w * stdDev + mean` is correct (`z` is `x1*w`). I still don't get the part about having to have the result to be between 0 and 1, but I see that you are happy with mbq's answer, so I am happy to live with my confusion.",2010-08-26 15:40:23.0,279.0,
2358,2145,0,Almost: replace *a* by 0 and divide by *k!*.,2010-08-26 15:49:48.0,919.0,
2359,733,0,@chl Thanks; indeed recently I have seen similar thing done in my lab; the result was that the clusters are not significant though :-/,2010-08-26 15:50:29.0,88.0,
2360,1991,0,"Ah, thanks: I was focusing on the first quotation only.  But that leaves us wondering specifically what King means by ""power.""  Evidently it's something that is gained by not ""shirking responsibility"" ;-).",2010-08-26 15:52:58.0,919.0,
2361,2131,0,"my dataset have 17 predictors(3 continuous and 14 categorical), obviously 1 class variable, and total 1000 obsrvations. The frequency dist. for class var. with train and test is train(bad) = 197, test(bad)= 103, trian(good)= 446, test(good)= 254",2010-08-26 15:57:34.0,861.0,
2363,2136,1,"Yup, but the conditional logit model can (as I said) take more than 2 values, which differentiates it easily from Chamberlain's model, just like the fact that Chamberlain is designed specifically for panel data. This is thus a relevant information ; precise description of the usual conditional logit isn't (and the description of both would take more than 600 chars).",2010-08-26 16:07:32.0,1091.0,
2364,2127,0,"1) yes, the question is somewhat ambiguous; there are, as you mention, many definitions of 'optimal': via information criterion, cross validation, etc. Most of the heuristic approaches I have seen to the problem proceed by stepwise predictor addition/removal: single pass forward addition or subtraction, etc. However, Hosmer & Lemeshow make reference to this method (a variant of work by Lawless & Singhal), which somehow 'magically' selects predictors by a single computation of an MLR (modulo some other stuff). I am very curious about this method...",2010-08-26 16:55:58.0,795.0,
2365,2114,0,"LARS is great, BTW. very cool stuff. not sure how I can jam it into the framework of Cox Proportional Hazards model, tho...",2010-08-26 16:56:36.0,795.0,
2366,2146,1,Can you make your question more concise?,2010-08-26 17:53:25.0,88.0,
2367,2089,0,"It is free as in beer (for non-commercial use), but alas, not free as in speech.",2010-08-26 18:08:07.0,1080.0,
2368,2146,1,Please try your question again in clear and simple language,2010-08-26 21:04:07.0,74.0,
2370,2151,0,Is dimension reduction an option?,2010-08-27 01:59:57.0,5.0,
2371,2151,1,"we have 4 classes, not really",2010-08-27 03:03:06.0,,CLOCK
2372,2152,2,I would say the same providing another link http://www.google.gr/url?sa=t&source=web&cd=1&ved=0CB8QFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.98.4088%26rep%3Drep1%26type%3Dpdf&ei=nlx3TIq-LJCk4Abn_Li3Bg&usg=AFQjCNHO-_yjWAJrRVnJms7MbcqaJkd8eg&sig2=sqERq2v68UvOhJDMviOklg,2010-08-27 06:40:29.0,339.0,
2373,2152,1,"And here is another one, directly related to multi-class problem: Multi-class ROC analysis from a multi-objective optimisation perspective, Pattern Recognition Letters 2006 27(8): 918-927 (http://j.mp/9AMgzq).",2010-08-27 07:38:55.0,930.0,
2374,2112,0,"@Adam Did you mean the example included in the help file for Anova? In this case, I grabbed the R transcript and put it there, http://j.mp/9pGtKY. HTH",2010-08-27 08:30:41.0,930.0,
2375,2154,0,"Better than naming it Part 2 (or n), make a link to the previous question inside this.",2010-08-27 09:10:00.0,88.0,
2376,2114,2,"The Glmnet software has a lasso'd Cox PH model:
http://cran.r-project.org/web/packages/glmnet/index.html
there is also a MATLAB version (not sure if it does a cox model though):
http://www-stat.stanford.edu/~tibs/glmnet-matlab/",2010-08-27 09:29:41.0,495.0,
2377,1817,1,"I had a look at this, and it's perfectly fine but has a bit too much of the feel of an undergraduate text book for my tastes. Not that there's anything wrong with that...",2010-08-27 09:44:20.0,174.0,
2378,2086,0,I was just being a smart ass. I hope there's no hard feelings. :),2010-08-27 10:41:42.0,144.0,
2379,2126,0,"Note that in actual computations, one should not be forming $A^T A$ directly; exploit the QR or singular value decomposition of $A$ for this.",2010-08-27 10:45:22.0,830.0,
2380,1817,1,"Very nice book indeed! I've started long ago to reproduce his analysis (with Design Expert and SAS) using R, but never find time to finish it. If you like to check it out, http://www.aliquote.org/articles/tech/dae/",2010-08-27 11:17:11.0,930.0,
2381,2115,0,"Thanks. It worked - this way I can plot both the Class random effects (level=1), and the student (level=2).",2010-08-27 11:30:46.0,108.0,
2382,1544,0,Thanks for your response John.  I'm not sure how I would go about using mixed-effects modelling to test from equivalence - do you have any examples?  Thanks!  Sam.,2010-08-27 12:27:35.0,867.0,
2383,2152,1,"Thanks for the other links, apparently that whole issue is popular, and its archive on science direct can be found here
http://www.sciencedirect.com/science?_ob=PublicationURL&_tockey=%23TOC%235665%232006%23999729991%23621242%23FLA%23&_cdi=5665&_pubType=J&view=c&_auth=y&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=ece739626e7c50b228e85480cb2407e3",2010-08-27 13:17:58.0,1036.0,
2384,2146,0,I think it highly unlikely that you will get answered without clarifying this question.,2010-08-27 13:48:09.0,5.0,
2385,1233,0,"I agree that this is the simplest method and will probably do the job. I think I would just suggest a minimum level for the c-line. If one or two cases are raising too many false positive alerts, just change the threshold to something higher.",2010-08-27 13:54:44.0,1036.0,
2386,2157,2,"P-values are not appropriate measures for control charts, because of the serial multiple comparisons involved and the possibility (likelihood, in this case) of serial correlations.  Control chart properties are better assessed in terms of expected lengths of out-of-control (OOC) runs and in-control runs under the null and alternative hypotheses, respectively.  On this basis, letting even one case trigger an OOC is almost never a good procedure.",2010-08-27 14:12:07.0,919.0,
2387,1959,0,"Could you tone down the criticism in this a little?  Your point is fair, but I would love to see Gary get involved in the site (if at all possible)...",2010-08-27 14:13:51.0,5.0,
2388,2112,0,@chl thank you! for some reason I couldn't find an on-line help for car Anova(). but this example is already in the R off-line help at ?Anova. I am starting to understand this method.,2010-08-27 14:19:11.0,1084.0,
2389,2112,0,"@Adam Ah ok, my answer was thus a little bit out... What I call on-line is indeed the help() or ? call under R. For external references, you can try http://www.rseek.org/ or http://search.r-project.org/ (much better than directly goggling about R). Also I just this post with a nice illustration: http://j.mp/dnJgW0.",2010-08-27 14:42:32.0,930.0,
2390,1934,0,Ditto with your second paragraph. If you know 2 infections is too low to be bothered with you can probably reasonably form a minimum estimate in which you want an alert raised.,2010-08-27 14:49:44.0,1036.0,
2392,2155,1,THIS IS FANTASTIC THANK YOU,2010-08-27 15:03:41.0,,CLOCK
2393,2157,0,"I was talking about the p-value of one case, so I think it is correct. Of course I agree with a general claim that this can (and should) be done better, still from the voice of the article lockhart cited I can conclude he seeks for statistics ""light"".",2010-08-27 15:04:17.0,88.0,
2394,2154,2,"The answer I supplied to your previous question suggests a version of the ""c-chart"" that is more rigorously justified, quite simple, and accordingly is easy to interpret. (It also raises the logically earlier question of whether control charting is an appropriate technique for you.) You did not respond to this, but now you re-raise the question, so I am wondering why: why do you think this solution will not work for you?",2010-08-27 15:10:33.0,919.0,
2395,2157,0,"I agree that your p-value calculation is correct, but the issue I am raising concerns whether that's the right way, or even a good way, to think about and design control charts.  For those familiar with adjusting for large numbers of multiple comparisons, a single p-value can be an excellent guide, but for others this way of thinking might be misleading.",2010-08-27 15:14:16.0,919.0,
2398,1959,0,"Normally I am reluctant to criticize (for example, I have yet to vote down any contribution in these forums), but the criticism seems warranted in light of the tone of that tweet.  Nevertheless, I share your interests and will delete the unnecessary parts of the response.",2010-08-27 15:19:51.0,919.0,
2399,1959,0,Thanks.  I'll delete my comments here too now so that this all just looks like a coherent train of thought.  :),2010-08-27 15:22:12.0,5.0,
2400,2112,1,"Here is the correct link, http://j.mp/c6nDvn.",2010-08-27 15:30:48.0,930.0,
2404,1544,0,equivalence of what?  Your question doesn't mention this.  Perhaps edit the question to be more precise.,2010-08-27 16:52:11.0,601.0,
2405,2112,0,"@chl Thanks, I saw this one before.",2010-08-27 17:10:11.0,1084.0,
2407,2167,1,"BTW, kernels are not really essential for SVM's. The ""heart"" of SVM is the principle of soft margin maximization. Going to kernel representation makes your problem dimensionality O(m^2) instead of O(d) where m is the number of examples and d is the dimension of your feature space, so if m^2 is more than d, you might be better off doing away with kernels
http://jmlr.csail.mit.edu/papers/v6/keerthi05a.html",2010-08-27 17:53:21.0,511.0,
2409,2168,0,+1 I was going to post the same answer.,2010-08-27 18:11:39.0,88.0,
2410,2168,0,"Thanks for the answer.  @mbq @ebony1: IMO we need to make more of an effort to post more serious machine learning questions on the site, in order to attract more of that community.",2010-08-27 18:23:58.0,5.0,
2411,2169,0,What is the distribution of $x$? I suppose we can assume that the weights are known.,2010-08-27 18:34:51.0,,user28
2412,2168,0,"@Shane I completely agree, but what about other SO site like http://metaoptimize.com/qa/?",2010-08-27 18:54:26.0,930.0,
2413,2169,1,"right, sorry, the weights are independent from $x$. The restrictions on the weights characterize, evidently, the class of 'coherent' risk measures. I have not seen any suggestions about how to set them, though, and this appears to be a black art. However, perhaps one would choose them to get the best discriminatory power (i.e. smallest standard error)...",2010-08-27 18:56:19.0,795.0,
2414,2168,0,"@chl: That's also an option, but it's not part of the StackExchange (it's controlled by one person, and on different software) and I personally would rather have these different data analysis communities mingle in one place.",2010-08-27 19:01:41.0,5.0,
2415,2168,0,"@Shane Well, ok it makes sense.",2010-08-27 20:05:15.0,930.0,
2416,2168,0,there's also Machine Learning stack exchange proposal http://area51.stackexchange.com/proposals/7607/machine-learning,2010-08-27 21:11:22.0,511.0,
2417,2168,0,"@Yaroslav, @chl, @Sahne I've started this topic on chat, I think it is more comfortable place to discuss than comments ;-) http://chat.meta.stackoverflow.com/rooms/170/stats",2010-08-27 22:39:08.0,88.0,
2418,2171,4,Should probably be community wiki?,2010-08-27 22:46:02.0,5.0,
2419,2146,0,"Cautious +1 to the edit. In truth, I think the question is still not perfectly clear, but I know I'd love to read the answers!",2010-08-27 23:14:00.0,174.0,
2420,2169,0,"could you answer Srikant's first question? I don't see how you're going to get anywhere unless you specify that, without using some form of resampling.",2010-08-27 23:28:07.0,805.0,
2421,2168,0,"chatroom requires 20 meta.stackoverflow reputation, and I have 1",2010-08-27 23:29:10.0,511.0,
2422,2169,0,"@Glen_b: I am looking for a non-parametric estimator. I just found 2 papers by Stigler which, though very dense, appear to give a form of the asymptotic variance for the parametric case.  I am now hunting down papers by Downton, Stillito, as referred to in Elamir & Seheult's papers on L-moment standard errors.",2010-08-27 23:35:58.0,795.0,
2423,2173,4,"1. IMHO time spent with R is well invested if you plan to do anything serious.
2. Also consider KNIME http://www.knime.org/ as RapidMiner alternative.",2010-08-28 00:01:20.0,22.0,
2424,2172,1,+1 for R and ggplot2,2010-08-28 00:01:46.0,22.0,
2426,2169,0,"Oh, okay, you mean asymptotically. But you will still need certain assumptions about the distribution for the results to apply, and the results will depend on characteristics of the distribution that are unknown (such as the variance).",2010-08-28 03:24:53.0,805.0,
2427,2172,0,"Awesome, great links! I already use R and ggplot2, but the visualizations from there seem more of the ""graphics for a report""-variety, than of the ""eye candy/visualization interesting in of itself""-variety I'm looking for. (ggplot2 is super beautiful, but it's not really meant to allow unlimited creativity.) Am I wrong?, or do you sometimes use R/ggplot2 as an input into another visualization tool?",2010-08-28 04:28:46.0,1106.0,
2428,2159,1,"Again some interesting tools. I knew about Numpy, Matplotlib and ReportLab, but Pyspread seems like an interesting idea. At least I would like to type Python expressions in spreadsheet cells. While it doesn't solve all possible problems, it could be good for prototyping and playing around with data.",2010-08-28 06:21:35.0,890.0,
2429,2168,0,"When you say ""can only be applied to linear models"" do you mean that you can prove that no application exit to something else than linear models?",2010-08-28 08:14:56.0,223.0,
2430,2168,0,"What do you mean by ""examples"" ? as you state it is seems to be something that can be interpreted as a dot product.",2010-08-28 08:16:42.0,223.0,
2431,2168,0,for me it is -1 until you answer my questions :),2010-08-28 08:17:33.0,223.0,
2432,2174,0,"`igraph` works in R also; for 3D openGL accelerated vis in R, use `rgl` & `misc3d` packages.",2010-08-28 09:41:15.0,88.0,
2433,2106,0,@Matt thank you for the very helpful edit. I was trying other things after the ~ except 1 - as in your answer. Thanks again!,2010-08-28 16:41:11.0,1084.0,
2434,2176,0,Thank you for a very thoughtful and instructive answer!,2010-08-28 16:42:30.0,1084.0,
2435,2106,0,"Ah, it seems that there's a bug in the current version of ez (v1.6.1). I've tracked it down to the fact that in converting to the formats that Anova() likes, ez was failing to turn something into a factor. I'm not sure whether this was always a bug, or whether Anova() recently changed in a way that broke ez. Regardless, I'm preparing ez v2.0, and this is one of the bug fixes. Stay tuned...",2010-08-28 17:16:34.0,364.0,
2436,2183,0,"@Srikvant.  Assume 5% is acceptable significance.  I'm seeking a more precise answer, one which exposes the idea that ""A leads B"" is a new statistic, the difference of pA and pB, and that it's corresponding confidence interval is not simply 2*MOE.",2010-08-28 23:39:37.0,,somebody
2438,2183,0,@somebody I fixed the answer. I hope it is what you want.,2010-08-29 00:34:31.0,,user28
2439,2106,0,"Thanks Mike, it seems to have happened fairly recently. I just finished an analysis last month using your package and haven't noticed any problems until this question came up.",2010-08-29 07:12:59.0,966.0,
2440,1720,0,"Sometimes we are interested in problems that come from from formulations of the total quantity of something. If you know the mean, you can go from the mean to the total (multiplying by the number of the observations). There is no way to go from the median to the total!",2010-08-29 08:22:41.0,339.0,
2441,2180,1,Has any one of those algos been implemented in R or other software?,2010-08-29 09:26:32.0,339.0,
2442,2180,2,"Yes, take a look at the R penalizedSVM package. Other packages of interest are : penalized, elasticnet, ppls, lars, or more generally: http://cran.r-project.org/web/views/MachineLearning.html",2010-08-29 09:33:19.0,930.0,
2443,2187,0,do you recommend to read all of them ? :),2010-08-29 10:41:03.0,223.0,
2444,2187,3,"In Psychology the Tabachnik & Fidell Book has a pretty good reputation. It is very understandable and applied and not too mathematic. However, examples are only in SPSS or SAS (no R!). But if your problem is covered in there, you will definitely solve it with the book. I recommend it as a good starting point. I don't like the Hair book (same level as Tabachnik & Fidell, but worse). And you gotta LOVE the Gelman. However, it is more complicated.",2010-08-29 12:00:38.0,442.0,
2445,2187,3,"HAIR et al is good if you don't like math and you want a step by step process. It's popular in management and business schools. If you can handle math, Hair et al can seem verbose.

Tabachnick and Fidell is popular in psychology. It's clearly written and does contain some mathematics. However, if you want a rigorous mathematical treatment, I'd look for an additional book to complement it.",2010-08-29 13:09:37.0,183.0,
2446,2181,0,"To what extent do you want: (a) mathematical rigour; (b) applications in particular software (e.g., R, SPSS, SAS, etc.); (c) domain-specific applications?",2010-08-29 13:12:27.0,183.0,
2447,2181,0,"Jeromy, allow me to take all of these with one blow: I'm a psychology student. And I reckon you're familiar with required statistical background... So there... =)
(I'm good with R and SPSS... but R has a greater priority)",2010-08-29 15:26:47.0,1356.0,
2448,2187,0,Thanks for giving accent to statistics in psychology.,2010-08-29 15:31:06.0,1356.0,
2449,2190,1,"Can you expand on this question a little?  What else to you know about the ""great prediction""?",2010-08-29 17:00:13.0,5.0,
2451,2190,3,"If you mean reading the forest by looking at each tree and trying to understand what is going on, don't do it, you'll fail to see anything in such a complex, randomized structure. RF is just a black box method, its models are not intended to describe data (as usual in machine learning).",2010-08-29 17:33:21.0,88.0,
2452,2138,0,I like the points with mean centrally.  It has a truer representation of the lines.  You could make the points smaller.,2010-08-29 19:18:56.0,601.0,
2453,2193,0,As I understand it is possible to reconstruct the structure but it is impossible to assign class specific lineage of nodes. Here is my problem. I have a binary problem (presence/absence) of variable that can be predictive of one of 4 classes (as single and as joint for example combination of 5 variables). I am getting high prediction relying on top 20 by VIMP out of 1500 variables). I have to extract the specific combinations at least some of them,2010-08-29 23:18:37.0,,CLOCK
2454,2177,0,"Thanks for the response, I did not include specific examples of methodologies such as LOESS smoothing line, or model selection types (like BIC or using an F-test) because they are non-central to my question. Thanks for the clear up and I was definitely referring to partial regression plots (the first article I had been exposed to it in had called them added variable plots). Although it is not really central to my question either so I just referred to the wikipedia page. I also edited multi-variate as indeed I am only predicting 1 variable. Thanks again for the response.",2010-08-30 12:32:58.0,1036.0,
2455,783,0,"Anyway, I can give a citation of Lehman about that: ""There is some convenience in such standardization since it permits a reduction in certain tables needed for carrying out various tests"".",2010-08-30 12:37:28.0,223.0,
2456,1332,0,depending on what you call 'the thruth'... I don't think I give so much value to statistical truth :),2010-08-30 12:39:31.0,223.0,
2457,2201,0,"This is a good idea, thanks for the links.",2010-08-30 12:41:35.0,1117.0,
2458,1360,0,"Do you think that by ""those"" he means ""everyone"" ? what was the context of this citation ? it seems a bit strong like this :)",2010-08-30 12:42:22.0,223.0,
2459,2072,0,Can you clarify what you mean by irregular? I had initially taken it to mean that you had two series of different discrete time intervals.,2010-08-30 12:42:39.0,1036.0,
2460,2206,0,I'll run with the idea that it's Monte Carlo -- thanks for the response.,2010-08-30 12:44:02.0,1117.0,
2461,2072,0,"Yes, I mean two time series with different random arrival times (not regularly sampled).",2010-08-30 12:56:30.0,5.0,
2462,2177,0,"Also I think my last statement sufficiently summarizes what I am interested in, and I don't see it as being entirely consumed by formal inference.",2010-08-30 13:37:58.0,1036.0,
2463,2201,0,Another useful link: http://stats.stackexchange.com/questions/1385/techniques-for-handling-incomplete-missing-data,2010-08-30 13:49:03.0,339.0,
2465,646,0,"I'm not sure how many of these will have the scope of ""marketing"" research, but I would also suggest the OP check out ICPSR. It is also good to know the quality of the sampling and data record keeping, which I imagine could be haphazard if you get data from a proprietary, non-education related source.",2010-08-30 15:32:19.0,1036.0,
2466,2200,0,"what? the question says 'blah blah blah would be $min(a,b)$, how do I find the min?' I answered the question.",2010-08-30 16:38:00.0,795.0,
2467,2145,0,"In fact, I got another form

$$ (n+1)!\\delta_{n+1} = \\gamma_{n+1}+\\sum_{k=1}^nk\\gamma_k\\cdot[(n-k)!\\delta_{n-k}] $$",2010-08-30 17:27:09.0,1043.0,
2468,2215,0,"Re: ""I think of Monte Carlo as any method that uses probabilistic sampling of data as input into a computation..."" this is a useful way for me to think of the problem. Thank you.",2010-08-30 17:46:11.0,1117.0,
2469,766,0,"In addition, `n!` = `Gamma(n+1)` for n >= 0. So try to look for a function called `Gamma` if you need to calculate the factorial (or log Gamma if you're calculating the log likelihood)",2010-08-30 18:43:09.0,961.0,
2470,2221,1,This is also why the area of a circle is *not* equal to $\\int_0^{2\\pi} \\int_0^R 1 d\\theta dr$.,2010-08-30 19:03:38.0,795.0,
2471,2213,0,"Although I didn't ask the question, I think it would be great to have some basic literature with the answers. Please.",2010-08-30 19:19:52.0,442.0,
2472,2213,0,@Henrik: You want basic literature on neural networks?  You might consider posting another question?,2010-08-30 19:39:20.0,5.0,
2475,2222,0,Thank you. Is it ok to generate the samples using `sample` and `replace=TRUE`? Is there any reason to use a package like `boot`?,2010-08-30 20:36:33.0,339.0,
2476,2222,0,"Typically it's done with replacement so you want to set that to TRUE.  As for why... the package is optimized so it should run faster.... never timed it.  That can be a problem if you set R large.  And, as you can see, the code is nice and concise.  It also has lots of features you wouldn't get easily rolling your own.",2010-08-30 23:10:18.0,601.0,
2479,2202,0,"your last explanation on finding the smallest intersection can be applied to all cases. Min `A∩B` is `max(a+b-1,0)`",2010-08-31 03:57:00.0,862.0,
2480,2224,0,I think it is better to put a comment and vote for closing the question. Indeed it may be a bit confusing to have an answer wich is a link to a question on the same site... I voted to close the question.,2010-08-31 07:38:31.0,223.0,
2481,2222,0,"boot.ci returns the confidence interval. Is there any (boot) function that gives the p.value? (as the ratio of the number of differences at least as high as the one observed, over the total number of generated samples)",2010-08-31 08:49:08.0,339.0,
2482,2222,0,"ok, I found a way to get it: `sum(b$t>=b$t0)/b$R`",2010-08-31 09:02:29.0,339.0,
2483,2234,0,"Do we have to assume that you are considering a fixed set of predictors, i.e. you are interested in getting a reliable prediction given $k$ predictors, or are you also interested in some kind of penalization on the $X_j\\quad (j=1\\dots k)$?",2010-08-31 11:29:14.0,930.0,
2484,2234,0,"I admit that for my personal interest, penalization wouldn't be necessary, and for the sake of knowledge here I would say both are relevant answers :)",2010-08-31 11:46:39.0,253.0,
2485,2234,0,For future reference: you might have been able to phrase this question in such a way that we would have allowed it as a non-CW question.  See http://meta.stats.stackexchange.com/questions/290/what-is-community-wiki,2010-08-31 13:07:31.0,5.0,
2486,2234,0,"Thank you for the link Shane.  A very interesting discussion you opened there.  After reading Thomasas answer, I believe this should still be a community wiki, since my intention was to find as many alternatives as possible (something i doubt any one person would be able to supply).
   Yet, again, thank you for directing me to that thread!",2010-08-31 13:22:00.0,253.0,
2487,2200,0,@shabbychef I agree; bad questions should be punished with no answers.,2010-08-31 14:21:09.0,88.0,
2488,2224,0,"@Robin: Thank you for the suggestion.  I do not see how it would be any more or less confusing to place a link in an answer or a comment, though.  I, too, have voted to close the question, but I first waited a day for the poster to respond to Shane's suggestion that this question be edited.",2010-08-31 14:22:26.0,919.0,
2489,2174,0,Also `matplotlib` plots are ugly; they may be nice for a long-years gnuplot user.,2010-08-31 14:23:30.0,88.0,
2490,2106,0,"@ Matt, I still get the same EZanova warning msg. for Anova I get another error msg: 
Note: model has only an intercept; equivalent type-III tests substituted.
Error in linearHypothesis.mlm(mod, hyp.matrix, SSPE = SSPE, idata = idata, : The error SSP matrix is apparently of deficient rank = 0 < 1",2010-08-31 14:43:45.0,1084.0,
2491,2227,0,What's the source?,2010-08-31 15:02:25.0,5.0,
2492,2236,0,+1 I was about to post the same answer!,2010-08-31 15:05:46.0,930.0,
2493,2224,1,my view is that answering a question is a step toward accepting it in the site.,2010-08-31 15:16:04.0,223.0,
2494,2106,0,"This may help with the type-III thing https://stat.ethz.ch/pipermail/r-help/2007-October/144240.html ; and I've edited my answer to reflect the updated version of ez, which has changed ""sid"" to ""wid"". I don't get your problem with the data you've given me.",2010-08-31 15:24:33.0,966.0,
2495,2106,0,"#Try this with the code I've written for the car Anova
#transfer data back to long to perform ezANOVA    
df2<-melt(df1, id.vars=""observ"", measure.vars=names(df1)[6:21])
df2$suc<-substr(df2$variable,1,2)
df2$cit<-substr(df2$variable,3,4)
df2$observ<-factor(df2$observ)
ez1<-ezANOVA(data=df2,
  within=.(suc, cit),
  wid=.(observ),
  dv=.(value)
  )",2010-08-31 15:28:08.0,966.0,
2496,2222,0,@gd047 : take into account that this is a one-sided p-value you're calculating.,2010-08-31 15:58:54.0,1124.0,
2497,2227,0,It was taken from Mike West's website: http://www.stat.duke.edu/~mw/fineart.html,2010-08-31 16:23:32.0,881.0,
2498,2240,0,Can you give an explanation of why permutation tests should not be used on datasets that you can't calculate all possible permutations?,2010-08-31 16:27:10.0,1036.0,
2499,2240,0,"@Andy W : First define ""permutation test"". for me, permutation tests are exact tests, using every permutation possible. That is impossible on larger datasets. The ""approximate permutation tests"" are in fact the simples Monte Carlo method around, and should be adressed that way. Next to that, the central limit theorem assures in most cases that the assumptions regarding the distribution of the test statistics are met when using large datasets. In complex testing, the use of permutation tests on large datasets makes calculation times unbearably long without adding any significant value. my2cents",2010-08-31 16:42:08.0,1124.0,
2500,2200,0,"No, that's not what he's asking for. He asked for the min of the intersection, as can be deducted by careful reading of the first of the two sentences in the opening post.",2010-08-31 16:51:40.0,1124.0,
2501,2242,0,I may be a bit confused but I think the above answer implicitly interprets confidence intervals as credible intervals. I am not sure if you can reason about the Prob(true polling percentage <= 34.5) using the classical concept of a confidence interval.,2010-08-31 17:04:38.0,,user28
2502,2224,0,@Robin: Thank you; that makes sense and it's a principle that will help in future cases.,2010-08-31 17:12:55.0,919.0,
2503,2222,0,"@John Christie : Could you show me how your last bootstrap code should be modified in order to draw resamples of size 5 with replacement from the first sample and separate resamples of size 6 from the second sample (instead of resampling from the total number of 11 values). As I read on page 18 of the linked pdf, this should be the correct way to draw the bootstrap samples.",2010-08-31 18:12:56.0,339.0,
2504,2240,0,"I didn't say anything like bootstrapping a permutation test. I came into this question after reading the last paragraph of [SECTION 14.5 | Summary], in the linked pdf.",2010-08-31 18:33:03.0,339.0,
2505,2238,0,+1 it is always nice to bootstrap something. On the other hand every challenge is easy with R: http://stat.ethz.ch/R-manual/R-devel/library/boot/html/tsboot.html,2010-08-31 19:47:47.0,88.0,
2506,2249,0,"Thank you very much for that.  I thought of the Mann-Whitney test, but many (most?) of the values will result in ties.  Won't this reduce the power of the analysis?",2010-08-31 21:38:04.0,,John
2507,2249,2,"Yes, of course it will; but as you're working with 10 observations per group, don't expect very much from this setting; check for yourself at http://statpages.org/#Power",2010-08-31 21:53:56.0,930.0,
2508,2186,0,why the Tinsley book? No reviews on Amazon suggests it ain't a big seller or particularly good.,2010-08-31 23:18:05.0,74.0,
2509,2238,0,Thanks Dirk and mbq. I have one time series I collected. I am not using a data generating process to produce different series. So in this case I understand that I cannot get std deviations? Please correct me if I'm wrong. Thanks again!,2010-08-31 23:51:38.0,443.0,
2510,2238,0,You could try 'chunked' resampling. Google for block bootstrap.,2010-09-01 01:14:32.0,334.0,
2511,2245,1,great question: see also this related question on correlation and causation http://stats.stackexchange.com/questions/534/under-what-conditions-does-correlation-imply-causation,2010-09-01 05:14:43.0,183.0,
2512,2200,0,@mbq - whats wrong with the question?,2010-09-01 06:00:17.0,862.0,
2513,2257,0,"thanks for this. I would like to higlight the groups using color such that all points from the same group have the same color. Can quite figure out what the convert command does, let me check the documentation though a layman explanation can also help. 
Thanks a lot!",2010-09-01 06:37:16.0,18462.0,
2514,2258,0,"LaTeX works, you must just stop editing for a while so that it will be applied to preview window.",2010-09-01 07:26:03.0,88.0,
2515,2258,0,"For LaTeX, enclose terms in $ signs.",2010-09-01 07:26:12.0,159.0,
2516,2258,0,Got it. Which tag can I use for multiline latex block?,2010-09-01 07:33:25.0,862.0,
2517,2256,1,You may also check out `rgl`.,2010-09-01 07:35:39.0,88.0,
2518,2258,0,"The align environment will work, but use \\newline instead of \\\\.",2010-09-01 07:45:25.0,159.0,
2519,2200,0,@saminny You could have formulated it better so that it would be less ambiguous and easier to understand. shabbychef just pointed it out with his answer.,2010-09-01 07:46:30.0,88.0,
2520,1305,0,Could you be a bit more specific about the values you have available and the values you want to be able to output.  From the current information it is hard to tell what you need.,2010-09-01 07:48:30.0,196.0,
2521,2240,0,"@gd047 Then I have misread your question. But you should really keep confidence intervals and p.values strictly separated. The confidence interval is estimated based on bootstrapping within each sample (although it is biased by definition), the permutation test is done by permutations over the complete dataset. Those are two completely different things.",2010-09-01 08:15:40.0,1124.0,
2522,2242,0,"We're not working in a Bayesian framework, so one can hardly speak of a posterior probability. I simply use the definition of a confidence interval, which is said to contain the true estimate with a probability of 95%. So it would be correct to say that the chance of having a true A (not the estimate) smaller than 34.5 is only 2.5%, by definition of the confidence interval.",2010-09-01 08:22:41.0,1124.0,
2523,2260,2,"Interesting point about the means. I suppose differences between means would suggest that the groups are different in some respect. And empirically I'd expect that where there's large differences between means, there's more likely to be differences in factor structure. Differences in means might also have implications for how the findings regarding the factor structure could be generalised. It might also increase correlations between items in general due to an effect the opposite of range restriction.",2010-09-01 08:35:28.0,183.0,
2524,2258,0,how do I use align environment? Is it a tag?,2010-09-01 08:41:23.0,862.0,
2525,2262,1,Could you explain what a 'peak' stand for in the context of your data?,2010-09-01 10:03:25.0,930.0,
2526,2262,1,Also do you have access to the underlying data? How are the curves drawn?,2010-09-01 10:09:50.0,8.0,
2527,2264,0,Please may we have an expected value tag?,2010-09-01 10:22:01.0,1134.0,
2528,2258,0,"Several points: 1. See my edits as to how to use Latex. 2. You may also want to see this thread: [Vote early, vote often](http://meta.stats.stackexchange.com/questions/314/vote-early-vote-often). 3. If this is homework could you please tag it as such so that people can provide hints instead of complete answers.",2010-09-01 11:00:54.0,,user28
2529,2265,0,Thanks for the comprehensive response. OpenMX does look like a good open source R package option for multi-group CFA. The IRT options also look interesting. Good point about the problems of hypothesis testing in CFA. Thanks for the references.,2010-09-01 11:12:05.0,183.0,
2530,2257,0,"`s3d <- scatterplot3d(x.mds$points[,1:3], color=as.numeric(gl(2,50)))` would highlight the first 50 points in black, the remaining ones in red; you can pass any logical vector or factor to suit your data. HTH",2010-09-01 11:12:43.0,930.0,
2531,2265,0,"I'd recommend lavaan as well, stupid I didn't think of it earlier.",2010-09-01 11:28:17.0,1124.0,
2532,2250,0,"With only 10 observations in each group, ordinal logistic regression is very unlikely to fit, let alone give correct estimates of the variances. I'd suggest the permutation test.",2010-09-01 11:54:28.0,1124.0,
2533,2264,0,"although not the same, you may want to see this related post: http://stats.stackexchange.com/questions/1848/bayesian-rating-system-with-multiple-categories-for-each-rating",2010-09-01 11:55:58.0,183.0,
2534,2269,1,This is an inappropriate place to ask this question. You need to deal with SAS directly or other forums directly related to utilizing SAS software.,2010-09-01 12:15:22.0,1036.0,
2535,2253,0,"Thanks everyone.  I went back and tried the Mann-Whitney test, using the method of listing the score of each individual as a column.  (FWIW, I use GraphPad Prism for the analysis.)  The results do appear reasonable, given what we already know about the biology.  Thanks very much for your help.",2010-09-01 12:15:46.0,,John
2536,2269,0,"Agree, now closed.",2010-09-01 12:42:06.0,88.0,
2537,2268,0,"Thanks for the link. I agree that the number of ratings for the product compared to average number for all products is significant. But the variance must be significant too. Suppose every product had zero variance in its ratings, then a single product rating would be sufficient to provide the ev.",2010-09-01 12:50:30.0,1134.0,
2538,2266,0,"As a newbie student maybe I shouldn't be too picky, but I'm not very happy with this answer. Why can't an ev be calculated? Can you prove that statement? Also 95% CI seems plain arbitrary.",2010-09-01 12:57:18.0,1134.0,
2539,849,0,"+1 cause it's a great answer that helped my a lot today. Sad that it only got one point so far. Go Brett, go!",2010-09-01 13:26:14.0,442.0,
2540,2263,0,"Hello Joris.

Thanks for the input.

All the peaks are gaussians - having a nice bell shape. The only thing that varies is the amplitude (height) and the SD (width).

I actually don't really know about this density function. COuld you explain it more or give me link towards it?

Thanks.",2010-09-01 13:48:46.0,1133.0,
2541,2263,0,"@chl: 'peak' is simply any area above my baseline. All peaks have gaussian distributions.

@csgillespie: I have access to the original data that plotted the histogram and the curves are drawn in GraphPad Prism using a 'Sum of three gaussians' fit.",2010-09-01 13:52:02.0,1133.0,
2542,2263,0,This is how the whole graph looks like: http://imgur.com/ipBiL.png,2010-09-01 13:53:16.0,1133.0,
2544,2263,0,"@Cadu Ok, so this does not resemble what is commonly refered to as AUC, since this term is more often used in reliability studies or when assessing performance of classifier, i.e. when you plot sensibility against (1-specificity).",2010-09-01 13:58:26.0,930.0,
2545,2263,0,"In full, the probability density function or pdf : http://en.wikipedia.org/wiki/Probability_density_function . The one for the normal distribution, which I use, is thoroughly described. Google will give you tons of hits on that. As you used the sum of gaussians, the approach I gave you should work. Mind you, it gives you the total area for each gaussian curve. So if you add the AUC's for the three curves, you end up with a higher area than under the combined curve.",2010-09-01 14:03:54.0,1124.0,
2546,2262,1,Maybe change your title because AUC is definitively misleading there...,2010-09-01 14:10:20.0,930.0,
2550,2232,0,"Thanks, I like the convergence of infinite series point-of-view!",2010-09-01 14:20:24.0,1106.0,
2551,2263,0,"Sounds pretty good for me, Joris.

However i am still unsure which one of those formulas is the correct one. If it is not too much, can you point it out which one you think would be the best one for this situation?

And, yes, i'll keep in mind that it is not the total AUC.

Thanks for your time again.",2010-09-01 14:23:11.0,1133.0,
2552,2250,0,"@Joris Good point! I forgot the sample size issue when I initially wrote my answer. The GLM would certainly yields poor estimates... Re-randomization test should be better, you're right. Thanks!",2010-09-01 14:26:30.0,930.0,
2553,2274,0,"Hey Chl. I will try this mclust package and see what it can do.

Thanks for the hint :)",2010-09-01 14:32:47.0,1133.0,
2558,2253,1,"Mann-Whitney can be a good choice, but be aware that it makes some specific assumptions about the relationships among the two groups.  It is good when results appear reasonable, but that cannot be the sole reason to adopt a statistical test: that would be getting into a circular argument (""we chose this test because it gives reasonable answers; our answers are significant because this test says so"").  At a minimum, make sure to graph all the data in a way that promotes visual comparison of the groups.",2010-09-01 14:56:58.0,919.0,
2559,2249,0,"@John: I suspect the KS test I proposed would be more powerful than Mann-Whitney because it can detect various alternatives that M-W cannot (such as a change in spread) and otherwise is doing a mathematically similar calculation.  This is difficult to study in full generality--in your case, each group's results has four degrees of freedom, so there are eight dimensions of differences to explore--but if you could be more specific about exactly *how* the two groups might differ (which is matter for science, not statistics, to decide), you would have a better basis for selecting a test.",2010-09-01 15:01:46.0,919.0,
2560,2260,0,"Good point I hadn't thought of it that way.  Thank you.  FYI I'm not sure how common it is to use this term, but Rosenthal (of Rosenthal and Rosnow, 2007) refers to the effect opposite of restriction of range as hypertrophy of range.",2010-09-01 15:08:25.0,196.0,
2565,2167,0,"@Yaroslav: Thanks for the reference.  Are you aware of any implementations of that ""Modified Finite Newton Method""?",2010-09-01 15:23:11.0,5.0,
2566,2266,0,E[X] is a property of the pop that you are drawing inferences about.  x-bar is an unbiased estimator of that value.  You don't have to use 95% -- adjust t accordingly.  But note the SE of x-bar is s/sqrt(n) not s/n. I'll see if I can edit this.,2010-09-01 15:31:26.0,1107.0,
2567,2266,0,"@Kingsford : sorry, my mistake. it's edited.",2010-09-01 15:39:35.0,1124.0,
2568,2266,0,"@Bart : an expected value is about the population, and a theoretical value. It can be seen as the limit of the sample mean when the sample size goes to infinity. You need to estimate it using the mean, but you can't calculate it unless you know the complete population. Which you don't. See : http://en.wikipedia.org/wiki/Expected_value",2010-09-01 15:40:19.0,1124.0,
2569,2263,0,"@Cadu : What's the software you're using? If it's merely for calculation, you shouldn't use the theoretical functions. In R, there is a function called dnorm() that does the calculation for you. On this wiki page you get more information on the probability density function for the normal distribution : http://en.wikipedia.org/wiki/Normal_distribution",2010-09-01 15:46:02.0,1124.0,
2570,2277,0,"Hmm, you got a point there, whuber.

In the graph that i posted i got the mean, which indeed it nothing more than the position in the X axis of the peak. The SD deviation should be half of the width of the peak. And i also got the amplitude of these peaks which is the height.

If i got it right, your suggestion is that i could calculate the area of the peaks simply by multiplying the height (amplitude) by the width (2*SD). And due to the fact that i am only interested in the ratios between the peaks, this should be enough.

Did i get you right?",2010-09-01 15:51:11.0,1133.0,
2571,2263,0,"@chi: I think you are mixing up two acronyms. 
ROC curve = Receiver Operator Characteristic curve
AUC = Area Under Curve",2010-09-01 16:03:26.0,25.0,
2572,2277,0,"Yes, you are correct.  You don't need to worry about the factor of 2, either--it's meaningless--because you just want ratios.  The problem with this approach is that when the peaks are close together, the tails of a peak's neighbors contribute somewhat to the peak's own height.  Thus, it's best to use a peak-estimation procedure that compensates for this.  That's what chl, for example, was referring to by a ""mixture of gaussians.""",2010-09-01 16:06:28.0,919.0,
2573,2282,1,"@Tal: I am basically happy to allow any R questions on here, but I just don't see how this has any relevance to the subject of data analysis.  This is a meta question about R itself, not even about using R.  We'll see what others think, but this seems more appropriate for stackoverflow, r-help, twitter, or a blog post.",2010-09-01 16:12:52.0,5.0,
2574,2275,0,"I think it would be ideal if you could present some of the data you finally have. At least for me, that would help a lot in thinking about an answer.",2010-09-01 16:13:12.0,442.0,
2575,2282,0,Regarding how to do this: I would suggest making your own version of the `library` function that would track each package when you load it.,2010-09-01 16:13:56.0,5.0,
2576,2282,0,Related meta questions: http://meta.stats.stackexchange.com/questions/1/how-to-answer-r-questions and http://meta.stats.stackexchange.com/questions/169/is-it-possible-to-integrate-data-from-so-com-questions-tagged-r-here.,2010-09-01 16:16:57.0,5.0,
2577,2277,0,"Okay then :)

Actually i was trying to use chl's approach but nothing so far.

On the other hand, i used Jori's approach to calculate the area and then i calculated the ratios between them. Afterward i did the same thing with this easy procedure of yours - and i must say that although the areas were different, the ratio was pretty much similar.

I guess this solves my issues.

Thanks everybody for the help :)",2010-09-01 16:18:06.0,1133.0,
2578,2282,0,"Lastly: regarding how to ftp; not sure how to do this from within an R function, but you can do it easily with `system(""ftp ..."")`.",2010-09-01 16:20:45.0,5.0,
2579,2266,0,"@Joris The wiki entry is much too advanced for me. Maybe I'm not asking for the right quantity. If by definition you need the entire population to calculate ev, I'd like a similar quantity that allows me to incorporate priors when my data is incomplete. So I can update the question, what would I call that?",2010-09-01 16:22:26.0,1134.0,
2581,2264,0,your entreaties have been heard and accepted ;-),2010-09-01 16:43:02.0,88.0,
2582,2282,0,"Thanks Shane. I'll go through the threads.  You are correct, this is more of a R community topic then a statistical one - I'll vote to close the question (I don't erase it since it is an example of a ""bad"" question for this website)",2010-09-01 16:56:23.0,253.0,
2583,2282,0,"Thanks @Tal.  This is similar in nature to this question: http://stats.stackexchange.com/questions/2269/access-tables-created-in-sas-enterprise-guide-client-into-sas-enterprise-miner-cl, which was also closed.",2010-09-01 17:00:20.0,5.0,
2585,2282,0,You should also reach out to @johnmyleswhite: http://twitter.com/johnmyleswhite/status/22395563278,2010-09-01 17:03:41.0,5.0,
2586,2282,0,Thanks for the pointer Shane!,2010-09-01 17:08:56.0,253.0,
2587,2282,1,"If you were looking for gathering data on _installed_ packages, the crantastic package with its function crantastic.submitInstalledPackages would be worth looking at..",2010-09-01 17:09:46.0,573.0,
2588,2282,0,Thank you Karsten - this is good to know.,2010-09-01 17:15:29.0,253.0,
2589,2275,0,"The actual probability after the fact is either 1 or 0.  So for example For test 1: I say 75% of a 1, and ends up a 1, test 2: 35% ends up 0, etc.  How can I quantify the accuracy of my estimate?",2010-09-01 18:14:36.0,1137.0,
2590,2285,0,Yes this looks like it might be what I am looking for.,2010-09-01 18:16:22.0,1137.0,
2591,2284,0,The Federalist paper is very interested and describes similar to above answer.  Thanks.,2010-09-01 18:30:29.0,1137.0,
2592,2269,5,I wholeheartedly disagree. We should cover statistics software here too. How many R questions are there?,2010-09-01 18:38:27.0,74.0,
2593,2280,0,Thanks. I've looked at some of your suggestions and they seem to be along the lines I was thinking. But can you recommend anything more specific and suitable for the lay person that I am.,2010-09-01 18:56:59.0,1134.0,
2594,2266,2,"@bart, think of this question: what is the expected value of the height of an American? The only way to know this value exactly would be to ask every single person his height and take the average. That's the expected value---it's a property of the full population. That's really hard. Instead, we get a sample, a subset of our population, ask them their heights and take the average. This is the sample mean---a property of our sample and an estimate of the expected value. As our sample gets larger, we should get a sample mean that gets closer to the true expected value (the law of large numbers).",2010-09-01 19:03:47.0,401.0,
2595,2263,0,"@Joris: Since dnorm(avg, avg, sd) = C * 1/sd, this formula for ""AUC"" simply proposes computing heights * sd / C (where of course C = 1/sqrt(2 pi) for Gaussians but can be some other constant for other scale-location families).  The constant factor disappears when computing ratios (which is what the question asks for), whence one might just as well compute heights * sd and be done with it.",2010-09-01 19:32:58.0,919.0,
2596,2277,0,"@Cadu: the ratios shouldn't be ""pretty much similar""; they should be *identical* (to within floating point error).",2010-09-01 19:34:48.0,919.0,
2598,2285,0,could you elaborate on how the bin counts would be compared to the probabilities?  I presume you have something quantitative in mind.,2010-09-01 20:35:54.0,919.0,
2599,2262,0,"The title should say ""peak height"" rather than ""peak mean"".",2010-09-01 20:46:40.0,25.0,
2601,2285,2,"@whuber: the probabilities can be plotted against the empirical frequencies to get what's called a calibration plot; the points should be roughly along the diagonal. I don't know if one could do a more quantitative analysis, this isn't an area I know a whole lot about, just a few things i've picked up here and there.",2010-09-01 21:10:28.0,495.0,
2602,2280,1,@bart -- I wish I could offer a safe automated way to do this but I don't think it exits.  See above for edits w/ a little more detail.,2010-09-01 22:54:38.0,1107.0,
2604,2292,0,"Point taken.

The problem appeared because the estimates of the smoothing parameters of the best fitting model (an ETS(M,A,N) ) are 0.0001 and 0.9999 which I think are the set limits for the possible values the estimates can take. So I was trying to find out what was going on. 

I was looking for standard errors because I'm used to finding out about a model's parameters' distributions whenever I have to estimate one..:) thinking that if they vary too much they are not too reliable.",2010-09-02 01:06:57.0,443.0,
2605,2292,0,I did bootstrap as advised earlier. I found that the standard deviation was not that useful because the distribution was very skewed.,2010-09-02 01:08:07.0,443.0,
2606,2279,0,Yes its trivial. I was able to come up with a simple proof using sigma algebra.,2010-09-02 01:39:48.0,862.0,
2607,2298,0,p.s. maybe this should be a community wiki? I'm not sure how that works...,2010-09-02 03:32:49.0,795.0,
2608,2167,0,"no, but Keerthi and Langford's pages have links to some software that may be related, since they both worked at Yahoo Research",2010-09-02 03:53:17.0,511.0,
2609,2292,0,"@noworries. Yes, they are the boundaries of the parameter region. A value close to zero is not a problem -- it indicates the associated state variable is extremely stable and essentially does not change over time. A value close to one usually indicates the model is trying to adapt quickly to each new observation which probably means the model is mis-specified.",2010-09-02 05:24:17.0,159.0,
2610,2298,0,if you don't give us more info on $f$ and on what type of characteristic then it seems to be community wiki but I am not sure neither :),2010-09-02 05:47:49.0,223.0,
2611,2298,0,"The fact that $f$ in undefined makes it a feed of a different multivariate visualization methods, so it should be CW. And now it is.",2010-09-02 07:20:09.0,88.0,
2613,2261,1,"Regarding your last point about GLM, this is basically how IRT models work: they can be viewed as generalized mixed-effects model with a logit function link and individuals treated as random effets (not items!). This yields the so-called marginal likelihood approach, where subjects' parameters are assumed to follow a standard normal distribution, for model identification purpose. The problem is that it has to be done on each subscale which are hypothesized to measure a single construct.",2010-09-02 08:32:10.0,930.0,
2614,2268,0,Good point. I just wanted to flag that the Bayesian option seems like the way to go. Of course @Kingsford has now provided a more rigorous explanation.,2010-09-02 08:49:29.0,183.0,
2615,338,1,"Still, none of those books claim that there is more than one entropy.",2010-09-02 10:25:52.0,88.0,
2616,2306,0,I should say for full disclosure that I have already posted this to the bioconductor list,2010-09-02 10:26:34.0,1150.0,
2617,2280,0,"You forget that some products don't have a score, or only very few. Lme is very likely to get convergence problems, and the estimates of the standard errors cannot be trusted.",2010-09-02 11:00:59.0,1124.0,
2618,2261,1,"Good point. And in fact, It would be better to use GEE models or mixed effect models instead of GLM, a standard GLM doesn't allow for random effects.",2010-09-02 11:04:04.0,1124.0,
2619,2266,0,"@Bart : You want to estimate the expected value, and you do that by modelling the sample means in any way. It sounds trivial, but it is crucial to understand what exactly you're doing. I see far too many students struggle with statistics because they don't understand the underlying concepts. I don't want to be rude, but it might be a good idea to look for a thorough introduction into statistics. It will help you tremendously in understanding what is going on.",2010-09-02 11:09:12.0,1124.0,
2620,2269,0,There is an ongoing discussion about this question on meta: http://meta.stats.stackexchange.com/questions/400/is-this-question-really-not-related-to-statistical-analysis,2010-09-02 11:11:45.0,,user28
2621,2307,0,"Thanks for that.  I'm not particular sold on SVM, just using that as an example.  So if you used random trees, you don't have to do cross-validation? Is that right.",2010-09-02 11:13:59.0,1150.0,
2622,2269,1,"of course you need some software/tool and can't do all statistics by just using calculator. If this is not a valid question then why do tags like ""R"", ""SAS"" exists here?

Are you guys planning a new site for ""Statistical Analysis Softwares""?",2010-09-02 11:26:25.0,263.0,
2623,2307,4,"yes, RFs include a random sampling of variables (typically $\\sqrt{p}$) when growing a tree and each tree is based on a boostraped sample of the individuals; variable importance is computed on so-called out-of-bag samples (those not used for building the decision tree) using a permutation technique. The algorithm is repeated for m trees (default m=500) and results are averaged to compensate uncertainty at the tree level (boosting).",2010-09-02 11:34:20.0,930.0,
2624,2281,2,"A 95% confidence interval by definition covers the true parameter value in 95% of the cases, as you indicated correctly. Thus, the chance that your interval covers the true parameter value is 95%. You can sometimes say something about the chance that the parameter is larger or smaller than any of the boundaries, based on the assumptions you make when constructing the interval (pretty often the normal distribution of your estimate). You can calculate P(theta>ub), or  P(ub<theta). The statement is about the boundary, indeed, but you can make it.",2010-09-02 11:44:24.0,1124.0,
2625,2307,2,It is important that it is called Random Forest not Random Trees; you may have problems with Google.,2010-09-02 11:52:11.0,88.0,
2626,2263,0,"@Harvey Yes, you're right: these are ROC that are used to show classification accuracy, but we use their corresponding AUC (or, AUC wrt. first bisector) to compare one classifier to another (or one predictive instrument to another). Now let's try ""AUC"" as a keyword in Pubmed and look at the results: How much deal with clinical evidence-based medicine or biomedical studies?",2010-09-02 12:12:39.0,930.0,
2627,2261,1,"FYI, this article from Tuerlinckx et al. 2006 provides a nice review of GLMMs: http://bit.ly/9k8vJs. Also, about the use of GEE as an alternative marginal model, it was proposed by Feddag, Grama, and Mesbah (2003) to estimate parameters of the Rasch Model (one-parameter IRT model), or loglinear multidimensional IRT models as defined by Kelderman and Rijkes (1994). It is currently available in the Stata `raschtest` package.",2010-09-02 12:33:14.0,930.0,
2628,2261,0,"@chl : thx for the reference, very nice reading material.",2010-09-02 12:48:52.0,1124.0,
2635,2242,1,"Let me correct myself: It would be correct to say that the chance of your lower limit to be bigger than the true value is only 2.5%, assuming normality of your sample mean. Which is a different phrasing (the chance is about the limit, not about A), but boils down to the same.",2010-09-02 13:27:35.0,1124.0,
2638,2146,0,"Do I understand it right that you have two sets of time series, and you want to check whether or not they differ significantly?",2010-09-02 13:59:52.0,1124.0,
2639,2281,4,"Joris, I can't agree. Yes, for any value of the parameter, there will be >95% probability that the resulting interval will cover the true value. That doesn't mean that after taking a particular observation and calculating the interval, there still is 95% conditional probability given the data that THAT interval covers the true value.

As I said below, formally it would be perfectly acceptable for a confidence interval to spit out [0, 1] 95% of the time and the empty set the other 5%. The occasions you got the empty set as the interval, there ain't 95% probability the true value is within!",2010-09-02 14:21:34.0,1122.0,
2640,2297,0,Perfect! Thanks!,2010-09-02 14:33:45.0,1146.0,
2641,2269,0,Reopened the question per this discussion: http://meta.stats.stackexchange.com/questions/400/is-this-question-really-not-related-to-statistical-analysis,2010-09-02 14:36:22.0,5.0,
2643,2281,0,"@ Keith : I see your point, although an empty set is not an interval by definition. The probability of a confidence interval is also not conditional on the data, in contrary. Every confidence interval comes from a different random sample, so the chance that your sample is drawn such that the 95%CI on which it is based does not cover the true parameter value, is only 5%, regardless of the data.",2010-09-02 15:02:43.0,1124.0,
2645,2280,0,"@Joris-Without a prior you can't include products with 0 information, but because there are thousands of products, having few observations per product is not a problem (even many with 1 obs will be OK). But still, without being familiar with the data, the context of the problem, and desired output/inferences we are only speculating as to what is appropriate.",2010-09-02 15:44:48.0,1107.0,
2646,2306,0,Please summarize any bioconductor results back here?,2010-09-02 16:01:50.0,5.0,
2649,2266,0,@Joris You are not being rude. I've tried studying maths and stats in a conventional way and I have to say it hasn't been very successful. What seems to work for me is to reason the problems out in my own special way and then see how that matches up with what others do.,2010-09-02 17:07:36.0,1134.0,
2650,2314,0,"I think there's still a generalization issue when using the same sample (1) to assess the classifier classification/prediction performance while tuning its parameters (eventually, with feature selection) and (2) use in turn its predictions on the whole data set. In fact, you are breaking the control exerted on overfitting that was elaborated using cross-validation. Hastie et al. provide a nice illustration of CV pitfalls, esp. wrt. feature selection, in their ESL book, § 7.10.2 in the 2nd edition.",2010-09-02 17:16:12.0,930.0,
2651,2298,0,"it's fairly easy for me to fit $f$ to linear model, so we should assume that has already been done, and we are looking at the _residual_ from that fit. As I mentioned in the question, I am looking for interesting facts about the function, so I don't yet know what I might find. Because of this open-ended nature, I thought maybe it should be a CW. I am still not sure on how one decides whether a Q should be CW.",2010-09-02 17:25:09.0,795.0,
2653,2281,0,"Joris, I was using ""data"" as a synonym for ""sample,"" so I think we agree. My point is that it's possible to be in situations, after you take the sample, where you can prove with absolute certainty that your interval is wrong -- that it does not cover the true value. This does not mean that it is not a valid 95% confidence interval.

So you can't say that the confidence parameter (the 95%) tells you anything about the probability of coverage of a particular interval after you've done the experiment and got the interval. Only an a posteriori probability, informed by a prior, can speak to that.",2010-09-02 17:46:47.0,1122.0,
2654,2316,1,You should delete this 'answer' and add it to your question as you have not really found the answer as evident by the last line.,2010-09-02 17:48:29.0,,user28
2655,2316,0,"Nice find, but when Simon writes

""view that single observation as a draw from a true probability distribution who's average you want...and you can view that true average itself as having been drawn from a probability distribution of averages""

he is describing a mixed or multilevel model.  I don't think there's a need to guess that ""K=25 seems to work well"" because the best linear unbiased predictor equations were worked out more than 60 years ago (BLUP). Excellent Bayesian estimators exist as well.  As Brad Efron said, ""Those who ignore Statistics are condemned to reinvent it""",2010-09-02 18:00:39.0,1107.0,
2656,2318,0,"I would even recommend the entire book from I. Guyon and coworkers, http://j.mp/anblwx. The ESL book from Hastie et al., http://j.mp/bW3Hr4, provides also interesting discussions around this 'hot' topic.",2010-09-02 18:24:46.0,930.0,
2657,2318,0,"I disagree with your claim; FS is interesting on its own for some explanatory information that it delivers (marker/SNPs selection is an example when it is a main aim of analysis). The feature selection overfit is of course a problem, but there are methods to omit it.",2010-09-02 18:25:58.0,88.0,
2658,2316,0,@Kingsford What's worong with K = Vb/Va?,2010-09-02 18:57:20.0,1134.0,
2659,2266,0,@Joris How do I calculate the ev from the CI? How does this approach make use of the prior information provided by the other products?,2010-09-02 19:06:21.0,1134.0,
2660,2316,0,"@Srikant It's the best ""answer"" I've got so far. I think I'll be trying it out because:
1. As currently stated Jorly's doesn't actually provide the ev, and there is no indication of how prior information is utilised.
2. Kingsford's answer could suffer convergence problems apparently, and there is a lot of work for me to figure out what it all means.
3. Simon's solution worked well for him, and I trust Simon.
4. I think in time I'll get a proof of his method
5. The method uses all the available data and is correct at the extrema when individual variance is small and large",2010-09-02 19:28:35.0,1134.0,
2661,2298,0,"At first blush this question seems purely mathematical: visualizing the function has nothing to do with the distribution of its arguments.  However, the reference to ""heteroskedasticity"" suggests you're really trying to visualize an object that is more explicitly represented as $f(x_1, ..., x_k) + \\epsilon$ or more generally as  $g(f(x_1, ..., x_k), \\epsilon)$ where $epsilon$ is a zero-mean random variable and $g(z,0) = z$ for all z.  Isn't this just a response-surface analysis?",2010-09-02 19:42:16.0,919.0,
2662,2314,0,"@chl: who said anything about tuning parameters? If additional things are performed, they should be repeated during cross-validation as well. Clearly modifying your algorithm until you get good cross-validated error rates is ""cheating"". BTW, I agree that cross-validation, especially leave-one-out, is not all that great.",2010-09-02 20:39:28.0,279.0,
2663,2316,0,"@bart - I don't think you'll run into convergence problems, but you're right that it takes awhile to learn the methods.  As for what's wrong with Vb/Va, I'm not sure how to answer because it's not clear to me how those values are being calculated.  But under the nested Gaussian assumption described, one good way to define 'best' is in terms of minimizing mean squared error (MSE).  This is what the BLUP equations do.  See, for example, [here](http://tiny.cc/4zfl1).",2010-09-02 21:00:07.0,1107.0,
2664,2321,1,"The inputs are assumed to be iid Gaussian, implying there is no direction of maximal variance.  But perhaps I have misinterpreted this assumption?",2010-09-02 22:07:06.0,919.0,
2665,2298,0,"@whuber I don't know enough RSM to say for sure (and the wikipedia page is not helping me). I am not necessarily trying to optimize the function (gasp), but am trying to find interesting facts about it. What makes this a statistical question is that I'm not terribly interested in the case where any of the $x_i$ are far from zero (assumed standard normal), and so what constitutes 'interesting' should somehow be weighted by relevance.",2010-09-02 22:54:51.0,795.0,
2666,2304,0,"ggobi looks interesting, but I am repelled by the `xml` data format. maybe I'll get over that, but it is not high priority.",2010-09-02 22:55:42.0,795.0,
2667,707,0,"@mbq: I have read the paper; Shao reports on a simulation study with only 40 observations, and shows that LOOCV underperforms the Monte-Carlo CV except in the case where no subselection is appropriate (the full feature set is optimal). 100 is way more than enough, at least for subset selection in linear models.",2010-09-02 23:14:06.0,795.0,
2668,2322,0,"Oh my, yet again; have you read the title of this article?",2010-09-02 23:49:54.0,88.0,
2669,2322,1,"Ok, to be clear -- I'm not saying LOOCV is a good idea for a big number of objects; obviously it is not, but Shao is not applicable here. Indeed in most of cases rules for LMs does not hold for ML.",2010-09-03 00:17:35.0,88.0,
2670,2316,0,@bart It is not driven by sound statistical principles as is evident by the hand waving he does. If you want a statistically sound solution you should really explore multi-level models that Kingsford refers to in his comments. See my [answer](http://stats.stackexchange.com/questions/1822/test-for-poolability-of-individual-data-series/1827#1827) to another question which illustrates the basic idea.,2010-09-03 00:22:41.0,,user28
2671,538,3,"Er... I'm not familiar with real stats by a long shot... but isn't ""exposing all hidden variables"" by definition impossible? How do you know when there's no more ""hidden"" variables?",2010-09-03 01:54:54.0,1170.0,
2672,1287,0,"Chapter 17 covers:
17 An overview of the GLM
17.1 linearity and the GLM
17.2 bivariate to multivariate statistics and overview
17.2.1 bivariate form
17.2.2 simple multivariate form
17.2.3 full multivariate form
17.3 Alternative research strategies

I hope this can help
Regards",2010-09-03 02:01:23.0,10229.0,
2673,2304,1,@shabbychef Then check out the interface from R called rggobi http://www.ggobi.org/rggobi/,2010-09-03 02:30:47.0,183.0,
2674,2326,1,Many questions: 1. Is it an experiment or an observational study? 2. State clearly what are your predictors and what is your outcome variable. 3. Try to be more specific. 4. What do you mean by sorting? 5. What is your sample? Is it n=100 pieces?,2010-09-03 03:44:30.0,183.0,
2675,1136,1,"A good model is never solely based on statistical tests and criteria. It should be a combination of literature review, experience, statistics and common sense.",2010-09-03 03:46:11.0,10229.0,
2676,2316,0,@Kingsford Thanks for your help and time.,2010-09-03 06:07:07.0,1134.0,
2677,2328,1,Do you really need NN? This method is rather considered obsolete (partially because it is very hard to generally answer your questions).,2010-09-03 06:36:53.0,88.0,
2678,707,0,"@shabbychef You've got me here; the second argument in my first comment is of course a junk, I had some other works in mind and overgeneralized. Nevertheless, I will still argue that Shao's paper is not a good reference for general ""LOO fails for large N"" since its scope is reduced to linear models.",2010-09-03 06:43:08.0,88.0,
2679,2314,0,"not it is not cheating, since CV shows you the approximation how algorithm will perform on new data. You only need to be sure that you haven't settled on something based on the whole set (this is a leak of information about the structure of the full set, so it can immediately bias all train parts).",2010-09-03 07:15:11.0,88.0,
2680,2318,0,"I was making the point that FS doesn't necessarily improve predictive importance, and can make it worse.  If finding the informative features is of intrinsic importance, then of course FS should be used, but it may well be that predictive performance is compromised if over-fitting the feature selection criterion occurrs (which happens rather easily).  For tasks like micro-array analysis, I would use (bagged) ridge regression for predictions and something like the LASSO for determing the key features (for gaining understanding of the biology).  There is no need to do both in the same model.",2010-09-03 07:28:20.0,887.0,
2681,2314,0,"@mbq - Ankino is correct, tuning your model to minimise a CV statistic is ""cheating"" and the CV statistic of the final model will have a substantial optimistic bias. The reason for this is that the CV statistic has a non-negligible variance (as it is evaluated on a finite set of data) and thus if you directly optimise the CV statistic you can over-fit it and you can end up with a model that generalises less well than the one you started with.  For a demonstration of this, in a machine learning context, see

http://jmlr.csail.mit.edu/papers/v11/cawley10a.html

Solution: Use nested XVAL",2010-09-03 07:40:01.0,887.0,
2683,2337,0,"Ideally, the question should be self-contained. Could you please give the relevant formulas as well? By the way, I may not have understood your question properly but won't euclidean distance give the same weight to all the variables? That metric seems to be one of the distance measures offered by the library you are using.",2010-09-03 10:42:31.0,,user28
2684,2326,1,To add to Jeromy's comment. It would be nice if you could state your research question. So far I am pretty confused.,2010-09-03 10:50:09.0,442.0,
2686,2338,0,"Hello Mike. Thank you for the answer, and for your package - it is really great!",2010-09-03 11:05:41.0,253.0,
2689,2281,0,"@ Keith : I see your point. So in the Bayesian approach, I take a diffuse prior to construct the same interval and call it a credible interval. In a Frequentist approach, if I can prove with absolute certainty that the interval is wrong, I have either violated assumptions, or I know the true value. In either case, the 95% confidence interval is not valid any more. The assumptions involved imply a diffuse prior, i.e. a complete lack of knowledge about the true parameter. If I have prior knowledge I shouldn't calculate a confidence interval in the first place.",2010-09-03 11:23:47.0,1124.0,
2691,2266,0,"@Bart. The expected value is a theoretical concept. Statistics is used to approximate it. If you can calculate the true expected value, there is no need for a confidence/credibility interval any more, because (Frequentist) the chance the true expected value is the true expected value, is 1, or (Bayesian) your true expected value is not a random variable. I gave you the frequentist approach, no strict priors involved. You use the information of the other products by calculating a common standard deviation. So if you want a Bayesian approach, my answer is indeed not what you're looking for.",2010-09-03 11:50:30.0,1124.0,
2692,2335,0,Could you be more precise about what you mean by cross-tabulating the ORs? Your factors have more than two levels?,2010-09-03 12:50:06.0,930.0,
2693,2335,0,"Yes, the factors have 3 and 6 levels respectively. I'm wanting a table of what the predicted odds are for each possible combination of `fac1` and `fac2`.",2010-09-03 13:22:42.0,229.0,
2695,2342,0,"Excellent! This is exactly what I was looking for, thanks!",2010-09-03 14:01:30.0,229.0,
2696,2298,0,"@Shabbychef: Right, your stipulation of standard normals establishes you want to know $f$ in a neighborhood of the origin that doesn't extend more than a few units away. RSM occurs to me because it includes methods that *efficiently* explore the behavior of a function, so if evaluating $f$ is expensive, RSM methods may be helpful. If evaluating it is not expensive, you have a purely mathematical problem. If each evaluation for a given set of arguments can produce a variety of results, you need the $\\epsilon$ term--but maybe you can eliminate it by averaging multiple evaluations at each point?",2010-09-03 14:15:19.0,919.0,
2697,2337,0,"instead of ""(multiply each dimension with 1/average of that dimension)"" don't you mean to multiply by the reciprocal of the *standard deviation* of the corresponding coordinates?  This is not a bad choice but it's sensitive to outliers.  Rescaling by a resistant measure of spread, such as the IRQ, would protect you from outliers and still accomplish what you intend.",2010-09-03 14:19:10.0,919.0,
2698,2345,2,Maybe it should be a comment? (even if it is a good comment :) ),2010-09-03 14:24:08.0,223.0,
2699,2344,2,There is not a single tree... But see @Shane's response for plotting one of them for illustrative purpose.,2010-09-03 14:48:41.0,930.0,
2700,2335,0,"Ok, @Bernd's answer is fine with me. Maybe have a look at the `Design` package from Franck Harrell; it has very nice functions along `lrm()` for GLMs and related stuff.",2010-09-03 14:52:41.0,930.0,
2701,2340,0,Thanks for your answer. I just found a useful paper on that issue: http://dx.doi.org/10.1007/BF01897163. It compares the usefulness of different normalization approaches with each other and with no normalization.,2010-09-03 15:05:22.0,977.0,
2702,2337,0,"@Srikant Vadali: I wanted to post the formulas, but I couldn't find a way to embed them here, and ASCII-formulas would have made things worse instead of better. Also, the book I referenced is freely available as an ebook, so everyone can look it up.",2010-09-03 15:07:17.0,977.0,
2703,2337,0,Check out http://meta.stats.stackexchange.com/q/386/930 for LaTeX typesetting,2010-09-03 15:09:58.0,930.0,
2704,2348,0,"Perhaps, CW? The question seems like a poll of what people use. How would you define the 'best' answer? I admit that I am a bit fuzzy when to apply CW. So, feel free to ignore this comment if you feel otherwise.",2010-09-03 15:22:57.0,,user28
2705,2281,0,"No, I'm afraid you still haven't quite got it. There is no requirement for a ""diffuse prior"" in either case. It is fine to calculate a confidence interval whether you have prior knowledge or not -- the point is that the confidence interval just doesn't care. A confidence interval guarantees its coverage probability absolutely, even in the worst case. It will not be ""the same interval"" as a credibility interval informed by a prior, at least not in general.",2010-09-03 15:47:27.0,1122.0,
2706,2281,0,"And as I said, it is perfectly acceptable, formally speaking, that at the end of your experiment you arrive at a particular confidence interval that you can prove does not cover the true value. This does NOT mean the interval was invalid or that it's not a 95% confidence interval. Of course if you reran the same experiment 100 times you must expect to get such a nonsense result less than 5 of those times, but the fact that you get provable nonsense 5% of the runs is formally okay as long as the confidence interval covers the value the other 95% of the outcomes.",2010-09-03 15:50:52.0,1122.0,
2707,2281,0,"And the transpose is true for a credibility interval -- it is perfectly acceptable to have values of the parameter that produce a credibility interval that's always wrong! As long as your prior says those values are rare.

Imagine a bag containing a trillion weighted coins -- one of which has heads probability 10%, and the rest are fair coins. Your experiment is: draw a coin from this distribution, flip it ten times, count the discrete # of heads, then state 95% credible interval on heads prob. If you get the ""10%"" coin the interval will ALWAYS FAIL TO COVER. Again, doesn't make it invalid.",2010-09-03 15:57:56.0,1122.0,
2708,2348,1,"I wouldn't mind leaving this as non-CW, especially if Colin can rephrase it slightly to allow for the possibility of one best answer.  That said, I can't envision how to do that...",2010-09-03 16:05:09.0,5.0,
2709,707,0,"@mbq I'm curious enough about this that I think it should be a question in its own right. I would guess that Shao's work extends to variable selection under logistic regression or Fisher LDA. possibly to classification with kernel methods for finite kernel space (e.g. polynomial kernels), b/c the model is linear in the kernel space. but I have no good intuition for whether it is appropriate for comparing models, using different kernels, say. I hope somebody has tested the results in these arenas. The fact that LOO is _impractical_ for large $n$ is another issue...",2010-09-03 16:07:06.0,795.0,
2710,2281,0,"In one of Jaynes papers

http://bayes.wustl.edu/etj/articles/confidence.pdf

He constructs a confidence interval and then shows that for the particular sample you can be 100% sure that the true value does not lie in the ""confidence interval"".  That doesn't mean that the CI is ""wrong"", it is just that a frequentist confidence interval is not an answer to the question ""what is the interval that contains the true value of the statistic with probability 95%"".  Sadly that is the question we would like to ask, which is why the CI is often interpreted as if it was an answer to that question. :-(",2010-09-03 16:24:03.0,887.0,
2712,2200,0,"before the question was edited to have proper LaTeX, when I answered it, it was very hard to parse. Look at this in plaintext: 'Max value of intersection would be min(a,b). how do I find the min?' 
With the proper markup, it is much easier to parse. (Though it looks like a homework question to me...)",2010-09-03 16:30:58.0,795.0,
2713,2228,0,"Many many thanks! However, installation fails in my system. Who should I report this to?",2010-09-03 16:33:42.0,1084.0,
2714,2298,0,"@whuber: as these things go, eventually $k$ will be sufficiently large, and the code will be sufficiently encumbered with bells and whistles that some sampling technique will have to be employed. I am not there yet, though.",2010-09-03 17:25:45.0,795.0,
2715,2298,0,"@Shabbychef: That points away from answers proffered by eng-carlin (because dimensionality reduction won't accomplish anything) and Josh Hermann (because a tornado chart looks purely at one coordinate at a time and further reduces the graph of $f$ from one dimension to zero) and favors answers akin to Jeromy Anglim's.  Along those lines, Mathematica offers great utilities to interpolate and visualize functions, given either as formulas or as sampled data.",2010-09-03 17:37:41.0,919.0,
2717,2352,0,There must be something in chemometrics books; the only man I know that uses LOO is also doing it.,2010-09-03 17:51:55.0,88.0,
2718,42,0,how's the performance of this? (I run screaming whenever I see the word 'Java'...),2010-09-03 18:01:28.0,795.0,
2719,2350,1,"Just to be clear: Are you applying RFs in an iterative manner, that is by selecting the top-ranked features (according to Gini index or decrease in MSE) from the whole input space? I know that RFs may not necessarily improve with increasing trees number, but what you are depicting seems to make sense (the best features allow to predict with 100% accuracy the OOB samples), although there is clearly a risk of overfitting when proceeding this way.",2010-09-03 18:08:07.0,930.0,
2720,2326,0,"Okay, more detail - you got it ;)",2010-09-03 18:10:36.0,1114.0,
2721,2345,0,"@robin-girard Perhaps it should. Actually I started it as a comment, but then decided that it does answer one of the questions: is the probability of the tree 0.25 or 0.5.",2010-09-03 18:12:36.0,279.0,
2722,42,0,"@shabbychef Seem quite good from what I've heard, but generally Weka is used as a first step to test several algorithms and look at their performance on given data sets (or a subset thereof), then one recode part of the core program to optimize its efficiency (e.g. with high-dimensional data calling for cross-validation or bootstraping), sometimes in C or Python.",2010-09-03 18:14:12.0,930.0,
2723,9,1,"@James + http://j.mp/9fVmtV, http://j.mp/aNDyf2, http://j.mp/9Gzzri :-)",2010-09-03 18:19:39.0,930.0,
2724,2304,0,Bump for rggobi. I just downloaded it after reading @shabbychef's comment and it changed/rocked my world.,2010-09-03 18:39:20.0,1076.0,
2725,2356,18,"Under rather mild assumptions, (a) Bayesian estimation procedures are admissible and (b) all, or almost all, admissible estimators are Bayesian with respect to some prior.  Thus it's no surprise that a Bayesian confidence interval ""yields the same or better results.""  Note that my statements (a) and (b) are part of the *frequentist* analysis of rational decision theory.  Where frequentists part company with Bayesians is not over the mathematics or even the statistical procedures, but concerns the meaning, justification, and correct use of a prior for any particular problem.",2010-09-03 18:48:20.0,919.0,
2726,2272,0,"I think the answers we have so far are very good, but I'd like to get some more votes and maybe a few more answers, so I'm placing a bounty.  Please vote!  And if you have another way of explaining it, post it - more perspectives are always welcome. (for some reason, this comment didn't go through when I placed it originally...)",2010-09-03 18:48:42.0,71.0,
2727,2251,1,"A huge plus one for point 2.  Other than going through human subjects protection training, I've never received the tiniest bit of training on data collection and storage.  Getting the data collection right is vastly more important than the analysis.",2010-09-03 18:54:34.0,71.0,
2728,2346,2,Small extension about plots: `plot.randomForest` shows how OOB error and in-class OOB error evolved with increasing number of trees; `varImpPlot` shows attribute importance measures for top attributes and `MDSplot` all objects plotted on the 2D projection of RF object proximity measure.,2010-09-03 19:25:18.0,88.0,
2729,2362,2,+1 Great point about setting the seed values.,2010-09-03 19:32:24.0,5.0,
2730,2356,1,"So, does the above comment imply that the answer to the OP's question is 'No such examples can be constructed.'? Or perhaps, some pathological example exists which violates the assumptions behind admissibility?",2010-09-03 19:44:08.0,,user28
2731,2346,0,+1 for citing the `MDSplot()` function. I must admit I often use RFs as a way to highlight clusters of individuals (based on the RF proximity measure) rather than selecting the best features. Clinicians often read much easily such plots than dotplot of var. importance...,2010-09-03 20:07:49.0,930.0,
2732,2365,2,"It is possible to be interested in the differences between Bayesian and frequentist statistics without it being a quarrel. It is important to know the flaws as well as benefits of ones preferred approach.

I specifically excluded priors as that is not a problem with the framework, per se, but just a matter of GIGO.  The same thing applies to frequentists statistics, for example by assuming and incorrect parametric distribution for the data.  That wouldn't be a criticism of frequentist methodology, just the particular method.

BTW, I have no particular problem with improper priors.",2010-09-03 20:52:49.0,887.0,
2733,2221,0,"Thanks for this response. To follow-up: is the solution to the the ""other formulation"" (wind vectors), 1/3, correct given the issues that you raise?",2010-09-03 21:56:55.0,401.0,
2734,2365,3,"Jaynes first example: Not one statistician in his right mind will ever use an F-test and a T-test on that dataset. Apart from that, he compares a two-tailed test to P(b>a), which is not the same hypothesis tested. So his example is not fair, which he essentially admits later on. Next to that, you can't compare ""the frameworks"". What are we talking about then? ML, REML, LS, penalized methods,...? intervals for coefficients, statistics, predictions,...? You can as well ask whether Lutheran service is equivalent or superior to Shiite services. They talk about the same God.",2010-09-03 22:22:11.0,1124.0,
2735,2365,0,"Could you clarify what is your data and what are the parameters you  would be estimating in your model? I am a bit confused on this point. Also, could you please use $$ instead of $ to center the formula? The font size is very small right now.",2010-09-03 22:23:09.0,,user28
2736,2356,1,"@Srikant: Good question.  I think the place to begin investigating is a situation where there are non-Bayes admissible estimators--not necessarily a ""pathological"" one, but at least one that provides some opportunity to find a ""contrary example.""",2010-09-03 22:29:08.0,919.0,
2737,2365,0,"@Srikant: The example in Felsensteins book is based on a Jukes-Cantor model for DNA evolution. Data is DNA sequences. You want to estimate the probability of change in your sequence, which is related to your branch length based on the mentioned formula. Branch lengths are defined as time of evolution : the higher the chance for changes, the more time that passed between the ancestor and the current state. Sorry, but I can't summarize the whole theory behind ML and Bayesian phylogenetic inference in just one post. Felsenstein needed half a book for that.",2010-09-03 22:29:34.0,1124.0,
2738,2365,0,I guess I just wanted you to clarify what variables in your equation was data and which ones were the parameter as it was not clear from your post especially to someone like me who is an outsider. I am still lost but I guess I would need to read the book to find out more.,2010-09-03 22:44:20.0,,user28
2739,2365,0,"@Srikant : I tried to clarify a bit more. Actually, P is the parameter that is used in the likelihood function for optimization, and the formula merely gives its relation to t, which can alternatively be used in the likelihood function. Sorry I can't be more clear. If Phylogeny interests you, I can surely recommend Felsensteins book, it's a gem. http://www.sinauer.com/detail.php?id=1775",2010-09-03 23:04:10.0,1124.0,
2740,2281,0,"@Keith: I'm not getting it. If you mean that the 10% coin only gives head 1 in 10 times, and you end up with 0 heads, you cannot compute a confidence interval. If you have 1 head in ten times, your interval will indeed not cover 50%. But I never claimed it covered. I just claimed it is unlikely it doesn't cover. I do NOT know the true value. Plus, all CI (Wald, score,Pearson,...) have a bad coverage on the edges of the probability space, definitely with only 10 cases. So I wouldn't state anything based on that CI. I'd use probability calculation to come to a conclusion. Like Bayes did.",2010-09-03 23:35:53.0,1124.0,
2741,2365,0,"It isn't clear to me why it is a problem that a flat prior on p implies an exponentially decreasing prior on t.  If that is inconsistent with biological knowledge, it simply means that a flat prior on p does not reflect actual prior knowledge.

I also don't see why it is a problem to use an improper flat prior on t (other than I would have thought it inconsistent with prior knowledge; the branch time can't be say a billion years, if it were we wouldn't be here yet, so it is inappropriate to use a flat prior).

Note that flat priors don't necessarily imply ignorance.",2010-09-03 23:45:34.0,887.0,
2742,2281,0,@Keith : but I got your point - the true value is not a random variable - I agree. My bad.,2010-09-03 23:47:27.0,1124.0,
2743,2365,0,"@Dikran : it's not a problem. It is a fact. The problem is that p and t are strictly related, and hence should give exactly the same model. That happens in an ML approach, but that doesn't happen in the Bayesian approach. In Felsensteins example, a truncation of the t-prior at 700 or larger makes that the credible interval doesn't cover the true value any more. In this particular case, i.e. the lack of prior knowledge, Bayesian inference just isn't feasible. There is no sensible ""uninformative"" prior that can be used.",2010-09-04 00:08:47.0,1124.0,
2744,2365,0,"@Dikran : Regarding the flat t-prior: the prior gets truncated. When truncated at 5(!), most of the mass of the prior on p is concentrated around the maximum p-value. With larger truncation values, this effect is even more pronounced. The point is-again- that it's impossible to find a sensible prior when you have no prior knowledge in phylogenetic inference.",2010-09-04 00:11:10.0,1124.0,
2745,2281,0,"Joris, my last comment was about a ""95% credible interval"" -- not confidence interval! If you have a bag with one trillion fair coins and a single 10%-heads coin, and your experiment has you draw a coin uniformly at random from the bag, flip it ten times and then state a credibility interval on the heads probability, your credibility interval will always be [0.5, 0.5] no matter what. Thus if you happened to draw the unfair coin, the credibility interval will always be wrong.",2010-09-04 03:59:07.0,1122.0,
2746,2281,0,"Also I can't agree that ""all CI"" have bad coverage on the edges. Any exact CI, and some approximate CIs, will guarantee that the coverage is always greater than the confidence parameter (e.g. the 95%), even in the worst case. This is true of the Blyth-Still-Casella and Clopper-Pearson intervals for a proportion.",2010-09-04 04:02:57.0,1122.0,
2747,2367,0,I believe Shao showed that k-fold CV is inconsistent if $k$ is fixed while $n$ grows.,2010-09-04 05:18:10.0,795.0,
2748,2367,0,The BIC has k growing with n.,2010-09-04 05:56:26.0,159.0,
2749,2365,0,"Joris, I think you are missing the point, a flat prior is not necessarily non-informative.  It is completely reasonable for the same state of knowledge/ignorance to be expressed by a flat prior on p and (say) a flat prior on log(t) (which is a very common Jeffrey's prior) rather than a flat prior on t.  Does the book investigate ideas of MAXENT and transformation groups for this problem? 

There isn't enough detail in your example, but from what I can tell even a truncated flat prior on t is likely to be inconsistent with prior knowedge about t.",2010-09-04 08:39:52.0,887.0,
2750,2365,0,"@Joris, also in your original comment you suggest the flat prior on t must be truncated, because otherwise the area under the density function is infinite.  This is not true, there are plenty of problems where improper priors work very well, so there is not necessarily a need to truncate the flat prior.",2010-09-04 08:49:51.0,887.0,
2751,2287,1,+1 That was a great example.,2010-09-04 09:01:13.0,,user28
2752,2369,3,"Food for thought, however the particular example is unfair as the frequentist approach is allowed to consider the relative costs of false-positive and false-negative costs, but the Bayesian approach isn't.  The correct thing to do according to Bayesian decision theory is to give an interval of [0,1] as there is no penalty associated with false-negatives.  Thus in a like-for-like comparison of frameworks, none of the Bayesians would ever be beheaded either.

The issue about bounding false-positives though gives me a direction in which to look for an answer to Jaynes' challenge.",2010-09-04 09:10:21.0,887.0,
2753,2369,1,"Note also that if the selected coin is flipped often enough, then eventually the Bayesian confidence interval will be centered on the long run frequency of heads for the particular coin rather than on the prior.  If my life depended on the interval containing the true probability of a head I wouldn't flip the coin just once!",2010-09-04 09:57:59.0,887.0,
2754,2348,0,"I've tried to change the question to make it less CW - not sure if I have succeeded :( @Shane @Srikant if you still think that it should be a CW, feel free to change it.",2010-09-04 10:50:46.0,8.0,
2755,2374,0,"If I understand correctly, your question encompasses at least two different topics: (1) use of FA in attitude or motivational scales, and (2) how to handle 'extreme' patterns of responses (ceiling/floor effects) in such scales?",2010-09-04 11:32:56.0,930.0,
2756,2372,0,"Technically, you are correct but do note that the confidence interval gives the set of parameter values for which the null hypothesis is true. Thus, ""are the observed data x reasonable given our hypothesis about theta?"" can be re-phrased as ""What true values of theta would be a compatible hypothesis given the observed data x?"" Note that the re-phrased question does not necessarily imply that theta is being assumed to be a random variable. The re-phrased question exploits the fact that we perform null hypothesis tests by inspecting if the hypothesized value falls in the confidence interval.",2010-09-04 15:32:22.0,,user28
2757,2186,0,"Just because it is the only book I know which combines exploratory MV analysis, statistical modeling, and psychometrics. Maybe not the best one actually, but interesting on its own.",2010-09-04 16:59:16.0,930.0,
2758,2367,1,"I will just silently remind that *IC <--> *CV correspondence from Shao paper works **only** for linear models, and BIC is equivalent only to k-fold CV with certain k.",2010-09-04 17:26:08.0,88.0,
2759,2111,2,"Find this article in the meantime, The lasso method for variable selection in the cox model, from Tibshirani (Stat. Med. 1997 16: 385-395), http://j.mp/bw0mB9. HTH",2010-09-04 21:30:44.0,930.0,
2760,2111,1,"and this more recent one (closely linked to the `penalized` R package), http://j.mp/cooIT3. Maybe this one too, http://j.mp/bkDQUj. Cheers",2010-09-05 08:29:41.0,930.0,
2761,2385,2,"The aim of visualization is not to show the data, but to show a claim that this data supports -- so first try to focus on what you want to show, to make this question answerable. Or, if you just want a pool of visualization ideas, make it a community wiki.",2010-09-05 13:08:37.0,88.0,
2762,2385,0,"@mbq I don't think it is that ambiguous. He wants to compare means of the scales between different groups, as well as the response rates between different groups. Visualization can be exploratory (the fact that he is making a report to senior management doesn't mean to me that it can't be exploratory).",2010-09-05 13:45:01.0,1036.0,
2763,2380,1,May I suggest that you edit your main question to include this information?,2010-09-05 15:50:49.0,196.0,
2764,2377,0,Do you require that the standard deviation remain constant with this transformation as well?,2010-09-05 16:04:35.0,196.0,
2765,2385,0,@Andy so my suggestion for CW; even exploratory visualization is made to check *something*.,2010-09-05 19:32:13.0,88.0,
2766,2365,0,"@Dikran : Guess you are missing the point : using the same uninformative prior gives two different models with Bayesian statistics on the same dataset. Not so with ML. The Bayesian can be very biased due to the nature of the model and the incompatibility of that model with infinite priors. You don't have to believe me. Felsenstein is the authority on phylogenetic inference, and his book explains you better than I will be able to. Reference in a previous comment.",2010-09-05 20:02:47.0,1124.0,
2767,2397,0,What exactly do you mean by limiting distribution in this case?,2010-09-05 21:24:39.0,352.0,
2768,2397,0,"ie, the one you get from Central Limit Theorem, let me update details",2010-09-05 21:28:01.0,511.0,
2769,2398,1,"but the covariance matrix of multinomial in question is singular, you showed it yourself...",2010-09-05 22:26:32.0,511.0,
2770,2398,0,"Oh, I see your problem! One of the elements, say the $d$th is completely dependent on the others. Probably if you chop off the last row and column of $C$ you will get that the $[p_1,p_2,\\dots,p_{d-1}]$ are normally distributed, but I'll have to have a think about it.  Surely this is solved somewhere already!",2010-09-05 22:41:08.0,352.0,
2772,2365,0,"@Joris, as I said a flat prior is NOT NECCESSARILY UNINFORMATIVE. Consider this, if two priors give different results, then the must logically represent a different state of prior knowledge (see early chapters of Jaynes book that set out desiderata for Baysian inference).  Therefore the ""flat p"" prior and ""flat t"" prior cannot both be uninformative.  Felsenstein may be an expert on phylogenetic inference, but it is possible that he is not an expert on Bayesian inference. If he states that two priors giving different results are both uninformative, he is at odds with Jaynes (who certailny was).",2010-09-05 23:09:15.0,887.0,
2773,2398,0,"One suggestion I found is to still use a Gaussian, but use pseudo-inverse instead of inverse and ""product of non-zero eigenvalues"" in place of determinant. For d=2 this seems to give the correct density form, but the normalization factor is off",2010-09-05 23:26:10.0,511.0,
2775,2400,0,"So then, sigma is the entropy of N(0,sigma) corresponding to squared error, and min(p,1-p) is the entropy of Bernoulli(p) corresponding to 0,1 prediction loss? Seems like quite a generalization!",2010-09-06 00:03:45.0,511.0,
2776,2228,0,"What error did you get? maybe try the following: install.packages(c('lme4','car','ggplot2','stringr'));install.packages('ez',dependencies=F,type='source')",2010-09-06 00:27:27.0,364.0,
2777,2401,2,"Do you know of any published research that looks at the relative merits of using likert scales as intervals vs ordinal data? Perhaps, the flaws of treating them as interval level scales are not serious enough to warrant a complex approach. If that is the case then your approach may simply be a wild goose chase.",2010-09-06 01:12:06.0,,user28
2778,2377,0,"no, I expect it will not, but the excess kurtosis should remain fixed. I would expect the transform to be monotonic, however, and preferably deterministic.",2010-09-06 02:23:41.0,795.0,
2779,2392,0,"I had been doing something like this: rank, then use the g-and-h transform to get a fixed kurtosis and skew. However, this technique assumes I actually know the population kurtosis, which I can estimate, but I am interested, philosophically, if there is a transform which preserves the kurtosis without me having to know what it is...",2010-09-06 02:26:04.0,795.0,
2780,2392,0,"@shabbychef:  Oh, well then sorry for not adding anything new. However, you added something new, I hadn't heard of the g-and-h formula before.  Do you have a freely accessible citation that provides it?  I stumbled onto one paper with it spelled out (http://fic.wharton.upenn.edu/fic/papers/02/0225.pdf) but the notion is a bit foreign to me (in particular is that e^Z^g or something else)?  I tried it like this... but the results seemed odd... a+b*(e^g^z-1)*(exp((h*z^2)/2)/g).",2010-09-06 05:04:41.0,196.0,
2781,2377,1,Yikes - woe unto the person that wants to prove a non-deterministic function is monotonic.,2010-09-06 05:05:54.0,196.0,
2782,2386,1,"There is nothing wrong with a scientist ""believing"" in something, especially as a Bayesian probability represents the degree of belief or knowledge regarding the truth of some proposition.",2010-09-06 07:25:56.0,887.0,
2783,2386,1,"...The problem arises only when a scientist cannot distinguish between a belief and a fact.  There is nothing unscientific in the belief that Bayesian or frequentist statistics are superior, as there is no objective test that can decide the answer (AFAIK), so the choice is largely subjective and/or a matter of ""horses for courses"".",2010-09-06 07:36:00.0,887.0,
2785,2378,0,I realise that there are countless variants of this algorithm. What I was interested in was which ones do people routinely use.,2010-09-06 07:50:24.0,8.0,
2788,2369,1,"Having though about this a bit more, this example is invalid as the criterion used to measure success is not the same as that implied by the question posed by the king.  The problem is in the ""no matter which coin is drawn"", a clause that is designed to trip up any method that uses the prior knowledge about the rarity of the biased coin.  As it happens, Bayesains can derive bounds as well (e.g. PAC bounds) and if asked would have done so, and I suspect the answer would be the same as the Clopper-Pearson interval.  To be a fair test, the same information must be given to both approaches.",2010-09-06 08:29:18.0,887.0,
2789,2345,0,"Thanks, I added some suggestion on one possible equivalence relation such that formula (2) would apply (I would have liked to add this as a comment but the number of characters was too high).",2010-09-06 08:46:29.0,1185.0,
2790,2406,1,"I would put it: ""Of the X% American/Australian/French/etc. adults who intend to buy a television, Y% intends to buy brand A, Z% intend to buy brand B,..."" So you immediately see both information in one sentence.",2010-09-06 08:49:23.0,442.0,
2791,2350,0,Yes that's what I am doing,2010-09-06 08:56:44.0,1150.0,
2793,2397,1,"What you're referring to is the asymptotic distribution of the **maximum likelihood estimator** of a multinomial. Also, the first equation should be n^{-1}, not n^{-1/2}.",2010-09-06 10:12:10.0,495.0,
2794,2406,0,@Henrik. Thanks. I agree. I guess I was trying to put it in slightly more general terms.,2010-09-06 10:16:50.0,183.0,
2796,2388,0,The use of box-plots seems doubtful there. I just try to plot 190 bxp on the same page and this doesn't seem to carry out so much information as can be seen here: http://j.mp/925AY8. P.S. I didn't even use the R base graphics but relied on the `grid` package for the layout. @Shane's suggestion might be better.,2010-09-06 14:23:22.0,930.0,
2797,2388,0,@chl Your right that is difficult to envision. The thinner you make the plots the more it just turns into a trend (like a time series). I don't think that would be a problem except that there is likely no logical ordering to the groups in this posters problem.,2010-09-06 14:47:28.0,1036.0,
2798,1297,1,"Also, isn't it possible to get CI (for the predictions) using bootstrapping ?",2010-09-06 14:58:08.0,253.0,
2800,2412,1,"Not to spoil the party, but how are random forest supposed to provided robustness to contamination by outliers is a mystery.",2010-09-06 15:27:04.0,603.0,
2801,2411,0,"Thank you kwak.  This article seems to be talking about boosting methods.  Do the results they present hold for the simple classifier case of a CART model? (on the surface it sounds like it, but I didn't go through the article enough to really know)",2010-09-06 15:28:18.0,253.0,
2802,2411,0,"The result they present holds for any convex loss function, and was initially discussed by Tukey. To sum things up, the measure of spread (Gini or entropy) used to quantify the quality of a node is sensitive to contamination by outliers (i.e. observations that are miss-labeled in the dataset).  This problem affects both the building and the prunning stage. Contamination of a dataset by observation with wrongly imputed label will typically cause the resulting tree to be much too complex (you can check this rather easely by yourself).",2010-09-06 15:38:22.0,603.0,
2803,2362,0,So @mbq when doing a CV is it valid to firstly do a random forest with all samples selected; doing it twice once with all and secondly with the top 10 variables (which can be quoted in a paper).  Then do a leave-one out cross-validation (selecting the 10 top genes each try) and quote the CV error from that?,2010-09-06 15:47:14.0,1150.0,
2804,2365,0,"@Dikran : The point is not whether a flat prior is uninformative. The point is that an satisfying uninformative prior cannot be defined due to the nature of the model. Hence rendering the whole method unusable if you don't have prior information, and thus leading to the conclusion that Bayesian inference in this case is inferior to the ML approach. Felsenstein never said a flat prior was uninformative. He just illustrated why an uninformative prior cannot be determined, using the example of a flat prior.",2010-09-06 16:03:44.0,1124.0,
2805,2411,0,Thank you Kwak!  And is there no loss function that is robust?,2010-09-06 16:11:35.0,253.0,
2806,2411,1,"no *convex* loss function. See this article ""A fast algorithm for the minimum covariance determinant estimator"" for an example of what can be done with non-convex loss functions (although not related to classification, the article is worth a read).",2010-09-06 16:14:31.0,603.0,
2808,2365,0,"@Joris - it may be that an uninformative prior cannot be constructed in this case, but nothing that you have written so far establishes that to be the case.  What does Felsenstein write about MAXENT and transformation groups (the two main techniques used to determine an uninformative prior for a particular problem)?  If he has not investigated those methods, how can he know an uninformative prior is impossible?  It looks to me that a flat prior on p corresponds a flat prior on log(t), which is a well known Jeffreys' prior.  Can you demonstrate that the flat log(t) prior is informative?",2010-09-06 16:22:09.0,887.0,
2809,2411,0,It is very enjoyable to ask questions when getting answers like the ones you give - thank you!  Last questions: Do you know of an easy way to implement such a loss function in R's CART? (rpart),2010-09-06 16:29:18.0,253.0,
2810,2411,0,The problem you point out seems easier to solve for regression than classification trees. I do not know how to change the spread criterion (from s.d. to Qn) in R but since Qn (and MAD) are implemented it should not be difficult. Best (let us know whether you had any success with a 'robustified' version).,2010-09-06 16:40:08.0,603.0,
2811,2394,3,"Good answer, John.  Note that kurtosis usually isn't worth looking at when a distribution is skewed: the skew already tells you there's going to be substantial kurtosis.  Thus, good applications are with datasets known a priori to be (approximately) symmetric.  Residuals often are this way: the kurtosis can be a quick numerical means to assess the shape of their distribution.  An additional point is that the sampling *variance* of the standard deviation is a function of the kurtosis, so if you're estimating SDs or variances, you might want to glance at the kurtosis to assess the precision.",2010-09-06 17:11:40.0,919.0,
2812,2379,2,Thank you for posting this.  It's an important (and potentially inspiring) discussion to have.,2010-09-06 17:27:30.0,919.0,
2813,2362,1,"@danielsbrewer I would do this in some other way (paying more attention to feature selection), but this is correct; yet it is more on the topic of benchmarking RF feature selection than on selecting best markers for your biological problem.",2010-09-06 17:39:35.0,88.0,
2814,2411,2,"@Tal CART is an equivalent to boosting, of a ""pivot classifier"" (the criterion that sits in each tree node, like some attribute grater than something or some attribute value in set something).",2010-09-06 17:47:23.0,88.0,
2815,2411,0,"True.  Since I was intending to try this on regression trees, I'll have a go at it in the next few weeks and will come back to report if something nice will be formalized.  Best.",2010-09-06 17:49:10.0,253.0,
2816,2412,3,"@kwak Still, this is a good answer; trees in RF don't see the entire set, so many of them will not be contaminated. Even better -- tracking in which leafs do OOB cases land can be used to find mislabeled objects and eliminate them. (As I recall now, this is mentioned in Breiman's paper about RF).",2010-09-06 17:50:57.0,88.0,
2817,2397,1,"In notation above, for d=2, X_n is the number of heads after n coin throws, so it's X_n/sqrt(n) that approaches Normal, not X_n/n, no?",2010-09-06 18:11:51.0,511.0,
2819,2397,1,"Yes, you're right. I was just confusing myself.",2010-09-06 18:37:09.0,495.0,
2820,2408,0,"What's a bit unsatisfactory is the freedom of choice, to get a valid density I need to ask for distribution of A x where A is some d-1 rank (d)x(d-1) matrix. Will the error of CLT approximation for finite n be equivalent for all choices of A? That's not clear to me",2010-09-06 18:39:07.0,511.0,
2821,2406,0,He should make sure the sample size from the new group is not too small however,2010-09-06 18:45:49.0,74.0,
2822,2408,1,"Yes, the error should always be the same. Keep in mind that the last element of the vector is functionally dependent on the other (d-1) elements (in both the finite sample and asymptotic cases).",2010-09-06 18:46:39.0,495.0,
2823,2382,0,"I think statistics is a separate area from probability theory, though.",2010-09-06 19:31:01.0,1106.0,
2824,2419,2,"Rpy, of course ;-)",2010-09-06 19:31:49.0,88.0,
2825,2415,0,What are ways in which people are trying to solve these problems?,2010-09-06 19:42:19.0,1106.0,
2826,2408,0,"It's not that the `last' element is dependent, Yaroslav's problem is that he doesn't like the idea of getting to choose which element to drop.  I agree with the answer you have given but I also think that a bit more thought and care is required here.",2010-09-06 21:30:33.0,352.0,
2827,2408,0,"@Yaroslav: Perhaps it would be good to have an idea of what application you have in mind here, because at this stage there are potentially lots of answers to your question.",2010-09-06 21:33:57.0,352.0,
2828,2415,1,"@grautur: That's four excellent questions (plus many more, because your response applies to every answer in this thread).  They all deserve elaborate answers, but obviously there's no space for that here: one question at a time, please!",2010-09-06 21:58:17.0,919.0,
2829,2427,4,Monte Carlo methods can give very accurate answers provided you do enough simulations.,2010-09-06 22:59:43.0,159.0,
2830,2056,0,"Yes, it does, but it describes resampling the subject means.  D&H advocate resampling the subjects and fitting the original model.",2010-09-06 23:00:55.0,187.0,
2832,2408,1,"Robby -- application I had in mind is here http://mathoverflow.net/questions/37582/how-big-is-the-sum-of-smallest-multinomial-coefficients  Basically integrals of Gaussian suggested by CLT give extremely good approximation to sums of binomial coefficients (for small n, even better than integrating Gamma representation directly!), so I was seeing if I can do something similar to get approximate sums of multinomial coefficients, which I need to get non-asymptotic error bounds for various fitters (like, maximum likelihood)",2010-09-06 23:23:45.0,511.0,
2833,2423,3,CW perhaps?....,2010-09-06 23:34:02.0,,user28
2834,2427,3,If you want a programming solution a monte carlo is the only approach. I do not see any reason why you will not get accurate answers using monte carlo. A mathematical/analytical solution may not be easy.,2010-09-06 23:49:37.0,,user28
2835,2427,0,"I have seen discussion about Monte Carlo and people said if you want to achieve 6 decimal places, it will take too long, or perhaps I'm confused with other similar problems. Since it's fairly easy to code a Monte Carlo approach, I guess it will be worthwhile to give it a try first.",2010-09-07 01:14:38.0,18.0,
2836,2420,2,"Technically speaking, a histogram is used to show summary information about a frequency distribution. I think you want to ask ""Is a bar chart satisfactory?"", to which I would answer Yes. If there are repeated measures on the categorical variables, then you can capture variability with a box plot, as mentioned in csgillespie's comment.",2010-09-07 02:22:19.0,1080.0,
2837,2406,0,@Neil. Good point.,2010-09-07 03:15:54.0,183.0,
2838,2423,1,see also question on mathoverflow regarding books on mathematical statistics http://mathoverflow.net/questions/31655/statistics-for-mathematicians,2010-09-07 03:18:37.0,183.0,
2839,2431,0,"I don't think that's been around for a while, and it's not very comprehensive either...one reason I started this post was because I found an error there, and also couldn't find some things I needed, like density formula for functions of random vectors",2010-09-07 03:30:55.0,511.0,
2840,2369,1,"Dikran, there needn't be ""Bayesians"" and ""Frequentists."" They're not incompatible schools of philosophy to which one may subscribe to only one! They are mathematical tools whose efficacy can be demonstrated in the common framework of probability theory. My point is that IF the requirement is an absolute bound on false positives no matter the true value of the parameter, THEN a confidence interval is the method that accomplishes that. Of course we all agree on the same axioms of probability and the same answer can be derived many ways.",2010-09-07 04:55:38.0,1122.0,
2842,2432,0,"discontinuities at known x-values, or at unknown x-values? (known x is easy enough)",2010-09-07 07:04:27.0,805.0,
2843,2432,0,@glen I updated the question: I'm interested in situations where timing of discontinuities is not known apriori.,2010-09-07 07:08:14.0,183.0,
2844,2433,1,"Re LOESS: Perhaps my terminology is not quite right. By LOESS I'm referring to models that predict Y from X using some form of localised curve fitting. e.g., as seen in most of these graphs: http://www.google.com/images?um=1&hl=en&biw=1076&bih=569&tbs=isch:1&sa=1&q=lowess+graph&aq=f&aqi=&aql=&oq=&gs_rfai=",2010-09-07 07:12:49.0,183.0,
2845,755,0,"Like it, though it does remind me of http://en.wikipedia.org/wiki/Swiss_Toni",2010-09-07 07:18:02.0,449.0,
2846,2420,0,@Josh - see updated question -> aren't bar charts for displaying quantities? : are dates quantities ? -,2010-09-07 07:51:35.0,414.0,
2847,750,6,I think this quote is more of a cynical but realistic view of how statistical data is mostly used in debates (i.e. selected to support a preconceived notion rather than produced to test a hypothesis),2010-09-07 09:12:43.0,127.0,
2848,2362,0,"@mbq, sorry to go on, but I am very new to the machine leaning classification area.  So if you wanted to select the best discriminating markers for my sort of problem how would you go about it? and what error would you report?  I appreciate all the input.",2010-09-07 10:18:10.0,1150.0,
2852,2434,1,"Your answering a slightly different question than the question asks. The question is asking the expected number of cells that would be empty after 50 jumps. Correct me if I am wrong, but I see no direct path from the probability a flea ends up in a certain square after 50 jumps to the answer how many cells would be expected to be empty.",2010-09-07 13:24:19.0,1036.0,
2853,2431,0,"It may be useful if you email the author. Perhaps, it is a misunderstanding and if not it will help in correcting the error in the next edition.",2010-09-07 13:53:27.0,,user28
2854,2419,0,I agree with mbq. Is there a very good reason why you have to do that in Python? Otherwise I'd use the workhorse R as a back-end as well.,2010-09-07 13:54:16.0,1124.0,
2855,2281,0,"@Keith. I should specify ""bad"" coverage. Too much coverage is also bad coverage. I'll state it differently : on the edges, the exact coverage does not coincide with the chosen coverage.",2010-09-07 13:57:23.0,1124.0,
2857,2434,1,@Andy W -- great comment; yet Monte Carlo can be used to do this last step ;-),2010-09-07 14:19:51.0,88.0,
2858,2423,2,"You could specify whether you need an introduction on applied statistics, or one on (theoretical) statistical inference. I.e., do you want the framework of testing, regression and ANOVA explained or do you want to know what the central limit theorem and the inequality of Chebiyshev have to do with the weak law of large numbers?",2010-09-07 15:11:25.0,1124.0,
2859,2430,3,You should keep in mind that achieving what you want might be impossible.,2010-09-07 15:33:30.0,279.0,
2860,2420,1,"@Tom - Yes, bar charts are used to display quantities, so you could use a simple dot plot or line chart too. Some would argue that bar charts have a low ""information-to-ink"" ratio, but it is a perfectly reasonable choice here. And yes, I would consider dates quantities, especially in this setting where the dates are ""days since some event"". My original comment was admittedly nitpicking: your original question asked ""For example is a histogram satisfactory..."", but histograms are for when the independent variable is discrete/continuous over some interval.",2010-09-07 15:41:06.0,1080.0,
2861,2228,0,"I tried, but  ERROR: dependencies ‘lme4’ are not available for package ‘ez’ 
* removing ‘/Library/Frameworks/R.framework/Versions/2.11/Resources/library/ez’",2010-09-07 15:47:22.0,1084.0,
2862,2427,1,"I don't dispute any of the three previous answers, but the (simple) analysis in the answer I have offered puts these remarks in perspective: if you want six decimal place accuracy for an estimate of a number that will be in the hundreds, the Monte Carlo simulation will take at least a year on a machine with 10,000 CPUs running in parallel.",2010-09-07 15:53:14.0,919.0,
2863,2429,0,"+1: i too learned from Lehman (very good reference), but this one is not mentionned nearly enough",2010-09-07 16:05:51.0,603.0,
2864,2228,0,"looks like lme4 isn't installing properly. try: install.packages(""lme4"", repos=""http://R-Forge.R-project.org"")",2010-09-07 16:09:07.0,364.0,
2865,2420,0,"One more thing: you have some votes for box plots but can you clarify if you have a single date for each category, or if you have multiple dates?",2010-09-07 16:10:03.0,1080.0,
2869,2392,0,"@drnexus: I did not want to bias the results by mentioning my technique. I learned about the g-and-h and the g-and-k distributions from Haynes et. al, http://dx.doi.org/10.1016/S0378-3758(97)00050-5 , and Fisher & Klein, http://econstor.eu/bitstream/10419/29578/1/614055873.pdf",2010-09-07 16:32:29.0,795.0,
2870,2431,0,"I have confirmation from author, so it'll probably be corrected in next edition",2010-09-07 16:40:39.0,511.0,
2871,2367,0,"Actually, I believe Shao shows that CV is inconsistent unless $n_v/n \\to 1$ as $n \\to \\inf$, where $n_v$ is the number of samples in the test set. Thus $k$-fold CV is always inconsistent for variable selection. Have I misunderstood? By $k$-fold CV I mean dividing the sample into $k$ groups and training on $k-1$ of them, and testing on 1 of them, then repeating $k$ times. Then $n_v/n = 1/k$ for $k$-fold CV, which never approaches 1.",2010-09-07 16:46:57.0,795.0,
2872,2431,1,"Could you, with the permission of the author, post the error briefly as a comment here? That will be useful to everyone.",2010-09-07 17:04:10.0,,user28
2873,2423,0,see also question http://stats.stackexchange.com/questions/414/introduction-to-statistics-for-mathematicians,2010-09-07 17:29:02.0,223.0,
2874,2228,0,"Still doesn't work. During EZ instalation Error in dyn.load(file, DLLpath = DLLpath, ...) : 
  unable to load shared library '/Library/Frameworks/R.framework/Versions/2.11/Resources/library/lme4/libs/i386/lme4.so':
  dlopen(/Library/Frameworks/R.framework/Versions/2.11/Resources/library/lme4/libs/i386/lme4.so, 6): Library not loaded: /usr/local/lib/libgfortran.2.dylib
  Referenced from: /Library/Frameworks/R.framework/Versions/2.11/Resources/library/lme4/libs/i386/lme4.so
  Reason: image not found
Error : package 'lme4' could not be loaded",2010-09-07 18:05:56.0,1084.0,
2876,2228,0,"It may be related to the error Attaching package: 'Matrix'

The following object(s) are masked from 'package:base':

    det",2010-09-07 18:41:12.0,1084.0,
2877,2442,0,"Just for fun, I did a brute-force calculation in Mathematica.  Its answer is a ratio of a 21,574 digit integer to a 21,571 digit integer; as a decimal it's comfortably close to 900/e as expected (but, since we're asked not to post a solution, I'll not give any more details).",2010-09-07 18:43:28.0,919.0,
2878,1136,2,"You can not eliminate human judgment -- nor would you want to. That said, making a model explicit does help reveal assumptions and open them up for discussion.",2010-09-07 19:12:07.0,660.0,
2879,2369,0,"I agree with the first point, it is a matter of ""horses for courses"", but examples which show where the boundaries lie are interesting and provide insight into the ""courses"" best suited to each ""horse"".  However, the examples must be fair, so that the criterion for success matches the question as posed (Jaynes is perhaps not completely immune to that criticism, which I will address in my answer which I will post later).",2010-09-07 19:47:20.0,887.0,
2880,2369,0,"The confidence interval only provides a bound on the *expected* number of false positives, it is not possible to put an absolute bound on the number of false positives for a particular sample (neglecting a trivial interval of [0,1]).  A Bayesian would determine an interval such that the probability of of more than five beheadings is less than some threshold value (e.g. 10^-6).  This seems at least as useful as a bound on the expected number of beheadings and has the advantage of being a (probabilistic) bound on what happens to the actual sample of courtiers.  I'd say this one was a clear draw.",2010-09-07 19:57:02.0,887.0,
2881,2447,0,"That's a neat idea: thanks for the reference.  However, the residuals to that particular fit look pretty bad, which makes me wonder how well it identifies potential changepoints.",2010-09-07 20:40:12.0,919.0,
2882,2447,0,whuber: i do not know how much you are familiar with the theory of quantile regression. These lines have a major advantage over splines: they do not assume any error distribution (i.e. they do not assume the residuals to be Gaussian).,2010-09-07 20:58:39.0,603.0,
2883,2420,0,@Josh - no problem - I understood your distinction - I think you are right about dates being a quantity and in fact that's what I was struggling to see and hence the original question.,2010-09-07 21:33:26.0,414.0,
2884,2420,0,"Yes, I have single date (last access time) per category",2010-09-07 21:35:16.0,414.0,
2885,2431,0,"Theorem 14.6, Sigma is singular so density isn't defined, this is fixed by instead considering distribution of $\\hat{p_i}$ which is $\\hat{p}$ with $i$'th component dropped (covariance matrix will be $\\Sigma$ with i'th row/column removed)",2010-09-07 21:50:26.0,511.0,
2886,2423,0,"Joris: well, internet is already pretty good for explanations, my motivation is having something to check against when I need a statistics related formula. For instance, recently I needed a formula for P(X=x|v'x=a) where X is multivariate gaussian and v is some vector, and none of my statistics books had it",2010-09-07 21:57:05.0,511.0,
2887,2412,3,"The problem is that outliers will make some 'bad' (i.e. contaminated) tree look better than good (uncontaminated) ones. This is called, masking effect and is easy to replicate with simulated data. The problem comes about because the criterion you use to evaluate trees is not in itself robust to outliers. I know i'm starting to sound like a fundamentalist mullah, but unless each and every tool you use is made robust, your procedure can be shown to be sensitive (at one level or another) to outliers (and hence not robust).",2010-09-07 22:20:44.0,603.0,
2888,2434,0,"@Andy W: Actually, the hard part was getting all those probabilities.  Instead of adding them at each cell, multiply their complements: that's the probability that the cell will be empty.  The sum of these values over all cells gives the answer.  Glen_b's approach beats simulation by seven or eight orders of magnitude ;-).",2010-09-07 22:36:18.0,919.0,
2889,2446,1,I don't think this is quite the same thing as ecological regression - that's about trying to infer something about the within-group slopes when you only have the group means. Here you have the within-group slopes.,2010-09-07 22:42:22.0,449.0,
2890,2056,2,"You also might like to see the recently-published: Ren, Shiquan , Lai, Hong , Tong, Wenjing , Aminzadeh, Mostafa , Hou, Xuezhang and Lai, Shenghan(2010) 'Nonparametric bootstrapping for hierarchical data', Journal of Applied Statistics, 37: 9, 1487 — 1498",2010-09-07 23:28:17.0,187.0,
2891,2362,2,"The main problem is that it is really hard to compare two models (model=learning method+feature selection method), but for simplicity you can just assume something (like I'll use RF and select top 10 attributes) and admit that you know that this may be suboptimal, but you agree on that while you are for instance satisfied with the accuracy. In that case your only problem is to remove bias of attribute selection. tbc.",2010-09-07 23:53:19.0,88.0,
2892,2362,2,"So, I would do a simple bagging: you create 10 (or 30 if you have a good computer) random subsamples of objects (let's say by random picking with replacement), train RF on each, get it's importance and return a rank of each attribute averaged over all repetitions (best attribute gets rank 1, second best 2 and so on; it can be averaged so the attribute that was 12 times 1st and 18 times 2nd have rank of 1.6), finally select 10 with best ranks and call them your markers. Then use a CV (LOO, 10-fold or preferably random sampling) to obtain an error approximation of RF using your markers. tbc.",2010-09-08 00:00:02.0,88.0,
2893,2362,2,"Report the ranks (hopefully they should be pretty near 1,2,3...), CV error with its deviation (just count standard deviation of results of each CV round) and OOB error (probably will be identical to CV error). DISCLAIMER: This is not a method for selecting optimal number of attributes -- you need RFE and nested CV to do that. DISCLAIMER2: I haven't worked with such a data, so I don't guarantee that your referees will be happy with it (though I believe they should).",2010-09-08 00:11:27.0,88.0,
2894,2456,0,"Splendid, thanks.",2010-09-08 00:45:41.0,88.0,
2895,2446,0,onestep:> Good point                                            Adam>: do you have access to the underlying data ? Or at least to the standard errors arround each slope  ?,2010-09-08 02:08:40.0,603.0,
2896,2463,1,excellent: +1 for the details,2010-09-08 02:30:48.0,603.0,
2897,2445,0,Thanks. I'll have a read.,2010-09-08 02:35:42.0,183.0,
2898,2447,0,@kwak This looks interesting. Not assuming a normal error distribution would be useful for one of my applications.,2010-09-08 02:36:46.0,183.0,
2899,2445,3,I found the bcp R package http://www.jstatsoft.org/v23/i03/paper which implements the Barry & Hartigan algorithm,2010-09-08 02:39:44.0,183.0,
2900,2447,0,"Indeed, what you get out of this estimation are actual conditionnal quantiles: in a nutshell, these are to splines/LOESS-regressions what boxplots are to the couple  (mean, s.d.): a much richer view of your data. They also retain there validity in non gaussian context (such as assymetric errors,...).",2010-09-08 02:51:52.0,603.0,
2901,4,1,"Are you interested in whether the distributions are of a different form (e.g., normal, poisson, etc.) or whether parameters are different (e.g., mean or sd of a normal distribution) or both?",2010-09-08 03:16:45.0,183.0,
2902,2448,0,"Hi Kwak, The Huettmann paper is interesting but I have a few questions. 1) Under Assessing the link Functions, what is meant by the ""mean coefficients derived from the 1000 comparisons"" (p44)? 2) What do the ""Predictions"" in Table2 refer to? Predicted probability or predicted number of nesting sites, or what? 3) The marbled murrelet data are claimed to be public domain. Do you know the source? Thanks.",2010-09-08 03:44:33.0,521.0,
2903,2454,0,Thanks for that. A customised solution might work best.,2010-09-08 06:03:08.0,183.0,
2904,2419,0,the only reason being that I have used R only very few times a year or so ago and python I'm using every day...,2010-09-08 06:03:21.0,961.0,
2905,2228,0,Are you using Linux or Mac? I can recall glimpsing over some problems installing the Mac version of lme4. There's quite a few posts on R-sig-mixed-models about this.,2010-09-08 06:25:25.0,966.0,
2906,2443,0,Thanks for this refined illustration.,2010-09-08 07:19:14.0,930.0,
2907,2472,0,awesome. thanks.,2010-09-08 07:45:28.0,183.0,
2909,2471,0,+1 I wanted to post something similar.,2010-09-08 08:22:34.0,88.0,
2910,2193,0,"Yes, so you just look at the top 20 you rely on, and find the patterns of similarity for the joint. The joint combinations giving you the prediction contain the information of the variable associations.",2010-09-08 10:22:18.0,1098.0,
2911,2193,0,Oh yeah and you just do correlation analysis like a,2010-09-08 10:23:08.0,1098.0,
2912,2146,0,@Joris Meys- exactly,2010-09-08 10:25:40.0,1098.0,
2914,2478,0,"Thanks. Does LDA require you to know the topics upfront? In our case we do not know to which topic each Document belongs and we will be using the similarity measure to perform clustering (EM- G-Means, or GAAC)",2010-09-08 10:59:12.0,1212.0,
2915,2478,0,"@ebony1 Nice reference to LSA, I made a similar answer some time ago at http://stats.stackexchange.com/questions/369/working-through-a-clustering-problem/2196#2196",2010-09-08 11:00:17.0,930.0,
2916,135,0,"In SAS, KS test is available in `proc npar1way`. In R, in addition to `ks.test()`, there is the `nortest` package which provides several other adjustment tests.",2010-09-08 11:15:20.0,930.0,
2917,2367,0,I probably shouldn't have said k-fold. The BIC is equivalent to leave-v-out CV where v=n[1-1/(log(n)-1)] and so it satisfies the Shao consistency criterion. I've updated my answer accordingly.,2010-09-08 11:23:26.0,159.0,
2918,2478,1,"@Joel: No, LDA doesn't assume that you know the topics for each document beforehand. BTW, just to be clear, LDA represents each document as a mixture of topics, not by just a single topic. So each topic will contribute to some fraction in the documents (and the individual fractions will sum to 1). Basically, LDA assumes that each word in the document is generated by some topic.",2010-09-08 11:37:44.0,881.0,
2920,2367,0,@mbq. I've now added the linear caveat.,2010-09-08 11:42:56.0,159.0,
2921,2482,3,Highlight your code and click on the button that looks like binary numbers...,2010-09-08 11:52:22.0,5.0,
2922,2478,0,"@ebony - thanks! At risk of a rephrasing the question and repeating myself, does LDA require you to know the number of discreet topics?",2010-09-08 12:02:01.0,1212.0,
2927,2228,0,"Looks like you're on a mac, in which case you might try building lme4 from source ( install.packages('lme4',type='source') ), but to do that you'll first need to install gfortran: http://cran.r-project.org/bin/macosx/tools/",2010-09-08 12:26:01.0,364.0,
2930,2209,4,And it is available for free download at the authors page: http://www.inference.phy.cam.ac.uk/mackay/itila/book.html,2010-09-08 12:37:47.0,247.0,
2931,2483,0,"This is not an answer, but the question reminds me of a presentation by Max Khesin on ""graphical models"" that he gave at a recent NY meetup: http://www.slideshare.net/xamdam/graphical-models-4dummies",2010-09-08 12:47:15.0,5.0,
2932,2367,0,@Rob Thanks. [more characters],2010-09-08 12:49:21.0,88.0,
2933,26,4,"Agreed, it's a valid question. It's also well stated as it asks for example usage and calculation. Surely the purpose of the site is to create a repository for ALL questions statistical.",2010-09-08 13:03:15.0,1212.0,
2934,2481,0,Kolmogorov test is for distributions hence object lying in infinite dimensional space... not obvious to get asymptotic power=1. t-test is about testing the mean of a real valued random variable?,2010-09-08 13:10:01.0,223.0,
2935,2481,0,"I haven't claimed that it is obvious, I just said that I have found this statement. So the natural question for me would be: what about other statistical tests - no matter what they are designed for.",2010-09-08 13:33:17.0,1215.0,
2936,2486,0,"Thank you, Joris. So ""my definition"" was just partially correct. ;) Do you probably know the asyptotic power of Friedman test (non-parametric repeated measures ANOVA)?",2010-09-08 13:43:04.0,1215.0,
2941,2459,0,"That's a nice observation.  However, I found it more bother than it's worth to exploit this explicitly.  Most of the programming amounts to setting up the transition matrix.  Once you do that, just square it and work with that.  By using sparse matrices, removing half the zeros doesn't save any time anyway.",2010-09-08 14:17:25.0,919.0,
2942,321,0,@kwak: Could you please delete your comment and post it as an answer so it can be accepted?,2010-09-08 14:18:02.0,220.0,
2945,2447,0,"@kwak: The residuals are heavily correlated with the x-coordinate.  For example, there are long runs of negative or small positive residuals.  Whether they have a Gaussian distribution or not, then, is immaterial (as well as irrelevant in any exploratory analysis): this correlation shows that the fit is poor.",2010-09-08 14:30:58.0,919.0,
2946,2445,0,@Jeromy: Thank you for the R package and for inserting the links to the references.,2010-09-08 14:32:07.0,919.0,
2951,2478,0,Yes. But there are variants of LDA (HDP-LDA) that do not require specifying the number of topics. See this paper: http://www.cse.buffalo.edu/faculty/mbeal/papers/hdp.pdf,2010-09-08 14:55:48.0,881.0,
2952,2473,0,Thanks. Nice idea cross classification.,2010-09-08 15:17:22.0,1154.0,
2953,2473,0,I'll come back with results.,2010-09-08 15:18:49.0,1154.0,
2954,2474,0,Thanks for your answer and reference. I will check differents models and do cross validation. Bayesian is a good idea.,2010-09-08 15:19:33.0,1154.0,
2956,2465,0,"Its a within subject design. With 4 slope values for each subject. Each slope value was calculated, in a different condition, over 5 independent variable values - and there are actually 4 measurements for each of these iv values.",2010-09-08 15:44:02.0,1084.0,
2957,2446,0,kwak:> I have the data! Not sure what is the best way to share data here.,2010-09-08 15:46:51.0,1084.0,
2959,2441,0,"I like the idea of creating a chain of indicators, but in your description it sounds like you still have to maintain full counts of fleas in all cells and run that, too, as a Markov chain.  So what is accomplished with the additional complexity of tracking the $n_{ij}$?",2010-09-08 15:53:12.0,919.0,
2960,2394,2,@whuber: actually the range of possible values of skew and _excess kurtosis_ allows distributions with some skew and zero excess kurtosis.,2010-09-08 16:49:40.0,795.0,
2961,2272,0,"Whew... great answers, all around.  The marked best answer is the one that worked best, on its own, for me, but the collection of answers is most helpful of all.  Thanks, everyone.",2010-09-08 16:59:24.0,71.0,
2962,2367,0,"ah, OK. much more sensible now.",2010-09-08 17:27:11.0,795.0,
2963,2355,0,"Beyond proofs, I'm wondering if there have been simulation studies of any of the five cases I list, for example.",2010-09-08 17:27:57.0,795.0,
2964,2459,0,"@whuber: I suspect the point of these problems is to learn problem solving techniques, rather than consume a lot of computational cycles. Symmetry, parity, etc, are classic techniques from Larson's book on problem solving.",2010-09-08 17:31:12.0,795.0,
2965,2447,0,"Whuber: the line you see on the plot is an estimate of the 90% conditionnal quantile (i.e. the function $f(x):p(y_i<f(x_i))\\approx 0.9\\; forall i$. This is true assymptotically for $f(x)$, not for each of the piecewise linear function taken individually.",2010-09-08 17:39:55.0,603.0,
2966,2492,10,For reference: I don't think that this needed to be community wiki.,2010-09-08 17:57:46.0,5.0,
2967,2355,0,Wanna make some?,2010-09-08 17:58:26.0,88.0,
2968,2492,1,I wasn't sure there was a 'right answer'...,2010-09-08 18:01:40.0,795.0,
2969,2492,0,See http://meta.stats.stackexchange.com/questions/290/what-is-community-wiki,2010-09-08 18:03:57.0,5.0,
2970,2492,2,"In a certain sense, this is true of all test of a finite number of parameters. With $k$ fixed (the number of parameters on which the test is caried) and $n$ growthing without bounds, any difference between the two groups (no matter how small) will always break the null at some point. Actually, this is an argument in favor of bayesian tests.",2010-09-08 18:07:28.0,603.0,
2971,2473,0,Do you know of any papers comparing scores computed based on PLS-DA or PLS-1/2 with regression scores coming from a factor model (which account for measurement errors)?,2010-09-08 18:42:31.0,930.0,
2972,2492,0,"For me, it is not a valid argument. Anyway, before giving any answer you need to formalize things a little bit. You may be wrong and you may not be but now what you have is nothing more than an intuition: for me the sentence ""In the era of cheap memory, big data, and fast processors, normality tests should always reject the null of normal "" needs clarifications :) I think that if you try giving more formal precision the answer will be simple.",2010-09-08 19:01:08.0,223.0,
2973,2441,0,"@whuber No, you need not maintain a flea position as a markov chain. Think of what I am proposing as a random walk for a cell. A cell initially is at position '1' from where it can go to 0, 1, 2, 3, 4, or 5. The probability of state transition depend on the states of the adjacent cells. Thus, the proposed chain is a on a re-defined state space (that of cell counts for each cell) rather than on the flea position itself. Does that make sense?",2010-09-08 19:41:35.0,,user28
2974,2483,0,"Insights into the warning process would help in precise answers: Are your events of interest (i.e., weather in your example) continuous variables which are discretized or are they truly discrete? How does the warning system work? In other words, what triggers a warning?",2010-09-08 19:58:44.0,,user28
2975,2493,1,"Do you have a the raw data from the GPS devices?  If so, you might consider sharing it here and we might be able to give some more explicit answers.",2010-09-08 19:59:54.0,5.0,
2977,2446,0,"Ok Adam, i don't need the data, but if you have access to all 150 data points, then, it is a completely different problem statistically (you should metion it in the question). The good news is this becomes much simpler to solve. I will respond in the answer zone.",2010-09-08 21:17:19.0,603.0,
2978,2446,0,"I erased my previous comment, as i had missunderstood the nature of your question.",2010-09-08 21:24:18.0,603.0,
2979,1433,12,"In statistics, bias is not the same as error. Error is purely random, bias is not. You have bias when you know that the expected value of your estimate is not equal to the true value.",2010-09-08 21:30:57.0,1124.0,
2980,2251,0,"I'd love to answer too, but I'm afraid there's nothing left to add to what Kingsford said.",2010-09-08 21:43:55.0,1124.0,
2981,2441,0,"It makes sense, but it seems like a step backwards, because isn't the number of states now much larger? In one model there are 900 states--the position of a single flea--and no more than four transitions out of each one. The computation only needs to be done for a single flea because they all move independently.  In yours it seems a state is described by a cell's occupancy along with the occupancy of its up to four neighbors. That would be an extremely large number of states and also a very large number of transitions among the states. I must be misunderstanding what your new state space is.",2010-09-08 21:50:10.0,919.0,
2982,2459,0,"That's a good point.  Ultimately some judgment is needed.  Project Euler appears to emphasize tradeoffs between mathematical insight and computational efficiency.  Glen_b mentioned symmetries that are worth exploiting first because there is more to be gained from them.  Moreover, by using sparse matrix arithmetic you will achieve the twofold gain automatically (whether you're aware of the parity or not!).",2010-09-08 21:56:59.0,919.0,
2983,2498,0,this is great! I'm slapping myself for not doing the experiments myself...,2010-09-08 22:35:17.0,795.0,
2985,2498,16,"On a side note, the central limit theorem makes the formal normality check unnecessary in many cases when n is large.",2010-09-08 23:19:31.0,1124.0,
2986,2497,0,"Firstly, would you be able to point me at the GIS forums where you've seen this dicussed? When it comes to sub-sampling I take it you mean sampling from the existing (sampled) data. By extrapolating a fit what kind of fit are you talking about? Lastly, I don't think the 2D kernel smooth approach you describe would work well for me as I would like the process to be automated. Is it not possible to directly combine both datasets and then kernel smooth them into one line?",2010-09-09 01:21:20.0,1227.0,
2987,2434,0,"@whuber , Thanks for the explanation. Indeed getting those probabilities in under a minute would be challenging. It's a fun puzzle and thanks for your input.",2010-09-09 01:33:24.0,1036.0,
2988,2493,1,+1 great question.,2010-09-09 01:42:55.0,352.0,
2989,2482,0,Thanks for the detailed answer Bruce!,2010-09-09 03:25:04.0,1210.0,
2993,515,1,I think your first example refers to Bertrand's paradox. Very nice illustration of the different ways to define a probabilistic space!,2010-09-09 07:48:50.0,930.0,
2994,2473,1,"Sorry for delay. You might find useful the paper ""Importance of data structure in comparing two dimension reduction methods for classification of microarray gene expression data"" from Caroline Truntzer (BMC Bioinformatics). Actually, it is not about factor model, but PCA model.",2010-09-09 08:17:17.0,609.0,
2995,2499,2,"Not to be rude, but isn't this a question that is already answered ad nausea on Wikipedia and the likes? Google gave me the answer within 15 seconds...",2010-09-09 08:32:42.0,1124.0,
2996,694,8,That's definitely my favorite. I always have to stop on this and laugh when scrolling over this page. It's just so bad!!,2010-09-09 09:02:48.0,442.0,
2997,2498,15,"yes, the real question is not whether the data are actually distributed normally but are they sufficiently normal for the underlying assumption of normality to be reasonable for the practical purpose of the analysis, and I would have thought the CLT based argument is normally [sic] sufficient for that.",2010-09-09 09:37:22.0,887.0,
2998,2509,0,I should point out that I have left out any scale factors (e.g. $\\sqrt{N}$ is common in these problems) but they can be added without trouble. What you choose for a scale factor will depend on the distribution of $g'(p_0)$.,2010-09-09 12:31:31.0,352.0,
2999,2510,0,"It would help immensely to clarify what you mean by ""globally"" and ""approximate""!  For example, h itself is a fine approximation to g according to many measures.  If you require that g and its approximator be relatively close pointwise for all of R^2, then you're not going to get any better than that.",2010-09-09 12:54:22.0,919.0,
3000,2497,0,"Check out the forums (both the old and new) at esri.com.  By ""fit"" I mean any reasonable (nonlinear) fit to the points.  A direct combination of two paths is highly problematic.  It would be tempting to just intersperse the union of all GPS readings in the two tracks, but there is no obvious unique way of connecting them up or of averaging them out somehow.  That's the basic problem to be solved here and I'm not aware that anybody actually has a general-purpose solution.",2010-09-09 13:03:13.0,919.0,
3002,2456,0,"Ok, indeed I rewrote it completely to have different scales on both sides, but it seems there is nothing in R that can do it natively.",2010-09-09 13:24:54.0,88.0,
3003,2456,0,"I'm sure that you could do it with ggplot2, but it would require a little effort.",2010-09-09 13:44:54.0,5.0,
3004,1646,0,"I'm having some trouble understanding the dependent measure in your question.  How is team effectiveness being measured?  When you say ""the measure of team effectiveness"" it sounds as though you are talking about your measure of ""team functioning"".  If so, isn't the dependent measure is necessarily criteria contaminated?",2010-09-09 14:43:33.0,196.0,
3005,2513,0,"Please may I suggest a new tag along the lines of ""artefacts""? (Or ""artifacts"" if you happen to be of the American persuasion).",2010-09-09 15:06:00.0,266.0,
3007,2507,0,"I presume you are referring to a situation where one first does a normality test, and then uses the result of that test to decide which test to perform next.",2010-09-09 16:07:23.0,25.0,
3008,2511,1,"this is a very good point. however, don't most hypothesis tests have arbitrarily low power against some alternative? _e.g._ a test for zero mean will have very low power when given a sample from a population with mean $\\epsilon$ for $0 \\lt |\\epsilon|$ small.  I am still left wondering whether such a test can be sanely constructed at all, much less whether it has low power in some cases.",2010-09-09 16:11:25.0,795.0,
3009,2511,1,"also, 'polluted' distributions like the one you cite always seemed to me to be at odds with the idea of being 'identically distributed'. Perhaps you would agree. It seems that saying samples are drawn i.i.d. from some distribution _without stating the distribution_ is meaningless (well, the 'independently' part of i.i.d. is meaningful).",2010-09-09 16:14:21.0,795.0,
3010,2506,0,This is a nice way of putting it. I am wondering if you can generalize this description to also apply to the kernel of 'kernel density estimation'.,2010-09-09 16:18:52.0,795.0,
3012,2510,0,I may be missing something but I agree with @whuber. I do not see why you need an approximation g(x) when you have h(x)^+ and in what sense will any other function g(x) be better than h(x)^+?,2010-09-09 16:36:47.0,,user28
3017,2506,0,"In a way, yes. One way to understand kernel density estimation is that you approximate the density of a point from some distribution as a weighted average of its similarities with a set of points from the distribution. So the notion of similarity does play a role here as well.",2010-09-09 17:14:44.0,881.0,
3020,2511,1,"(1) You're right about the low power, but the problem here (it seems to me) is that there is no gradual step from ""finite"" to ""infinite"": the problem seems not to have a natural scale to tell us what constitutes a ""small"" departure from the null compared to a ""large"" departure. (2) The distributional form is independent of considerations of iid.  I don't mean that, say, 1% of the data will come from a Cauchy and 99% from a Normal.  I mean that 100% of the data come from a distribution that is almost normal but has Cauchy tails. In this sense the data can be iid for a contaminated distribution.",2010-09-09 17:39:07.0,919.0,
3022,2510,0,"In light of your clarification, are you asking for a polynomial approximation to the entire function g or to the data (x_i, g(x_i))?  (The solutions to those questions are somewhat different...)",2010-09-09 17:42:00.0,919.0,
3024,2499,8,"I absolutely hate wikipedia answers for stats. There are rambling, symbolic messes. I am looking for a gem of an answer that can explain the answer in plain English, as I believe that that shows a deeper level of understanding than a math equation. There are many popular ""plain English"" questions on here, and for good reason.",2010-09-09 18:04:48.0,74.0,
3025,2510,0,"@ Sri. Indeed i do have h(x) (the alpha are known) and therefore h(x)^+. But h(x)^+ is not a polynome. For reason pertaining to computational issues and ease of derivation of some properties, i need to approximate this function by a polynomial. Since h(x) [and therefore h(x)^+] are convex functions, i though, maybe, there should be a way to approximate them by some polynome, over the entire domain (R^2).",2010-09-09 18:43:51.0,603.0,
3027,2510,0,@ Whuber: Im really looking for an approximation to the entire function g(x_i). The criterion is such that we have somethin practical to work with (i.e. the criterion is essentially L2 distance between g and \\hat{g}). See also my response to Sri's comment above.,2010-09-09 18:45:31.0,603.0,
3029,2519,2,"+1: indeed, all three answers here are logically consistent with one another.",2010-09-09 19:03:23.0,603.0,
3030,2516,6,+1: this question usually exposes some interesting point of views.,2010-09-09 19:05:58.0,603.0,
3031,2520,0,"This will get the ball rolling. $g(x)$ has indeed as simple analytical form, but not a polynomial one. It will make my problem simpler if i could re-express $g(x)$ as a (or even several) polynome(s), even very complicated one(s). Wiggly is okay, so long as i can make it more precise by adding more terms.",2010-09-09 19:12:43.0,603.0,
3032,2518,0,"That's weird... intuitively, this seems to contradict the Law of Large Numbers.",2010-09-09 19:12:50.0,666.0,
3033,2518,0,Carlos:> can you be more specific ?,2010-09-09 19:25:45.0,603.0,
3034,2521,1,"Fantastic. Although your contribution answers my problem, i would like to use it to ask the following question: assuming now that we only consider a compact $B\\in\\mathbb{R}^2$. What is not the non-trivial answer?",2010-09-09 19:31:59.0,603.0,
3035,2441,0,@whuber I see the problem now. I think my proposal amounts to defining a markov chain on the state of the grid. Define the state of the grid as $\\{n_{ij}\\}$. Then we have a markov chain on the grid's state as the cell counts of all the cells at any one time period are dependent on the cell counts of all the cells in the previous time period. I agree with you this is an impractical proposal as the state space increases dramatically.,2010-09-09 19:45:14.0,,user28
3036,2521,2,@kwak: Least squares!  The standard example of a compact subset is a finite set of points and the L^2 norm sums the squares of residuals.  With measurable compact subsets you need to *integrate* the squares of residuals.  There's also a classical L-infinity (sup norm) theory of approximating functions by polynomials (Weierstrass' Theorem) or even with sets of arbitrary real powers (Muntz-Szazs Theorem).  Rudin's textbook on real and complex analysis has an accessible account (I'm looking at a 1974 printing).  I believe these extend from R^1 to higher dimensions.,2010-09-09 19:49:03.0,919.0,
3037,2521,0,"@kwak (continued): The L^2 theory constructs orthonormal sets of polynomial functions (such as Legendre Polynomials and Chebyshev Polynomials).  You obtain the coefficients of good approximations simply by taking the inner product with the basis elements, exactly as in finite-dimensional linear algebra.  Different bases have different approximation properties.  Arbitrary (measurable) subsets B likely require variants of these systems adapted to their specific geometry.",2010-09-09 19:51:26.0,919.0,
3038,2521,0,"Ok, thanks very much. In some odd way this solves my problem (limits what i can do).",2010-09-09 19:53:31.0,603.0,
3039,2518,0,"The LLN basically states that the larger your sample is, the better it represents the ""real"" probability distribution. In your example, the more house numbers I examine, the closer to 50% the number of odd-numbered houses will be. So it sounds weird that it becomes easier for you to break through the band, since it shrinks in proportion to the square root of $n$. (Am I making sense here?)",2010-09-09 19:54:59.0,666.0,
3040,2518,0,"You do make sense in a theoretical framework. This is addressed by Andre's comment below. For matters of measurement errors, incomplete models, ect... you will nonetheless observe differences (even minute ones) in measured income between the two groups that will not shrink to 0, regardless of sample size.",2010-09-09 20:01:01.0,603.0,
3044,2492,1,"The thread at ""Are large datasets inappropriate for hypothesis testing"" discusses a generalization of this question.  (http://stats.stackexchange.com/questions/2516/are-large-data-sets-inappropriate-for-hypothesis-testing )",2010-09-09 20:17:48.0,919.0,
3046,2473,0,"Thx, I will look at that paper.",2010-09-09 20:52:41.0,930.0,
3047,1537,0,"Because (n+k)/2 is not necessarily integral, consider rewriting the probability as Pr(S_n = 2^{2k-n}) = 2^-n Comb(n,k), 0 <= k <= n.  (There's also something fishy about your equating 2^k x = k.)",2010-09-09 21:08:15.0,919.0,
3048,2355,2,"I do; I'm going to have to learn a lot more R, though, to share the results here, though.",2010-09-09 21:16:44.0,795.0,
3052,56,6,"It should be pointed out that, from the frequentists point of view, there is no reason that you can't incorporate the prior knowledge *into* the model. In this sense, the frequentist view is simpler, you only have a model and some data. There is no need to separate the prior information from the model.",2010-09-09 22:29:26.0,352.0,
3055,471,4,"Back when this was a \\$3.95 and then a \\$4.95 paperback, I bought copies by the dozen and gave them away to friends, clients, and anyone else who might be interested.",2010-09-09 22:54:01.0,919.0,
3056,370,7,"Both are worth a periodic re-reading, maybe once a decade, just to refresh the ideas.  Concerning Tukey: it's great to sit down just with pencil and paper once in a while and do a deep analysis of an interesting dataset.",2010-09-09 22:55:49.0,919.0,
3057,2518,0,@Carlos -- but convergence does not mean equality; this is guaranteed only for unreachable limit of infinity. So there is no contradiction ;-),2010-09-09 23:32:56.0,88.0,
3058,93,0,when you say very small how small are you talking about? Unfortunately is is common to see people using sophisticated methods applied to a handful of data. If the problem is not having a good study to start with one should look at alternatives and make a case for a better study or data collection,2010-09-09 23:58:29.0,10229.0,
3059,2526,0,"This demonstration seems to assume the expected amount of money is zero, but it's not.  Furthermore, you appear to assume that the net of doublings minus halvings must be nonnegative, which is also incorrect.  Collectively these errors yield the correct limit, but that's an accident.",2010-09-10 02:15:23.0,919.0,
3060,2447,0,"@Kwak: Thank you for the clarification.  But if the estimate is true only asymptotically and globally, why should we expect it to perform at all well in identifying changepoints?  Is there some additional property the broken line regression has, of which I am unaware, that provides some comfort in this respect?",2010-09-10 02:36:03.0,919.0,
3061,720,0,"I am confused by the remark about changing units, because by definition AIC is unitless (it's an adjusted maximum log likelihood).  A change in the data units would not change the maximized likelihood at all and therefore would not change the AIC either.  (Regardless, your recommendation to pay attention only to the difference is not in question.)",2010-09-10 02:41:22.0,919.0,
3062,1646,0,"@drknexus In typical team studies, each team member is asked about for example their level of agreement with statements like ""my team is performing well"", ""team members get along well with each other"", ""my team handles conflict constructively""... The question is thus the degree to which such beliefs are shared by team members.",2010-09-10 02:52:57.0,183.0,
3063,1537,0,should your variable `money` be `x`?,2010-09-10 03:15:35.0,183.0,
3064,2034,0,"The situation I mention above is standard practice in meta analytic studies of correlations. The ""combining apples and oranges"" critique is of course commonly raised regarding meta analysis. However, in defence of the practice, there is often substantial theoretical and empirical evidence that different measurements of the same construct (e.g., different intelligence measures) are highly correlated.",2010-09-10 03:31:32.0,183.0,
3065,2034,0,"@Jeromy: Let's see whether I have understood. X and Y could be different measures of ""intelligence."" In study A, 100 people randomly drawn from a special school are tested. They all have comparable (low) levels of intelligence and r = 0.50. In study B, 50 gifted students are tested; they all have comparable (high) levels of intelligence and r = 0.45. If the raw data were to be combined, r would be almost 1.00. In study C, 50 more low-level people are studied and r = 0.45. The r from the combination of studies A and C would lie close to 0.45 or 0.50. How would a meta-analysis handle this?",2010-09-10 04:02:06.0,919.0,
3066,2511,0,"I don't follow (2). It seems you are flipping a heavily biased coin, and usually drawing a normal random variate, and very rarely a Cauchy. Since you specified this distribution, the words 'identically distributed' are meaningful. However, to say that some observations are identically distributed _without_ giving the distribution seems meaningless, because that distribution could be: first uniformly select an integer $i <= 1000$, then draw a random variable from distribution $i$ out of 1000 different distributions. my Q: how could one have a sample which is *not* 'identically distributed'?",2010-09-10 04:36:19.0,795.0,
3068,2034,0,"typically X would be one variable (e.g., intelligence) and Y would be a different variable (e.g., average grade at university). Each study might use a slightly different measure of intelligence and a slightly different measure of average grade. Some meta analytic studies apply corrections for range restriction,  reliability of measurement, and other factors. Of course in the ideal situation the same measure would be used in all studies, and the raw data would be available, but meta analysis aims to synthesise results in the published literature as best as possible.",2010-09-10 07:14:41.0,183.0,
3069,2517,1,I do not know if Mark and Shern had your view in mind but just to re-phrase your point- if the model for the data under the null is 'wrong' then you will reject the null hypothesis for a large enough data.,2010-09-10 08:40:53.0,,user28
3070,1537,0,"@Jeromy: Yes, I've changed it. @whuber: You're correct, I've tried to make the probability a bit clearer. BTW, integral->integer in your comment.",2010-09-10 08:43:23.0,8.0,
3071,2513,0,"Thank you, mbq.",2010-09-10 09:05:07.0,266.0,
3072,2513,0,Could you add the calculated T as formula using latex?,2010-09-10 09:10:56.0,1124.0,
3073,2507,2,"I refer to the general utility of normality tests when used as  method to determine whether or not it is appropriate to use a certain method. If you apply them in these cases, it is, in terms of probability of committing an alpha error, better to perform a more robust test to avoid the alpha error accumulation.",2010-09-10 10:42:59.0,442.0,
3074,2447,0,@Whuber:> i appended my answer with some mathematical details and a much needed scientific citation. Best.,2010-09-10 11:24:13.0,603.0,
3075,2537,0,"Are you working with R or Stata? Are the items locally independant (i.e. the probability of choosing any one item does not depend on response to other items)? Could you confirm that what you call ""conditional fractions"" are just the % of males/females that choosed a given item, adjusting for other effects in your model?",2010-09-10 12:56:18.0,930.0,
3077,2537,0,@chl : that's exactly what I want to achieve.,2010-09-10 13:37:41.0,1124.0,
3078,2500,0,"Very nice! I'm going to use your example with the circle to explain kernel methods, as it is the best visualization I met up til now. Thanks!",2010-09-10 13:47:06.0,1124.0,
3079,2447,0,"@Kwak: Thank you!  I see how the case \\tau = 0.5 might be effective.  (BTW, you probably meant to write \\tau instead of \\alpha in the formula.)  It would be interesting to compare its performance to methods specifically designed to identify changepoints.",2010-09-10 14:02:16.0,919.0,
3080,2483,0,"I think it would be ideal if you tell us your real question. Then we can give a lot better answers. The changed example also needs more clarification which will be specific to this example and will probably lead us away from your real question. I know that revealing your real question comes with the potential cost of somebody ""stealing"" it. But I think that is the price for getting good answers here.",2010-09-10 14:24:10.0,442.0,
3082,2538,0,"@Joris Maybe I will have to rework my response because it seems to me I'm not entirely answering your question; indeed, I just talked about estimating main effects of your covariates, but we can also imagine model with random slope or random intercept, or a combination thereof to derive proper estimates for each gender.",2010-09-10 15:01:07.0,930.0,
3083,2538,0,"this is already more than I hoped for. I've seen a comparison between gee and glmm before, and the output differed sometimes quite substantially, hence my choice for gee. I'll check out the gee package. I suppose I could use the same approach more or less for the interaction terms, as I want the odds per item and per gender (hence the interaction terms). Or am I wrong there?",2010-09-10 15:25:13.0,1124.0,
3084,2538,0,"@Joris Ok, I've somewhat UPDATED my response. Under the GLMM, you can fit separate intercept and/or slope for both gender, which is not exactly the same as an interaction term (which as formulated there would also be viewed as a fixed effect). Whatever the solution you choose, you need to account for a different probability of endorsing any one of your 5 items depending on the gender of the respondent. More generally, you will useful references by just looking at *Differential Item Functioning* (DIF) on the web. I just remember that the `difR` package has interesting functionalities for this.",2010-09-10 15:43:54.0,930.0,
3085,2538,0,"indeed, I want to see gender and item as a fixed effect. The research hypotheses are formulated that way. The only randomness in the whole thing is the personId. But I'll read in on the topic using your links. Thanks for the valuable help.",2010-09-10 15:47:28.0,1124.0,
3086,2538,1,"@Joris Other links that might be useful: http://j.mp/bzetkQ, http://j.mp/ac20UQ, http://j.mp/a1UNRb. HTH",2010-09-10 15:52:20.0,930.0,
3087,2507,0,"Hello Henrik, you bring an interesting case of multiple comparisons which I never thought of in this case - thanks.  (+1)",2010-09-10 16:59:19.0,253.0,
3088,2511,0,"*Q: how could one have a sample which is not 'identically distributed'?* an example of non-iid sample would be in time series (GARCH), where the observations are generated by a mixture of Gaussian distributions whose parameter are themselves random variables. In the context of the example above, a non iid sample would be one generated by \\alpha percent Gaussian + (1-\\alpha) percent Cauchy, where \\alpha is a r.v. in (0,1). Then draws from this mixture would be non-iid.",2010-09-10 17:48:27.0,603.0,
3089,2447,0,"@Whuber:> with the caveat that this is not an 'online' algorithm. If you add observations, the locations of the change points will vary (although this behavior is fairly easy to 'fix', for some reasons, the author have never explored that direction).",2010-09-10 17:59:16.0,603.0,
3091,2541,2,"Without reference to the number of parameters you try to estimate, or equivalently the kind of model your are working with, it seems rather difficult to give you a clear answer.",2010-09-10 18:36:26.0,930.0,
3093,2509,0,"Yup, and page 51 of van der Vaart's ""Asymptotic Statistics"" gives the distribution of that quantity as Gaussian with covariance matrix $$E[g'^2]/E[g'']^2$$ I don't see why consistency is that important here...Taylor expanding around the biased estimate p* instead of p0 seems to produce Gaussian with the same variance by this argument",2010-09-10 18:43:49.0,511.0,
3094,2535,1,"Cronbach's alpha is essentially a measure of internal consistency, that it is of inter-items correlations. It suffers from many pitfalls though, especially the fact that it increases with increasing the # of items (it may even be negative, although it is a ratio of variances!). I think we shall focus here at describing variations/homogeneity at the level of individuals within teams, so thinking of mixed-effects modeling or hierarchical models is a good starting point.",2010-09-10 19:10:51.0,930.0,
3095,1198,0,"Thank you, Carlos, for the recommendation.  It is indeed a great read, despite (or perhaps because of) its age.  I'm especially impressed that the authors (Box, Hunter, & Hunter) appeal to *permutation* distributions, rather than arguing for normality, as the ""ultimate"" justification for the classical tests (t, F, etc.).",2010-09-10 19:50:39.0,919.0,
3096,2531,0,isn't there a copy/past typo in the first equation ?,2010-09-10 19:53:23.0,603.0,
3097,730,10,This sentence itself is a model (an epistemological one),2010-09-10 20:00:18.0,603.0,
3098,2547,4,It seems like you already had a reasonable answer on the other site?,2010-09-10 20:40:51.0,5.0,
3099,2548,0,+1 Beat me.....,2010-09-10 20:50:17.0,88.0,
3100,1520,0,"Probability of observing value above median is one half, but your quantity is log-normally distributed for large n, which has different mean and median, so you shouldn't expect that probability to approach 1/2.",2010-09-10 21:33:02.0,511.0,
3102,2548,7,Breakdown isn't an issue for a descriptive statistic of an entire population.,2010-09-10 22:06:33.0,919.0,
3103,2546,1,Beautiful reference--and spot on relevant.  Thank you.,2010-09-10 22:21:10.0,919.0,
3104,2509,0,"Sure, a biased estimate converging to $p^*$ is going to have a similar central limit theorem, i.e. $p^* - \\hat{p}$ must then be assumed to converge. This is simply replacing $p_0$ by $p^*$.",2010-09-10 23:37:24.0,352.0,
3105,2549,0,"Actually I think nowadays the skew in a lot of countries is more towards seniors, not tots.",2010-09-11 00:09:34.0,830.0,
3106,2549,0,"Perhaps, it is skewed the other way but the general point stands. For skewed distributions a median may make more sense than the mean.",2010-09-11 00:21:30.0,,user28
3108,2550,1,"I think you meant ""The median is more resistant to such errors than the mean"". I agree with your comments though, and I believe the US census typically reports median's for many categories in official reports (not just the age) for basically all of the same reasons. Income is maybe even a better example than age to illustrate such points.",2010-09-11 01:54:37.0,1036.0,
3112,2539,0,"@srikant: Thanks very much. This is DEFFINITELY going in the right direction. Plus you have clearly outlined your thought process, and explained things as you are going along. Very, very good. Give me a little time to reflect on your answer and make sure that it addresses all the issues I need to address.",2010-09-11 07:22:13.0,1216.0,
3113,2539,0,"@srikant: The more I read your answer, the more I love it - its a shame that I can't vote it up, whilst I'm doing a little bit more reading around the subject area. I have a couple of further questions for you. 1). In the Bayesian model you described above, are there any underlying assumptions of the observed variables regarding: i). normality ii). independence (i.e. zero auto correlation)?  If there are any assumptions what are they and how does that affect the ""appropriateness"" of bayesian model, if one (or both) of the underlying assumptions are ""violated"" ?",2010-09-11 07:33:08.0,1216.0,
3114,2539,0,"Second question: You have (understandably), assigned a nominal scale to the observed variable (which light bulb is switched on), which you called 'Severity'. Will the Bayesian model still be an appropriate one, if instead of using a nominal scale, I use an ordinal one - i.e. there is no 'ranking' between the signals given. For example Level1, .. LevelN merely indicate the level of a building in which the attact event is predicted to occur.",2010-09-11 07:38:53.0,1216.0,
3115,2558,0,"A question though (on other polynomials): my aim is to approximate a multivariate gaussian by polynomial expansions: which of the different approaches (Chebyshev, Hermite,  Laguerre) is considered best for approximation ?",2010-09-11 07:47:15.0,603.0,
3116,2539,0,"Srikant: Could you please elaborate on this statement: ""In order to compute the required probability, P(S=s|W=w) you need to have some sense of P(S=s) which you can estimate or guess based on previous experience.",2010-09-11 07:52:20.0,1216.0,
3117,2539,0,"@Srikant: Could you please elaborate on the following statement (i.e. how do I do what you suggested, in practise): ""You can then assess the accuracy of the device by looking at the following probabilities: P(S=s|W=s).""",2010-09-11 07:53:49.0,1216.0,
3118,730,1,"but see a nice discussion around this quote on Gelman's blog, http://j.mp/9SgIBO",2010-09-11 10:21:25.0,930.0,
3119,2549,0,"I just updated my answer on math.stackexchange to emphasize just that point.  People look for symmetry and can incorrectly impose symmetry when it isn't there.  When you report the median, you give an answer that is symmetric -- the median splits the population in half -- even though the distribution is not symmetric.",2010-09-11 14:11:38.0,319.0,
3120,2542,6,Here's a page on exactly how good the normal approximation of the t distribution is for n=30.  http://www.johndcook.com/normal_approx_to_t.html,2010-09-11 14:25:20.0,319.0,
3121,2545,2,"Your example illustrates the value of robust statistics.  The *sample median* estimates the location parameter of a Cauchy distribution well.  One could argue that the weakest link in using a t-test with 30 samples is the t-test, not the 30 samples.",2010-09-11 14:35:20.0,319.0,
3122,2550,0,"Oops: thanks for catching that typo, Andy.",2010-09-11 14:54:38.0,919.0,
3123,2547,0,@Shane: But perhaps different sites hold the potential to garner different answers from different points of view?,2010-09-11 14:56:20.0,919.0,
3124,2561,0,"Edward Tufte is a statistician.  Started his career with BA and MS in statistics from Stanford, taught and wrote books about statistics for political scientists and is a fellow of the ASA.",2010-09-11 15:13:09.0,1107.0,
3125,2563,2,I am a bit unclear as to what exactly you are asking and how data analysis enters the picture? Please elaborate.,2010-09-11 16:37:35.0,,user28
3126,2539,0,Reg assumptions- Normality is irrelevant as the variables are discrete. Using the empirical proportions to estimate P(W=w|S=s) is equivalent to assuming independent bernoulli trials for the events; an assumption which may or may not be reasonable given your context. Your estimates for this probability will be 'off' if this assumption is violated. And consequently your assessment of the reliability of the device will also be affected.,2010-09-11 16:45:16.0,,user28
3127,2539,0,"Reg nominal vs ordinal- you have your terms switched. Ordinal implies ranking and nominal does not imply ranking. Yes, the model will hold for both ordinal and nominal data. Do note that if you have ordinal data we can develop a better model by exploiting the ordering of the data. Currently, for ordinal data the model is not as powerful as it can be as it does not use the ordering information present in the data.",2010-09-11 16:49:20.0,,user28
3128,2539,0,Reg P(S=s): This is application specific but perhaps you could take a random sample of your target population to which you wish to generalize and assess P(S=s),2010-09-11 16:50:19.0,,user28
3129,2539,0,"Reg criteria for P(S=s|W=s): I do not know of any explicit criteria. Obviously, this should be as close to 1 as possible. So, perhaps compute how far the vector {P(S=s|W=s)} is from a vector of 1s by computing the Mean Squared Error. That way you have a scalar measure of the performance of the device which you can then compare across several potential designs of the warning system.",2010-09-11 16:52:43.0,,user28
3130,1406,0,"Unfortunately it is popular among those that don't understand or trust statistics.  Once had it quoted to me when I gave a scientist an estimate of the number of hours it would take me to design and analyze his experiment.  He didn't seem to appreciate the fact that noise can look like effects, and effects of interest can be hidden from intuition by noise.",2010-09-11 17:05:56.0,1107.0,
3131,1330,3,Love this one -- a wonderful bonus of being a statistician.,2010-09-11 17:13:07.0,1107.0,
3132,2542,0,That's good stuff.  Thanks for sharing.,2010-09-11 17:53:41.0,1108.0,
3134,2563,0,"How is ranking not related to data analysis?  You're trying to combine user data (up/down votes) to generate a rank statistic that conveys information.  Admittedly, more work on this has been done in machine learning than statistics, but that's on topic, right?",2010-09-11 18:56:10.0,251.0,
3135,2574,0,"""That is different from describing your dataset with an estimated  density or histogram."" --- In case of pdf we are also somehow ESTIMATING the density. Are we not? Can you shed more light on your last paragraph. Thanks.",2010-09-11 19:44:54.0,1102.0,
3136,2563,0,@ars It *seems* on topic but the question was/is a bit unclear to me. An algorithm can mean many different things depending on context.,2010-09-11 19:47:49.0,,user28
3137,2574,1,"@Harpreet You are not estimating the shape of the PDF since as @Dirk indicated it has closed form, you just specify its parameters (e.g. $\\mu$ and $\\sigma^2$ for a gaussian). It will not necessarily ""fit"" the data. Now, there exist several kind of non-parametric density estimates, where you only use the data at hand (plus some kernel specifications or window span, etc.); see e.g., online help for the `density` R function.",2010-09-11 19:57:24.0,930.0,
3138,2575,2,"I must admit I never fully understand why Gelman advocates the use of histogram with small bin width; why not using stripchart plot or raw data with superimposed kernel density estimates, which much better convey the empirical distribution of the observed data?",2010-09-11 20:06:31.0,930.0,
3139,2575,1,"@chl: There are of course other good visualization methods to get a sense of sampling variability.  But on the narrower comparison of histogram v. pdf under discussion here, I think his point is well made.",2010-09-11 20:18:52.0,251.0,
3140,2577,0,"It's not absolutely necessary; I was hoping whether there was such a technique so there's more rigour than eye-balling. If nothing exists, well, this is it, but perhaps there is.",2010-09-11 21:04:52.0,840.0,
3141,2561,0,"@Kingsford My fault! I was initially thinking of another citation, not from Tufte and didn't remove my first words... I UPDATED my response. Many thanks!",2010-09-11 21:11:41.0,930.0,
3142,2574,0,"@chl: Thanks! PS: How do you include these formatting in text - like italics, selection etc - in comments?",2010-09-11 21:27:17.0,1102.0,
3144,2573,0,"Could you please clarify whether this question concerns data (whose distribution could be represented by a histogram) or theoretical constructs (such as a pdf, which describes a probability distribution).",2010-09-11 21:42:33.0,919.0,
3145,2573,0,"@whuber: I am asking it from perspective of application towards data analysis. Whether to employ histogram or probability dist. func (pdf), and why?",2010-09-11 21:59:13.0,1102.0,
3146,2577,0,"Oh sorry, I didn't mean to imply there weren't any techniques, only that you might be better served with collapsing into categories rather than being misled by a distinction between ranks x and x+1.  For example, I think the scoring method you propose is a fine way to start.",2010-09-11 22:01:53.0,251.0,
3147,2573,3,"But where does the pdf come from?  By definition, a pdf describes a theoretical probability distribution.  Do you perhaps mean the edf (empirical distribution function)?",2010-09-11 22:38:37.0,919.0,
3148,2563,0,@Srikant: there was a comment earlier about *closing* the question which I was responding to.,2010-09-11 22:56:18.0,251.0,
3149,2367,0,"@mbq: No --  the AIC/LOO proof by Stone 1977 does *not* assume linear models.  For this reason, unlike Shao's result, it's widely quoted; see for example the model selection chapters in either EOSL or the Handbook of Computational Statistics, or really any good chapter/paper on model selection.  It's only a bit more than a page long and worth reading because it's somewhat neat for the way he avoids having to compute the Fisher information/Score to derive the result.",2010-09-12 02:45:40.0,251.0,
3150,2367,0,"@Rob: right, it's not k-fold.  In fact, Shao devises the BICV procedure (Balanced Incomplete CV) in the referenced paper because the LKO procedure is so computationally expensive (C(n, k) subsets).",2010-09-12 02:48:17.0,251.0,
3151,2583,0,"I don't think it is relevant whether I know the true model in the wild. If there is a 'true' model, I would prefer a method which is more likely to find it.",2010-09-12 03:49:05.0,795.0,
3152,200,0,"+1 for this answer. I'll embellish it by saying that ""meaningful"" should also be considered in the context of decision making. If a given pattern is reliably extracted from the data generating process, is there a business/research decision that will be made differently by its presence? If so, you can estimate the Value of Information with respect to the effort required to do the data mining and the potential (monetary) gain from confidently making a different decision.",2010-09-12 03:56:17.0,1080.0,
3153,2583,0,"@shabbychef: I don't disagree.  But note: ""If there is a 'true' model"" *and* it's under consideration .. how would you know this a priori?",2010-09-12 04:08:01.0,251.0,
3154,430,1,"ditto. I usually put it in terms of 52 cards, and the goal is to find the ace of spades.",2010-09-12 04:08:32.0,795.0,
3155,2583,0,"Note also that my second paragraph actually makes the point in your comment.  This is a nice property, but it's not all clear how applicable it is in the wild; even though it's comforting in some sense, it may be misguided.",2010-09-12 04:13:58.0,251.0,
3156,2400,0,"Yes. The entropy for square loss is constant and the entropy for 0-1 loss is min(p,1-p). What's also interesting is that these have strong correspondences to divergences too. The square loss to the Hellinger divergence and 0-1 loss to variational divergence. Since entropies defined like this they are necessarily concave functions and it turns out the f-divergence built using f(p) = -entropy(p). Bob Williamson and I have explored some of this in our paper: http://arxiv.org/abs/0901.0356 . It's fun stuff.",2010-09-12 05:39:17.0,1201.0,
3157,2580,0,do you know the probability distribution of the values in **each bin** ? The y axis label makes me think that this could be Poissonian or Multinomial ? (assuming a model gives you the mean in each bin),2010-09-12 07:47:55.0,961.0,
3158,2574,0,"@Harpreet This is just Markdown syntax, as for editing a post through the online editor: `*ab*` gives *ab* (italic) `**ab**` gives **ab** (bold) `$\\sqrt{2}$`=$\\sqrt{2}$",2010-09-12 08:00:35.0,930.0,
3159,2585,1,"Should this be community wiki? I know that more than one reasonable answer could be given, but it seems like a question where reputation should be awarded.",2010-09-12 08:43:32.0,183.0,
3160,2531,0,I don't see it?,2010-09-12 09:54:53.0,1124.0,
3162,2581,0,"Say, you have a portfolio of assets and you want to estimate the sensitivity of the diversification benefit to a severe change in correlations (therefore, the need to stress correlation matrix). Stressing the eigenvalues seems like a mathematically attractive idea, however, it doesn't necessarily reveal that much - wouldn't be more useful to stress actual correlation because you can express a view about specific variables.",2010-09-12 10:59:48.0,1250.0,
3163,2580,0,"The data is essentially drawn from two Poisson processes, but there are hidden variables that I can't correct for, leading to overdispersion. Thus, a negative binomial is definitely a better model. (see the new image/text I added above). I need to show that my nb model fits better quantitatively.",2010-09-12 11:36:40.0,54.0,
3164,2580,1,How about a metric like Mean Squared Error between actual vs predicted values?,2010-09-12 11:42:44.0,,user28
3165,2478,0,"@ebony - thanks for your input. I'll leave the question open for a while longer, to encourage other answers.",2010-09-12 15:48:10.0,1212.0,
3166,2592,0,"What do you call 'Pca matrix' or matrix of eigenvalues? There is only a vector of eigenvalues, and eventually a matrix of rotation.",2010-09-12 16:21:45.0,930.0,
3167,2592,0,"Sorry, my inaccuracy - By 'PCA matrix' I meant matrix of rotation",2010-09-12 16:25:43.0,1260.0,
3168,2558,1,"I'm doesn't matter so much _which_ polynomial class you use (up to numerical issues), but you can get better results by the choice of nodes where you sample the function (in this case the bivariate gaussian density) in order to compute the coefficients in front of the polynomials. For this problem, the Chebyshev nodes are optimal, at least when interpolating on a finite interval. see http://en.wikipedia.org/wiki/Chebyshev_nodes or any decent numerical analysis text (e.g. Cheney & Kincaid) or http://pages.cs.wisc.edu/~amos/412/lecture-notes/lecture09.pdf",2010-09-12 16:33:13.0,795.0,
3169,2558,0,should say 'it doesn't matter'...,2010-09-12 17:20:49.0,795.0,
3171,2580,0,"hrmm - I like that idea, Srikant. It's a lot simpler than what I was thinking, but still makes sense. Throw into a an answer below so I can credit it and send some rep your way.  I'm still interested in hearing other methods, but this may work for now.",2010-09-12 18:13:42.0,54.0,
3172,2588,0,"I found that page too, but it does not mention any relationship to the upper tail, so I was unsure.",2010-09-12 18:21:26.0,977.0,
3173,2482,0,"+1, nice and complete answer, with code to boot!",2010-09-12 18:32:39.0,251.0,
3174,2307,0,"+1, good answer and serendipitous for me -- much thanks for the paper references, especially the review.",2010-09-12 18:36:13.0,251.0,
3175,2367,0,"@ars But AIC must exist, convergence is poor, and so on. My whole crusade about this topic is against using it mindlessly to support some strange claims like that LOOCV is better/worse than 10-fold on 10 object set or that it works in case of machine learning.",2010-09-12 18:39:25.0,88.0,
3176,2367,0,"@mbq: I was only objecting to the strongly worded comment on ""*IC"" / ""*only* for linear models"", not your crusade where I happily cheer you on.  Your latest comment is spot on.  This is the essence of the objection in my answer, i.e. false sense of comfort through stronger claims than supported by theory.  Model selection is far from a settled science -- and for good reason: as the model space becomes more complex, it's quite hard to prove widely applicable results.",2010-09-12 19:24:38.0,251.0,
3177,2599,0,Thanks! The Wikipedia article on hierarchical clustering does not link to that one.,2010-09-12 20:32:07.0,977.0,
3178,2599,2,"Oh right.  Fixed now under ""see also"" links, thanks for pointing that out!",2010-09-12 20:38:19.0,251.0,
3179,2588,1,Well upper tail is by definition P(X>x).,2010-09-12 22:18:59.0,,user28
3180,2585,0,"I am more or less looking for opinions, so there's no real ""solution"" to my question. Though I would be happy to award reputation to good responses...if it's possible.",2010-09-12 23:19:32.0,1253.0,
3181,2585,0,Doesn't look like I can change it back into a regular question - could a moderator help with this?,2010-09-12 23:28:15.0,1253.0,
3182,2545,0,"John:> ""One could argue that the weakest link in using a t-test with 30 samples is the t-test, not the 30 samples"". Very true, and also the assumption that the data is *iid*. Also, the median is MLE for Cauchy distributed random variables (and hence efficient), but in general you could need more than 30 observations.",2010-09-12 23:54:36.0,603.0,
3183,2558,0,"Yes, indeed. But do the Chebyshev node generalize to multivariate functions ? I have tried to search the web and the only class of polynomials that is considered in the multivariate case seems to be the Hermite.",2010-09-12 23:58:37.0,603.0,
3184,2605,0,@kwak I would be interested in knowing the exact solution as my proposal is more of a hack rather than a principled approach. Could you provide a pointer like a reference/ a term to search for?,2010-09-13 00:15:42.0,,user28
3185,2605,0,"@Srikant:> i don't think it's an 'hack' v.s. principled approach difference. Your answer is in term of eigenvalues whereas Edward's question specifically mentions individual correlation coefficients. As a statistician, i understand that eigenvalues  have better statistical properties and are easier to work with than individual correlation coefficients (in this case the e.V.'s   will have the positive side effect of 'pooling' the correlation coefficients). But your simulation will no longer be in terms of individual assets, which may (or may not) be important for Edward.",2010-09-13 00:28:03.0,603.0,
3186,2605,0,@kwak Can you shed some light on the structure of the objective function? Why is it formulated the way it is?,2010-09-13 01:51:40.0,,user28
3187,2560,0,"While I accepted the other answer, I'd like to say thanks for this one too (which is also quite useful)!",2010-09-13 02:08:49.0,386.0,
3189,2605,0,"IMO, the above is a really helpful answer. +1 from me.",2010-09-13 02:23:13.0,,user28
3190,2604,0,"Thanks. That clarifies a bunch of things and opens up a whole slew of new questions which I'll have to do some research on. I guess my main question is, does what you're saying mean that something more simple, like just taking root mean squared error, isn't a valid way to approach this problem? I'll grant that it's probably not as robust and won't give me a p-value, but it's something I could do quickly while I try to track down a copy of the book you reference. Any thoughts would be appreciated.",2010-09-13 02:58:32.0,54.0,
3191,520,0,"I'll give you +1 (so that you now have your 2 original upvotes :), just because I agree that with many data points, Python may be a good option; now the minor point, IMO, is that there're not so many clustering algorithms available in SciPy compared to R.",2010-09-13 06:16:10.0,930.0,
3194,2607,0,"@ kwak - that you! The sign of a small correlation can become important in certain optimisation problems. For example, a small negative correlation (which, statistically might have been positive just as likely due to insignificance) can be combined with 2 large volatilities (and as a result large negative covariance), which will lead to misleading optimisation results. Had the sign been positive, the results would have been very different. Zero correlation would at least not give preference to either.",2010-09-13 07:34:38.0,1250.0,
3195,2607,0,"By the way, I agree that the matrix might become indefinite. However, wouldn't it be feasible to simply make the negative eigenvalues = 0 and recalculate and rescale the matrix so that the diagonal values are 1? Example of the procedure can be found here: http://comisef.wikidot.com/tutorial:repairingcorrelation",2010-09-13 07:39:20.0,1250.0,
3196,2605,0,"@kwak that was very helpful! It is possible to estimate the limits to stress testing numerically by increasing the stress levels until the minimum eigenvalue of the matrix no longer is >= 0. I have also been exploring stress testing the entire matrix by a single parallel shift (ie each correlation +/- x stress level). Again, it is easy to estimate the limits to stress testing numerically. Rule of thumb: max upward stress is roughly equal to the smallest eigenvalue of a pos-def matrix. Max downward stress is more complicated. Is there a convenient way to solve this problem analytically?",2010-09-13 07:50:26.0,1250.0,
3197,2610,0,"Thanks for this excellent answer! In fact, the hierarchical clustering module you showed is already part of scipy. Also, scipy provides an implementation of k-means, so I could easily use that.",2010-09-13 08:36:32.0,977.0,
3198,2530,0,"Thanks.  I've done some more searching and your formula appears to be more commonly used (and using a ratio of variances seems simpler and more intuitive to me).  I tried both formulas on two data sets; the results were qualitatively similar in one case and quantitatively identical in the other, so I guess the results of the two versions of the test converge at some point.  I'm going to proceed using your formula.  If anyone else has any insight into the two formulae, it would be great to hear it.",2010-09-13 09:01:33.0,266.0,
3199,2610,0,"Ok, I didn't look in details into this. For k-means, you need to pay attention to the fact that we generally need two outer loops for validating cluster solution (one where you vary the # of clusters and another for varying the seed -- the objective being to minimize the RSS); then you can use the Gap statistic to choose the optimal # of clusters.",2010-09-13 09:18:19.0,930.0,
3201,2605,0,"@Srikant:> The solution is a k dimensional ellipse centered at $a_0$ (the actual observed correlation coefficients). This is like asking 'how far i can walk in the direction given by -w until i meet the boundary of the ellipse (which is represented by the first constraint). The actual distance is $\\max.\\; a_0'w-a'w$, but since the first term is a constant, it drops from the objective function. So this is given by min. a'w",2010-09-13 10:30:55.0,603.0,
3202,2605,0,"@Edward:> the numerical approach is okay when $p$ is small. What you do there is that you take a bunch of points at random inside the $k$ dimensional $(-1,1)$ hypercube and then you test each of your random points you test (using the value of the smallest eigenvalue of the resulting matrix) whether that point is inside the $k$ dimensional ellipse that i describe which bound the zone of admissibility. When $k$ increases you approach is no longer feasible because the ellipse will occupy an ever smaller part of the hypercube (this is known as the curse of dimensionality) meaning that the odds...",2010-09-13 10:39:07.0,603.0,
3203,2605,0,...of having a randomly selected point falling inside this ellipse (your acceptance rate) will become ever smaller. You would have to make exponentially more tries to have a fixed number of PSD matrices. THat's why it's better to have an explicit formulation of the boundary of the ellipse.,2010-09-13 10:43:18.0,603.0,
3204,2605,0,"@Edward:> your second question (analytical solution). If you want to know by how what quantity $\\delta$ you can shrink all of your correlation coefficients towards 0 while still having a PSD matrix, this is easy. If you want to know by how what quantity $\\delta$ you can shrink all of your correlation coefficients towards -1 while still having a PSD matrix, this is more complicated. Which one you want?",2010-09-13 10:49:54.0,603.0,
3205,2607,0,Edward:> your second point: i need to think about this but i have the impression that what you propose amounts to shrinking all your correlation coefficients by a small amount towards 0.,2010-09-13 10:57:46.0,603.0,
3206,2605,0,I am interested in the 2nd. Allow me to describe the problem in more detail. I have some initial correlation matrix. I want to stress each correlation in the matrix by the same constant simultaneously (except the diagonal; lets call this global parallel stress since it affects the entire matrix by the same constant). I start with adding 0.01% to each correlation and check if the matrix is still PSD. I continue increasing the stress levels by small increments. Eventually I encounter a stress level above which the matrix would no longer be PSD. Let's call this the upper stress boundary.,2010-09-13 11:42:06.0,1250.0,
3207,2605,0,I repeat the same procedure for negative stresses and find the lower stress boundary. My general observation was that the minimum eigenvalue initial correlation matrix is roughly equal to the upper stress boundary. Do you think that an analytical solution for this problem is too complicated?,2010-09-13 11:46:04.0,1250.0,
3208,2605,0,"Generally, I am more interested in the upper stress boundary. I stress the entire matrix at the same time because it is a simple way to reduce diversification benefit across all market variables.",2010-09-13 11:55:24.0,1250.0,
3209,2605,0,"Edward:> maybe open a new threat (that links to this one). Since you marked this one as solved, you're not getting a lot of traffic here (and i think other people in this forum have alot of experience with issues similar to yours). I think your problem is now different enough from the initial one and specific enough  that it deserves a threat of its own.  Also you might want to cross post it in http://www.or-exchange.com/",2010-09-13 12:26:13.0,603.0,
3210,2598,0,"As @Edward pointed out, please add some details.",2010-09-13 12:40:02.0,88.0,
3212,2612,0,"Thanks a lot for your reply! In principle i'm also interested in how to solve a concrete problem like the one i described (thus thanks for the WinBUGS tip). But at the moment i'm trying to do a simulation study for a seminar paper in which i'd examine the performance (coverage rates etc.) of MI under model misspecification. I suppose i'll just forget about the variance components if I can't find a solution and focus on the fixed effects, but it's frustrating to give up.",2010-09-13 13:15:38.0,1266.0,
3213,2612,0,@Rok Great idea for the simulation! I'll look forward for this particular issue. I suppose you already search on the r-sig-mixed mailing and Gelman's book on multilevel regression...,2010-09-13 13:20:52.0,930.0,
3214,2605,0,Good idea! Will do.,2010-09-13 13:22:51.0,1250.0,
3215,2613,4,"The McNemar test is designed for paired data, isn't it? So you cannot compare its output with that of Pearson or Fisher which assumes independent samples...",2010-09-13 13:40:15.0,930.0,
3216,2613,0,"OK, can you put your comment as answer?",2010-09-13 14:08:18.0,603.0,
3217,2617,1,"There are many ways to interpret (or misinterpret) your description, Daniel, so let me know where this attempt at a clarification might be wrong: You have a set of (x,y) coordinates.  You apply a function to them to obtain a set of (x,y,z) values, a ""terrain map.""  The z values can in principle lie in [-2.25,2.25] but actually they lie in [-1.75,1.75] with the middle 40% of them in [-0.4,0.4].  You seek a transformation f so that the collection of f(z) is uniformly distributed in [-1,1].  (If this characterization is correct, there's a simple solution involving a linear transform of the rank.)",2010-09-13 14:58:25.0,919.0,
3218,2612,0,"I looked now, tanks for the references! Unfortunately, there's nothing on MI in the r-sig-mixed archives; and Gelman only gives the basic formula on how to combine inferences from MI when we have variation within and between imputations given (§25.7).",2010-09-13 15:14:20.0,1266.0,
3219,2607,2,"@kwak: Have I misinterpreted that last statement?  The eigenvalues of a 3 x 3 correlation matrix, for instance, are determined by three correlation coefficients (a,b,c).  For (a,b,c) = (-0.9, 0.5, -0.1), the ev's are (2.07415, 0.91532, 0.0105346).  Changing c from -0.1 to -0.01 changes the ev's to (2.03383, 0.99151, -0.0253402).  Thus, a relatively small *decrease* in absolute value of a single coefficient can destroy the positive definiteness of the matrix.",2010-09-13 15:16:41.0,919.0,
3220,2619,1,I would recommend that you to split this across three separate questions.,2010-09-13 15:20:50.0,5.0,
3221,2619,0,"Actually, now that I read this a little closer, I see that the answer for each is very closely related.",2010-09-13 15:25:22.0,5.0,
3222,2615,0,You may want to explicitly link to the previous question on this issue as @kwak's answer on that question may be relevant here.,2010-09-13 15:35:27.0,,user28
3223,2615,0,Good idea! Added.,2010-09-13 15:51:14.0,1250.0,
3224,2615,0,"Also, on re-reading your question it is not clear what you mean by ""minimum eigenvalue initial correlation matrix"". Could you clarify what you mean by that term?",2010-09-13 15:57:32.0,,user28
3225,2617,0,@whuber - This sounds like a perfect interpretation.,2010-09-13 16:07:00.0,1267.0,
3226,2619,0,"I felt that the heart of the question is comparing two different distributions, I just happen to list three different ways to do it.",2010-09-13 16:14:58.0,559.0,
3227,2607,0,+1 counter example: i had assumed that making the ellipse more spherical was a transformation that preserved the convexity of the associated quadratic form. Corrected (i.e. strike in the response).,2010-09-13 16:19:00.0,603.0,
3228,2622,0,I actually have the Agresti textbook on my desk right now and I have been using it. The problem is that I didn't know what specific methodology I should be using.,2010-09-13 16:32:54.0,559.0,
3229,2558,1,"ah, yes, for some reason I thought the function was separable.  In higher dimensions, I only have experience in spline interpolation, e.g. for finite-element method compuations. (oddly enough, my thesis was on the topic). the big takeaway in planar meshing to minimize interpolation error is to minimize the maximum angle appearing in the triangulation. the margin does not permit me to be more detailed than this ;) you can look up any of the papers by J. Shewchuk (see 'finite element quality' at http://www.cs.cmu.edu/~jrs/jrspapers.html) or N. Walkington & G. Miller, etc. etc. hth.",2010-09-13 16:34:25.0,795.0,
3230,2615,0,"Minimum eigenvalue initial correlation matrix: 1) calculate the eigenvalues of the correlation matrix prior to stress testing (i.e. initial matrix), 2) select the smallest eigenvalue, 3) this eigenvalue will be roughly equal to the upper stress boundary.",2010-09-13 17:02:22.0,1250.0,
3231,2623,1,"if your series is non stationnary, the ACF will decline very slowly, to the point of being useless (it essentially a constant). What do you mean by 'have any meaning' ?",2010-09-13 17:02:54.0,603.0,
3232,2603,0,The location changes after each attempt,2010-09-13 17:47:26.0,64.0,
3233,2607,0,@kwak: Thank you.  Issues involving positive definiteness can be subtle.,2010-09-13 17:48:35.0,919.0,
3234,2613,2,"Don't know if your udpate calls for a new answer, anyway I'll continue to comment on. For a given # of cases, power for detecting an effect can be increased by selecting more controls than cases (which also come into play in CIs width). Power increases as one moves from a 1:1 to a 1:2 matching but the benefits are less interesting above that ratio. However, it is still useful to have more controls in case of exclusion during sensitivity analysis or things like that. If you're interested in CC design, look at this Lancet series, http://j.mp/aZWRrg",2010-09-13 18:29:50.0,930.0,
3235,2625,0,"The problem with this method is that I never actually have a complete map. The my noise function generates [x,y] independently from [x+n,y+m], and should have, on its own, a uniform distribution. I'm trying to make the function uniform. A uniform distribution over the map is simply a byproduct. (Granted, I didn't really make this clear in the question; +1 for the idea.)",2010-09-13 19:07:22.0,1267.0,
3236,2622,2,"@Elpezmuerto Very briefly, to complement @ars answer, question 1 can be answered with a conditional or trellis plot, e.g. sth like `dist ~ occ | isLand` using Lattice, or see the `coplot()` function in the `vcd` package -- this is for exploratory purpose; question 2 calls for a prediction model; depending on the variable you consider as your outcome, it may be logistic regression (e.g. if Y=isLand), a linear regression (e.g. if Y=distance), or directly a log-linear model providing you categorize your continuous measurement; question 3 is clearly a log-linear model as suggested by @ars.",2010-09-13 19:10:03.0,930.0,
3237,2622,1,"@Elpezmuerto @ars Thanks to the work of Laura Thompson, Agresti's book is available in R too, http://j.mp/9fXheu :-)",2010-09-13 19:12:25.0,930.0,
3238,2620,0,"This is looking good, but I'm having trouble interpreting my results (which I edited in above.) I think it means my starting distribution isn't exactly normal, but I can't seem to tweak it to work. I'm using `x = (1 - erf(x))/2`. Am I missing a constant scalar of some sort?",2010-09-13 19:17:12.0,1267.0,
3239,2622,2,@chl: that's a great find! Thank you.  @Elpezmuerto:  There's a series of examples in Agresti concerning crabs -- I'm pretty sure there's a continuous variable (size of crab?) along with a color (range) and a boolean (can't recall).  So fairly close to your case -- it's probably instructive to read through those examples which span at least 2 chapters (one chapter is logistic regression I believe).,2010-09-13 19:33:44.0,251.0,
3240,2613,0,"good link, thanks.",2010-09-13 19:35:19.0,603.0,
3241,2603,0,"as @Edward said. I could ask such a thing in a homework actually... Now Martin, can you figure out what's the chance you never find that treasure?",2010-09-13 19:47:02.0,1124.0,
3242,2622,0,"@ars These are esp. chapters 4 and 5, with carapace width and weight as continuous variables and spine condition as another categorical (ordinal) variable, used in Poisson and Logistic regression :)",2010-09-13 19:51:01.0,930.0,
3243,2620,1,"@Daniel: Don't think you're missing a constant, but scaling might help.  Think your intuition about normality is correct though.  I updated my answer with some suggestions.",2010-09-13 19:56:09.0,251.0,
3244,2622,0,"@Ars, I actually have been using the crab examples before you mentioned it. So atleast I know I am on the right track",2010-09-13 20:18:52.0,559.0,
3245,2625,0,"@Daniel: Hmm, your actual situation is a little unclear.  Maybe you could edit the question to say more about what your doing.  Anyway, I'm operating on the assumption that the references to normality may be a bit of a red herring.",2010-09-13 20:23:28.0,919.0,
3246,2617,0,"@Daniel: Which probability integral are you using?  If you used the one appropriate for the upper (green) curve, then the lower (red) curve should be perfectly horizontal.",2010-09-13 20:24:27.0,919.0,
3247,2617,0,@whuber - I'm using cubic interpolation between integral points generated from a hash function.,2010-09-13 20:32:53.0,1267.0,
3248,2496,0,"Thank you, I will look into this, I already know how to do a regression analysis of this data, but maybe the chapter also has a solution to my problem.",2010-09-13 20:43:56.0,1084.0,
3249,2617,0,"@Daniel: Although I'm quite familiar with all those terms, without more details it's hard to say whether the problems with the lower red curve are to be expected (e.g., if you're using a spline with a small number of knots) or are actually serious.  I'm startled by the amount of error in the tails, though.",2010-09-13 20:46:57.0,919.0,
3250,2617,0,"@whuber - I'm taking the X,Y coordinates of the four corner points, running them and a seed value through MurmurHash2, and using cubic interpolation calculate get the points between the corners. Since all this can be done independently for each point, I want the *function* to be uniform, not only the map.",2010-09-13 20:57:52.0,1267.0,
3251,2632,0,I think this may be the way I'm going to go. I fear you're right about the hash function not being completely normal.,2010-09-13 20:59:45.0,1267.0,
3252,2632,0,"@Daniel: I suspect it doesn't matter if it's normal or not. If you want to know how normal it is, generate N numbers from a normal distribution, sort it, and plot against the numbers in your table. If your table is normal you'll get a straight line (QQ plot). If not, it will not be straight. Useful thing.",2010-09-13 21:14:12.0,1270.0,
3253,2632,0,@Daniel: There's a really brain-dead simple way to generate a pretty good normal random number. Just add together 12 uniform numbers from the range -0.5 to 0.5.,2010-09-13 21:16:56.0,1270.0,
3254,2496,0,"It'll still be a regression analysis, just one that takes unto account (i.e. corrects) the fact that 4 measurements on the same observations are correlated with one another.",2010-09-13 21:18:45.0,603.0,
3255,2621,0,Nice explanation. Thanks!,2010-09-13 21:23:32.0,1102.0,
3256,2574,0,Thanks chl! Is it true in just stack exchange or its an html formatting?,2010-09-13 21:25:55.0,1102.0,
3257,2617,0,"@Daniel: Aha!  Do you want the function to have a uniform distribution on each rectangle, or does it have to be uniform *globally*?  (Until now the latter was my interpretation, but your comment suggests it's the former.)  The first interpretation has a definite mathematical answer.",2010-09-13 21:34:56.0,919.0,
3258,750,5,"There are three kinds of lies: Lies, damned lies and fake statistics.",2010-09-13 22:01:51.0,1124.0,
3259,2617,0,"@whuber - The *limit* of the distribution (if that makes any sense) should be uniform. But any single point should be also. So if I pick, say (45.32,-12.1), it should have equal probability of being any number [-1,1]. (45.32,-12.2) should also have equal probability of being any number [-1,1], though in reality it will be very very close to the number selected before. Any integral point [(0,0),(1,4),etc] already has uniform distribution. But interpolation between points skews the distribution.",2010-09-13 22:15:47.0,1267.0,
3260,2558,0,"*the big takeaway in planar meshing to minimize interpolation error is to minimize the maximum angle appearing in the triangulation.* meaning Delaunay triangularization ? this is nice because its quiet simple to do. Thanks for the papers, i will have a look.",2010-09-13 22:25:20.0,603.0,
3261,2617,0,"Interestingly enough, the distribution of points generated from the hash and 1 dimensional cubic interpolation seems to not be normal at all. It looks almost uniform, but after generating a billion values, there is still noticeable noise in the PDF.",2010-09-13 22:25:57.0,1267.0,
3262,2617,0,"@Daniel: I'm still lost, but I'm getting there.  Let's see: you are generating hash values (which, if it's a good hash, should act like independent uniformly distributed random numbers) at points with *integral* coordinates.  Then you're interpolating those values to all other coordinates.  You find that the distribution of all values is not uniform.  You would like to make it so by transforming the interpolated values.  Am I close now?  If so, should the transformation be predetermined or can it depend on the hashes at the integral coordinates?",2010-09-13 22:48:34.0,919.0,
3263,2558,0,"A small maximum angle is implied by a large lower bound on minimum angle ($\\pi - 2\\alpha$ where $\\alpha$ is the lower bound on angle); Delaunay Triangulations maximize this lower bound over all triangulations. Shewchuk explains this fairly well, IIRC. My thesis less so ;) You are more free however, because you are not constrained to include certain edges (meshing algorithms are usually applied to weird shapes), and can put your mesh points anywhere. Practically speaking, you want to use Strang & Persson's meshing code, in Matlab, or use Shewchuk's Triangle, which is a high quality standalone.",2010-09-13 23:12:24.0,795.0,
3264,2619,0,"For `occupants` what you've got is an ordinal variable, so I wouldn't think of it as categorical. Especially with 8 values, it's almost continuous.",2010-09-14 00:07:09.0,1270.0,
3265,2617,0,"@whuber - You seem to be much less lost than you make yourself out to be. If I understand your question, I believe I want a predetermined transformation. I know how I could transform any given map so that it's uniform. But I need a transformation that generally works all the time. So the I can add on to my map indefinitely, and it will retain uniformity (and probably get more and more uniform, too.)",2010-09-14 00:37:28.0,1267.0,
3266,2636,4,The distribution of age is certainly not log normal.,2010-09-14 00:51:17.0,159.0,
3267,2622,0,"@chl: totally impressed by your consistent thoroughness in all responses! You're setting a high bar though.  :)  @Elpezmuerto: totally on the right track.  I'm guessing you're more or less aware of the methodology and just need to start applying it to your data.  Then ask specific questions if you hit a wall or are unsure.  It's the only way to learn modeling, I think.",2010-09-14 01:25:27.0,251.0,
3268,2633,0,"Oops, you're using agglomerative methods, so the ""1 or 2 clusters"" part doesn't make sense -- the ""inverse"" applies, you don't want n-1 or n-2 singletons, etc, i.e. letting clustering work for a bit before applying validity criteria shouldn't be problematic.",2010-09-14 01:29:13.0,251.0,
3269,2036,1,"@Gong-Yi: nice, thanks for updating with the answer!",2010-09-14 01:32:35.0,251.0,
3270,2639,0,No one to explain?,2010-09-14 02:18:56.0,1102.0,
3271,2639,0,Do you really mean you have 1 observation from A? It seems like you are likely to make an error with so little data.,2010-09-14 03:17:16.0,795.0,
3272,2497,1,"+1, Very informative, thank you.",2010-09-14 03:27:28.0,251.0,
3273,2639,0,"Its not one observation, it is one distribution or realization with 40000 values.",2010-09-14 03:38:14.0,1102.0,
3274,2639,0,"So say a sample has size n = 40000.  Then you have one such sample of random variates from A, and 100 replicates each for B, C and D?",2010-09-14 03:55:52.0,251.0,
3275,2641,1,"Great question. I would add ""odds"" and ""chance"" in there too :)",2010-09-14 05:28:41.0,74.0,
3276,2641,3,I think you should take a look at this question http://stats.stackexchange.com/questions/665/whats-the-difference-between-probability-and-statistics/675#675 because Likelihood is for statistic purpose and probability for probability.,2010-09-14 06:04:20.0,223.0,
3279,2650,1,"If you need a command to do that, you better provide us at least with the language/package you use. Are we talking SAS, SPSS, Stata, SPlus, Minitab, R, ... ?",2010-09-14 09:36:46.0,1124.0,
3280,2604,2,"imagine that you had a set of points (x,y) and you were considering whether you might fit a straight line or a quadratic. If you compared the RMSE, the quadratic *would always beat the straight line*, because the line is a quadratic with one parameter set to zero: if the least squares estimate of the parameter is exactly zero (which has zero probability for continuous response), it's a tie, and in every other case the line loses. It's the same with the Poisson vs the negative binomial - a free Negative Binomial can always fit at least as well as a free Poisson.",2010-09-14 10:28:42.0,805.0,
3281,2651,0,"Thanks a million for your valuable answer, it really does help, for your previous remark, i'm a stata user.",2010-09-14 10:53:58.0,1251.0,
3282,1315,0,"You're getting lots of useful ideas for different distributions. To answer your question ""I want to know what statistical distribution this is"", the usual method is a QQ plot, easy to generate in R.",2010-09-14 11:25:03.0,1270.0,
3283,1315,0,"Of course, if all you want is a smooth curve, take all your data, sort it to make an empirical CDF, smooth it, and take the first derivative numerically. That's your curve.",2010-09-14 11:29:03.0,1270.0,
3284,2651,1,"In Stata, I wouldn't know a command to check that easily. When using implemented goodness of fit tests, you have to be aware you cannot compare the values across non-nested models in many cases, and definitely not across different frameworks.",2010-09-14 11:54:18.0,1124.0,
3285,2651,0,"@Ama Maybe worth looking at the UCLA Resources Center for Stata, http://j.mp/9xBk6U",2010-09-14 12:14:36.0,930.0,
3286,2654,0,"Thanks for the suggestion. Perhaps, my description makes it sound like a sigmoidal process, but the data is typically not as smooth as a sigmoidal process might suggest. The theory and the data suggest that some form of relatively abrupt change occurs, which leads to the commencement of the rise from values around 0 to values around 1. Thus, it's something like a segmented nonlinear regression with unknown change points.",2010-09-14 13:26:14.0,183.0,
3287,1234,0,"In quiet periods price volatility tends to be lower, so prices closer to the mean can be considered outliers. However, because you use MAD for (presumably) an entire trading day (or even longer) these outliers are less than 3 MAD away from the median and will not be filtered. The reverse is true for busy periods with higher price movements (acceptable price movements will be filtered). Thus the problem reduces to properly estimating the MAD at all times, which is the issue to begin with.",2010-09-14 13:39:17.0,127.0,
3288,2604,0,"Nice explanation - I get what you're saying now. I think my case is a little different, because I'm not doing regression to get a fit, but rather, I'm basing the extra NB parameter on outside information (I expect the var/mean ratio to be N).  Since Poisson is the special case where N=1, what I'm really comparing is the choice of N. I agree that if I was doing regression, the NB would always be able to find a better fit, because it's less constrained. In my case, where I'm choosing a value for N up front, it would certainly be possible to choose some crazy value of N that makes the fit worse.",2010-09-14 13:40:22.0,54.0,
3289,2654,1,"The sigmoidal functions become step functions with appropriate prarameters, but are you meaning you are wanting something like a level shift in time series analysis?",2010-09-14 13:47:30.0,229.0,
3290,2622,0,"@Ars, I felt I hit an information wall but you guys are getting me on the right track. I am also considering using a generalized linear model because max occupants is a discrete response variable that is a count as a possible outcome. Agresti (page 74) suggests generalized linear model then.",2010-09-14 13:51:06.0,559.0,
3291,2496,0,Then I think think I have explained my question properly. I already understand how to do a regression analysis in a within subject design. What I don't understand is how to adjust df when comparing means (and correlating) slope values acquired in a repeated design.,2010-09-14 14:06:42.0,1084.0,
3292,2496,0,"In fact a simple regression line, that minimizes distance the vertical distance, is not appropriate for finding the relation between two dependent variables, one has to calculate the minimized euclidean distance.",2010-09-14 14:08:04.0,1084.0,
3295,2496,0,"Adam1:> once you control (by fixed or random effect) for the correlation between measurement, you will also increase the number of parameters of your model, hence decrease the number of df.",2010-09-14 14:17:13.0,603.0,
3297,2648,0,Does $Y$ increases monotoneously ? (it's not clear from your description whether the first derivative of $Y$ is always $\\geq0$),2010-09-14 14:31:52.0,603.0,
3298,2496,0,"Kwak:> not sure how to explain myself better. I think you are trying to help me do a regression, but that is not my question - I already understand that, what I am trying to do is to apply a t-test or a correlation to individual slope values. I am asking if I am loosing df treating each slope value as 1 df even though it was calculated over repeated measurements.",2010-09-14 14:39:53.0,1084.0,
3299,2651,0,"Joris, Chl, many many thinks to both of you, your advices are really so valuable!!!!",2010-09-14 14:40:53.0,1251.0,
3300,2615,1,Apparently a solution had been founded on the OR exchange forum (http://www.or-exchange.com/questions/695/analytical-solutions-to-limits-of-correlation-stress-testing). Edward:> Maybe you should repost mark this question as solved,2010-09-14 14:53:02.0,603.0,
3301,2617,0,"@Daniel: I don't think your hope is going to play out exactly. A fixed transformation can be devised to uniformize the data within any given cell.  That distribution will, of course, depend on the hash values at the four corners.  The whole grid thereby becomes an equally-weighted mixture of uniforms which have varying ranges (and those ranges are correlated among neighboring cells).  There will be tendency for fewer values to be generated near the extreme end of the z-interval.  However, for any given set of (x,y) coordinates, you can generate several independent sets of z's... (continued).",2010-09-14 15:05:16.0,919.0,
3302,2617,0,"... With this set, you can either create a lookup table (as suggested by Mike Dunlavey) or, more simply, fit a spline to the edf for an efficient approximation to the rank transform I suggested.  That empirically-determined, simulation-based transform should work well for all future realizations of z's using the same grid (or, I believe, grids of similar size and shape).",2010-09-14 15:07:41.0,919.0,
3303,2639,0,"@ars: Yes. However A,B,C,D are not necessarily related.",2010-09-14 15:15:06.0,1102.0,
3304,2604,0,I'm certainly going to read up on the smooth tests of goodness of fit that you suggested though.  Thanks for the informative answers.,2010-09-14 15:31:55.0,54.0,
3305,2647,2,The distinction between discrete and continuous variables disappears from the point of view of measure theory.,2010-09-14 15:48:30.0,919.0,
3306,2639,0,"ok, I'm going to delete my 'answer', because I have no idea what your problem is and what you are trying to solve.",2010-09-14 16:31:54.0,795.0,
3307,2660,0,"Just in case it helps for further reading, link to the wikipedia article for KS test: http://en.wikipedia.org/wiki/K-S_Test",2010-09-14 16:59:56.0,251.0,
3308,2653,0,Thank you very much Chl :),2010-09-14 18:24:45.0,1251.0,
3309,2661,0,Is your question related to some extent to assessing model goodness of fit or the comparison of two nested models?,2010-09-14 19:28:38.0,930.0,
3310,2628,0,thanks for change the title Jeromy. better this way.,2010-09-14 19:29:12.0,1154.0,
3311,2661,0,"@chl, both, i have data and need to determine the proper fit and how well it fits",2010-09-14 19:43:01.0,559.0,
3312,2657,0,"I don't really understand how the conclusion you reached about the issue of tossing a coin is related to the question asked. Maybe you could expand a little bit on that point? Statistical tests seem to be relevant to the extent they help to infer observed results to a larger population, whether it be a reference or general population. The question here seems to be: Given that the sample is close to the population of test takers for a fixed period of time (here, one year), is classical inference the right way to reach a decision about possible differences at the individual level?",2010-09-14 19:43:49.0,930.0,
3313,2660,0,"Thank you, ars.  I figure anyone can search Wikipedia themselves, but you have made it more convenient by providing the link.",2010-09-14 20:07:26.0,919.0,
3314,2647,1,@whuber yes but an answer using measure theory is not that accessible to everyone.,2010-09-14 20:09:06.0,,user28
3315,2647,2,"@Srikant: Agreed.  The comment was for the benefit of the OP, who is a mathematician (but perhaps not a statistician) to avoid being misled into thinking there is something fundamental about the distinction.",2010-09-14 20:36:32.0,919.0,
3316,2663,2,"+1 for the sensible discussion; a few points though.  Inferential machinery is unavailable for population analysis, but in many modeling cases, I'd question whether one ever has *the* population data to begin with -- often, it's not very hard to poke holes.  So it's not *always* an appeal to a super population as the means to deploy inference.  Rather than ""super population"", the better way is to assume a data generating process yielding, for example, the year to year test taking cohorts in question.  That's where the stochastic component arises.",2010-09-14 20:41:59.0,251.0,
3317,2664,0,"This was an OR question to begin with :), i'm glad an elegant solution was found.",2010-09-14 21:10:16.0,603.0,
3318,2539,0,"Thanks for the response. Apologies for the delay in accepting your answer, I had not realized you had answered my questions.",2010-09-14 21:15:23.0,1216.0,
3319,2668,0,Here I am thinking of all of frequentist/bayesian probability and all of descriptive/exploratory/inferential statistics.,2010-09-14 21:57:29.0,1108.0,
3321,2663,1,"I don't think there as any disagreement here, except for the lack of inferential machinery for population analysis.  Randomization tests are applicable to populations and can reasonably test whether the data generating process is likely due to a random generating process versus a systematic generating process.  They do not assume random sampling and are a rather direct test of chance versus systematic variation.  Our traditional tests happen to stand in pretty well for them.",2010-09-14 22:45:07.0,485.0,
3322,2663,0,"That's true re: ""lack of inferential machinery"".  Careless wording on my part, especially since I liked the point you made about randomization tests in your answer.",2010-09-14 23:02:42.0,251.0,
3324,2667,0,+1 really nice question.,2010-09-15 00:09:05.0,352.0,
3325,2667,0,"I'm not immediately sure how to solve this, but it seems reasonable that an estimate of the cdf can still be obtained. It might not be continuous, but I don't see that as a huge problem.",2010-09-15 00:11:30.0,352.0,
3327,2641,0,"Wow, these are some _really_ good answers.  So a big thanks for that!  Some point soon, I'll pick one I particularly like as the ""accepted"" answer (although there are several that I think are equally deserved).",2010-09-15 01:13:24.0,386.0,
3328,2671,4,"For the formulas, you can get a jump start at http://meta.math.stackexchange.com/questions/480/math-markup-diagrams-etc-pointers-please .",2010-09-15 02:34:19.0,919.0,
3329,2664,0,when you get a chance please update your answer with the proof for the benefit of future readers.,2010-09-15 03:23:10.0,,user28
3330,2670,0,"You're assuming independence and a probability of 1/12 of being born in January. Neither are quite correct (e.g twins, triplets etc exist and have a tendency to be found together after birth, and the proportion of births that are in January isn't actually 1/12, though it's pretty close).",2010-09-15 03:24:38.0,805.0,
3331,2635,1,Your question is not clear. Could you please edit the text and add a bit more context?,2010-09-15 03:25:47.0,,user28
3332,2604,0,"Sorry about not realizing that the data didn't come into the choice of overdispersion parameter. There may be some argument for doing it your way, but if the external estimate is likely to reflect what you actually observe, the NB still may have some advantage depending on circumstances.",2010-09-15 03:28:14.0,805.0,
3333,2671,0,Thanks whuber :-),2010-09-15 03:47:32.0,521.0,
3334,2606,1,"This was the right answer for my particular situation, even though Glen_b's response helped me learn more.  So more upvotes for him, accepted answer for Srikant. Everybody wins - thanks all.",2010-09-15 04:35:43.0,54.0,
3335,162,2,"Spring force is proportional to the deformation, so this is not a least squares regression!",2010-09-15 05:31:36.0,795.0,
3336,162,1,"Nice try! Depends on the spring. For example, if the spring constant is 1/sigma, works great ;)",2010-09-15 05:54:10.0,74.0,
3337,2648,0,@kwak The data itself varies from time point to time point. The line of best fit is generally either stable or monotonically increasing.,2010-09-15 06:52:37.0,183.0,
3338,2670,0,@stat-teacher LaTeX works here. Just enclose formulas in dollar signs $...$,2010-09-15 07:08:27.0,183.0,
3339,2657,0,"@chl Yes, but it seems to OP is trying to infer an underlying probability of success. The tests compare the observed proportions to the theoretical distribution to determine if there is a difference for a given level of confidence. You are testing for any form of randomness, not just sampling error randomness.",2010-09-15 09:02:35.0,229.0,
3340,2675,2,"A little bit more detail would be ideal to get the question. At least, how does your data look like, who are the subscribers, what do they have to do with average time, and how is 'average time in service' defined?",2010-09-15 11:05:08.0,442.0,
3342,2675,0,"I agree with Henrik. The more precise your question is, the better the answer will be.",2010-09-15 12:12:02.0,8.0,
3345,2654,0,"My assumption is that a latent change occurs that triggers the initial rising process. I'm also interested in identifying the exact time point when this occurs. Thus, I see this as a true step function, whereas I would have thought a sigmoidal function could be parameterised to approximate this, but actually, it would still be continuous in the sense that it has a second derivative.",2010-09-15 14:10:04.0,183.0,
3346,2652,0,Thanks. I'll explore some of these models.,2010-09-15 14:17:19.0,183.0,
3347,2680,0,@Srikant: Would you mind explaining which two independent variables are being summed?,2010-09-15 14:39:47.0,919.0,
3348,2681,1,"@G. Jay: Could you elaborate a little on what ""standard and important result"" this is and how it applies in this situation?",2010-09-15 14:40:37.0,919.0,
3349,2673,0,"Great point!  Note, however, that this report would only be directly relevant for a room full of six and seven year olds ;-).  Seriously, if you are concerned about the statistics of the question (which I think is great), then it gets interesting once you realize people control birth month and even day of the week (doctors induce labor, etc.) and that over time the degree of control as well as what is favored can change.  But please don't take these remarks as quibbles: I'm only pointing out some intriguing consequences of the path you have indicated here.",2010-09-15 14:47:47.0,919.0,
3350,2648,0,"Jeromy:> then, i don't think a Sigmoid function is what you want (because of symmetry assumption built unto it).",2010-09-15 15:02:56.0,603.0,
3351,2680,1,I'd assume red and non-red cars.,2010-09-15 15:23:24.0,1287.0,
3352,2680,0,"@whuber @msalters yes that is correct. My answer addresses the OP's first qn. No of cars per hour is poisson. No of cars per hr = No of red cars per hr + No of non-red cars per hr. Hence, No of red cars per hr is poisson.",2010-09-15 15:26:18.0,,user28
3353,2682,0,"Sorry, I had thought it was clear that the color here should be taken as red or non-red. The chance of a car being red obviously doesn't depend on a previous car being blue or green, but does depend on the previous car being red.",2010-09-15 15:29:43.0,1287.0,
3354,2680,0,"Hadn't realized it this explicitly, but I can indeed get at the results I need by treating my ""all cars"" not as an independent variable, but as the sum of two variables with related distributions. Then σ(cars) = σ(red cars) + σ(non-red cars) = σ(red cars) * (1/p(red))",2010-09-15 15:37:19.0,1287.0,
3355,1924,0,"The difference between $I$ and $H$ is that $H$ is expected second derivative, whereas I is expected product of first derivatives. $I$ is not always the same as $-H$, for instance, for uniform distribution",2010-09-15 15:39:12.0,511.0,
3356,2681,0,"Sure.  I said ""standard"" because I have seen it usually covered in the first few chapters of books on stochastic processes. (Didn't mean to seem snooty;  hope it didn't come off that way).  I said ""important"" because a) Poisson processes form the foundation for renewal processes in general (where interarrival times are not necessarily exponentially dist'd but are independent with dist'n F), and b) decomposition of Poisson processes in particular gives, for instance, a handy way to  simulate continuous time Markov chains with a computer.  Durrett has a nice discussion of this point.",2010-09-15 15:45:31.0,1108.0,
3357,2681,0,"By the way, as whuber noticed, the process determining which cars are ""red"" and which ones ""not-red"" should be independent of the Poisson process;  otherwise, all bets are off.  :-)  The OP did mention independence, but it can be tricky to make things perfectly clear.",2010-09-15 15:48:58.0,1108.0,
3358,2680,0,@Srikant: Cool!  Thanks for the reference to Raikov's Theorem.  (I might have spent a lot of time trying to think of a counterexample otherwise...),2010-09-15 15:50:26.0,919.0,
3359,2673,0,"@whuber I agree with you, my post was rather simplistic (I just found it interesting!) Two other points of interest. First, in the report it gives different birth rates classified by race. Secondly, I'm a bit confused why the Monday birth rate isn't much higher (since the rate is low over the weekend). Instead, it seems Tuesday is the day of choice.",2010-09-15 15:54:05.0,8.0,
3360,2681,0,@G. Jay: Thank you!,2010-09-15 15:56:33.0,919.0,
3361,162,2,"no, no, the point is that in static equilibrium, the sum of forces would be zero; assuming equal spring constants, you would be minimizing the sum of absolute deviations, i.e. $L_1$ regression, not least squares. This ignores the fact that the springs would have to be freely floating on the stick, so they would shift so that deformation would not be entirely in the $y$ direction, resulting in something like a Principal Components fit, but with absolute errors.",2010-09-15 16:19:10.0,795.0,
3363,2667,0,"Like Robin says, definition of entropy depends on choice of measure. For discrete distributions it's clear-cut, but for continuous there's no agreement on the right measure to use, so you have a spectrum of different entropies. Perhaps you should ask why you need entropy in a first place, ie, what exactly do you expect to learn from it?",2010-09-15 17:06:25.0,511.0,
3364,1693,0,"thanks, useful links",2010-09-15 17:11:21.0,511.0,
3366,2228,0,"Its not only lme4. this is the instalation output now: Attaching package: 'Matrix'

The following object(s) are masked from 'package:reshape':

    expand

The following object(s) are masked from 'package:base':

    det


Attaching package: 'lme4'

The following object(s) are masked from 'package:stats':

    AIC",2010-09-15 17:26:59.0,1084.0,
3369,2673,0,"would *you* (as a hypothetical obstetrician) like to come in on Monday mornings to deliver babies, lol?",2010-09-15 18:14:19.0,919.0,
3375,2106,0,"updated ez, but still doesn't work 2: Anova() failed for some reason (maybe there are too few Ss?). should having too few Ss matter?",2010-09-15 19:17:40.0,1084.0,
3376,2228,0,"now it works, not sure what fixed it.",2010-09-15 19:18:06.0,1084.0,
3378,2644,0,"I'm using a hash function to determine the integral points, which is why they have uniform distribution already. Continuity is important, however, between the points. I don't want completely random noise.",2010-09-15 19:26:33.0,1267.0,
3379,2688,0,"What R package did you use for HC3 estimation? `sandwich`, `contrast`?",2010-09-15 19:46:25.0,930.0,
3380,2617,0,"I generated 10 billion numbers, found the best fit Normal distribution, and applied the inverse probability transform. It seems to work, but my data's not quite normal.",2010-09-15 19:46:28.0,1267.0,
3381,2688,0,"Another question while we are in: What is the design you are considering, I mean is there any clustering or multiple predictors, or is it a simple linear regression? This may help the reader to better understand the context of your study.",2010-09-15 20:47:17.0,930.0,
3382,2695,4,That's very layman ;-),2010-09-15 21:24:57.0,88.0,
3383,2688,0,Have you tried re-expressing the dependent variable to stabilize variance?,2010-09-15 21:34:23.0,919.0,
3384,2682,0,@MSalters: thank you for the clarification.  I'll leave my answer up for amusement value ;-).,2010-09-15 21:35:11.0,919.0,
3385,2691,30,"Good question. I agree with the quote as well. I believe there are many people in statistics and mathematics who are highly intelligent, and can get very deep into their work, but don't deeply understand what they are working on. Or they do, but are incapable of explaining it to others.I go out of my way to provide answers here in plain English, and ask questions demanding plan English answers.",2010-09-15 21:43:29.0,74.0,
3387,333,0,Looks like Louis Bachelier started it all in 1900 http://en.wikipedia.org/wiki/Louis_Bachelier,2010-09-15 21:58:06.0,74.0,
3388,2699,1,They have ratings break downs by gender and age groups as well.  I suspect that they may average within those groups and then taking the average of the group means for their reported score.,2010-09-15 22:04:57.0,196.0,
3389,2699,0,"@drnexus Yes, good point about age, but I'm not sure about gender...",2010-09-15 22:07:21.0,8.0,
3390,2686,1,Hint: being more concise will probably help you get an answer.,2010-09-15 22:09:49.0,88.0,
3394,2548,0,@whuber (+1) ...about the first remark.,2010-09-15 22:41:15.0,88.0,
3395,2636,0,"I do not think you can infer age is log-normally distributed just from the fact that it is always positive. The gamma and the Weibull distributions are also always positive, so why not choosing those ones?",2010-09-15 23:08:27.0,582.0,
3396,2695,2,"It's a little mathy, but the best way to understand something is to derive it.",2010-09-16 01:08:28.0,7.0,
3397,2695,15,You have an exceptionally well-educated grandmother :-).,2010-09-16 01:44:58.0,919.0,
3398,2703,1,"Welcome to the stats site @Joel!  If you get a chance, you might also contribute to the discussion on our proposed distributed StackExchange data analysis project: http://stats.stackexchange.com/questions/2512/what-should-be-our-first-polystats-project.",2010-09-16 02:11:33.0,5.0,
3400,2687,1,"The mean at different levels of the repeated measures factor could change, even when r = 1.0 between scores at different time points.",2010-09-16 03:14:23.0,183.0,
3402,2674,0,very good. I'll sum the two and see how that works!,2010-09-16 04:45:18.0,795.0,
3403,2691,4,"This was asked on the Mathematics site in July, but not as well and it didn't get many answers (not surprising, given the different focus there).  http://math.stackexchange.com/questions/1146/intuitive-way-to-understand-principal-component-analysis",2010-09-16 05:03:44.0,919.0,
3404,2688,0,I´m using the sandwich package for the HC3 estimator. I´m using just simple linear regression. Intuitively I feel more comfortable with the bootstrap version and I guess I´ll stick with that. //thx for the input,2010-09-16 05:19:58.0,1291.0,
3405,2701,0,"Note that it's often not possible to calculate the mean (without making parametric assumptions about the distribution) as the longest observed time is usually too short for the survival curve to reach zero, often it's not even close. You can estimate some of the quantiles, such as the median time, however, though which ones depends on the proportion who experience the event.",2010-09-16 06:09:56.0,449.0,
3408,2654,0,"@Jeromy I'm still not sure why a sigmoidal function isn't appropriate without seeing the shape of the data, but you might find this paper useful: http://www.unc.edu/~jbhill/tsay.pdf",2010-09-16 09:23:34.0,229.0,
3409,2698,1,"To add to this, when you have (near-)equal semiaxes (i.e. the ellipsoid has a (near-)circular slice), it indicates that the two pieces of data corresponding to those axes have (near-)dependency; one can talk about principal axes for an ellipse, but circles only have one radius. :)",2010-09-16 09:43:26.0,830.0,
3410,2712,0,"Thanks, but I'm trying to avoid 'confidence interval'. I seek for some formal distance metric to interpret error. I know what I'm gone pick, I just need metric 'how sure am I' (in %).",2010-09-16 09:50:22.0,1313.0,
3412,2712,1,"It seems you need some since I'm not familiar with, so I hope someone else will give you a more satisfying answer; nevertheless getting/assuming distribution would be a inevitable starting point for such an analysis.",2010-09-16 11:46:28.0,88.0,
3413,2717,1,You should consider adding the information that you have already tried UPGMA (and others that you may have tried) :),2010-09-16 11:49:07.0,977.0,
3416,2717,0,done...........,2010-09-16 12:15:38.0,1316.0,
3418,854,2,"I agree that computing resources allow to easily perform permutation and exact tests, or long runs of Monte Carlo simulations, but here is a paper (from *Statistics in Medicine*) where the authors argued against the systematic use of Fisher's exact test and rather recommended choosing the test according to the question that is posed: http://bit.ly/9qflht.",2010-09-16 12:20:34.0,930.0,
3420,2680,0,Pardoxically the Central Limit Theorem states that if you keep adding independent identically distributed Poisson variables the distribution will converge to a normal distribution. Go figure... http://en.wikipedia.org/wiki/Central_limit_theorem,2010-09-16 13:11:34.0,521.0,
3421,2712,0,"I have already used mine metric.
Thanks in any case.",2010-09-16 13:41:34.0,1313.0,
3422,2680,0,@thylacoleo I do not think there is a paradox because of two reasons. CLT applies to a suitable normalization of the sum and not to the sum itself. You need to control the growth in the numerator by a suitable scale factor as otherwise there is no convergence.,2010-09-16 13:46:44.0,,user28
3426,2675,0,"Average time in service is simply the number of days from when they first appear in our system, to when they choose to leave.  The data available is the subscriber identifier, the current state (i.e. whether or not they've left yet) and the date they joined (and also the date they left if applicable)",2010-09-16 14:08:10.0,1284.0,
3427,2678,0,"Excellent thanks, i'll read up on that shortly.",2010-09-16 14:09:54.0,1284.0,
3428,2698,3,"I would be more cautious here, J.M.  First, just to clarify, by ""near-dependency"" you must mean ""nearly independent.""  This would be true for a multinormal variate, but in many cases PCA is performed with data that are markedly non-normal.  Indeed, the clustering analyses that follow some PCA calculations can be viewed as one way to assess a strong form of non-normality.  Mathematically, circles *do* have principal axes, but they are just not uniquely determined: you can choose any orthogonal pair of radii as their principal axes.",2010-09-16 14:11:11.0,919.0,
3429,2712,0,"Consider adding your solution as an answer; it is fair to do so, users that will read this in future will get the solution and you could earn some reputation.",2010-09-16 14:18:51.0,88.0,
3430,2711,4,"Requests to ""avoid explanation"" and a reluctance to use probability statements suggest an intention to use statistics to sanctify a result rather than understand or improve it.  That can make for a difficult conversation, because statisticians do not share this point of view.",2010-09-16 14:20:47.0,919.0,
3431,2678,0,Is there any software around to assist in the analysis?,2010-09-16 14:26:11.0,1284.0,
3432,2729,0,pam actually gives that out of the box (when using plot on the pam).  I'm still open for other solutions :),2010-09-16 15:53:27.0,253.0,
3433,2729,0,"@Tal I know, but you haven't mentioned it.",2010-09-16 16:07:23.0,88.0,
3434,2698,1,"Yes, sorry, I suppose ""the principal axes of a circle are indeterminate"" would have been a better way of putting it.",2010-09-16 16:13:22.0,830.0,
3435,2701,0,"@onestop: thanks, I fixed my answer to note the mean/censored issue.",2010-09-16 16:23:59.0,251.0,
3436,1653,4,"@AndyF Not `lm()`, unless you move to mixed-models with the `nlme` or `lme4` package, but there's a handy way to handle repeated-measurements through appropriate specification of the `Error` term in `aov()`, see more details on Baron & Li tutorial, §6.9, http://j.mp/c5ME4u",2010-09-16 17:25:46.0,930.0,
3437,1723,0,"@aL3xa I think the randomization approach is better appropriate given your research field; notwithstanding the fact that usual parametric tests provide good approximation to exact permutation tests, non-parametric tests also imply some kind of assumption (e.g. on shape of the distribution). I even wonder how we might really define what is a deviation from normality in small-sample study. I think you should ask for further discussion about this point in a separate question.",2010-09-16 17:35:55.0,930.0,
3438,2663,0,Thanks a lot for your answers.,2010-09-16 18:52:03.0,1154.0,
3439,2663,0,sorry. I have difficulties to understand how i would compute permutations and what conclusions I'll be able to made for them.,2010-09-16 18:53:16.0,1154.0,
3440,2700,1,+1 for really caring to answer.,2010-09-16 18:55:35.0,851.0,
3441,2686,0,I agree with you entirely. My question is long and rambling but that is my problem. I do not really know what my question is. Can you help me?,2010-09-16 19:11:24.0,104.0,
3442,2738,0,Thanks gd047 a good answer indeed.,2010-09-16 19:19:45.0,253.0,
3443,2731,0,"Thanks ars, I didn't know of this one: +1",2010-09-16 19:20:11.0,253.0,
3444,2729,0,"True, I simply forgot:  +1  :)",2010-09-16 19:20:34.0,253.0,
3445,2741,0,"So a set of ""If-Then"" rules is basically a forest of single-level decision trees (i.e., single binary splits on a single predictor)?",2010-09-16 19:30:28.0,1121.0,
3446,2741,1,"@user1121 Yes, but only in case of binary classification.",2010-09-16 19:35:45.0,88.0,
3447,2729,2,Also I still think that this is a very good method.,2010-09-16 19:39:16.0,88.0,
3448,2716,0,"Hey Peter! Good to see you here. This is a really good, simple, no math answer.",2010-09-16 19:40:02.0,29.0,
3449,749,0,"Indeed, this is too much!",2010-09-16 20:49:35.0,930.0,
3450,2748,1,It might help to explain more about your study design: how are cases and controls sampled?,2010-09-16 20:58:19.0,449.0,
3451,2746,0,Perhaps you could indicate what PSD acronym stands for?,2010-09-16 21:03:21.0,930.0,
3452,2746,0,positive semi definite?,2010-09-16 21:05:42.0,795.0,
3453,2738,2,"@Tal Galili You are welcome Tal! If you find out some other method for choosing the number of clusters, share it with us.",2010-09-16 21:12:05.0,339.0,
3454,2751,0,"Well, we answer quite at the same time. Nice reference for `metan`!",2010-09-16 21:12:52.0,930.0,
3455,2746,0,"@shabbychef Well, let's go this way; I was thinking of this too but others might not grasp the title at first sight...",2010-09-16 21:14:42.0,930.0,
3456,2729,0,I'll look into it more then - thanks :),2010-09-16 21:19:10.0,253.0,
3457,2746,0,Correction added!,2010-09-16 21:22:52.0,1250.0,
3458,796,1,Meta-response: Good point.  But your data might not behave like mine! :-),2010-09-16 23:56:56.0,919.0,
3459,2751,0,Thansk everyone - I only have one group - I'm trying to calculate Orwin's (1983) modification of Rosenthal's Failsafe N and it seems like this is only possible using Cohen's d. Any suggestions?,2010-09-17 00:07:51.0,,Jay
3460,2654,0,Thanks the paper looks good,2010-09-17 02:13:13.0,183.0,
3461,2761,0,"I'm sorry, what distinction are you making between the large sample formula and the small sample formula?  Are you calculating the variances using a population formula in large samples rather than using a sample estimate of the population variance?",2010-09-17 05:53:05.0,196.0,
3462,2772,0,do you have a link to the articles (free pdf or publisher)?,2010-09-17 07:16:12.0,183.0,
3463,2739,1,Do this question and the answer refer solely to machine-learning?,2010-09-17 08:12:33.0,442.0,
3464,2752,0,+1 nice links. Thanks.,2010-09-17 08:15:31.0,442.0,
3465,2765,0,"Howell is very good, but not very mathematical.",2010-09-17 08:30:35.0,442.0,
3467,2712,0,"OK, I will do that. Thanks!",2010-09-17 08:45:05.0,1313.0,
3468,2780,0,"+1 Great link, I missed this reference -- so I guess now it can be handled within R without any problem :) I put the PDF at this address in case someone want to read it: http://j.mp/bnnwsu",2010-09-17 09:08:23.0,930.0,
3469,2781,1,Thanks. The idea of using a clustering procedure had occurred to me. I imagine the challenge would be to adequately capture and weight the possible individual-level curve features in a theoretically meaningful way. I'll have a look at see how it works in kml.,2010-09-17 09:08:49.0,183.0,
3470,2781,1,"Well, it works pretty well although the interface is awful (and I know the guy who build it :) -- I used it two months ago for separating clinical groups based on individual profiles on developmental measurements (Brunet-Lézine).",2010-09-17 09:12:09.0,930.0,
3471,2772,0,@Jeromy. Links added.,2010-09-17 10:59:02.0,159.0,
3473,2781,0,Thank you for the functional data analysis link.,2010-09-17 11:16:18.0,919.0,
3474,2775,0,"If the entries are generated from a Normal distribution rather than a uniform, the decomposition you mention ought to be SO(n) invariant (and therefore equidistributed relative to the Haar measure).",2010-09-17 11:47:03.0,919.0,
3475,2781,1,Here's another primary reference for FDA: http://www.psych.mcgill.ca/misc/fda/,2010-09-17 12:02:37.0,364.0,
3476,2787,0,What would be the goals of qualitative and quantitative testing?,2010-09-17 13:25:28.0,,user28
3477,2789,1,"I have never heard of this approach, but it seems somehow reasonable. But, ""experiment"" should rather be a random factor, than a fixed one. These two experiments are just two randomly drawn from the population of possible experiments with this design. They are in no way fixed.",2010-09-17 13:32:51.0,442.0,
3478,2789,0,"Ah, you're indeed correct that experiment should be a random factor. I'll update the answer to reflect this. Thanks!",2010-09-17 13:35:03.0,364.0,
3479,2792,4,"Baayen's textbook, *Practical Data Analysis for the Language Sciences with R*, may still be found at http://bit.ly/aQ1KGb",2010-09-17 13:59:15.0,930.0,
3480,2775,0,Interesting. Can you provide a reference for this?,2010-09-17 14:39:46.0,30.0,
3481,2786,0,M. : Nice reference: This appears to be the most efficient solution (asymptotically).,2010-09-17 14:46:17.0,919.0,
3483,2792,0,"Thanks. I just had it in a kind of unusable Version with ""Draft"" in the background.",2010-09-17 17:34:43.0,442.0,
3484,2765,0,It's plenty mathematical. More computational.,2010-09-17 18:30:26.0,1334.0,
3487,2786,0,"@whuber: Heh, I picked it up from Golub and Van Loan (of course); I use this all the time to help in generating test matrices for stress-testing eigenvalue/singular value routines. As can be seen from the paper, it's essentially equivalent to QR-decomposing a random matrix like what kwak suggested, except that it is done more efficiently. There's a MATLAB implementation of this in Higham's Text Matrix Toolbox, BTW.",2010-09-17 19:14:47.0,830.0,
3488,2772,2,"0  down vote
	

Gappy:> this is not an answer to your question, but an alternative, more recent, and often more potent in out of sample forecasting, approach to this problem:

Large Bayesian VARs, see this recent paper http://ideas.repec.org/p/cpr/ceprdp/6326.html",2010-09-17 19:56:02.0,603.0,
3489,2786,0,M.:> Thanks for the matlab implementation. Would you by any chance know of a Haar pseudo-random matrix generator in R?,2010-09-17 19:58:44.0,603.0,
3490,2786,0,"@kwak: No idea, but if there's no implementation yet, it shouldn't be too hard to translate the MATLAB code to R (I can try to whip one up if there really isn't any); the only prerequisite is a decent generator for pseudorandom normal variates, which I'm sure R has.",2010-09-17 20:06:23.0,830.0,
3491,2786,0,"M.:> yes i'll probably translate it my self. Thanks for the links, Best.",2010-09-17 20:13:19.0,603.0,
3493,2740,0,"OK - I think this is the right conceptual answer - I hadn't appreciated the difference between the effect of the correlation across repeated measures on the within group power (improved) and between groups power (degraded).  Of course, given that this is a clinical trial, what I really care about is the power to detect a group x time interaction.  I believe I should be using in g*power the ""repeated measures, within-between interaction"" model, which does show increasing power with increasing correlation over the repeated measure.",2010-09-17 23:11:20.0,1290.0,
3495,527,2,"@robin: ""matrix"" in this context is standard analytical chemistry terminology; it refers to the medium where the entities to be analyzed for (the ""analytes"") can be found. For instance, if you're analyzing for the concentration of lead in tap water, lead is the analyte, and water is the matrix.",2010-09-18 00:49:04.0,830.0,
3496,2806,2,"There are loads of public SVD algorithms. See http://en.wikipedia.org/wiki/Singular_value_decomposition#Implementations. Can't you use or adapt one of them? Also, R is open-source, and under a GPL licence, so why not borrow its algorithm if it does the job?",2010-09-18 03:16:10.0,159.0,
3497,2806,0,"@Rob:  I'd like to avoid practically writing a linear algebra library, and I also want to avoid the GPL's copyleft.  Also, I've looked at bits and pieces of the R source code before and it's generally not very readable.",2010-09-18 03:25:47.0,1347.0,
3498,2810,0,The Wikipedia algorithm cites this and is equivalent to this for the case of finding one principal component at a time.,2010-09-18 03:26:12.0,1347.0,
3499,2806,2,Am I missing something? You have >10K features but <1K samples? This means the last 9K components are arbitrary. Do you want  all 1K of the first components?,2010-09-18 03:59:16.0,795.0,
3500,2806,1,"In any event, you can't escape having to implement SVD, though thanks to much numerical linear algebra research, there are now a lot of methods to choose from, depending on how big/small, sparse/dense your matrix is, or if you want just the singular values, or the complete set of singular values and left/right singular vectors. The algorithms are not terribly hard to understand IMHO.",2010-09-18 05:39:04.0,830.0,
3501,1921,1,"Nice reference! In the same vein, Categorical data analysis: Away from ANOVAs. (transformation or not) and towards logit mixed models. T. Florian Jaeger (JML 2008 59), http://j.mp/a8h763.",2010-09-18 07:42:54.0,930.0,
3502,2799,0,+1 the classic.,2010-09-18 08:21:23.0,442.0,
3503,2680,0,@Srikant I'm pretty sure the CLT applies to sums. http://cnx.org/content/m16948/latest/,2010-09-18 08:28:43.0,521.0,
3504,2680,0,"@ Srikant I'm pretty sure the CLT applies to sums. http://cnx.org/content/m16948/latest/  I think the paradox is resolved because the mean of the sum of n iid Poisson distributions is n*λ, whilst the SD  is sqrt(n*λ). See ""Sums of Poisson-distributed random variables"" http://en.wikipedia.org/wiki/Poisson_distribution#Properties  And for large  λ (say >1000) the Poisson can be approximated as the normal distribution with mean λ and variance λ. http://en.wikipedia.org/wiki/Poisson_distribution#Related_distributions",2010-09-18 08:36:02.0,521.0,
3506,2782,4,"Is it a quotation? I'm just wondering how trying alternative testing procedures (not analysis strategies!) may not somewhat break control of Type I error or initial Power calculation. I know SAS systematically returns results from parametric and non-parametric tests (at least in two-sample comparison of means and ANOVA), but I always find this intriguing: Shouldn't we decide before seeing the results what test ought to be applied?",2010-09-18 09:57:47.0,930.0,
3507,1701,0,I used the kmeans R function with a random initialization so the results were different for each clustering attemp. Now I am reading the paper suggested above by Suresh.,2010-09-18 10:19:05.0,221.0,
3509,2636,0,@Rob: @nico: I'm sure you're right. It was a poor choice of example. Typically we model pharmacometric parameters like volume and clearance.,2010-09-18 12:53:10.0,1270.0,
3510,2820,1,"Hi Tal, thanks! Bonferroni does not seem appropriate to me - if one of my SNPs *is* causal and others are associated with it, there should be a signal, and Bonferroni has always looked too conservative to me (I usually prefer Holm's stepwise correction). The FDR you link to and p.adjust do not consider *combined* evidence (and the FDR still requires me to understand the correlation of my tests, the original question). multcomp may help, though at first glance it seems like it deals more with multiple tests within a *single* model, whereas I have *multiple* models. I'll dig deeper...",2010-09-18 14:12:53.0,1352.0,
3511,2796,0,"Lot's of things are known about Fisher Information, it's integrals of fisher information that I'm not sure about. I'm not familiar with what you say about Fisher Information turning into some known divergence on integration",2010-09-18 14:43:09.0,511.0,
3512,2822,1,"Wow, thanks for going to all this trouble! I understand your qualms about bootstrapping, and I'm almost convinced. I think my main complication is the numerical covariate I have that will certainly be necessary (either by itself or in interaction with genotype), and that seems to rule out mt.maxT and plink, although I may need to look into plink again. But I will certainly dig through the references you provided!",2010-09-18 15:56:52.0,1352.0,
3513,2822,0,"You can always work with the residuals of your GLM to get ride of your covariates, though you lost some Df that will be difficult to account or reintroduce afterwards (e.g. for computing p-value).",2010-09-18 16:08:45.0,930.0,
3514,604,0,I also don't get how you get perfect fit with a line -- what if your points don't lie on any line?,2010-09-18 18:06:19.0,511.0,
3515,2822,0,"Hm, residuals from my logistic regression? Would that be legitimate?",2010-09-18 18:09:35.0,1352.0,
3516,2822,0,"Yes, why not? It is not uncommon to remove the variance accounted for by other covariates and then move on 2nd-level analysis with your residualized data. It is often faster (for instance, plink is pretty slow with categorical covariates, while it's ok with continuous ones; `snpMatrix` or simply `glm()` performs quite better on this point but you cannot embed a lot of SNPs within `glm()`...); the problem is that getting the corrected p-value at the end of your 2nd analysis is rather tricky (because you have to account for the parameters already estimated).",2010-09-18 18:27:33.0,930.0,
3517,2822,0,"For an illustration of how people are working with residuals, see for example p. 466 of Heck et al. Investigation of 17 candidate genes for personality traits confirms effects of the HTR2A gene on novelty seeking. *Genes, brain, and behavior* (2009) vol. 8 (4) pp. 464-72",2010-09-18 18:31:55.0,930.0,
3518,2809,0,Thanks for the links; MASAL approach looks interesting.,2010-09-18 18:35:19.0,930.0,
3519,2811,0,Thanks for the article of Muthén!,2010-09-18 18:36:32.0,930.0,
3520,2822,0,"I think the Nyholt's paper is worth looking at. I'm just re-reading it and it seems to provide good reference and a well-founded framework. Maybe you'll have to check if there isn't more recent publications on this topic, e.g. http://j.mp/9jHTpI but I just look at the abstract.",2010-09-18 18:39:35.0,930.0,
3521,2822,0,"Last but not least, Nicodemus has been very active for promoting the use of Random Forests when working with SNPs, and RFs are great because you get measure of variable importance and % of classification error on resampled subjects, see e.g. http://j.mp/cstgeO. There is also a paper on how to use haplotype block instead of PCA like suggested in the aforementioned paper, http://j.mp/c4V45i",2010-09-18 18:46:08.0,930.0,
3522,2826,0,"Thanks. Unfortunately, few of my data are numeric, but this is a useful phenomenon to know about. Incidentally, there was a nice programme about this on BBC Radio 4 a few years ago - looks like it's still available in RealAudio: http://www.bbc.co.uk/radio4/science/further5.shtml",2010-09-18 19:24:44.0,1343.0,
3523,2778,0,"I have found the book by Maxwell and Delaney, and having read already 20-30 pages, I must say it is very nice... I'll keep on reading and I think I'll find the answers I'm looking for, thanks!",2010-09-18 19:58:22.0,1320.0,
3524,2820,0,"Hello Stephan.  I understand, sorry for not helping more.  Good luck!  Tal",2010-09-18 20:14:01.0,253.0,
3525,2810,0,"OK, I see the link now.  This is a fairly simple approach, and like Wikipedia mentions, there are advances upon this basic idea.  On reflection though, you're going to have to deal with some kind of trade-offs (convergence in this case).  I wonder if you're asking the right question here.   Are there really no good bindings to linalg libraries for D?",2010-09-18 20:26:54.0,251.0,
3526,2828,0,can you give more information about what attributes are available about the models?,2010-09-18 20:42:21.0,795.0,
3527,1815,2,See also this question  (asked with specific reference to designining clinical trials): http://stats.stackexchange.com/questions/2770/good-text-on-clinical-trials/,2010-09-18 21:02:27.0,266.0,
3528,2828,0,"This is kind of an open question, so my question would be -- what kind of attributes do I need to be able to measure complexity? At the most basic level, a probability model is a set of probability distributions, and I fit the model to data by picking the best fitting member",2010-09-18 21:32:21.0,511.0,
3529,2835,0,you don't need to tweak it. Quantreg will happily digest irregularly spiced time series.,2010-09-18 21:57:54.0,603.0,
3530,2214,0,We need a finite mean (first moment) to compute an L-estimator (don't we?). Levy distributed r.v. do not come with such niceties. Correct me if i'm wrong.,2010-09-18 22:02:30.0,603.0,
3531,2835,0,"No doubt about that. But does it do *local* quantile regression? - If not, one can play around with the weights and just calculate lots of models",2010-09-18 22:06:46.0,1352.0,
3532,2830,0,"Hm...the paper is all about regression, I wonder if this can be used for discrete probability estimation. Also, I don't really understand the motivation he gives for it -- gdf is a degree of sensitivity of parameters to small changes in data, but why is it important? I could choose a different parameterization where small changes in parameters in original parameterization correspond to large changes in the new parameterization, so it'll seem more sensitive to data, but it's the same model",2010-09-18 22:07:39.0,511.0,
3533,2830,0,"Yaroslav:> *I could choose a different parameterization where small changes in parameters in original parameterization correspond to large changes in the new parameterization, so it'll seem more sensitive to data * can you give an example (involving an affine equivariant estimator)? Thanks,",2010-09-18 22:24:39.0,603.0,
3534,2830,1,"DoF in linear regression work out to the trace of the hat matrix or the sum of sensitivities -- so the motivation/concept aren't all that far out.  Tibshirani & Knight proposed Covariance Inflation Criterion which looks at covariances of model estimates instead of sensitivities.  GDF seems to have been applied in a number of model procedures like cart and wavelet thresholding (Ye's paper on adaptive model selection has more details), and in ensemble methods to control for complexity, but I don't know of any discrete estimation cases.  Might be worth trying ...",2010-09-18 23:11:29.0,251.0,
3535,2830,0,"Don't know about ""affine equivariant estimators"", but suppose we rely on maximum likelihood estimator instead. Let q=f(p) where f is some bijection. Let p0,q0 represent MLE estimate in corresponding parameterization. p0,q0 are going to have different asymptotic variances, but in terms of modeling data, they are equivalent. So the question comes down to -- in which parameterization is the sensitivity of parameters representative of expected risk?",2010-09-18 23:15:55.0,511.0,
3536,2833,0,"Good advice most of the time, but what about the case of large datasets, where you can't feasibly look through all the data manually?",2010-09-19 00:51:01.0,1347.0,
3537,2811,0,"@chl, No problem, Thank you for all the resources you listed here.",2010-09-19 03:25:56.0,1036.0,
3538,2817,1,Good grief... constructing the covariance matrix is never the best way for SVD. I displayed an example of why explicitly forming the covariance matrix is not a good idea at math.SE: http://math.stackexchange.com/questions/3869/3871#3871 .,2010-09-19 03:30:04.0,830.0,
3539,2782,3,"@chl good point. I agree that the above rule of thumb can be used for the wrong reasons. I.e., trying things multiple ways and only reporting the result that gives the more pleasing answer. I see the rule of thumb as useful as a data analyst training tool in order to learn the effect of analysis decisions on substantive conclusions. I've seen many students get lost with decisions particularly where there is competing advice in the literature (e.g., to transform or not to transform) that often have minimal influence on the substantive conclusions.",2010-09-19 05:45:50.0,183.0,
3540,2106,0,What's the error message?,2010-09-19 07:06:42.0,966.0,
3541,2811,0,"Good point about latent groups. I've seen several applications of latent class analysis & cluster analysis where it seems to be just carving up a continuous variable int categories such low & high (http://jeromyanglim.blogspot.com/2009/09/cluster-analysis-and-single-dominant.html). However, I do have some individual-level longitudinal data which visually look like they are coming from  categorically distinct data generating processes (e.g., always high, always low, gradual increasing, low-then-abrupt-increase, etc.) and within categories there is more continuous variation of parameters.",2010-09-19 09:22:23.0,183.0,
3542,2844,0,"For the 2nd question, you mean the eigenvectors or the component loadings summary?",2010-09-19 09:40:04.0,930.0,
3543,2844,0,"Hi chl - I mean the output from ""summary(pc.cr)"" - for some reason, I can't find it. (doing something like summary(pc.cr)[[1]] will get me only part of the table)",2010-09-19 09:44:07.0,253.0,
3544,2782,0,@chl no it's not a quotation. But I thought it was good to demarcate the rule of thumb from its rationale and caveats. I changed it to bold to make it clear.,2010-09-19 10:11:22.0,183.0,
3545,2781,1,"I found this introduction to FDA link by Ramsay (2008), particularly accessible http://gbi.agrsci.dk/~shd/public/FDA2008/FDA_Sage.pdf",2010-09-19 10:27:29.0,183.0,
3546,2782,1,"Ok, it makes sense to me to try different transformations and look if it provides a better way to account for the studied relationships; what I don't understand is to try different analysis strategies, although it is current practice (but not reported in published articles :-), esp. when they rely on different assumptions (in EFA vs. PCA, you assume an extra error term; in non-parametric vs. parametric testing, you throw away part of the assumptions, etc.). But, I agree the demarcation between exploratory and confirmatory analysis is not so clear...",2010-09-19 10:29:29.0,930.0,
3547,2735,0,Thanks for the link!,2010-09-19 10:36:25.0,1119.0,
3548,2814,0,"Indeed this is the same proof as the one provided by Paul Rubin on or-exchange.com. Multiplying each correlation by a constant has a very significant drawback though - it is a very very small stress test. It is possible to apply a much larger stress test by adding a constant to each matrix, not multiplying. However, it seems the analytical solutions for that are a little less obvious.",2010-09-19 12:15:13.0,1250.0,
3549,2809,0,Thanks for the article and R package links,2010-09-19 12:55:56.0,183.0,
3550,2807,0,Electrical potential difference is an example of a real-world quantity that can be negative.,2010-09-19 13:03:21.0,582.0,
3551,2833,1,"@dsimcha It also depends of sample size per group. It is known, for example, that when the samples are of equal size the t-test  is robust against departure from the homoscedasticity assumption; if $n_1\\neq n_2$, then the probability of a type I error will be $<\\alpha$ if the larger $\\sigma^2$ is associated with the larger sample, and *vice versa*. See Zar, JH *Biostatistical Analysis* (4th Ed., Prentice Hall, 1998) for further references.",2010-09-19 13:17:37.0,930.0,
3552,2828,3,"What, precisely, is ""complexity""?  (This is not a flippant question!)  In the absence of a formal definition, we cannot hope to make valid comparisons of something.",2010-09-19 13:30:33.0,919.0,
3553,2415,1,"Concerning the first bullet (drug trials): even people who otherwise might not be interested in medical experimentation should read the NYTimes article *New Drugs Stir Debate on Basic Rules of Clinical Trials* (http://www.nytimes.com/2010/09/19/health/research/19trial.html?pagewanted=1&_r=1&th&emc=th ).  The statistically literate reader will immediately see the unstated implications concerning experimental design and using p-values for decision making.  There is a statistical resolution, somewhere, to the life-and-death conundrum described in this article.",2010-09-19 13:35:16.0,919.0,
3554,2852,0,Could you explain how data were collected and what they intend to measure? Are the different columns of your data matrix independent measurements?,2010-09-19 14:23:14.0,930.0,
3555,2849,0,what's the other dimension of the panel (i.e. N)?,2010-09-19 14:41:27.0,603.0,
3556,2852,0,Thank you. These are biological data and each column are indepent data under the same measure method. I only want to know the difference between the first column band each one of others.,2010-09-19 15:52:51.0,,Chuangye
3557,2849,0,"thanks Jeromy for your reply,my second dimension is N=16.",2010-09-19 16:27:19.0,1251.0,
3558,1653,0,@chl Thanks - I suspected lm couldn't do it.,2010-09-19 16:31:31.0,702.0,
3559,2849,0,Sorry i mean Kwak,2010-09-19 16:36:57.0,1251.0,
3560,2849,0,"Ama>: imho that's much too little. Even if the observations were drawn from a single process (i.e. a single run of 4*16=64 *differentiated* observations), the granger test for causality would probably require more data points. Now in your case you would also have to account for the loss of degrees of freedom to control for the fact that your observations are coming from different process.",2010-09-19 16:52:24.0,603.0,
3561,2811,0,"@Jeromy, I don't think the work I cited would discourage people from using such methods to identify latent groups. I would say the point of the work is that you can't use such methods to solely infer the existence of groups, because you will always find groups, even in random data. It is up to more subjective interpretation whether those groups you find are real or are simply artifacts of the method. You could identify some logical theories that generate such processes and then see if the groups identified fit within those theories.",2010-09-19 16:59:23.0,1036.0,
3562,2849,0,"sure, you are right, but is there any possibility to bypasse this issue??",2010-09-19 16:59:44.0,1251.0,
3563,2846,0,Great topic - You'll get many +1's for this question...,2010-09-19 17:07:02.0,253.0,
3564,1653,0,"@AndyF `aov()` is built on top of the `lm()` function but include additional argument, called *Special* terms, like `Error`.",2010-09-19 17:44:29.0,930.0,
3565,2856,0,"Thanks, nice reference!",2010-09-19 17:45:57.0,930.0,
3566,2828,0,That's what I'm asking essentially,2010-09-19 17:49:14.0,511.0,
3567,2856,0,"Thanks, I like the reference too!",2010-09-19 18:03:19.0,1307.0,
3568,2847,0,"+1 I also use FactoMineR but I remember that when I tried it's PCA method on a really large dataset, I never got results.",2010-09-19 18:11:01.0,339.0,
3569,2828,1,"But can't you at least give us a hint as to what aspect of a model you're trying to capture in the word ""complexity""?  Without that, this question is just to ambiguous to admit one reasonable answer.",2010-09-19 18:14:42.0,919.0,
3570,2859,0,"Nice remark! Apart from looking for a plausible interpretation of the data, do you think the use of an exact permutation test is not an option there?",2010-09-19 18:22:24.0,930.0,
3571,2847,0,"@gd047 It failed for me too, though it is based on an SVD (might be optimized to handle large data set :)",2010-09-19 18:26:52.0,930.0,
3572,2834,0,"would you mind clarifying what you mean by ""(a) our conclusions should not depend on the particular sample used for the comparison""?  I'm having trouble due to the ambiguity of ""sample"" in this context: does it mean ""statistical sample"" (a set of data presumed to represent a process or population) or ""environmental sample"" (a bit of water, soil, air, or tissue, typically).  With either meaning I can't quite draw the logical line to your conclusion that this ""precludes any method based on correlations.""",2010-09-19 18:36:54.0,919.0,
3573,2859,2,"@chl: I think an exact permutation test would work fine; I implicitly applied two of them in my response.  The reason why it should work is that the *actual* test statistic itself will not be affected by ties, because none of the data in the first column is tied within anything, so that their ranks are all uniquely determined.  For the same reason, a Wilcoxon test that does *not* adjust for ties should give correct results!",2010-09-19 18:40:38.0,919.0,
3574,1653,0,"aov() is simply a wrapper to lm().  It does some contrast coding behind the scenes and packages the result in the ANOVA style.  All of it is modeled by lm().  In the article I referenced above, it tells you how to set up coding to do repeated contrasts in regression models, including lm().",2010-09-19 18:51:29.0,485.0,
3575,2859,0,Thanks for this.,2010-09-19 18:55:54.0,930.0,
3576,2833,2,"@dsimcha re large datasets: depends on what you mean by ""large"". Many observations? Use good graphics (boxplot, jittered dotplots, sunflowerplots). Many independent variables? Yes, you have a point there... But if you have so many IVs that you can't plot the DV against each IV, I'd question using an ANOVA at all - it looks like it may be hard to interpret in any case. Some smart machine learning approaches may be better (Brian D. Ripley: ""To paraphrase provocatively, 'machine learning is statistics minus any checking of models and assumptions'."")",2010-09-19 19:13:43.0,1352.0,
3577,2834,0,"@whuber Well, I mean the collection of observed data (e.g. glucose concentration) which, ideally, should be representative of the likely range of what is being measured. Relying on correlation may be misleading because it depends on the sampled units (e.g. patients in an hospital): we can get a higher correlation just by getting one or more extreme measurement on either scale, although the relation between the two methods is still the same. Hence, the idea is that the distribution of the measure of interest should not influence our conclusion about methods comparability. (...)",2010-09-19 19:16:55.0,930.0,
3578,2834,0,"@whuber (...) What we want to assess is the *agreement beyond the data*, not the relationship in the data (I'm quoting Carstensen 2010 p. 8-9).",2010-09-19 19:17:16.0,930.0,
3579,2814,0,"what exactly is your question, then?",2010-09-19 19:55:00.0,795.0,
3580,854,0,@chl: I don't have (free) access to the article. I'm very interested in the arguments. What can be more exact than Exact?,2010-09-19 20:11:10.0,506.0,
3581,854,2,Here is the electronic copy in case you want to read it (it's very instructive and includes a lot of references): http://j.mp/b2Wbpn.,2010-09-19 20:20:25.0,930.0,
3582,2828,0,amount of data needed to identify a good member of the model,2010-09-19 21:38:19.0,511.0,
3583,2814,0,"Same as before - I am looking for an analytical solution for correlation matrix stress testing. Solutions have been suggested for stress testing utilising multiplication but not addition of stress levels (addition was my original question above). Either way, thank you everyone.",2010-09-19 22:02:14.0,1250.0,
3587,1164,6,"Tests relying on many assumptions are more powerful when those assumptions are satisfied. We can test for significance of deviation assuming that observations are IID Gaussian, which gives mean as statistic. A less restrictive set of assumptions tells us to use median. We can go further and assume that observations are correlated to get even more robustness. But each step reduces the power of our test, and if we make no assumptions at all, our test is useless. Robust tests implicitly make assumptions about data and are better than classical only when those assumptions match reality better",2010-09-19 23:20:07.0,511.0,
3588,2846,1,"@Tal .. thanks :-) But I wanted to know more and possibly expand the Wiki article, which is not too informative.",2010-09-19 23:45:37.0,1307.0,
3589,2814,0,"ah, I think I see: you want to add a fixed amount to every off-diagonal element? I think my other answer can be adapted to that case.",2010-09-20 04:14:54.0,795.0,
3590,1884,0,"See also http://en.wikipedia.org/wiki/Wald–Wolfowitz_runs_test , where the mean number of runs matches that given here.",2010-09-20 04:51:32.0,795.0,
3591,2221,0,"yes, it is correct. Both formulations are a little bit fishy, though. Imagine if the dart board were 30 feet in diameter. I don't think either of these models would be appropriate in that case",2010-09-20 05:27:01.0,795.0,
3592,2852,0,"When you say you ""want to compare the difference between the first column [and the] other columns"" are you implying that row 1 represents 6 different measurements on the same object, subject, unit (whatever), row 2 likewise, etc? So we have only 6 things being measured. Or are we looking at 36 different units being measured? And you have not explained why you want to ""compare the difference..."". What are you trying to test?",2010-09-20 06:35:54.0,521.0,
3593,2885,1,"I've read that paper and it seems like Stochastic Complexity fixes the problem of not being able to distinguish between models of same dimensions, but introduces a problem of sometimes not being able to distinguish between models of different dimensions.  Geometric distribution is assigned infinite complexity, surely not what we'd expect for such a simple model!",2010-09-20 06:45:50.0,511.0,
3594,1857,0,"That rule of thumb mainly applies to classical methods like l2 regularized maximum likelihood, L1 regularized methods can learn effectively when number of adjustable parameters is exponential in the number of observations (ie, Miroslav Dudik, 2004 COLT paper)",2010-09-20 07:03:45.0,511.0,
3595,2885,0,"Very good point about infinite stochastic complexity (SC). Solutions to the problem of infinite SC exist, but are not very elegant; Rissanen's renormalization works well in linear models, but is not easy to do for the Poisson/Geometric problem. The MML (or SMML) encoding of Poisson/Geometric data is fine though.",2010-09-20 07:07:59.0,530.0,
3596,2846,0,Great incentive :),2010-09-20 07:19:51.0,253.0,
3597,2853,1,+1 for the great answer that wraps everything up. How to apply the mentioned numerical procedures is nicely and applicably described in Tabachnik and Fidell's Using Multivariate Statistics (for SPSS and SAS): http://www.amazon.com/Using-Multivariate-Statistics-Barbara-Tabachnick/dp/0205459382/ref=sr_1_1?ie=UTF8&s=books&qid=1284969084&sr=8-1 (But see the Erratas on the accompanied web page),2010-09-20 07:53:29.0,442.0,
3598,2888,0,"Yes, the Wu et al. 2009 is a nice paper. Incidentally, I've been working on GWAS and ML during the last two years; now I'm trying to go back to clinical studies where most of the time we have to deal with imperfect measurements, missing data, and of course... a lot of interesting variables from the point of view of the physicist!",2010-09-20 08:37:51.0,930.0,
3599,2836,1,"Just a quick note: minimum description length is very powerful and useful, but it can take ages to obtain results, especially when using normalized maximum likelihood withslighltty larger datasets. I once took 10 days running FORTRAN code to get it for just one model",2010-09-20 09:02:18.0,447.0,
3601,2892,0,"Do you have in mind any particular application, that is do you seek for general advices about how many EVs we need to consider apart from any application (i.e. on a pure mathematical side) or should it apply to a specific context (e.g. factor analysis, PCA, etc.)?",2010-09-20 10:32:13.0,930.0,
3602,2892,0,"I'm interested more in the mathematical side, ie eigenvalues as a property of the data  underlying a correlation matrix. If it makes sense to discuss this in terms of specific context, feel free to do so too.",2010-09-20 11:00:01.0,1250.0,
3603,2894,0,Any reason why you marked this as CW? The question seems specific enough to allow for *one* right answer and thus should not be CW.,2010-09-20 14:24:05.0,,user28
3604,2834,0,"Thank you; that clarifies your position well.  This is essentially an exercise in *calibration* except that we do not appear to have a reference standard for comparison; we merely assume that the physical samples chosen by the experimenter cover some range of true concentrations.  Thus, as you write, correlation *per se* is not necessarily a useful measure of agreement among the two methods.  Typically though, especially for chemical analyses, the true concentration is known (because the experimenter introduced a known amount of a substance into the matrix).",2010-09-20 14:25:48.0,919.0,
3606,2884,1,"At this point you need to consider how the data were generated and you need to more formally specify your study objectives.  The Kruskal-Wallis test, as I recall, looks for a shift of location but assumes homogeneous variances, which obviously is not the case here (compare V4 to V6).  The data-generation process may suggest useful analyses (e.g., these data might be rescaled counts or they could be autocorrelated or the low values might represent measurement noise only).",2010-09-20 14:38:19.0,919.0,
3607,2834,0,"@whuber That's right. In the absence of a gold standard, we are merely interested in the extent to which the two methods yield ""comparable"" results, hence the idea of relying on so-called limits of agreement. Although the true measure may be known in advance, each measurement instrument carry its own measurement error -- at least for those I used to deal with in the biomedical (e.g. blood glucose concentration) and neuropsychological (e.g. depression level) domain.",2010-09-20 14:42:39.0,930.0,
3608,2894,0,"Although it doesn't answer your question, you may find this answer to be helpful in your quest http://stats.stackexchange.com/questions/1807/how-to-perform-students-t-test-having-only-sample-size-sample-average-and-popul/1836#1836",2010-09-20 14:44:25.0,1036.0,
3610,1929,0,"Srikant:> No. The definition has to have two important feature of the univariate median. a) Invariant to monotone transformation of the data, b) robust to contamination by outliers. None of the extentions you propose have these. The Tukey depth has these qualities.",2010-09-20 16:57:58.0,603.0,
3611,2894,0,"@Srikant Because I didn't understand the nature of CW posts. (I went just now and looked it up on meta -- oops.) I thought I was being helpful. Alas, it looks like there's no way to undo it.",2010-09-20 17:01:14.0,1376.0,
3612,2891,0,Is this compositionnal data (i.e. do the lines sum to a constant)?   http://en.wikipedia.org/wiki/Compositional_data,2010-09-20 17:01:58.0,603.0,
3613,2890,0,"I don't really understand ""model mimicry"", but cross-validation seems to just postpone the task of assessing complexity. If you use data to pick your parameters *and* your model as in cross-validation, the relevant question becomes how estimate the amount of data needed for this ""meta""-fitter to perform well",2010-09-20 17:09:21.0,511.0,
3615,1929,0,@kwak What you say makes sense.,2010-09-20 18:25:06.0,,user28
3617,2894,1,@josh that's fine. I was just curious about your choice.,2010-09-20 18:25:54.0,,user28
3618,2904,0,"are BetaH, BetaM, BetaL also parameters (i.e. things to estimate)?",2010-09-20 18:31:58.0,603.0,
3619,1929,0,"@Srikant:> Check the R&S paper cited by Gary Campbell above ;). Best,",2010-09-20 18:38:45.0,603.0,
3620,1929,0,"@kwak On thinking some more, the taxicab metric does have the features you mentioned as it basically reduces to univariate medians. no?",2010-09-20 18:40:07.0,,user28
3621,2904,0,Kwak -- yes; I'll clarify that in the question statement.,2010-09-20 18:40:28.0,53.0,
3622,1929,0,"@Sri:> no. For instance: if you pre-multiply $x,y$ by a non singular,symmetric $2 \\times 2$ matrix $A$, the ranking of the $|(x_i,y_i)-(m_x,m_y)|$ will change. The taxicab metric is not affine invariant. The median is invariant to a even larger group of transformation (it is monotone invariant). Same for robustness, an arbitrary small contamination of your dataset by an observation $x_i$ located at $+\\infty$ will cause the location of $m_x$ to shift without bounds, hence changing, again, all the rankings of the $|(x_i,y_i)-(m_x,m_y)|$. The taxicab metric has a breakdown point of $0$.",2010-09-20 18:57:42.0,603.0,
3623,1929,0,"The median, again, has a break down point of 50 percent (the bivariate tukey depth has a break down point of 33 percent (1/(1+p)) and is monotone invariant.",2010-09-20 18:58:37.0,603.0,
3624,2686,0,very good question (somehow i just saw it). +1.,2010-09-20 19:05:01.0,603.0,
3625,2737,0,Shabbychef:> i think Farrel specifically mentioned an argument against this approach in his question.,2010-09-20 19:06:00.0,603.0,
3626,578,0,In chemometrics the instrument responses are often nonlinear and heteroscedastic.  At a minimum that imposes a certain amount of caution when conducting and interpreting the regression.,2010-09-20 19:06:19.0,919.0,
3627,2896,3,Why would one need to invoke the CLT when sampling from a known (or assumed) Gaussian distribution?  The mean of even a sample of one will be Normally distributed!,2010-09-20 19:09:50.0,919.0,
3628,2894,1,"Google ""adaptive sampling"" and ""sequential sampling"".  If you're still stuck, include ""Wald"" as a keyword and then work forward historically (i.e, look at papers that reference Wald's work on sequential sampling, then look at papers that reference them, etc.).",2010-09-20 19:10:59.0,919.0,
3629,1929,0,@kwak I see the point. I will not delete this incorrect answer for the benefit of future readers.,2010-09-20 19:13:21.0,,user28
3631,2908,0,"Yes, I've read his remarks in (I believe) JASA, and he seems to really try to be respectful to statisticians. Very unlike the way he treats economists.",2010-09-20 19:18:21.0,666.0,
3632,2908,1,"I agree that his books are full of bluster, but the ideas are IMHO valid, and in many cases, scary.",2010-09-20 19:41:40.0,247.0,
3633,1931,0,phv:> one can ask for 'the' generalization to preserve (in higher dimensions) some of the interesting properties of the median. This severly limits the number of candidates (see the commenting after Srikant's answer below),2010-09-20 20:10:15.0,603.0,
3634,1931,0,@Whuber:> then notion of ordering can be generalized to R^n for unimodal distributions (see my answer below).,2010-09-20 20:11:05.0,603.0,
3635,1929,1,@Srikant:> there are no incorrect answer to phv's questions because there are no 'good answers' either; this area of research is still under development. I simply wanted to point out why it is still an open problem.,2010-09-20 20:13:22.0,603.0,
3636,2908,0,+1 Loved your blog post. Explains Taleb's ideas very well.,2010-09-20 20:27:37.0,666.0,
3639,2894,0,"Is there anything wrong with computing the sample mean, the sample variance and a confidence interval?",2010-09-20 20:48:04.0,352.0,
3640,2911,0,"Thanks for the links! The question is open to any statistical software -- I use Python and Stata from time to time, so I wonder if confirmed users may bring interesting recommendations there.",2010-09-20 20:51:34.0,930.0,
3641,2904,0,"I looks to me like you should only need 3 guesses, one for when each of $H'$, $M'$ and $L'$ is largest.  But, I may simply misunderstand the question.",2010-09-20 20:55:39.0,352.0,
3642,2911,0,Absolutely; although I would add that the recommendations in the above links could really apply to any statistical project (regardless of the language).,2010-09-20 20:58:44.0,5.0,
3643,2911,0,"Definitely, yes! I updated my question at the same time.",2010-09-20 21:03:00.0,930.0,
3644,1931,0,"@kwak: could you elaborate a little?  The usual mathematical definition of an ordering of a space is independent of any kind of probability distribution, so you must implicitly have some additional assumptions in mind.",2010-09-20 21:14:45.0,919.0,
3645,2901,2,But doesn't the third (central) moment do this in a more stable and practical way?,2010-09-20 21:16:58.0,919.0,
3646,2905,1,I'm curious: what is the trick?,2010-09-20 21:20:06.0,795.0,
3647,2894,1,"@Robby McKilliam: But what data do you use?  This question arises before any data have been collected.  If you collect values one at a time and compute a CI after each new one is added to the dataset, you cannot use standard formulas for the intervals due to the correlated multiple comparisons you are making.  Thus, you need a *stopping rule* that optimizes the sum of the statistical risk of your estimator and the cost of collecting each additional sample.",2010-09-20 21:22:44.0,919.0,
3648,1865,0,why has this question/answer pair been imported _in toto_ from the math SE?,2010-09-20 21:28:39.0,795.0,
3649,2913,0,"i.e. the data matrix is m-by-n, where m is (usually) greater than n. Forming the covariance matrix from that will always give you an n-by-n matrix no matter how you increase or decrease m. (That's part of why using the normal equations for least-squares seduces a lot of people...)",2010-09-20 21:38:24.0,830.0,
3650,2894,1,"@whuber thanks! I'm still digesting the material, but I think that this is exactly what I'm looking for. If this were an answer, I'd accept it...",2010-09-20 21:41:32.0,1376.0,
3651,2913,0,"@whuber: Well, computing the sample covariance matrix does depend on the number of data points as well and this computation costs something like O(N*P^2) for N data points having P features each. Besides, many of the fast iterative algorithms for PCA (e.g., the Expectation Maximization algorithm for PCA) have a time complexity dependent on N. It can really make a difference if N is large. So one can't say in general that the computational complexity will not at all depend on the number of data points.",2010-09-20 22:15:08.0,881.0,
3652,2913,0,"@ebony1: But O(N*P^2) is O(N), which is exactly what I was saying.  If your N really is around 1000, you can't improve computational performance (which is already going to be fast unless P is huge) by more than a factor of 2.  In most cases that's hardly worth sweating over and in this case, for reasons ably pointed out in your problem statement, it seems like subsampling may not be a good idea.",2010-09-20 22:18:15.0,919.0,
3653,2901,1,"@Whuber:> the third is measuring overall asymmetry, which is not the same thing as tail asymmetry. Because of the higher exponent, the value of the fifth is nearly entirely determined by the tails.",2010-09-20 22:31:26.0,603.0,
3654,2914,0,"Just to throw a couple of ideas on the subject, if the study discloses standard regression statistics you could focus on the t stats and p values of the coefficients.  If the RSquare of the model is high; but, one or more of the variables have a t stat < 2.0; this could be a red flag.  Also, if the sign of the coefficients on some of the variables defy logic that is probably another red flag.  If the study does not disclose a hold out period for the model, that may be another red flag.  Hopefully, you will have other and better ideas.",2010-09-20 23:12:15.0,1329.0,
3655,2909,0,How accurate do you need it? Could you just simulate the walk and avoid fully-functional solutions?,2010-09-20 23:20:43.0,53.0,
3658,604,0,"I understand your question, and I share your conundrum.  I would think that any method to estimate something would be able to replicate its exact estimates if the latter are used as the dependent variable.  The method you use has its own constraints (linear relationships, normal distribution, or whatever).  But, the estimates by definition reflect the constraints of the method you use.  So, you should replicate the estimates exactly.",2010-09-20 23:35:40.0,1329.0,
3659,2914,0,One way is to see how the model performs on other (but similar) data.,2010-09-20 23:36:49.0,5.0,
3660,2919,0,Is Doug Bate's mixed-effects model the same as a hierarchical model?,2010-09-20 23:45:52.0,1329.0,
3661,2919,1,yes: http://en.wikipedia.org/wiki/Multilevel_model,2010-09-20 23:56:47.0,603.0,
3662,2909,0,"good point. I don't need atomic-physics level accuracy. in fact, 3 sigfigs is probably just fine....",2010-09-21 00:11:39.0,795.0,
3663,2737,0,The question is somewhat ambiguous; my takeaway was that the OP was looking for 'buzzwords'.,2010-09-21 00:13:21.0,795.0,
3664,2905,0,Do you mean the _eigenvectors_ of $\\tilde{C}$ in 3? not _values_?,2010-09-21 00:16:51.0,795.0,
3665,2905,0,no. $\\lambda_1$ is a scalar.,2010-09-21 00:27:52.0,603.0,
3667,2833,0,"Good comment, +1.  Even though this specific question is about ANOVA, I was thinking on a more general level about the question of plots vs. tests when I wrote my response.",2010-09-21 01:28:46.0,1347.0,
3668,1931,0,"@Whuber:> You state: ""R1 can be ordered but R2, R3, ... cannot be"". R2,..,R3 can be ordered in many ways by mapping from Rn to R . One such way is the tukey depth. It has many important properties (robustness to some extend, non parametric, invariance,...) but these only hold for the case of unimodal distributions. Let me know if you want more details.",2010-09-21 01:47:08.0,603.0,
3669,2925,0,what do you mean by 'verify'? does this mean the third party controls the external event? this would contradict the pmf here.,2010-09-21 02:39:30.0,795.0,
3670,2905,0,This is a very odd procedure; has it been published somewhere?,2010-09-21 02:42:57.0,795.0,
3671,2913,0,@whuber: My question wasn't as much about a specific case of 500/500 split. It was just an example. Maybe something like 10% in-sample/90% out-of-sample would have been a better example. My question in general was about the reliability of the embeddings of the out-of-sample data points if you do PCA using a small sample of the data and use the learned projection to compute the embeddings of the out-of-sample data points.,2010-09-21 02:51:28.0,881.0,
3672,2925,0,"Obviously the third party should not control the external event for the question to make sense. As an example of what I mean, suppose we agree that the protocol is this: If it rains tomorrow in New York X=0 otherwise X=1. The draw is based on an external event and is verifiable in the sense that someone can check if it really rained post-facto.",2010-09-21 02:58:55.0,,user28
3674,2929,0,"So, this would mean when having two distributions to compare, I could plot two Lorenz curves and calculate two Gini coefficients for them. This should then be comparable with creating one QQ plot with the advantage that the Lorenz/Gini solution would allow me to compare the mentioned distributions numerically, whereas the QQ-plot only allows me to compare visually. Correct?",2010-09-21 03:56:29.0,608.0,
3675,2931,1,"Wow, thanks Andrew Moore's tutorial on cross-validation is world class.",2010-09-21 04:08:38.0,1329.0,
3676,2927,0,Thanks for the links to the AIC and BIC tests.  Do they add much value vs Adjusted R Square that does a similar thing by penalizing models for adding variables?,2010-09-21 04:10:47.0,1329.0,
3677,2929,0,"What question are you trying to answer? If you are asking, ""are these two distributions the same?"", then you should use the QQ plot for a visual and the Kolmogorov-Smirnov test for the similarity of the distributions. This test basically uses the absolute maximum distance from the 45-degree line in the QQ plot as its statistic. The KS test can also be thought of as the maximum distance between two cumulative distribution functions plotted in the same figure. A CDF is a general concept; a Lorenz curve is a special case of the CDF when the domain of the random variable is from 0 to 1.",2010-09-21 05:28:26.0,401.0,
3678,2937,0,Aren't these tests not restricted to hypotheses testing (testing the null hypothesis that they are the same)? I would like to have a similarity measure that quantifies their (dis)similarity in addition to simply testing IF they are (dis)similar and preferably in some comparable form (e.g. 20% dissimilar or 0.2 (i.e. a normalized value between 0 and 1 as a measure for similarity) etc.). Would you recommend to use the p value for that? Thanks in advance!,2010-09-21 05:40:39.0,608.0,
3679,2929,0,"I have a distribution ranging from 1 to 1024 (screen coordinates) with frequencies of user activities at these coordinates. I produce a density plot of this distribution and, since I have a number of activities I would like to compare, I have several of such distributions. So, to answer your question, my rage is naturally not [0,1]. However, if I normalize the screen between 0% and 100% I can fit it that way (just like in economic problems where the income level is scaled between 0% and 100% in 20% steps). Does that sound reasonable?",2010-09-21 05:47:51.0,608.0,
3680,2929,0,"I'm sorry, but I'm still not understanding the hypothesis that you want to test. Is it, ""clicks are uniformly distributed across the screen""? If so, you can use the KS test to answer whether your distribution of clicks is statistically different from a normal distribution. You wouldn't need to normalize or think of this as a Lorenz curve; just think of it as a CDF and the KS test statistic is the maximum absolute distance between your CDF (with a domain from 0 to 1024) to the CDF of the uniform ""reference"" distribution (the 45-degree line).",2010-09-21 06:01:17.0,401.0,
3681,2929,0,The KS test can compare one empirical distribution to a known theoretical distribution or one empirical distribution to another empirical distribution. It is a nonparametric test for equality of distributions. See the Wikipedia page: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test,2010-09-21 06:02:40.0,401.0,
3682,2905,0,"@Shabbychev:> no, but i had the opportunity to work on a related problem (just not one involving time series) a while ago (same problem as this one http://stats.stackexchange.com/questions/2572/correlation-stress-testing )",2010-09-21 06:45:22.0,603.0,
3683,2935,1,Gaetan:> Elane seems to suggest that the slope in the summer are different from the slopes in the winter (not just the intercept).,2010-09-21 06:50:29.0,603.0,
3684,1735,0,"Elaine:> what do you want to do? From your question i gather that you want a relative ranking of the coefficient for winter and one for summer. Is this correct, or just an ersatz for something else you have not been able to do.",2010-09-21 06:52:10.0,603.0,
3685,2927,1,"@Gaeten, Adjusted R-squared will increase when an F-test of the before vs after model is significant, so they are equivalent, except normally calculating an adjusted R-squared doesn't return a p-value.",2010-09-21 07:33:42.0,521.0,
3686,2927,1,"@Gaeten - AIC & BIC are more general than F-tests and adjusted R-squared which are usually limited to models fit by least squares. AIC & BIC can be used for any ,model where the likelihood can be calculated and degrees of freedom can be known (or estimated).",2010-09-21 07:44:14.0,521.0,
3687,2929,0,"I simply want to know how similar they are. Lets say I have clicks across the screen for one distribution and eye fixations (measured with an eye tracker) for the other. Now I want to know what is the % of similarity between them using some normalized qualifier/measure. This could be accompanied by a test of course, and I take your advice on that, but in addition to such a test, I still would like to quantize the similarity somehow.",2010-09-21 07:55:52.0,608.0,
3688,2937,0,"Ampleforth:> yes, you can use the p-value as a continuous (not just 0-1) and normalized measure of the differences between the two distributions. It'll be more accurate (in the sense of precise,  or powerfull) than Gini's I.",2010-09-21 08:29:00.0,603.0,
3690,2913,0,"@Ebony:> Whuber's point is still valid, in the sense that even with 1%-99% split, your proposal does not affect the (very low) O(n) complexity of the overall procedure so from a theoretical point of view your concerns are void. Shifting to a practical point of view, you can perform PCA on 100K's of observations and 10K's features in seconds on a modern computer. It's hard to imagine a setting in which splitting the sample would be worthwhile.",2010-09-21 08:45:49.0,603.0,
3691,2890,0,"@Yaroslaw: I don't really understand your issue with cross-validation, but to be honest I am no expert there. However, I would really like to make a point for measuring model mimicry. Therefore, see my updated answer.",2010-09-21 08:46:20.0,442.0,
3693,2828,0,Yaroslav:> Shouldn't that be *Amount of data needed to identify when a model is **not** a good description of the data*?,2010-09-21 09:01:03.0,603.0,
3694,2896,0,Good point! Didn't RTQ properly.,2010-09-21 09:22:29.0,229.0,
3695,2929,0,"@Ampleforth But it seems to me that in this case you should rather focus on some kind of distance (e.g. euclidean) between eye coordinates on the screen and mouse position for each event (I assume that you have 2 (x,y) pairs -- eye + mouse -- for each mouse click). Then you can derive a measure of overall error (e.g. RMSE).",2010-09-21 09:38:18.0,930.0,
3696,2700,1,The tutorial was really great. Could you suggest any further tutorials as a follow-up?,2010-09-21 09:54:57.0,1250.0,
3698,2913,1,"@kwak: Okay, consider this -- if P >> N, then it's suggested that you do PCA using the eigen-decomposition of X'*X (which is NxN) rather of the covariance matrix X*X' (which is PxP). In this setting, the computational complexity is going to be O(N^3). If N is reasonably large (even though smaller than P) I don't think you can ignore the cubic complexity.",2010-09-21 11:21:13.0,881.0,
3700,2737,0,I google searched time-based boxcar and came up with very little. Can you point us to a worked example?,2010-09-21 13:03:43.0,104.0,
3701,2907,0,do you have a url that you can point me at. I have never done a time series analysis in my life. But it looks as if this is what I need to teach myself. Is there a worked example that I can look at that uses time series filters?,2010-09-21 13:06:25.0,104.0,
3702,2884,0,"Thank you,whuber!",2010-09-21 13:14:27.0,,Chuangye
3704,2936,0,What would be a specific example for an external event that I could use for $Y$?,2010-09-21 13:58:31.0,,user28
3705,2913,4,"@ebony1: It is true that the situation changes when P >> N.  That's an interesting refinement of your original problem, because in effect any model is going to be a gross overfitting: more explanatory variables than data.  That does not augur well for the prospects of obtaining stable results via subsampling.  At a minimum, it points to the importance of retaining as much data as possible.",2010-09-21 14:08:24.0,919.0,
3706,2944,1,"@2 is clever.  It might even be better to draw correlated random subsamples: e.g., randomly partition the data into $k$ parts of approximately $n/k$ values each and run $k$ PCAs for a computational cost of only $1 / k^2$ of a full PCA.",2010-09-21 14:11:01.0,919.0,
3707,2937,2,"+1 for the second suggestion.  The Q-Q plot is one of the strongest and most insightful ways to compare two empirical distributions, because it graphically displays variations between the two throughout their range.  It is also a natural graphical adjunct to the KS test, whose statistic is the maximum deviation between the Q-Q plot and its reference line (Y==X).",2010-09-21 14:15:28.0,919.0,
3709,2851,0,Thanks! I will try to get hold of the book.,2010-09-21 14:33:33.0,1084.0,
3711,2907,0,"@farel:> if you click on the blue words, you will be directed to actual websites. The last document in particular contains working out examples (repeated in the edted version of my post).",2010-09-21 14:57:17.0,603.0,
3712,2939,0,"Note that (as I understand it), the K-S test is for testing empirical data against an a priori distribution. It's not appropriate for comparing two empirical distributions, nor is it appropriate to compare empirical data against an a priori distribution whose parameter values were estimated from the empirical data.",2010-09-21 15:07:19.0,364.0,
3713,2947,0,Your paper includes very nice figures :),2010-09-21 15:19:47.0,930.0,
3714,2936,1,"As an example I first thought of old numbers games where they used some decimal point of specific stock reports in the news paper. Of course you would worry about the true randomness of this metric though, so I imagine there are better examples.",2010-09-21 15:31:31.0,1036.0,
3715,2915,0,Should this be CW as it is asking for a collection of resources? What would constitute a 'correct' answer for such a question?,2010-09-21 15:37:31.0,,user28
3716,2939,1,"@Mike, you can use the K-S test to compare two empirically derived distributions, see Charlie's prior answer and comments http://stats.stackexchange.com/questions/2918/lorenz-curve-qq-plot/2929#2929",2010-09-21 15:39:48.0,1036.0,
3717,2944,0,"@Whuber:> thanks. However i have two questions: a) PCA on the full dataset is not the same as the mean of PCA on the parts (do you know of a way to recover the 'total-sample' pca from the 'partial-sample' ones?). b) wouldn't we (under your proposal) lose the interpretation of the 'partial-sample' PCA in terms of sampling variation ? In other words, do the distribution of the 'partial-sample' pca constructed from correlated sample have a statistical interpretation ?",2010-09-21 15:41:56.0,603.0,
3718,2947,0,@chi: They were all created in R using ggplot2. It's a fantastic graphics production system!,2010-09-21 15:42:11.0,364.0,
3719,2915,0,"I am new here, so I don't know what CW is?  Does it mean Community Wiki or something?  And, what does this mean?  Should I have posted this question somewhere else within this site?  And, where? and how?",2010-09-21 15:50:30.0,1329.0,
3720,2935,0,"kwak, you raise an excellent point.  I have been told that linear regression can handle that whereby you have dummy variables for the intercepts of the seasons and another one for their respective slope coefficients.  Unfortunately, I have not used such a method.  So, I can't explain it further.",2010-09-21 15:55:53.0,1329.0,
3721,2939,0,"@Andy, Ah, I took point 3 from http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm as having the corollary that you can't compare two empirical CDFs, but I see that my assumption wasn't appropriate. Good to know, thanks!",2010-09-21 16:02:47.0,364.0,
3722,2942,0,"Thank you, the paper is quite helpful.",2010-09-21 16:05:14.0,1381.0,
3724,2915,1,When you ask a question or edit a question there is a check box that appears at the bottom of the question box towards the right hand side that says 'Community wiki'. You need to check that box *if* you wish to make a question or answer CW. The idea behind CW is enable the community to edit the questions/answers collaboratively and to prevent users from gaining rep for certain type of questions. See this link for more details: http://meta.stats.stackexchange.com/questions/6/what-should-our-faq-contain/198#198,2010-09-21 16:18:24.0,,user28
3725,2937,0,@whuber: good point about KS statistic being the maximum deviation.,2010-09-21 16:26:45.0,795.0,
3726,2937,0,"You can use the K-S test statistic as a measure of dissimilarity. For a two-sided test, the K-S test statistic is the largest absolute difference between the two distribution functions. As the distribution functions lie in [0,1], the difference also lies in [0,1]. If the two distributions differ slightly the p-value will tend to 0 as the sample sizes increase (since the power of the test increases) but the K-S test statistic will tend to a limit.",2010-09-21 16:33:21.0,449.0,
3728,2920,0,"For MSD, if MSD = t_{alpha,2n-2}*sd*sqrt(2/n), is SE = MSD*n/(t_{alpha,n}*sqrt(2)) correct?",2010-09-21 16:35:22.0,1381.0,
3729,2950,2,kmeans() (you can find it by typping ?? folowed by the name of what you want as in ??kmean),2010-09-21 17:36:02.0,603.0,
3730,2950,0,Another useful way to find functions is the `apropos` function.,2010-09-21 17:38:11.0,5.0,
3731,2950,3,@kwak: why not post your comment as an answer?,2010-09-21 17:38:45.0,5.0,
3733,2945,1,"I would agree that Cross Validation solves the problem of measuring model complexity. Maybe I'm asking the wrong question, because a practical question is the sample complexity of the fitting procedure. Cross-validated learner would try different models and pick the one with lowest cross validation error. Now the question is -- is this learner more likely to overfit than one that fits a single model by maximum likelihood?",2010-09-21 18:30:22.0,511.0,
3735,2945,0,"Yaroslav Bulatov:> yes but you can use ML only to compare nested models. Insofar as you specified (in your question) mentioned models with the same number of parameters, then they cannot be nested.",2010-09-21 18:39:43.0,603.0,
3737,2851,1,"Hurlbert, S.H. (1984) Pseudoreplication and the design of ecological field experiments. Ecological Monographs, 54, 187–211.",2010-09-21 20:34:04.0,603.0,
3738,2953,4,"You can also do `RSiteSearch(""k-means"")`",2010-09-21 20:42:23.0,582.0,
3739,2953,0,Thanks: i did know of this feature ;),2010-09-21 21:09:33.0,603.0,
3740,2950,0,"@all:> i propose to change the title of the post, so as to make it easier to find for someone looking for the R implementation of other procedures (beside k-means).",2010-09-21 21:11:13.0,603.0,
3741,2939,0,"However, point 3 does imply that you can't use K-S to test whether your data come from a normal distribution with mean and sd *estimated from the data*. This is a popular error among the psychology students I meet.",2010-09-21 21:28:00.0,1352.0,
3742,2945,0,Another issue is that cross-validation doesn't add to our understanding of model complexity. Measures like AIC/BIC make it clear that lots of parameters encourage overfitting. Now the question becomes -- what aspects of the model besides dimension increase capacity to overfit?,2010-09-21 21:56:49.0,511.0,
3743,1858,0,Did you find any paper finally?,2010-09-21 22:00:31.0,930.0,
3744,2950,0,"@kwak Yes, I think it's better to change the title because k-means vs. kNN seems ambiguous.",2010-09-21 22:07:11.0,930.0,
3745,1858,0,@chl Not yet; but thanks for reminder.,2010-09-21 22:11:37.0,88.0,
3747,2945,0,"Yaroslav:> Again, very good point.",2010-09-21 22:28:07.0,603.0,
3748,1858,0,There's no hurry :) Didn't find anything interesting myself; maybe Pubmed isn't the right search engine for this particular case...,2010-09-21 22:34:03.0,930.0,
3749,1858,0,@chl That's also my problem here. It really seems n<<p has became a synonym for biomed data.,2010-09-21 22:46:27.0,88.0,
3750,2888,0,"BTW, I just came across a paper that made me think of this question...it's very rare for Machine Learning papers to talk about confidence intervals, but here's a notable exception http://www.ncbi.nlm.nih.gov/pubmed/19519325",2010-09-21 22:50:39.0,511.0,
3752,2954,0,"I believe the k nearest-neighbors test is a multivariate analysis. Perhaps I have confused cluster analysis with classification, but I don't think either apply in this situation. I already have categorized the variables into groups, and now want to see if these groups statistically differ from one another. This test is different that an ANOVA or linear model. Any suggestions? Thanks.",2010-09-21 23:03:10.0,,Sharon
3754,2820,0,"Hello Stephan,
I still think you can still use the method=BY (for Benjamini Hochberg Yekuteli Procedure) in p.adjust in R, as pointed by Tal.

Definitely, using Bonferroni can be conservative.",2010-09-22 03:44:28.0,1307.0,
3755,2960,0,But the question is: how would you verify that to someone on the other side of the world?  See item b) in the question.,2010-09-22 05:16:54.0,71.0,
3756,2963,0,"Tangent: Dirk Eddelbuettel wrote an R package (http://cran.r-project.org/web/packages/random/index.html) to access random numbers.  Not verifiable, however.",2010-09-22 05:27:53.0,71.0,
3757,2962,2,"I'm not aware of such a function, but perhaps someone could look at the formulas in Olejnik and Algina  (2003) http://www.cps.nova.edu/marker/olejnik2003.pdf and write a function",2010-09-22 05:34:03.0,183.0,
3758,2962,2,"@Jeromy Nice reference! This one is worth looking too: Recommended effect size statistics for repeated measures designs (BRM 2005 37(3)), http://j.mp/cT9uEQ",2010-09-22 06:56:03.0,930.0,
3759,2954,0,"@Sharon I'm a little bit puzzled about the term 'randomization test' (the only paper I found using this keyword, http://j.mp/9OCB9I, seems rather to focus on power considerations) -- I rest on my idea that you could use cross-validation to assess the classification accuracy in your data, e.g. leave-one-out or k-fold CV for this purpose (available in `kknn` and `class` packages). Now, I don't know on which basis your classes were defined, but it is generally meaningless to use ANOVA to test if the groups differ on the variables that were used to define class membership.",2010-09-22 07:30:34.0,930.0,
3760,2962,2,"@chl Thanks. Apparently, ezANOVA() in the ez package in R reports generalised eta squared.",2010-09-22 07:49:19.0,183.0,
3761,2958,0,Can you provide more detail Justin?,2010-09-22 07:54:06.0,521.0,
3763,2954,0,@Sharon; can you explain your data a bit better (I'm a (sort of) ecologist and use isotopes all the time so you can be quite technical if it helps)?,2010-09-22 11:54:48.0,1390.0,
3765,2958,0,"What else do you know about the distribution apart from its 16th percentile (which is what i assume you mean by ""16% quartile"")?",2010-09-22 12:09:02.0,449.0,
3766,2963,0,This works perfectly.,2010-09-22 12:56:36.0,,user28
3768,2967,0,"Thanks for the great suggestion. I did a quick analysis using isoreg in R. It was certainly robust to the outliers (i.e., the very slow times). It was a little bit more stepped than would seem appropriate if it was updating its estimate based on every observation. I also assume that it also wouldn't deal well with the situation where the runner experiences an injury or perhaps changes their running style and thus the runner's latent performance actually declines.",2010-09-22 13:22:25.0,183.0,
3769,2970,0,"Thanks. I have a concern with the nonlinear regression approach (or the linear regression of transformed x and y). 1. It assumes the functional form is known; 2. least squares is influenced by outliers, which I would argue are less relevant to the quantity of interest; 3. The quantity of interest is more like median of a positively skewed running time distribution with outliers, whereas least squares would estimate the mean.",2010-09-22 13:28:48.0,183.0,
3770,2967,0,Jeromy:> first point/ have you tried fudging with the bandwith selector ? Second point/ Injury is a valid example (running style is not since it does not impact latent [potential] speed): see edited answer.,2010-09-22 13:30:09.0,603.0,
3771,2967,0,"Thanks. I'll play with the bandwidth selector. As for running style, I guess your argument would be that the runner could go back to their original running style and return to their previous speed. Sometimes I'd be interested in seeing the person's new potential in terms of perhaps a new running style that they have committed to.",2010-09-22 13:35:20.0,183.0,
3772,2967,0,@Jeromy:> see updated answer.,2010-09-22 13:40:04.0,603.0,
3773,2968,0,"+1 Because, it is a great idea.",2010-09-22 13:47:02.0,442.0,
3774,2820,0,"suncoolsu, I think that this method only works when the correlation is positive (not negative) between the variables.  Cheers.",2010-09-22 13:57:04.0,253.0,
3775,2970,0,@Jeromy: I'm sure you're right. Just sharing my ignorance.,2010-09-22 14:24:23.0,1270.0,
3776,2884,0,"Whuber,but I foud ""The Kruskal-Wallis test  does  not  assume  population  normality  nor  homogeneity  of  variance,  as  does  the parametric ANOVA,  and  only  requires  ordinal  scaling  of  the  dependent  variable."" that you can see from http://www.jstor.org/pss/1165320 or http://oak.ucc.nau.edu/rh232/courses/EPS525/Handouts/Understanding%20the%20One-way%20ANOVA.pdf.",2010-09-22 14:52:44.0,,Chuangye
3777,2701,0,"That is great thanks, the graphpad page was perfect - Fig 6.4 gave me what I needed to know.  We havent yet got 50% dead, so technically it's not possible to work out, but i did some forecasting to get to that point.",2010-09-22 15:13:36.0,1284.0,
3778,2947,0,What do you mean with fitted CDF?,2010-09-22 15:44:54.0,608.0,
3779,2929,0,"I believe that would be too noisy. My wish to focus on distributions rather than sequences of data points is based on the fact that the data is rather noisy. I think distributions provide the general tendencies rather than individual pattern. Also, I am summarizing over many users which I can combine in that way to detect the main trends. So, I basically have two PDF (or CDFs) and would like to know how similar they are. I like the idea of the Gini coefficient that uses the relative area under a Lorenz curve but might need something slightly different.",2010-09-22 15:51:45.0,608.0,
3780,2929,0,"By the way, thank you all for continuing this discussion. I just realized that they became rather lengthly :)",2010-09-22 15:53:02.0,608.0,
3781,2947,0,"@Ampleforth, in that paper, I fit a distribution to empirical data, so by ""fitted CDF"" I meant the theoretical CDF of the fitted distribution. Sorry, I see how I could have been more clear!",2010-09-22 17:06:35.0,364.0,
3782,2976,1,"Would factor analysis not do the job for qn 1? Question 2 is a bit vague. 'Relationship' seems a synonym for 'correlation' or at least one form of relationship is linear relationship and correlation captures that. Perhaps, you need to clarify qn 2.",2010-09-22 17:08:32.0,,user28
3783,2951,0,"many thanks for pointing this out to me, I had completely missed the reference to the weights in the documentation.",2010-09-22 17:42:54.0,1007.0,
3784,2977,0,"@Srikant I know all this. CV is a means for finding ""best"". What is the definition of ""best""?",2010-09-22 18:13:26.0,1134.0,
3785,2977,0,"@bart 'best model' = a model that 'best' captures global patterns while avoiding local features of a data. That is the best I can do for a non-math description. Perhaps, someone else can elaborate a bit more or be more specific.",2010-09-22 18:15:38.0,,user28
3786,2977,0,"@bart: ""best"" means the the function that fits the training data the best, and that ""generalizes"" well to the validation/unseen-test set data. I think this is quite clear from Srikant's answer. There are many ways to formally define a good generalization behavior. In a non-formal sense, you can think of it as finding a function that is ""smooth"" and not much wiggly. Trying to fit solely on the training data may lead to the wiggly looking function whereas smoothness usually ensures that the function will do reasonably well on both training and validation/test data.",2010-09-22 18:27:35.0,881.0,
3787,2977,0,@ebony: You are missing the point. I've rephrased the question to hopefully make it clearer,2010-09-22 18:34:28.0,1134.0,
3788,2980,0,"I thought the meaning of 'external event' was clear. To clarify, because of the constraints imposed on the protocol, the external event should be the source of the random generation and the draw must be verifiable. Your answer addresses verifiability but not the external event part. Nevertheless, +1 for an interesting answer. Out of curiosity, doesn't the algorithm breakdown if both Bob and Alice cheat and report a value (0 or 1 as they choose) *without* flipping a coin?",2010-09-22 19:09:32.0,,user28
3789,2807,6,"@nico:  Sure it can be negative, but there's some finite limit to it because there are only so many protons and electrons in the Universe.  Of course this is irrelevant in practice, but that's my point.  Nothing is **exactly** normally distributed (the model is wrong), but there are lots of things that are close enough (the model is useful).  Basically, you already knew the model was wrong, and rejecting or not rejecting the null gives essentially no information about whether it's nonetheless useful.",2010-09-22 19:39:17.0,1347.0,
3790,2947,0,"Oh, please don't apologize. My lack of statistics is rather large and that is the only problem here ;) Also I did not read your paper, but only glanced through your graphs which I really liked.",2010-09-22 19:57:07.0,608.0,
3791,2984,1,Is $\\lambda$ a parameter that is free to be chosen?,2010-09-22 21:49:03.0,352.0,
3792,2984,0,@Robby:> thanks. I slightly appended the text to make the distinction between parameters and hyperparameters clear.,2010-09-22 22:12:04.0,603.0,
3793,2979,0,> this is not the same model (i.e. Kelly's model is non linear in the parameters).,2010-09-22 22:35:14.0,603.0,
3794,2835,0,> yes it does local quantile regression as well (see the vignette (http://cran.r-project.org/package=quantreg).,2010-09-22 22:40:08.0,603.0,
3795,2984,0,"@kwak: I'm sorry to say I haven't a clue what this means. What do the symbols p, q, lambda, x, y, m and beta signify?",2010-09-22 23:05:43.0,1134.0,
3796,2985,0,You understand the question. I'll follow the links.,2010-09-22 23:09:13.0,1134.0,
3797,2984,0,"@bart:> My answer is essentially the same as Srikant's. Where he provides an intuitive explication, I wanted to add a more rigorous one for the benefits of future visitors that may have the same question as you, but are more familliar with math than non-formal language. All the symbols you mention are defined in my answer (altough, again, this is done formally).",2010-09-22 23:35:03.0,603.0,
3798,2985,0,You should know that these links are unlikely to take you anywhere `practical'. If you are trying to *build* something using cross validation (or some other sort of model selection) then in practice it is likely to always come down to something heuristic and a bit ad-hoc (although I agree this is unsatisfying).,2010-09-22 23:36:43.0,352.0,
3799,2985,0,Now we are getting somewhere. http://en.wikipedia.org/wiki/Minimum_message_length seems to be what I was thinking. Thanks!,2010-09-22 23:48:45.0,1134.0,
3800,2985,0,"No worries. This is just reflection, not practical.",2010-09-22 23:50:31.0,1134.0,
3801,2984,0,"@kwak: Where, for example, is p defined?",2010-09-22 23:54:18.0,1134.0,
3802,2986,0,"@user1396: many thanks! I was thinking I had to do this 'by hand'. : ) I ran `community.to.membership(ag, z$merges, steps=12)` on the output of running `fastgreedy.community()` with the graph provided as an example in the docs and the output was `$membership
 [1] 2 2 2 2 2 0 0 0 0 0 1 1 1 1 1

$csize
[1] 5 5 5`
I am confused by the repeated values in the $membership vector. How should I read them? I thought this would contain the vertice names. many thanks!",2010-09-23 00:41:25.0,1007.0,
3803,2984,0,"@bart:> line 1 (p,q)>= 1 means p and q are taken to be numbers larger than 1. Latter on (line 1 of last §), it says that for that particular property to hold p should be equal to either 1 or 2.",2010-09-23 00:45:51.0,603.0,
3804,2986,0,@user1396: I think I got it; a '2' in the i-th element in the membership vector is telling me the i-th vertex belongs to community 2. correct? :),2010-09-23 00:47:25.0,1007.0,
3805,2979,0,"Paul-- believe it or not, we tried this as a starting point for the  optimization (taking into account the concerns you raised), but the estimator was way off from where we'd expect it to converge to. No good explanation why. We do have a reasonable workaround, which I'll be sharing with the question shortly.",2010-09-23 01:15:09.0,53.0,
3806,2979,0,"@kwak:> what do you mean by ""this""? Not the same model as 2sls or not the same model as what the GHK simulator is usually applied to, ie not a multinomial probit? I know it's not a multinomial probit, that's why I suggested a slight variant of the GHK simulator. As to whether 2sls applies, doesn't the equation for W have some error term, say, e, and doesn't the full model imply E[e|Z,X] = 0 and E[(H,M,L)|Z] not 0?",2010-09-23 01:17:15.0,1229.0,
3807,2976,0,You have stated what you want to do. What is your question? Is it about implementation or whether your analysis approach is appropriate? or something else?,2010-09-23 01:21:15.0,183.0,
3808,2980,0,"Hi Srikant, yes, the algorithm requires at least one honest party. You have no internal source of randomness at all? Then you could take the SHA-256 of the PDF version of The New York Times every day, or the SHA-256 of the volume and closing price of all the stocks in the Dow Jones Industrial Average, or the really the secure hash of anything that can be mutually observed and that you can't influence. If you want one bit, take one bit of the SHA-256. If you want a normal distribution, take the whole thing (256 bits) as a uniform deviate and use the Box-Muller transform to get a normal deviate.",2010-09-23 02:47:10.0,1122.0,
3809,2980,0,"Yes, your modification using the hash of NYT/stock prices should also work although it has the one weakness of requiring at least 1 honest party. Interesting way to approach the issue.",2010-09-23 03:06:53.0,,user28
3810,3000,0,"Criteria will be 10% difference in probability of good vs bad outcome. Or lets say since it will be logistic regression, odds ratio = 2. alpha= 0.05, power=80%, I do not yet know what the pooled variance on the continuous variable is but let us assume that the standard deviation is 7mmHg.   Sequential analysis would be good but the final outcome is two  years after the measurement is taken.",2010-09-23 03:36:42.0,104.0,
3811,2988,0,Do you expect some dropouts during follow-up? Are there any other covariates to be included in your model?,2010-09-23 06:34:13.0,930.0,
3812,2979,0,"> you state *You could avoid the problem altogether by simply estimating* which is certainly correct. Nonetheless, the model you propose is a different one than the one Kyle wanted to fit (it is not a reformulation of the same model).",2010-09-23 07:37:05.0,603.0,
3813,2989,0,Thanks alot (both for the clever initial suggestion and for the folow thru).,2010-09-23 07:39:16.0,603.0,
3814,2835,0,"Nice, I didn't know that - thanks!",2010-09-23 08:18:55.0,1352.0,
3817,3009,0,Thanks for the references.,2010-09-23 09:21:59.0,183.0,
3820,3009,0,"I pointed to similar books in my own response, sorry didn't see your response while I was writing... Card's book is very great!",2010-09-23 09:36:32.0,930.0,
3822,3005,0,"@kwak Ok, the LARS algorithm seems largely more sophisticated than simple thresholding on variable importance, but the point is that I don't see a clear relation between the penalty parameter and the # of variables that are asked to be kept in the model; it seems to me we cannot necessarily find a penalty parameter that would yield exactly a fixed # of variables.",2010-09-23 09:51:08.0,930.0,
3823,3010,0,Wow. That's a great set of references. I'm assuming that husbands and wives would not be interchangeable. I wonder if that matters.,2010-09-23 09:51:37.0,183.0,
3825,3010,0,"It depends on the questions asked in the questionnaire. I came across a clinical study dealing with erectile function and sexual life (wife + husband), in this case the role of the respondents are not symmetric.",2010-09-23 09:55:19.0,930.0,
3826,3009,0,"@chl Great, that you mentioned the Thompson/Walker-paper about ""The Dyad as the Unit of Analysis"".",2010-09-23 10:00:39.0,307.0,
3827,3005,0,"@chl:> S-PLS you mean ?(you wrote LARS which is a different thing from either algorithm you discuss). Indeed, there is a monotoneous relationship between the penalty parameter and the # of component, but it is not a linear relationsip and this relationship varies on a case per case basis (is dataset/problem dependant).",2010-09-23 10:04:11.0,603.0,
3829,2963,0,+1 I use it often to make seeds.,2010-09-23 10:06:12.0,88.0,
3830,3005,0,"@kwak L1-penalty may be achieved using LARS, unless I am misleading. Your second point is what I have in mind in fact; have you any reference about that point?",2010-09-23 10:21:59.0,930.0,
3832,3005,0,"@chl:>* L1-penalty may be achieved using LARS, unless I am misleading* i didn't know that (and sort of doubt it). Can you provide a reference ? Thanks. for your second question: look On the “degrees of freedom” of the lasso

Hui Zou, Trevor Hastie, and Robert Tibshirani
Source: Ann. Statist. Volume 35, Number 5 (2007), 2173-2192. (there are many ungated versions).",2010-09-23 10:47:08.0,603.0,
3833,3005,1,"@kwak Check out Tibshirani's webpage, http://www-stat.stanford.edu/~tibs/lasso.html and the `lars` R package; other methods include coordinate descent (see JSS 2010 33(1), http://bit.ly/bDNUFo), and the Python `scikit.learn` package features both approaches, http://bit.ly/bfhnZz.",2010-09-23 10:58:16.0,930.0,
3834,2989,0,"No problem, kwak; remember only that it does premultiplication, though modifying the algorithm to do postmultiplication shouldn't be too hard.",2010-09-23 11:45:26.0,830.0,
3835,3002,2,"Monotonic *and increasing*, of course.",2010-09-23 13:32:59.0,919.0,
3836,2733,1,"Besides that fact that the former link wasn't really related to *mathematical* statistics, the latter one is neither: ""Data Mining with STATISTICA Video Series"" - http://www.youtube.com/user/StatSoft#g/c/B804A810436AFB03",2010-09-23 14:00:53.0,653.0,
3838,2971,1,"Best is the model with lowest future error and cross-validation gives you that estimate. The reason for c(Complexity)+e(Error) formulas is because you could use error on training data as estimate of future error, but that's overly optimistic, so you add a term to make this estimate unbiased, which is usually some function of model complexity",2010-09-23 14:32:33.0,511.0,
3839,2959,0,Thanks for the pointers. Will need to hit the library to make sense of it all.,2010-09-23 14:53:10.0,1393.0,
3840,2961,0,"Thanks, that was an interesting paper.",2010-09-23 14:53:50.0,1393.0,
3841,2852,0,"Thank you,Thylacoleo.These are DNA divergence data of six different groups.I want to know the difference between the first column band each one of others with P value.",2010-09-23 15:00:35.0,,Chuangye
3847,2917,0,"I am not sure if most CIs are really computed via t-values or rather via z-values. However, on bigger ns (> 30) this shouldn't make much of a difference.",2010-09-23 16:45:44.0,442.0,
3848,2891,0,"A typical ratio would be 1042:42. 1042 turn left, 42 turn right.",2010-09-23 17:13:32.0,,Pierre 303
3849,2989,0,M.:> it should be okay: i only need Q (i.e. qmult(eye(p))),2010-09-23 20:23:54.0,603.0,
3850,3005,0,@chl:> thanks. (i also recommend the lars paper www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf),2010-09-23 20:27:50.0,603.0,
3853,2975,0,"It seems to me no valid comparison is possible among these series unless there is some persistent spatial phenomenon producing the ""temperature signature.""  Otherwise they all measure different things.  If there is a spatial pattern, then you really have a problem of spatial interpolation, not of time series normalization.",2010-09-23 21:56:58.0,919.0,
3854,2901,0,"@Kwak: Thank you for clarifying your meaning.  Of course, the same response could be applied to any odd moment: they measure asymmetry further and further out in the tails.",2010-09-23 22:07:46.0,919.0,
3855,2901,0,"@Whuber:> Of course. Note that even for a fair tailed distribution like the gaussian, by the 7th moment you are already in effect comparing the max to the min.",2010-09-23 23:12:54.0,603.0,
3856,2945,0,"If overfitting is the tendency of a model fitting procedure to fit noise in addition to signal, then we can look at a given procedure to see where such tendencies could arise.  Perhaps due to a lack of imagination or knowledge, while considering a few different procedures, I couldn't boil this down to something that can't be restated as ""number of parameters"" (or ""effective number of parameters"").  We could flip this on its head and ask: all else equal, what happens when we introduce noise to our data?  Then we arrive at measures such as Ye's GDF.",2010-09-23 23:22:11.0,251.0,
3857,2945,0,"@ars:> suppose you compare two models for $y$ a) $y=\\alpha_1*xz$ and $y=\\alpha_1*x$. They have the same number of parameters, but the first one is sensitive to noise in both x and z.",2010-09-23 23:29:34.0,603.0,
3858,2945,0,"@kwak: That doesn't meet my ""all else equal"" case -- sorry if that was unclear.  I'm really trying to arrive at an answer to Yaroslav's question, perhaps restated: is there something other than dimensionality to account for tendency to fit noise in addition to signal?  And keeping the considered case very simple, I'm failing ...",2010-09-23 23:36:08.0,251.0,
3859,3002,0,Of course. Thanks. I've updated the answer.,2010-09-24 03:06:23.0,159.0,
3860,3032,0,wow. i am an embarrassed nOOb. thanks for the pointer.,2010-09-24 03:33:32.0,1410.0,
3861,3034,0,"This is great Dason, thanks so much. Rob's code was great but your example made it obvious to me what was happening -- thanks for taking the time to do that!",2010-09-24 03:39:19.0,1410.0,
3864,3038,0,"what do you mean by ""this assumes the null hypothesis to be true"" ?",2010-09-24 08:39:27.0,223.0,
3865,3038,0,"If you want to be able to control the probability of declaring wrongly ""there is a difference""  you need to separate the two hypothesis (did I already mentionned I love this quote: http://stats.stackexchange.com/questions/726/famous-statistician-quotes/728#728 ;) )",2010-09-24 08:40:19.0,223.0,
3866,2775,0,"> the problem with this method is that you can't control the ratio of smallest to largest eigenvalue (and i think that as the size of your randomly generated dataset goes to infinity, this ratio will converge to 1).",2010-09-24 09:28:03.0,603.0,
3867,3045,0,"> the problem with this method is that you can't control the ratio of smallest to largest eigenvalue (and i think that as the size of your randomly generated dataset goes to infinity, this ratio will converge to 1).",2010-09-24 09:28:29.0,603.0,
3868,2783,0,"> Not random: two matrices generated from the same Whishard will not be independant from one another. If you plan to change the Whishart at each generation, then how do you intend to generate those Whishart in the first place?",2010-09-24 09:32:47.0,603.0,
3869,3045,0,"> besides, the method is not very efficient (from a computationnal point of view)",2010-09-24 09:36:01.0,603.0,
3870,3045,0,"Your ""random matrix"" is a specially structured one called a ""diagonal plus rank-1 matrix"" (DR1 matrix), so not really a good representative random matrix.",2010-09-24 10:08:08.0,830.0,
3871,3038,0,@Robin the p value of a null hypothesis significance test is the probability of seeing as or more extreme data than that observed assuming the null hypothesis is true; but perhaps I could word the statement above better.,2010-09-24 10:26:31.0,183.0,
3872,3038,0,@Robin I modified the question to try to make my point clearer,2010-09-24 10:32:39.0,183.0,
3873,2783,0,@kwak: I don't understand your question: the Bartlett decomposition will give independent draws from the same Wishart distribution.,2010-09-24 11:03:38.0,495.0,
3874,3047,0,I was wondering if someone would provide a Bayesian approach. Excellent. Thanks.,2010-09-24 11:41:32.0,183.0,
3875,2783,0,"> Let me rephrase this, where do you get the scale matrix of your whishart distribution from ?",2010-09-24 12:16:44.0,603.0,
3876,2971,0,On the other hand reasoning in light of Runge phenomenon (Physics inspirations again) drives to a conclusion that future error is something about Complexity/Train_Error.,2010-09-24 12:25:26.0,88.0,
3877,2783,0,"@kwak: it's a parameter of the distribution, and so is fixed. You select it at the start, based on the desired characteristics of your distribution (such as the mean).",2010-09-24 12:40:23.0,495.0,
3880,2901,0,"@Kwak: Two quick follow-up questions; no need to respond if you don't want.  (1) ""Fair tailed""??  (2) What are the min and max of a Gaussian?",2010-09-24 13:29:44.0,919.0,
3883,3051,4,Have you seen [this](http://rss.acs.unt.edu/Rdoc/library/TTR/html/MovingAverages.html)?,2010-09-24 14:52:24.0,830.0,
3884,3051,0,"Can you give some background on this ""slide"" idea?",2010-09-24 15:01:01.0,5.0,
3885,3055,0,"Thank you Shane, but this seem to find local minima (or maxima) - i.e. a single point in a region. My data ( as any biological data) IS NOISY> I don't really care about point minima themselves but about larger regions which are low.",2010-09-24 15:53:49.0,634.0,
3886,3055,0,"If you have local maximum and minimum points, you can easily calculate the differences.  So you want to know instances when the differences are both large in magnitude and in ""duration""?  Is this time series data?",2010-09-24 15:59:20.0,5.0,
3887,3055,0,"@david Perhaps, you can iteratively use this function. Use the function to identify a minima. Drop that point and surrounding points (say x points within some tolerance level). You can choose a tolerance level (e.g., +- 10 counts) which would define a flat region for your application.  Find a new minima on the new dataset. Will that work?",2010-09-24 16:01:07.0,,user28
3888,3055,0,@shane The analogy that comes to mind is that of valleys in a mountainous region. I think the goal is to identify all the valleys and the issue is some valleys are 'deeper' and some are 'shallow' relative to the mountains.,2010-09-24 16:02:57.0,,user28
3889,3055,0,"@Shane It's not a time series, these are coordinate along the genome (chromosome).",2010-09-24 16:03:43.0,634.0,
3890,3055,0,"@srikant: this function gives you all the local maximum and minimum indexes.  You can easily use these to calculate the depth of the valleys.  What I'm missing in this question is whether there is some other factor to be considered (e.g. ""width"")?",2010-09-24 16:05:04.0,5.0,
3891,3052,0,Could you provide a small data sample?,2010-09-24 16:20:43.0,5.0,
3893,3052,0,@Shane see update,2010-09-24 16:43:03.0,634.0,
3894,3052,0,"@David Thanks.  As both the answers imply, time series analysis can be applied here since you have ordered observations.",2010-09-24 16:50:52.0,5.0,
3895,2901,0,"@Whuber; the explanation for (2) is quiet subtile. The basic idea is based upon 'a quantile alternative to kurtosis' (J.J.A. Moors). Just as the kurtosis is determined by the normalized sum of (E_7-E_5) and (E_3-E_1) (where E_i is the ith octile), the 8^th central moment is determined by the normalized sum of (G_15-G_14) and (G_3-G_1) (where G_i is the ith 100/16 percentile) -the top and bottom 5% of the data.",2010-09-24 17:39:26.0,603.0,
3896,2901,0,The same applies to the odd moments.,2010-09-24 17:45:00.0,603.0,
3897,3052,0,"This is kind of hard to answer without know exactly what you're looking for. Can you maybe circle the points on the plots that you're looking to capture? What do you consider a ""valley""? how low does it have to go and what are you looking to return? It's hard to formulate a solution without knowing the question, ie thresholds and such.",2010-09-24 17:48:37.0,1409.0,
3898,2935,0,Gaetan:> yes linear regression can handle slope dummy.,2010-09-24 17:56:04.0,603.0,
3899,2901,0,"@Kwak: I'm not grasping the subtlety of attributing a min and max to a distribution that doesn't have one! ;-)  I suspect you may be referring to sampling theory, whereas I read the original question as one about theoretical distributions.",2010-09-24 18:00:51.0,919.0,
3900,2901,0,"@whuber:> Oh, then indeed i missunderstood your comment. Yes min (max) was a poor choice of terms.",2010-09-24 18:13:22.0,603.0,
3901,3053,0,"This does not return what the asker wanted, but 5.33 5.00 6.33. However, it looks quite interesting. Can you explain your idea, because I don't get it.",2010-09-24 18:14:07.0,442.0,
3902,3057,0,"@cxr Thank for your response. I has a look at `surveillance` and `DCluster `, but could you please be a bit more specific? They are both relatively large packages and their aim seems quite specific. I'm not sure where to begin.",2010-09-24 18:37:44.0,634.0,
3903,3052,0,"@ Shane♦ Thank you. As I have no experience with time-series analysis also, could you leave a few pointers of where should I start?",2010-09-24 18:39:08.0,634.0,
3904,3061,0,Duplicate? http://stats.stackexchange.com/questions/3062/how-do-we-find-concordance-between-two-sets-of-non-linear-non-monotonic-data,2010-09-24 19:13:01.0,5.0,
3905,3061,0,@user1417: You should edit a question rather than posting a new one.,2010-09-24 19:13:28.0,5.0,
3906,3061,0,You should give a more precise reference than 'Lin's concordance correlation coefficient'.,2010-09-24 19:24:28.0,603.0,
3907,3052,0,@David: Why not start by looking at the drawdowns function that I posted.  Walk through the example and look at the algorithm.  It's very simple.,2010-09-24 19:41:12.0,5.0,
3908,3052,0,"@David: By the way, I hate to state the obvious, but just looking at the regions you selected in your graph, it looks like you chose the area in the lowest quartile.  Is that a sufficient criteria?",2010-09-24 19:44:51.0,5.0,
3909,3064,0,"Oh, and feel free to retag - my browser isn't working with the auto-suggest, so I'm having a hard time seeing what's out there.",2010-09-24 19:55:12.0,71.0,
3910,3058,0,Please don't downvote without providing a comment.  How am I supposed to know what's wrong?,2010-09-24 19:59:02.0,71.0,
3911,3065,0,"Obviously you can apply the same approach using something else than the moving average, this was just to give an idea.",2010-09-24 20:38:43.0,582.0,
3912,3065,0,"+1 Thank you very much, nico. Let me see if I got you right: at the end, this is basically like setting some global threshold and defining any point with value < threshold as a part of a valley. The sampling etc. is just used to get some meaningful measure (quantile) to set the threshold. Why can't we use a single threshold for the entire points, I mean, if we did enough simulations we would get straight (read and yellow) lines. Also, correct me if I'm mistaken, but this does not take into account the surrounding environment but examines the absolute value of each point.",2010-09-24 21:29:29.0,634.0,
3913,3065,0,"@David B: of course, you could use a global threshold and that would probably save you some calculation time. I guess choosing something like 1/3 of the global mean could be a start. This swapping process is probably more helpful if you use some other statistics than the moving average, it was mostly to give an idea. Anyway the moving average will take into account the surrounding, in the example it will take into account a window of 10 points.",2010-09-24 21:35:14.0,582.0,
3914,2945,0,"ars: for maximum likelihood estimation, curvature of the parametric manifold affects sample complexity",2010-09-24 21:58:55.0,511.0,
3915,2945,0,"Yaroslav Bulatov:> even more intuitively, the median is unaffected by shifts concerning <50% of the sample, yet it's still a one parameter location model (just like the mean).",2010-09-24 22:08:31.0,603.0,
3916,3054,0,"BTW, I once wrote about a usage of this function to implement the notion of ""quantile loess"" : http://www.r-statistics.com/2010/04/quantile-loess-combining-a-moving-quantile-window-with-loess-r-function/",2010-09-24 22:11:50.0,253.0,
3917,3052,0,it sounds like you want to build histograms of depths of coverage for each span from 1 to 100k (or so).  Then you probably might like to color-code your overall genomic chart where each vertical bar's intensity depends on for how many histograms it fell into the bottom percentile (or so).,2010-09-24 23:13:47.0,486.0,
3918,3060,0,Seems interesting. Do you know of a simple R implementation?,2010-09-25 01:54:38.0,104.0,
3919,3008,0,Do you have a worked case in R?,2010-09-25 02:07:25.0,104.0,
3920,2988,0,"Let me suck a dropout rate out of my thumb - 20%.  We will indeed collect many variables for instance, age, trauma score but I wanted to keep things as simple as possible for the power calculation. I have often found it useful to discuss a primary model and then secondary models that are loaded with more finesse and nuance.",2010-09-25 02:11:12.0,104.0,
3921,2766,0,"Regarding the wikipedia link that you gave which requires data to be normally distributed, however my data is not presumed to be normal or of any other distribution. Further, what is the best way to compare distributions if you think the mentioned method is not? Thanks !",2010-09-25 03:49:38.0,1102.0,
3922,2766,0,"@Harpreet: true enough, however, if your distribution has a finite variance, the central limit theorem ( http://en.wikipedia.org/wiki/Central_limit_theorem ) describes the asymptotic distribution of the sample mean. Since you presumably do not know the population variance, you would have to estimate that as well. see also http://en.wikipedia.org/wiki/Student's_t-statistic#Prediction",2010-09-25 03:58:17.0,795.0,
3923,3070,0,"+1, good points.  On this: ""unpleasant ideas about memory management"" .. interesting, can you elaborate?",2010-09-25 05:27:26.0,251.0,
3924,3070,1,"my memory is going _somewhere_; my experience with Java outside of Matlab usage indicate it is the likely culprit, and running in `-nojvm` appears to help...",2010-09-25 05:42:58.0,795.0,
3925,3070,0,Thanks shabby!!,2010-09-25 05:44:23.0,1102.0,
3926,3071,0,Nice one too. Thanks ars!,2010-09-25 05:45:06.0,1102.0,
3927,3070,0,"@shabbychef: ah, thanks.",2010-09-25 05:56:21.0,251.0,
3928,2975,0,"It is complex and messy real-world data. The ""thermal anomaly"" is (I hypothesize) variable in time but consistent in location. Overlaid on this is a different large-scale regional temperature gradient, seasonal and day/night variations, noise, instrument variability, etc. The flight paths come and go in different directions (so the mean temp of two flight paths should not be equal, even if season+time-of-day are removed, because of the large-scale regional gradient, etc.)...",2010-09-25 07:14:29.0,957.0,
3929,2978,0,"This is helpful, thank you. And addresses the simplified problem stated above. Unfortunately, as commented above, the real world is much more complex than what I initially described. I'm still working on it...",2010-09-25 07:15:46.0,957.0,
3930,2988,0,"Ok, but usually the expected % dropout, the number of covariates, and whether covariates are measured with errors (see e.g., http://j.mp/9fJkhb) enter the formula (in all case, it will increase the sample size).",2010-09-25 07:37:32.0,930.0,
3931,3053,0,"@Henric I use this trick frequently, yet user1414's code return this roll with slide 1, not 2, as intended by OP. Check out `(c(0,0,x)+c(0,x,0)+c(x,0,0))/3` to see what I mean (and how does it work). The proper formula would be: `(c(0,0,x)+c(0,x,0)+c(x,0,0))[1:(length(x)-3)*2+1]/3` (we must cut 0-padding at the beginning and select even elements then.",2010-09-25 08:00:54.0,88.0,
3932,3058,0,"It wasn't me, but this is slow (but not much slower than `rollapply`).",2010-09-25 08:06:01.0,88.0,
3933,3072,0,"+1 Not tada, rv/windowsize ;-)",2010-09-25 08:24:54.0,88.0,
3934,3072,0,"This marg... comment box is too narrow for this code, so I've posted a new answer.",2010-09-25 08:31:46.0,88.0,
3935,3054,0,"You may add a 0 at the end of x (`x<-c(x,0)`) to get the last element of answer.",2010-09-25 08:36:22.0,88.0,
3936,3070,0,"My favorite example of MATLAB strange built-in codes is shuffle, which reorders the data with the ordering returned by sorting a freshly created random vector.",2010-09-25 08:49:05.0,88.0,
3937,3054,1,"@mbq; that is making a strong assumption that the observation is 0. I had been mulling this point and T-Burns is making the same assumption (an unobserved 0). I would prefer perhaps to pad with NA and pass in the `na.rm = TRUE` argument to `mean`. The answer won't be the same as what the OP requested, but it seems more useful. I'll edit my answer to include this.",2010-09-25 08:59:38.0,1390.0,
3938,3054,0,"@ucfagls Yet this is easy to change and as you said this assumption was made by the OP. On the other hand, I would be even more restrictive and removed the last average.",2010-09-25 09:24:45.0,88.0,
3939,3058,2,"wasn't me either, but as mentioned by yourself, pre-allocation of the result object will help with the speed issue. One trick, if you don't know, or it is tedious/difficult to determine, the size of the result object you need. Allocate something reasonable, perhaps pre-filling with NA. Then fill in with your loop, but add a check that if you are approaching the limit of the preallocated object, allocate another big chunk, and continue filling.",2010-09-25 09:43:32.0,1390.0,
3940,3058,1,"@mbq; Speed of results, whilst important, isn't the only consideration. Instead of having to reinvent the while and handle all the indexes etc in the custom solutions, the one-linear that is `rollapply` is much easier to understand and grep the intention of. Also, `rollapply` is likely to have had many more eyeballs checking its code than something I might cook up one afternoon. Horses for courses.",2010-09-25 09:45:50.0,1390.0,
3941,3069,1,"This should be community wiki, IMO.",2010-09-25 12:01:46.0,5.0,
3942,175,8,"Sharpie, I wonder if it's time to accept a ""best answer""or at least explain why you are not satisfied with the answers on offer so far.",2010-09-25 13:42:30.0,521.0,
3943,3058,0,"ucfagls, thanks for adding some info on pre-allocating when you don't know the ultimate size of the object - that's helpful.  And while I agree that rollapply is probably almost definitely the way to go, I a) wanted to provide an example that would calculate the edge cases with less than three values, and b) just wanted to use `while()`, which I've yet to have a use for ;)",2010-09-25 14:43:16.0,71.0,
3944,3075,0,"Thanks for your answer.  Regarding not knowing the true status: we do have extensive questionnaires and are well aware of the BCG vaccine issue with the skin test - in fact, these blood tests are supposed to resolve that issue because they use a different set of antigens than the PPD you're used to.  That's almost a separate question, however, and one we're going to be working on a bit later - right now, my interest is in making this test 'longitduinally aware'.",2010-09-25 14:50:42.0,71.0,
3945,3075,0,"... especially because some individuals do flip from negative to positive, and that's often a product of their typical nil and TB results making small fluctuations - nil down a bit, TB up a bit, and suddenly they're positive.  Next test, they've gone back to being negative.  I can see that as I review individual results, but I'm not sure how to appropriately incorporate my intuition into a model.",2010-09-25 14:54:01.0,71.0,
3946,3075,0,"Finally, while I have tried taking the log results, that doesn't seem to be sufficient to get them even close to normality.  They are very, very skewed, and the truncation at the high end further complicates this by adding a noticeable blob of density at the ceiling.  Interestingly, however, the sample-wide nil and TB result distributions are quite similar, with the only difference being that that blob on the ceiling is much larger for the TB results.",2010-09-25 14:57:14.0,71.0,
3947,3075,0,Thanks for taking the time to read and answer this beast of a question!,2010-09-25 14:57:47.0,71.0,
3948,2915,0,I don't know what starting a bounty mean?  I have been asked to start one.  What are its implications?,2010-09-25 16:06:10.0,1329.0,
3949,1001,0,"Indent your code with 4 spaces to get it converted into code. Or, equivalently, select it and use the button with binary data.",2010-09-25 17:05:36.0,88.0,
3950,1001,0,mdb: Not sure what you mean. I would not ask if it would all be correct and without question. I am not sure what you mean with indenting. I hope it has nothing do to with using this website 'right' ;),2010-09-25 19:22:40.0,608.0,
3951,3078,0,"Thank you for your input. I will read more about the Anderson-Darling test, which I have not heard of . Regarding the Chi-Square test, doesn't this test require that the distribution is Chi Square? I agree that all other assumptions are very relaxed...",2010-09-25 19:28:40.0,608.0,
3952,1001,0,"I think it's just to remind you that this is a Markdown enabled website, so you can benefit from easy syntax highlighting (which facilitates reading) through md markup (http://j.mp/9bQHMC) -- or the utilities provided in the on-line editor.",2010-09-25 21:38:14.0,930.0,
3953,3069,0,Would you care to explain why you couldn't also look at R?,2010-09-25 23:32:31.0,334.0,
3954,3069,0,"@DirK: I've hardly heard of R. Moreover I wanted to learn some programming language like Python, and then again too I don't think R is anywhere close to python, IMO. I hope it answers your question.",2010-09-25 23:37:38.0,1102.0,
3955,3069,1,"Poke around a little here and at StackOverflow in terms of what people recommend for *statistical analysis and programming*. Many of us feel that there is no real alternative to R.  But just like  beauty, this is in the eye of the beholder, so good luck.",2010-09-25 23:41:07.0,334.0,
3956,2897,0,"the result you state for [sub]gaussian tails does not look right. according to the bound [$A\\sqrt{p}$] you cite, the $p^{th}$ norm of a centered gaussian variable would not [in the limit] exceed 1. but the $p^{th}$ norm of a rv tends to its ess sup, which is $+\\infty$ for a gaussian variable.",2010-09-26 02:00:25.0,1112.0,
3957,3078,0,"Ampleforth, I am not sure that is the case.  I think you can test all sorts of data with the Chi Square test.",2010-09-26 04:18:22.0,1329.0,
3958,3076,0,"+1 I like the second point (I use roxygen + git). The first point makes me think also of the possibility to give your code to another statistician that will be able to reproduce your results at a later stage of the project, without any help.",2010-09-26 09:52:17.0,930.0,
3959,3082,4,"I just can't understand why people think this swapping one is 'simpler' or 'more naive' than FY... When I was solving this problem for a first time I have just implemented FY (not knowing it has even a name), just because it seemed the simplest way to do it for me.",2010-09-26 09:56:18.0,88.0,
3960,3044,0,Thanks for the link and the pointer to the `equivalence` package.,2010-09-26 09:59:49.0,930.0,
3961,3082,1,"@mbq: personally, I find them equally easy, although I agree that FY seems more ""natural"" to me.",2010-09-26 10:29:21.0,582.0,
3962,3079,0,I´ll look into the matching package. My first analysis does control for the same factors as implemented in the match procedure in a multivariate linear regression model. Thx for the feedback..,2010-09-26 11:50:40.0,1291.0,
3963,3082,2,"When I researched shuffling algorithms after writing my own (a practice I have since abandoned), I was all ""holy crap, it's been done, and **it has a name**!!""",2010-09-26 13:55:04.0,830.0,
3964,3081,0,"I particularly like the likelihood ratio as a means of aggregating evidence in meta-analysis; if you have sufficient data to compute them for each study, you simply compute the product across studies to represent the aggregate evidence for/against a hypothesis.",2010-09-26 14:36:21.0,364.0,
3965,3086,0,How many individuals are used to estimate the item intercorrelations? What software is used for computing both correlations?,2010-09-26 15:42:05.0,930.0,
3966,3073,0,"thanks for translating. I figured it would be an easy exercise, and I learned some R from it",2010-09-26 15:59:52.0,795.0,
3967,3089,1,"@shabbychef Yes, it's the MLE of correlation in a two-way table, see Hamdam (1970), http://j.mp/9SN7Lk, or Brown & Benedetti (1977), http://j.mp/aJjjzu.",2010-09-26 17:11:20.0,930.0,
3968,3088,0,But aren't these steps already part of meta-analysis?,2010-09-26 17:14:57.0,930.0,
3969,3086,0,matrix is 5000 x 300. Software is SAS. PROC CORR and PLCORR. Convergence was not reached so the number of iterations have been changed to 100 iterations.,2010-09-26 17:29:12.0,1154.0,
3970,3091,1,"You may be interested in this related question, http://stats.stackexchange.com/q/2628/930.",2010-09-26 18:30:37.0,930.0,
3971,3093,0,"Thanks for both of your responses. I will dig a little further into some of the references you suggested chi. I guess what I was getting at in my question, was aside from the issue of spatial effects, if I actually did have population-wide independent data, would classical hypothesis tests suffice - since they are based on sampling theory? Is multi-level modelling the only approach here? Forests aren't really nested in counties, rather I'd say they make up a measured proportion of each county, and those measurements are correlated in space...",2010-09-26 20:16:16.0,,kip
3972,3093,0,"@kip Would you mind explaining ""they make up a measured proportion of each county""? I initially thought there was a % of Lyme disease recorded at each site (in your case all the forests in each country).",2010-09-26 20:29:16.0,930.0,
3973,3093,0,"@kip About your second point, well if you consider you have a set of measurements collected on a fixed period of time on all possible statistical units (this is what you called your population), you can apply inferential procedures provided you are willing to assume there is an evolving generating model underlying the observed data (but see the related question I put in comment for a thorough discussion, e.g. by @ars, on this point).",2010-09-26 20:29:53.0,930.0,
3974,3089,0,"@chl thanks, I thought I that was the case, but it always seems like a too remarkable fact because the cutoffs are unknown.",2010-09-26 20:31:35.0,795.0,
3975,3089,1,"@shabbychef Yes, the threshold remains unknown. To my opinion, John Uebersax provides a great overview on TCs correlation, http://j.mp/bnhem7.",2010-09-26 20:34:57.0,930.0,
3976,3088,3,"@chl: True, but the point is that these steps get to the essence of the question.  A meta-analysis would be helpful only when there are many studies (not just two) and their merits have already been carefully evaluated.  The question before us is really asking how one goes about evaluating the quality of a study, or pair of conflicting studies, in the first place.  Cyrus has pointed to some of the many aspects of this; a reasonable treatment usually requires one or two semesters of university-level study.  In this light I think his use of the term ""heroic"" is somewhat understated!",2010-09-26 20:38:33.0,919.0,
3977,3095,2,Thanks for your (always) enlightened remarks! You provide a more thorough response than mine wrt. OP (I must admit I'm a little bit biased toward epidemiology). [I'll +1 ASAP],2010-09-26 20:40:18.0,930.0,
3978,3081,0,"I commented on the (ir)relevance of meta analysis after Cyrus's answer, but upvoted this response for everything else, especially the bullet points.",2010-09-26 20:40:59.0,919.0,
3979,3080,0,Maybe this should be CW?  There will not be a unique answer to this question and multiple perspectives and approaches might emerge.,2010-09-26 20:42:23.0,919.0,
3980,3088,1,"@whuber Yes, I agree with you and @Cyrus. Of course, assessing the quality and trustiness of previous studies is a mandatory step (and it takes time to review every studies, especially when we have to contact authors because informations are missing in the MS); I just thought this was part of the meta-analysis, and the ""statistical part"" reduces to bringing a quantitative summary of trustworthy results.",2010-09-26 20:50:34.0,930.0,
3981,849,1,+1 For the example.,2010-09-26 21:27:40.0,776.0,
3982,3093,0,"@chi Sorry for the confusion, I meant a proportion for forest cover, and a binary indicator for presence or absence at each country. I think I get what you are saying here, would the assumption of not having an evolving generating model essentially be an assumption of 'stationarity'?",2010-09-26 21:41:44.0,,kip
3983,3095,0,"Thanks a lot for this explanation. It really helps. The distinction between process and population is what I was missing (among other stuff!).. that the process is stochastic, and will vary from place to place and time to time",2010-09-26 21:47:13.0,,kip
3984,3080,2,@whuber I would vote against CW because even if there are different perspectives there is likely to be one *best* approach. This is similar to how the same hypothesis can be tested using different frameworks/models but there is likely to be one best approach.,2010-09-26 22:17:32.0,,user28
3985,2761,0,"The unpaired student t test has two formulas.  The large sample formula is applied to samples with more than 30 observations.  The small sample formula is applied to samples with less than 30 observations.  The main difference in those formulas is how they calculate the pooled standard error.  The small sample formula is much more complicated and counterintuitive.  And, in reality it really makes very little difference.  I have tested that several times.  That's why I think most people have forgotten about this distinction.  And, they use most of the time the large sample formula.",2010-09-26 22:45:11.0,1329.0,
3986,3066,0,@suncoolsu: many thanks! I have just followed your advice and ran prcomp. I also stored the loadings matrix it produced. But how can I use this matrix to group together the pages?,2010-09-26 23:00:03.0,1007.0,
3987,3087,1,"""Let's be generous, too, and suppose you are actually selecting distinct pairs of indexes uniformly at random for your shuffles"". I don't understand why that assumption can be made, and how it is generous. It does seem to discard possible permutations, resulting in an even less random distribution.",2010-09-27 01:14:05.0,1421.0,
3988,3085,1,"+1. That demonstrates that the probability for a given card to end up in a given position approximates the expected ratio as the number of shuffles increases. However, the same would also be true of an algorithm that just rotates the array once by a random amount: All cards have an equal probability to end up in all positions, but there is still no randomness at all (the array remains sorted).",2010-09-27 01:30:05.0,1421.0,
3990,3081,0,"@whuber @Gaetan's question assumes that one study is closer to the truth. I try to take a step back and situate variations in results between studies within a meta-analytic framework, acknowledging the possibility that the studies may be of equal quality, but that random sampling or substantive differences may be the explanation.",2010-09-27 02:43:35.0,183.0,
3991,3081,0,"@whuber Even with two-studies it would be possible to form a meta-analytic estimate of the effect of interest. Of course, the confidence interval of the estimate of effect may be large. But a high degree of uncertainty is to be expected if only two studies have been conducted and they are giving conflicting results.",2010-09-27 02:44:04.0,183.0,
3992,3090,0,"Thanks a lot for the answer, I'll do the inquiry when I'll have more time",2010-09-27 04:56:38.0,223.0,
3993,3070,1,"@mbq: `shuffle` might be in a toolbox, is not stock matlab. could hardly be worse than builtin `randperm` which returns sort index of a random vector. Again, this is probably the wrong algorithm (I just learned about the Knuth-Fisher-Yates shuffle here on stats.SE)..",2010-09-27 05:14:06.0,795.0,
3994,3070,0,"@shabbychef True, I was trying to write about `randperm` and I've missed the name. And it is not a bad algorithm (it is correct and quite elegant), it is just blatantly inefficient because of a redundant sort.",2010-09-27 07:19:12.0,88.0,
3995,3089,0,"Thanks for explanation and references. But what i would know is when values obtained from tetrachoric are slightly different of person coefficient. Because they're supposed to be close. Indeed, in many programs, for estimation, Pearson r are used as start values for algorithm which compute tetrachoric correlation.",2010-09-27 07:33:07.0,1154.0,
3996,3085,0,"@Thilo: Sorry I don't follow your comment. An ""algorithm  rotates by a random amount"" but there's still ""no randomness""? Could you explain further?",2010-09-27 08:21:59.0,8.0,
3997,3085,0,"If you ""shuffle"" an N-element array by rotating it between 0 and N-1 positions (randomly), then every card has exactly the same probability to end up in any of the N positions, but 2 is still always located between 1 and 3.",2010-09-27 08:26:39.0,1421.0,
3998,3085,1,"@Thio: Ah, I get your point. Well you can work out the probability (using exactly the same idea as above), for the Pr(A in position 2) and Pr(A in position 3) - dito for cards B and C. You will see that all probabilities tend to 1/3. Note: my answer only gives a particular case, whereas @whuber nice answer gives the general case.",2010-09-27 08:46:09.0,8.0,
3999,3082,0,+1 A really nice question,2010-09-27 08:46:29.0,8.0,
4000,3093,0,"@kip Finally, I like @whuber point of view: ""you're modeling a process, not a population"", which seems pretty close to what I had in mind.",2010-09-27 09:27:46.0,930.0,
4001,3102,0,"Thanks. The idea of modelling the two curves together seems like a good one. However, I have theoretical reasons for wanting to quantify the degree to which the shape of the curves for the 100 and 400 metres are the same. Does the approach you mention do this?",2010-09-27 09:28:06.0,183.0,
4002,3102,1,"@Jeromy:> I'm afraid not. On the other hand, i would pay attention to the issue of sample size and model complexity as 200 observations ain't a lot.",2010-09-27 11:42:14.0,603.0,
4003,3080,0,"@Srikant: In any particular case I can imagine you could amass a strong defense to support your assertion.  In general, though--which is the present situation--the best answer will depend on the context.  As a simple (and incomplete) example, contemplate the differences between evaluating a pair of designed physical experiments (such as measuring the speed of light, where historically most of the confidence intervals have missed the truth!) and an observational study in the social sciences.",2010-09-27 15:03:40.0,919.0,
4004,3087,1,"@Thilo: Thank you.  Your comment deserves an extended answer, so I placed it in the response itself.  Let me point out here that being ""generous"" does not actually discard any permutations: it just eliminates steps in the algorithm that otherwise would do nothing.",2010-09-27 15:23:04.0,919.0,
4005,3116,2,"I agree with you and about a decade ago collected a bunch of such problems for a course (see http://www.quantdec.com/envstats/homework/class_03/paradox.htm ).  However, there is a strong pedagogical counter-argument: Probability itself can be confusing, so if you start off with counter-intuitive examples, you risk losing your audience forever (like Augustus DeMorgan, a pioneering probabilist, who later in life completely gave up on probability as hopelessly difficult!).  So caution is in order here, especially if you want to *motivate* people in an *introductory* setting.",2010-09-27 15:32:34.0,919.0,
4006,3080,0,"@whuber Perhaps, we should continue this conversation on meta. I admit that I am still fuzzy about when to use CW and when not to but to take up your point: the very best answer to this question would then be that the answer is context dependent and explain why via a few examples. In any case, I somehow feel that this question should not be CW but I am unable to articulate any more reasons beyond the ones I have outlined above.",2010-09-27 15:55:20.0,,user28
4007,3115,1,Have a look at my question here: http://stats.stackexchange.com/questions/1881/analysis-of-cross-correlation-between-point-processes,2010-09-27 16:35:52.0,582.0,
4008,3070,1,"@mbq: the other good part about `randperm` is that it is affected by the seeding of `randn`, whereas a mex'ed version of Knuth-Fisher-Yates perhaps cannot access the randn seed 'internally', and a pure .m version of shuffle would probably be too slow.",2010-09-27 16:38:31.0,795.0,
4009,3100,0,"When are two curves ""consistent,"" Jeromy?  (Without a clear definition, many different answers are possible.)  Your second initial thought is suggestive, but not dispositive.  For example, unless you choose just the right way to express the results--should they be times, speeds (miles per hour), or inverse speeds (hours per mile), for instance?--then you might fail to identify and quantify a ""consistency"" that is really present.  In particular, I would **expect** $\\theta_2$ to be smaller for the shorter race.",2010-09-27 16:45:07.0,919.0,
4010,2897,0,Thanks for catching that.  I forgot the exponent on the RHS; it's corrected now.,2010-09-27 16:52:17.0,89.0,
4011,3119,0,"Please elaborate : do you want to scale x2 so it lies between 0 and 0.5 like x1, or do you want to be able to predict x1 from x2?",2010-09-27 17:00:17.0,1124.0,
4012,3116,0,"I think it causes polarization. The students who are not interested in mathematics/probability will become confused, and the inquisitive/interested students will be inspired to learn more. Like you said, it might be best to exercise caution. Nothing could be worse than a confusing teacher presenting a confusing example!",2010-09-27 17:12:43.0,1118.0,
4013,3087,2,"This problem can be analyzed fully as a Markov chain on the Cayley graph of the permutation group.  Numerical calculations for k = 1 through 7 (a 5040 by 5040 matrix!) confirm that the largest eigenvalues in size (after 1 and -1) are exactly $(k-3)/(k-1) = 1 - 2/(k-1)$.  This implies that once you have coped with the problem of alternating the sign of the permutation (corresponding to the eigenvalue of -1), the errors in *all* probabilities decay at the rate $(1 - 2/(k-1))^n$ or faster.  I suspect this continues to hold for all larger $k$.",2010-09-27 17:48:33.0,919.0,
4014,3119,0,x1 and x2 are predictions from 2 different classifiers. I want to scale x2 so it lies between 0 and 0.5 like x1.,2010-09-27 17:57:12.0,,Bob
4015,3121,1,"I'm not sure what this means; do you have the numbers 1-100 in a bag, and are going to pick them out one at a time without replacement?",2010-09-27 18:05:22.0,795.0,
4016,3119,0,"try squaring your x2. it will look more like x1, then.",2010-09-27 18:07:25.0,795.0,
4017,3121,1,"@shabbychef: The formula in that case is n = 0, because the average and median are both 50.5! :-)",2010-09-27 18:13:22.0,919.0,
4018,3121,2,"@whuber: yes, this is why I'm not sure what is being asked. although the question could be: how many numbers must I pick until I pick one of 49,50,51 or 52, with 95% probability...",2010-09-27 18:32:22.0,795.0,
4019,3108,1,Is there an explanation for the appearance of 7?,2010-09-27 18:52:29.0,,user28
4020,3119,0,Let me explain the problem in another way. After adjusting one solution to the scale of the other I want to fit linear regression so that intercept=0 and slope=1.,2010-09-27 19:16:39.0,,Bob
4021,3119,0,"@Bob run a linear regression to get $x_1 = m x_2 + b$, then transform   as $\\hat{x_2} \\leftarrow m x_2 + b$. Then a linear regression of $\\hat{x_2}$ vs $x_1$ will have intercept 0 and slope 1.",2010-09-27 19:28:36.0,795.0,
4022,3119,0,"@shabbchef: You're right, but the relationship between $x_1$ and $x_2$ will still be curvilinear.  One needs to find *nonlinear* re-expressions of the variables before removing any ""tilt"" via linear regression.",2010-09-27 19:49:00.0,919.0,
4023,3121,2,"@shabbychef: I completely agree with you; my comment was just an amusing way to emphasize the vagueness.  The *answers* are obvious--sequential methods or standard experimental design methods, with a reference to the literature on the relative power of robust methods to handle the case of the median--but exactly *which* answer is appropriate (or best) will depend on how the numbers are being obtained and assumptions about their possible distribution.  E.g., if the numbers are in the range $[1,100]$ *and that's all you know,* I can guarantee 2% relative error with 95% confidence in 9228 draws.",2010-09-27 20:23:30.0,919.0,
4024,3121,1,"(continued)...But if you use a sequential method and assume normality and it turns out all the values are around 98 - 100, then you can stop after about a half dozen draws!",2010-09-27 20:26:05.0,919.0,
4025,3125,2,"You assumed a uniform distribution, so why do a simulation?  An exact answer is straightforward to obtain.  (Indeed, my facetious comment still applies: by assuming a uniform, you know *a priori* that the mean is 50.5, so you don't need to pick *any* numbers at all!)  But if *all* you know is that the numbers are in the range [1,100], you have to plan for the worst case, which will require up to 9000+ draws (when half the values are 1 and half are 100).  First we need to hear from the OP concerning what assumptions are valid to make here.",2010-09-27 20:30:14.0,919.0,
4026,3119,0,(Sorry about misspelling your handle: it's the cleverest one around and so deserves to be typed correctly!),2010-09-27 20:31:50.0,919.0,
4027,3125,0,"@whuber, I assumed the numbers where NOT put back in the ""pool"" (that is why with 100 draws you have 100% of the trials giving the good result). Also, the simulation works for ANY set of 100 numbers, I just used 1:100 because that's what the question asked (well, I guess at least), but it can be any set of 100 numbers :)",2010-09-27 20:49:32.0,582.0,
4028,3121,1,"@whuber: I would think this was a homework question, but it was so ambiguously worded.",2010-09-27 20:50:35.0,795.0,
4029,3126,0,Thanks for the link on the R-help mailing list. There is also this one on s-news: http://j.mp/8Zol8W.,2010-09-27 20:52:31.0,930.0,
4030,3125,0,"Anyway, you can easily modify the script to allow for resampling the same number multiple times (add `replace=TRUE` in the `sample` call). The result turns out to be around 5000 in that case",2010-09-27 21:06:16.0,582.0,
4032,3119,0,@whuber the OP states he wants to 'scale x2' to get a linear fit with given slope and intercept. I live to serve.,2010-09-27 21:37:00.0,795.0,
4033,3119,2,"@shabbychef: Ditto.  I think I got lucky this time in guessing the intent.  We have to recognize that non-statisticians often are unaware of the technical distinctions we make.  After all, a great deal of statistical consulting amounts to figuring out what someone might *really* be asking you!  Here the best clue is that the OP himself suggested a nonlinear re-expression (""link"").  The scatterplot he posted helps immensely in understanding the situation, too.  The combination of those useful clues seemed sufficient to venture an answer rather than to keep probing for clarifications.",2010-09-27 21:43:39.0,919.0,
4034,3121,2,"@shabbychef: Yep, the ambiguity is certainly there.  The followup about the median could make for some difficult homework, though: you have to make a lot of assumptions and the analysis is not found in elementary courses.  (Under some distributional assumptions the median is a more efficient estimator of central tendency than the mean, but most of the time in practice it's less efficient.)",2010-09-27 21:47:05.0,919.0,
4035,3108,0,"My general hand-waving explanation is this: people avoid {1, 5, 10} because they are too obvious and therefore ""not random"". Numbers less than 5 - well who wants a small RN! People then tend to go for the middle number between 5 and 10. I've tried this example six times now (in classes of size ~100) and it's worked each time.",2010-09-27 21:49:28.0,8.0,
4037,3125,1,@nico: Now you're getting to the heart of the matter: the trick is to find a method that works for the full range of possible pool contents consistent with one's assumptions.  All a simulation can do is indicate what can be achieved by sampling when one particular assumption is true.  Your modification already reveals what's at stake: the difference between a sample of 98 and one of 5000 is enormous.  This suggests looking a Bayesian and/or sequential sampling methods instead of designing a fixed sample size based on a frequentist assumption.,2010-09-27 22:09:29.0,919.0,
4038,3108,2,"And of course, 17 is the least random number.  http://www.catb.org/~esr/jargon/html/R/random-numbers.html  but my favorite random number is 37: http://jtauber.com/blog/2004/07/09/37_is_a_psychologically_random_number/  (though, also see http://scienceblogs.com/cognitivedaily/2007/02/is_17_the_most_random_number.php )",2010-09-27 23:17:01.0,251.0,
4039,3111,1,"This is my favorite example too (HIV test), but unsure if conditional probability is too ""advanced"" given the introductory nature (plenty of studies showing that it's not too intuitive).  If you do teach this, I recommend perusing Gigerenzer and the frequency method: http://library.mpib-berlin.mpg.de/ft/gg/GG_How_1995.pdf",2010-09-27 23:21:00.0,251.0,
4040,3111,0,"@ars:> maybe first you state them all the relevant informations in table form, then the problem ""what do you think is  p(AIDS|test=1)?"", then the counter intuitive punchline, only then  you show them the problem re-casted as a 'tree' (where the final 4 nodes are all possible cases) and the branches show the respective probability. In my experience, the last leg need not be understood by everybody, but it has to convey the importance of having a principled way of thinking about these issues.",2010-09-28 00:08:12.0,603.0,
4041,3100,0,"@whuber Thanks for the point about $\\theta_2$. The analysis is motivated by a theoretical and qualitative interest in consistency. Thus, my question is asking ""what is a good way to quantify consistency"". I.e., what is a good definition of consistency?",2010-09-28 00:51:03.0,183.0,
4043,3066,0,"Hello Laramichaels,
please find my answer below.",2010-09-28 04:05:03.0,1307.0,
4044,3101,0,Can you explain why this is a pyramind and not a triangle?  Or a triangle and not a ladder?  I'm not being snarky -- I'm wondering if I'm missing something significant that this visualization is trying to convey.,2010-09-28 05:03:31.0,251.0,
4045,3125,0,"@whuber: OK, I understand what you're saying now. Actually, to clarify on my previous comment: the code works for any 100 number set, but it does not give the same result. If we only use numbers between 90 and 100, for instance, 2 or 3 draws are enough to estimate the mean. Still, an interesting problem",2010-09-28 07:03:40.0,582.0,
4046,3089,0,"@shabbychef Sorry, I didn't read your updated answer before posting mine. The bias would increase when the cutoff depart from the mean, but it will be even worse when the departure is assymmetrical wrt. joint density.",2010-09-28 07:43:22.0,930.0,
4047,3089,0,Thanks a lot. Great word. It will be helpful for me.,2010-09-28 08:06:34.0,1154.0,
4048,3136,3,"@Skarab Maybe I'm totally off, but wouldn't you expect that the frequency of any word will be inversely proportional to its rank in the frequency table of words, according to Zipf's law (http://j.mp/9er2lv)? In this case, check out the `zipfR` package.",2010-09-28 09:29:57.0,930.0,
4049,3136,1,I agree with @chl - it would be minor miracle if your data was normally distributed. Perhaps another question about what you want to do with the data would be worthwhile. Don't reinvent the wheel!,2010-09-28 09:32:54.0,8.0,
4052,3129,3,"I agree with you, although most of the sample size calculation that are done when devising RCTs are based on parametric models. I like the bootstrap approach, but it appears that very few studies rely on it. I just found those papers that might be interesting: http://bit.ly/djzzeS, http://bit.ly/atCWz3, and this one goes in the opposite direction http://bit.ly/cwjTHe for health measurement scales.",2010-09-28 11:22:16.0,930.0,
4059,3140,0,"Can you please make your question a bit more understandable? It seems as you want to use Cross-Validation to make classification itself, but this is nonsense.",2010-09-28 14:50:24.0,88.0,
4060,411,0,Thank you for this great overview :) Are there books you know that especially discuss such distance measures for distributions in detail? I am particularly interested in non-parametric measures that operate with minimal assumptions.,2010-09-28 15:03:02.0,608.0,
4061,3136,2,How could your data be distributed according to a model that gives non zero probability to negative occurrence?,2010-09-28 15:05:49.0,603.0,
4062,3100,1,"That's not a statistical question, LOL!  But (to stave off possible objections) I agree that its answer can be usefully informed by statistical thinking.  ""Consistency"" depends on what you are studying and what use you will make of a decision that two response curves are ""consistent"" or not.  In some applications it would be enough that they are both increasing or decreasing; in others it would amount to a test of equality of all parameters.  Without more information, one can only guess where along this spectrum your needs fall.",2010-09-28 15:06:45.0,919.0,
4064,3130,1,It's *binomial* estimation.  (There is no marking or recapturing at all. which leads to hypergeometric estimation.),2010-09-28 15:11:05.0,919.0,
4065,3136,0,What is the reason for doing this test?,2010-09-28 15:12:36.0,919.0,
4066,3134,1,"The answer of 25000, which is obviously ridiculous no matter what the true state of affairs may be, indicates that a power calculation is not appropriate here.  The answer of 9228 I obtained guarantees that no matter what the distribution may be (provided only that its values all lie between 1 and 100), 95% of all simple random samples (without replacement) with n=9228 will lie within 2% of the mean.  Power is not relevant!",2010-09-28 15:31:33.0,919.0,
4067,3123,1,"The asymptotic distribution of the median is Normal with variance inversely proportional to the square of the pdf at the median.  Thus, experimental designs for median estimation have to make some specific assumptions about the nature of the distribution near its middle.  (See http://mathworld.wolfram.com/StatisticalMedian.html for example.)  Usually people study a range of parametric alternatives to compare the efficiency of the median to that of the mean; relative efficiencies translate via the usual square-root law into relative sample sizes.",2010-09-28 15:35:51.0,919.0,
4069,3089,1,"@chl: not a problem. I like that you posted R code (I am slowly learning R); my version is in homebrew Matlab, which I cannot share. Having code that the OP can test is definitely the way to go.",2010-09-28 16:40:17.0,795.0,
4070,3145,2,"Huh, factors are well handled using the `factor` command, so no need to rely on dummy coding (otherwise, it's accessible through `model.matrix(lm(x~a*b))` for my toy example).",2010-09-28 18:01:10.0,930.0,
4071,3145,2,"@chl thanks. I know very little R, but suspected this simple algebraic trick could be applied in any case, or at least illustrate what is going on 'under the hood'",2010-09-28 18:05:48.0,795.0,
4072,3145,0,"Yes, definitely! this was just a precision. (and you have my +1)",2010-09-28 18:14:52.0,930.0,
4073,3144,0,"Thanks! It did work, question: it did not force the condition that coefficients sum up to 0, but minimized the sums for both factors. Is there a way to force the ==0 condition, or it simply does not do that since it would affect the error ?",2010-09-28 19:26:09.0,1439.0,
4074,3148,0,"I've performed a Chi-Squared test, but that was based on the subjective results that I received from user surveys. They sound quite similar though?",2010-09-28 19:37:35.0,1441.0,
4075,3148,0,"Yes, you could do a chi-squared test as well if that is familiar to you. Both the test I recommended and the chi-squared test will give you the same answer.",2010-09-28 19:43:23.0,,user28
4076,3066,0,"@suncoolsu: I am dealing with a similar problem, but I want to ""cluster"" the individuals that have the same ""dynamics"" (actually I have a huge number of timeseries per regions and I want to model them). I was thinking to use pam with the correlation distance (1-rho). Is this a recommended way? Could you please suggest some paths to explore?",2010-09-28 19:59:12.0,1443.0,
4077,3125,0,I think this is what I was looking for nico. The numbers arent put back into the pool.,2010-09-28 20:05:38.0,,Greg
4078,3148,0,Well I'd rather use a different method because I have already implemented the Chi-Squared test on other results..I'm just not sure if this test is too similar to the Chi-Squared one or not..?,2010-09-28 20:34:14.0,1441.0,
4079,3125,0,"by the way, how can I copy the formula and change the numbers?",2010-09-28 20:44:53.0,,Greg
4080,3143,2,Please **do not crosspost** simultaneously here and on SO.,2010-09-28 20:47:17.0,334.0,
4082,3101,0,"Hi ars, it could well be the triangle of evidence. The reason for the word pyramid is probably historical, I am simply using it as it was taught to me.  BTW, it shouldn't be a leader, since the width of the triangle also reflects the abundance of such an evidence in practice (Or at least, that's my guess :) )",2010-09-28 21:09:48.0,253.0,
4083,3125,0,"@Greg, I am not completely sure what you are asking... if you want to change the numbers just change the `numbers` variable. For instance `numbers <- c(1, 5, 18, 32, 36, 42, 95, 97)` or whatever you want to use :) The rest of the script will run for any set of numbers.",2010-09-28 21:10:06.0,582.0,
4084,3144,1,@Vytautas: I think you're forgetting the reference level for your factor which is not shown (since it's the baseline for the others).,2010-09-28 21:14:14.0,251.0,
4085,3125,0,"@nico, Im asking how I can run the script for myself. Like is there a website I can cut and paste it to? I tried on here but it didnt display a graph.",2010-09-28 21:17:02.0,,Greg
4086,3101,0,"I hadn't come across this before, so just curious for any insight into what it was saying.  Your point about abundance of evidence makes sense.  Thanks.  :)",2010-09-28 21:32:23.0,251.0,
4087,3144,1,"@Vytautas or take a look at *Interpreting model matrix columns when using contr.sum*, http://j.mp/9pNFQe.",2010-09-28 21:43:49.0,930.0,
4093,1931,0,"@Kwak: A continuous mapping from R^n to R cannot induce a total order.  Technically, I was wrong, because any bijection from R^n to R will ""order"" R^n via the order on R, but this order will not be compatible with any of the metric structure on R^n (which is an essential part of the concept of a ""median""), and that was the spirit of my comment.",2010-09-28 22:31:51.0,919.0,
4096,3156,0,Hi. Please see my edit above. Thanks.,2010-09-28 23:25:34.0,1102.0,
4097,3156,0,"What's p?  what's h?  Start at the beginning and just say what you're measuring, why the distributions are different, and what you want to accomplish.",2010-09-28 23:28:48.0,601.0,
4098,3156,0,Perhaps if I guess some things it would help??  Are the p's the probability of the mean showing up in each distribution?  You want to say what one is farthest fromthe mean?... closest to it?,2010-09-28 23:46:38.0,601.0,
4099,3156,0,"p is the probability, under the null hypothesis, of observing a value as extreme or more extreme of the test statistic. This might be of help to you: http://www.mathworks.com/help/toolbox/stats/ttest.html ... I used Matlab's 'ttest' function.",2010-09-28 23:53:44.0,1102.0,
4100,3156,0,@John: This is my question (http://stats.stackexchange.com/questions/2639/how-to-compare-different-distributions-with-reference-truth-value-in-matlab) where I have defined my problem about what I want to accomplish. Thanks.,2010-09-28 23:55:29.0,1102.0,
4101,3066,0,"@Musa .. Can you be bit clearer. I don't think I understand the ""dynamics"" mentioned by you. Definitely _pam_ is OK for clustering. But you can also try the R packages pvclust and hopach as mentioned by me. Also, SOM (self organizing maps) are a different way of looking at clustering. Please see Ripley and Venable (2002) book - MASS for further details. The book offers a thorough treatment of clustering.",2010-09-29 00:26:35.0,1307.0,
4102,3157,0,"Thanks for your answer. Basically I wanted to compare my distributions using CI, for which I guess we don't need any assumptions, and I found ttest function on Matlab which does that, so I used it. Can you suggest how to compare them?",2010-09-29 00:54:24.0,1102.0,
4103,3148,0,@Mark The two tests give you the same information in terms of what you would conclude regarding the effectiveness of method 1 vs method 2 but the way they approach the issue is different. Any particular reason to avoid the chi-square? Feel free to choose either one.,2010-09-29 01:28:14.0,,user28
4104,3134,0,"Conceded whuber. I'll modify the answer to reflect this, although the question appears to have changed in the interim. BTW it's ""with replacement"".",2010-09-29 02:31:20.0,521.0,
4105,3100,0,"@whuber Thanks. I see the translation of a theoretical question into a statistical question as one of the most important skills that a data analyst can acquire. In most areas of statistics, there are multiple ways of making the translation, with a body of knowledge existing on when and why you would apply one approach over another. Thus, I'm interested in knowing whether any standard methods exist for quantifying consistency of two fitted curves and when and why you would apply one approach over another. I'll have a think about how I can edit the question to make my specific aims clearer.",2010-09-29 04:14:33.0,183.0,
4106,1013,0,you mean the standard deviation...,2010-09-29 04:16:36.0,795.0,
4108,3066,0,"@suncoolsu: Sorry for the poor formulation! I have 200 timeseries that I want to model (i.e. to simulate). I think that I can cluster ""similar"" (i.e. having the same behavior over time: the straight forward approach is to use the correlation) timeseries and simulate only the cluster timeseries...",2010-09-29 06:57:13.0,1443.0,
4110,3159,0,@grautur I'd like to add this one too: http://mkweb.bcgsc.ca/circos/tableviewer/,2010-09-29 07:26:35.0,930.0,
4112,3136,0,I want to estimate if the huge result of the Information Extraction is  correct. I want to check if the distribution of the entities found in the text follows my expectations (I know the domain and the text corpus).,2010-09-29 09:06:59.0,1389.0,
4113,3066,0,"@Musa as you say that your data is a time series, the first step should be removing all the trends from the data .. as you don't mention the size and type of your data (ie size -- moderate, large etc and type -- count data, continous data?), I will assume your data is moderate sized and continuous. There are better methods than clustering if you just want to model the time series. Please check out the (one of the many) time series package in R named zoo. There are great books: http://bit.ly/dmsWtl and http://amzn.to/9DmG6b to accomplish this task. Clustering may not be v.informative in ur case",2010-09-29 09:37:01.0,1307.0,
4114,3163,0,@Misha What's wrong with @Joris's response on stackoverflow where you cross-posted?,2010-09-29 10:08:12.0,930.0,
4115,3163,0,link to StackOverflow Question: http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval,2010-09-29 11:09:25.0,183.0,
4116,3125,0,"@Greg: Ah, OK! Well, you need to install R, you can download it from http://cran.r-project.org",2010-09-29 11:24:11.0,582.0,
4120,3163,0,@chl & @Jori:I'm sorry for crossposting but I looked at stackoverflow immediately before doing so and could only find the comment by @Bertelsen suggesting a crosspost. Looking at the time I cannot explain why I didnt find @Joris post. The answer by @Jori was just what I was looking for.,2010-09-29 11:38:28.0,1291.0,
4130,3165,0,"In your Q2, do you mean `cex.lab` instead of `cex.axis`?",2010-09-29 12:43:17.0,1390.0,
4131,3166,0,"It won't fit text labels to the maximum available space in the margins (which is what the OP ask for, to my understanding).",2010-09-29 12:54:22.0,930.0,
4132,3166,1,"@chl; my opening line says I can't think of an answer to Q1, but Q2 *can* be done via my example, substituting in the maximum values for cex.lab in place of where I have `2` and `3` now. That was just an example of how to label the plot separately and never claimed to answer Q1. Maybe I misread/misinterpretted what Tal wanted with his Q2 though? Your reading of Q2's meaning seems obvious if Q1 is TRUE, hence I thought Tal was aksing the Q2 I answered...?",2010-09-29 13:11:01.0,1390.0,
4133,3166,0,(+1) That's right.,2010-09-29 13:19:08.0,930.0,
4134,3168,0,Ouaouh! You're just summarizing in a few words a lot of R good practices!,2010-09-29 13:49:11.0,930.0,
4136,3165,0,"Why would you want to do this? Surely when you include your graph within another document, the text sizes will now look too big.",2010-09-29 14:18:30.0,8.0,
4137,3170,0,That's not working for me (you probably need to edit out the `cex.lab = mycex` part of the call to `plot()` to get that example to run) as the y-axis label is half outside the plot window and the x-axis label extends across the axis and into the plot region.,2010-09-29 14:21:44.0,1390.0,
4138,3134,0,Thanks about the BTW.  I mis-typed and intended to say *with* replacement.,2010-09-29 14:33:06.0,919.0,
4139,1013,0,@shabbychef: You are right. Fixed.,2010-09-29 14:35:05.0,56.0,
4140,411,0,"@Ampleforth: I'm not familiar with any one source that discusses many of these distances in general, and what I have seen is very much from a mathematician's and not a statistician's perspective (hence my question).  I just happened to see a reference to a book *Probability for Statisticians* by Shorack which apparently discusses many such distances.",2010-09-29 14:46:53.0,89.0,
4141,3169,0,"Yes, I am aware of TeXexample.net. Besides showing all fancy features, I'm a bit disappointed in the number of 'normal' line plots they have (showing different styles)",2010-09-29 15:02:22.0,190.0,
4142,3168,0,"This is not completely what I was looking for but thanks! The command in point 2 is very useful, only why use a bitmap format (png) when the result is pdf? Isn't it way better to have svg, pdf or pgf output?",2010-09-29 15:03:59.0,190.0,
4144,3168,0,@Peter See my updated response.,2010-09-29 15:24:17.0,5.0,
4145,3163,0,@Misha Since you have obtained the answer on SO perhaps you can post that answer (with proper attribution etc) here and accept it for the benefit of future readers. That way the question will be self-contained here and readers do not have to click on the SO link to find out the answer.,2010-09-29 15:54:02.0,,user28
4146,3157,2,"@Harpreet When constructing confidence intervals, say a 95% CI for a difference of means, you are explicitely assuming a sampling distribution (t or z, i.e. that of your test statistic under $H_0$).",2010-09-29 16:28:10.0,930.0,
4148,3163,0,Sorry Misha. You're caught in the growing pains of the 'R' overlap between stats.stackexchange.com and SO.,2010-09-29 18:32:14.0,776.0,
4149,3163,0,"@Brandon @Misha Yes, but it's just a little bit annoying, so maybe the complete answer can now be pasted here as suggested by @Srikant.",2010-09-29 19:08:06.0,930.0,
4152,3165,0,"This is really an R-code question, not a stats question.",2010-09-29 19:18:32.0,601.0,
4153,3064,0,"Is your dependent variable continuous or discrete? Or, perhaps, the underlying test result is continuous and it is converted into a discrete answer (i.e., 'positive', 'negative') depending on some cut-off? Could you also clarify why an individual would flip from negative to positive despite not being exposed to TB? A specific example (with some numbers thrown in) of such a flip may help.",2010-09-29 19:21:19.0,,user28
4154,2611,0,"A very interesting question. I look forward to hearing from your results, if you want to share them...",2010-09-29 19:25:24.0,930.0,
4156,3163,0,"@chl, annoying for you. But you have to remember that other people who are new to the community are going to be oblivious to what's wrong.",2010-09-29 19:45:51.0,776.0,
4157,3171,0,"I am not sure I see what the issue is. Don't you associate the responses of each respondent over time with some sort of dummy id (to preserve anonymity)? If so, you just have a repeated measures design with missing data. The missing data would be for those respondents who responded to a survey in the past but do not do respond to a later survey. Whether a respondent's answer changes or does not change over repeated measurements is not a data or sampling artifact but an indication of the stability or unstability of response for that individual.",2010-09-29 19:57:21.0,,user28
4158,3171,0,"Thanks Srikant. I would agree in theory, but in practice the physical well-being of the subjects depends on their anonymity and we can't risk anything that might identify them from their answers. I expect there probably exists a robust system to solve this, but we're not security experts and neither can we afford one.",2010-09-29 20:17:01.0,1343.0,
4159,3163,0,"@Brandon Oh, no! It wasn't intended to be read as a reproach (the word ""annoying"" was perhaps too excessive, sorry I'm a French guy and I certainly did not find the good term); just to say that we often end up with posts on stackoverflow and this SE, with partly overlapping answers or no response at all...",2010-09-29 20:24:24.0,930.0,
4160,3169,0,"Nice that you cited TeXexample.net! Maybe too much for that purpose, anyway `tikz` is very powerful for illustrations and diagram.",2010-09-29 20:38:03.0,930.0,
4161,3177,0,"Thanks! It is solved now, though with **hist** only.",2010-09-29 22:14:39.0,1102.0,
4162,3176,0,why would you want to do that ?,2010-09-29 22:36:49.0,603.0,
4163,3171,0,"So, as I understand it, you have no way to tell which response belongs to which respondent as you do not record any id consistently across respondents. Is that correct? I would imagine that you do know the sample size of each poll though, right?",2010-09-29 22:48:06.0,,user28
4164,3176,2,"General remark -- try to give questions reasonable, informative titles and make them clear and understandable for people not working in your field. Also good question has a bigger chance for a satisfying answer.",2010-09-29 23:35:58.0,88.0,
4165,3064,1,"The examples are really helpful to visualize the data. Another question regarding your caveat: ""the values clump at the floor and the ceiling and that the data are not normal."" Can you tell me if (a) the data on the lower end of the scale look normal and (b) the data on the upper end of the scale look normal?",2010-09-30 00:23:22.0,,user28
4166,3178,4,"Thank you for providing this comment!  But unless you can quantify the amount of skewness, that fact by itself is not very useful.  Plenty of distributional families are skewed but have practical normal approximations, such as the Chi-square (Gamma) and Poisson, and plenty more can be strongly skewed but rendered close to (or exactly) Normal through a simple re-expression of the variable, such as the Lognormal.  Could you perhaps amplify your answer to explain how the knowledge of the skewness could be used to estimate p-values from reported ORs?",2010-09-30 03:57:26.0,919.0,
4167,3179,0,"""clear, polished, and most importantly, succinct manner"" Sounds like ggplot2 to me.",2010-09-30 04:49:08.0,776.0,
4168,3179,1,"I'm looking not just for R libraries, but also any specific types of graphs. My knowledge of graphs is limited to scatter, box, qq, histograms, violins, kernel density estimations, etc. Any slightly more obscure graphs that can reveal more about the data than those would be fantastic.",2010-09-30 04:52:27.0,1118.0,
4169,3163,0,@chl C'est pas grave mon ami. Quel que fois je ne trouve pas les mots non plus - encore en anglais! :),2010-09-30 04:57:17.0,776.0,
4170,3159,0,"@chl:  very handy, I hadn't seen it.  Thanks for pointing it out!",2010-09-30 06:08:01.0,251.0,
4171,3180,0,Does this study involve only one disorder (with low prevalence as I understand) or are there multiple diagnoses assessed by multiple indicators?,2010-09-30 06:18:42.0,930.0,
4172,3182,1,"@ars Let's add Hadley's R tools to enhance GGobi experience, e.g. `DescribeDisplay` and `clusterfly`.",2010-09-30 06:28:38.0,930.0,
4173,3180,0,@chl Just one disorder,2010-09-30 06:43:50.0,183.0,
4174,3180,0,Do the scales overlap to some extent (i.e. shared constructs across the questionnaires)?,2010-09-30 06:45:31.0,930.0,
4175,3180,0,@chl they correlate but they are conceptually distinct; I've updated the question a little bit to reflect your two queries.,2010-09-30 06:49:26.0,183.0,
4176,3182,0,"Hi ars, as I wrote on my answer - my experience with ggobi is that it doesn't handle large datasets well.  Do you have another experience with that ?",2010-09-30 08:09:00.0,253.0,
4177,3179,2,parallel coordinates sounds like another one to mention.  Also methods of dimensionality reduction might be helpful.,2010-09-30 08:09:48.0,253.0,
4178,3181,4,I think this might need to be a wiki question :) ...,2010-09-30 08:13:12.0,253.0,
4179,3170,0,Nice answer John.  Why do you think this is not a good idea ?,2010-09-30 08:27:20.0,253.0,
4180,3165,0,"John - you are right. But when I get to these, it is always borderline for me if to put them here or stackoverflow.  Thanks for the answer either way.",2010-09-30 08:28:16.0,253.0,
4181,3182,0,"@Tal The problem comes from not relying on glyph for screen display/rendering, which is common to R base graphics. This was discussed at the latest DSC conference (http://j.mp/bpOhBH). Actually, there is an ongoing project with Qt as a backend, and a new port of GGobi, to enhance interactive display of large data sets.",2010-09-30 08:37:56.0,930.0,
4182,3188,0,Thanks for the links about `rflowcyt` and Acinonyx.,2010-09-30 08:43:11.0,930.0,
4183,3188,0,"BTW, `rflowcyt` has been deprecated with recent releases of Bioconductor, it is now recommended to use `flowViz`. Anyway, both rely on `lattice`.",2010-09-30 08:53:19.0,930.0,
4184,3066,0,"@suncoolsu another formulation: what I have is some panel data with very short time series (9 points for each of them). I want to model them, but not all of them (I will do a Monte Carlo simulation and do not want to simulate 200 time series). I was thinking that by clustering I can reduce the number of time series to simulate to manageable order...",2010-09-30 08:58:11.0,1443.0,
4185,3169,0,@chl Thanks for fixing the typo.,2010-09-30 10:00:33.0,1355.0,
4186,3170,0,the sizes on each side could vary enough that that would be distracting.  I think a good idea could be made of it by simply combining the min() of each separate axis to the min of all of the variables.  But that isn't what you asked for.  It could then be wrapped into a nice function you use frequently called getMaxLabelCEX(),2010-09-30 10:28:55.0,601.0,
4187,3156,0,None of this answers why you're comparing these distributions to a mean and why simple analysis of the confidence interval could possibly help.  You don't compare a distribution to a mean just because it has some numeric property.  That's why you can't get an answer for your question.,2010-09-30 10:43:28.0,601.0,
4188,3066,0,"@musa .. is there a specific reason that you don't want to simulate 200 time series? Using hierarchical modeling (using WinBUGS, OpenBUGS or JAGS), you can easily model such data sets. It will be definitely better than clustering and then proceeding to inference (and/or simulation).",2010-09-30 11:17:44.0,1307.0,
4189,3066,0,"@suncoolsu I have some time constraints (I am working) and want to have a quick solution. At a later stage I am also thinking to do a complete inference/simulation. My goal is to be able to forecast from the model. I have 2 questions:1)is it reasonable to proceed, for now, with clustering+inference or should I concentrate on some other models e.g. random effects model? 2)assuming that I want to model the whole time series what is the best way of doing it, could please give me a reference that I can quickly access?",2010-09-30 12:15:19.0,1443.0,
4190,3066,0,"I am looking, as a first try, for a solution where the simulation is not needed or it should run very quickly (less than 5 minutes)...",2010-09-30 12:41:46.0,1443.0,
4191,3188,0,"chl, it is good to know - thank you.",2010-09-30 13:50:44.0,253.0,
4192,3182,0,"chl, it's great to know.  After seeing the latest release was on 2008: http://www.ggobi.org/downloads/ I thought this project was dormant, but looking at their blog I see they are having some activity - http://ggobi.blogspot.com/  I hope they'll grow even more.  Best, Tal",2010-09-30 13:53:58.0,253.0,
4193,3198,0,"All the answers are similar, but slightly different. I don't think it really matters.",2010-09-30 13:58:47.0,8.0,
4194,3195,0,Point taken about CW! I made it that way because stats is not my field so I assumed my question would require some correction or extension. Will consider this more carefully next time. Thanks!,2010-09-30 14:08:42.0,1343.0,
4195,3199,0,2x2 matrix's upper left corner is only one element... Can you reformulate the question?,2010-09-30 14:11:44.0,88.0,
4196,3199,0,"@mbq: I tried to reformulate, let me know if it's still unclear (high values and low values refer to block matrices inside the big matrix)",2010-09-30 14:17:40.0,900.0,
4197,3171,0,"@Srikant, more-or-less correct; individuals that conduct the polling *may* remember who said what, but there is no physical record between response and respondent. Polling is more continuous than punctual: roughly 6 interviews are conducted per week though this can vary wildly as external factors that disrupt access to the group prohibit a more consistent sampling. We have 3 years of data. Since I arrived I've been enforcing putting dates on the results, though many old records are un-dated.",2010-09-30 14:22:05.0,1343.0,
4198,3199,0,"Much better now, thanks. Still, the more details you'll put here (Are those clusters sharp or smooth? Is it all noisy? How large should be those clusters?) the more useful answer you get.",2010-09-30 14:27:10.0,88.0,
4199,3191,0,"Great job chl!  Would it be o.k. by you if I where to publish this on my blog? (I mean, this text is cc, so I could, but I wanted you permission any way :) )
Cheers,
Tal",2010-09-30 14:49:02.0,253.0,
4200,3191,0,"@Tal No problem. It's far from being an exhaustive list, but maybe you can aggregate other useful links at a later time. Also, feel free to adapt or reorganize in a better way.",2010-09-30 15:07:36.0,930.0,
4201,3183,0,"Thanks for the two novel libraries. My main conflict with these two is that I am submitting my report via paper copy, so interactive graphics might not be fully used. The graphics of Mondrian look pretty complex. I'll give it a look-see.",2010-09-30 15:20:13.0,1118.0,
4202,3183,0,"@Christopher For Mondrian, you have the ""equivalent"" R version through `iplots` cited by @Tal. About Paraview, you have the option to save a screenshot of your viz. `DescribeDisplay` is the way to go for exporting dynamic visualization from GGobi, http://cran.r-project.org/web/packages/DescribeDisplay/index.html.",2010-09-30 15:27:23.0,930.0,
4203,3191,0,"+1 This is a nice list.  You might consider ""accepting this"" so that it's always on top; given that it's CW, anyone can keep it updated.",2010-09-30 15:34:48.0,5.0,
4204,3201,0,It can't be done unless you come to some objective criterion; probably most of possible ratings can be constructed with some combination of your parameters.,2010-09-30 15:39:21.0,88.0,
4205,3191,0,"@Shane Well, I am indebted to you for providing a first answer with so useful links. Feel free to add/modify the way you want.",2010-09-30 15:45:02.0,930.0,
4206,3205,0,"(+1) The 1st display really conveys a lot of information, and it will allow to see clusters (wrt. time or individual), if any.",2010-09-30 15:51:42.0,930.0,
4207,3205,0,"@chl Thanks.  But I think it may illustrate some of the problems to which the OP is reacting: it does not clearly show the extent to which units may be active for less than one day.  It could be improved in this regard (e.g., by coloring all streaks less than a day long), but it still does not lend itself well to gauging the extent of the problem.  What it does provide is the ability to distinguish one unit from another.  Displaying the two together on a common time scale suggests that if one single graphic won't do, maybe a collection of related graphics will.",2010-09-30 15:57:10.0,919.0,
4208,3191,0,I republished it here.  Great list!  http://www.r-statistics.com/2010/09/managing-a-statistical-analysis-project-guidelines-and-best-practices/,2010-09-30 16:03:47.0,253.0,
4209,3066,0,"@ musa .. in genomics there is a specific modeling strategy for ""time course experiments"". You can use it to get to see if the modeling strategy fits your data. Genes correspond to pages and individuals correspond to patients. I also listed 2 references in my previous comment.",2010-09-30 16:18:15.0,1307.0,
4210,3209,1,"This looks nice, but won't it just pick highest-variance attributes and biggest clusters of cross-correlated ones?",2010-09-30 16:21:38.0,88.0,
4211,3196,3,"+1 for the reference: it's a lengthy treatise on practical die testing.  Halfway down the author suggests using a KS test and then goes into ways to identify specific forms of deviation from fairness.  He's also well aware that chi-square is an approximation for small numbers of rolls per face (e.g., for 100 rolls of a 20-sided die), that power varies, etc., etc.  In short, anything the OP might like to know is clearly laid out.",2010-09-30 16:25:31.0,919.0,
4212,3188,0,"Very thorough answer, Tal! Plot generation time should not be a huge issue. I've been doing most of my graphs with the base package, and the issue of having the graphs look nicer was for when I decide to use a graph for the paper. I had considered using a scatterplot matrix for the numerical variables, but since many of them are of different units (some are in dollars, others in sqft), the only valuable information I'd get would be general trends, but with ~8 numeric variables, an 8x8 SPM is a bit cluttered.",2010-09-30 16:49:58.0,1118.0,
4213,3211,0,"@Gaetan Well, for PCA you have to find a suitable numerical coding for variable such as ""textual content""...",2010-09-30 17:02:17.0,930.0,
4214,3148,0,"I want to avoid it because I have already used a chi-squared test for another part of the study. However, I don't know if I'm being stupid by saying that. I guess it wouldn't really matter how many times I used a test within a paper.",2010-09-30 17:08:41.0,1441.0,
4215,3209,0,"Alternatively, one can perform *multiple correspondence analysis* or *multiple factor analysis* for mixed data (if numerical recoding happens to be not realistic for some variables), and the rest of your idea (computing factor scores and looking at variable loadings on the 1st dimension) applies as well.",2010-09-30 17:16:08.0,930.0,
4216,3182,1,"@Tal: My experience is that it's quite slow when refreshing/repainting the views, e.g. when adding a variable or dragging to rearrange displays in the PCP.  Still, it is usable though not as interactive as advertised with large data.  @chl: That's really good to know, thanks!",2010-09-30 17:16:45.0,251.0,
4217,3182,1,@ars @Tal Here are the links on Qt interface for R (http://j.mp/d1AJp7) and GGobi (http://j.mp/cUOvfp). See also Hadley's Github repository!,2010-09-30 17:37:19.0,930.0,
4218,3202,8,"I don't mind the downvote, but a comment would be appreciated (so I can learn myself, understand what was wrong, and improve my future responses).",2010-09-30 17:53:17.0,930.0,
4219,3212,2,str(variable) is your best friend.,2010-09-30 18:21:15.0,776.0,
4220,3202,3,"+1, good answer and also seconding chl's comment about leaving an explanation for downvotes to help both the author and readers understand what might be lacking.  Thanks.",2010-09-30 18:31:16.0,251.0,
4221,3211,0,"That's not the issue I am raising.  PCA can handle dummy variables as you suggest.  PCA is incredibly powerful and flexible that way.  But, it is the interpretation of the principal components that gets really challenging.  Let's say the first principal component starts like this: 0.02 years of experience - 0.4 textual content of reviews + 0.01 associations...  Maybe you can explain it.  An expert performance is proportional to years of experience, but inversely proportional to textual content of reviews?  It seems absurd.  But, PCA often does generate counter-intuitive results.",2010-09-30 18:42:19.0,1329.0,
4222,3214,0,Can you provide an example of the outcomes you consider in your experiments? What is your sample size (in each group)?,2010-09-30 18:43:03.0,930.0,
4223,3211,0,"@Gaetan Still, I reiterate my opinion that the problem lies in how you choose to represent your variables (or how you find a useful metric). I agree with you about the difficulty of interpreting a linear combination of variables when dealing with non-continuous measurements or a mix of data types. This is why I suggested in another comment to look for alternative factorial methods. Anyway, developing scoring rules based on user preferences or expert reviewing (as is done in clinical assessment) also calls for some kind of statistical validation (at least to ensure scores reliability).",2010-09-30 19:01:03.0,930.0,
4224,3202,0,Thank you for the excellent answer.,2010-09-30 19:03:37.0,1458.0,
4225,3206,0,"Thanks! If you do not have access to the original data but only a table of regression coefficients, is the Bonferroni adjustment your only choice?",2010-09-30 19:08:18.0,1458.0,
4226,3206,4,"Presumably you also have the p-values :-).  But with only those and the coefficients, it's hard to imagine what else you might do besides a Bonferroni adjustment.  (I always make such an adjustment whenever reading any paper with multiple tests: it's a quick way to winnow out the results that are likely to be junk.)  Most people also provide summary statistics for the variables: you can use ranges or sds along with the coefficients to estimate how much effect each explanatory variable might have on the predictand.",2010-09-30 19:17:13.0,919.0,
4227,3206,0,"Thanks for your explanation, esp. on cross-validation. I appreciate your last argument, i.e. that we also have to look for theoretical relevance (beyond p-values).",2010-09-30 19:20:13.0,930.0,
4228,3215,2,"Could you confirm that (a) d1 and d2 are the side lengths (and not angles); (b) that you are assuming the angle between them is a right angle (for otherwise the atan formula is suspect); and (c) that you are interested in the distribution of one of the other angles of this right triangle?  Also, presumably, the SD of each length distribution is much smaller than its expectation because the triangle shouldn't have any appreciable probability of a negative side length :-).",2010-09-30 19:25:25.0,919.0,
4229,3215,0,"Exact. I've rephrased the problem to make it a bit clearer. And yes, the SD will be small relative to the dimensions.",2010-09-30 19:47:32.0,77.0,
4230,3215,0,"Using formulas for multiplication and addition, you can try Taylor expansion.",2010-09-30 20:04:10.0,88.0,
4231,3211,0,"@Gaetan, Yes some of your comments make a lot of sense, and you're right in saying that it is not merely a statistical exercise but involves elements that are more subjective. The reason being that the intent from a user/customers standpoint might differ. Assuming he's doing a search for an expert, then i just add filters to allow him to select experts >X number of years of experience and so on But let's say he's narrowed down to 2 experts, and wants an independent comparison. So i'm just looking for a generic method to compare any two experts.",2010-09-30 20:19:52.0,1459.0,
4232,3211,0,"@Gaetan, So yes i'm assuming i can somehow quantify all of the attributes, which again might be subjective. But anyhow i see some interesting points in this discussion and will think some more on that before commenting further.",2010-09-30 20:26:10.0,1459.0,
4233,3211,0,"@chl numerical coding for ""textual content"", could be as simple as some sort of sentiment analysis and giving them a binary value of good/bad?",2010-09-30 20:30:33.0,1459.0,
4234,3211,0,"@Sidmitra, your comments make sense.  Maybe you will derive good use out of PCA after all.  I would suggest you may run PCA several times by deriving the simplest and most explainable models that still explains most of the variance between experts.  By doing so, hopefully you may end up with a PCA model with principal components that are readily explainable to others and to yourself.",2010-09-30 20:43:29.0,1329.0,
4235,3217,0,"Great idea! I have latitudes and longitudes of all datapoints already, so such a task would be relatively elementary. I was thinking the maps library would be a good way to go, unless there's something better.",2010-09-30 20:43:39.0,1118.0,
4236,3211,1,"+1 for pointing out this is not a statistical exercise.  At best, PCA can describe relationships within a particular data set and, conceivably, simplify the data by identifying near-collinearities.  It is not apparent how it can inform us about how to *rank* the experts.",2010-09-30 21:03:53.0,919.0,
4237,3209,2,"It seems to me the first component will merely point out a strong direction of commonality among the experts.  How could it possibly tell us who is better and who is worse, though?  That requires additional information concerning the relationships between these variables and the quality of being a ""good"" or ""bad"" expert.  If we believe all the variables are monotonically associated with goodness or badness, then perhaps PCA can help us explore the frontier of extreme (or maybe just outlying!) experts.  Watch out though--even the monotonicity assumption is suspect.",2010-09-30 21:09:03.0,919.0,
4238,3217,2,"@Christopher You can also do this with `ggplot2` (esp. if you don't need to draw country boundaries), http://had.co.nz/ggplot2/coord_map.html. Otherwise, `maps`, `gmaps` are better. There's also `GeoXp` and an R interface to GRASS. BTW, Mondrian has a plugin for geographical data :)",2010-09-30 21:12:38.0,930.0,
4239,3209,0,"@whuber I see the point, thanks. Maybe you could add this in your own response (which is very welcomed)?",2010-09-30 21:41:02.0,930.0,
4240,3216,0,"There was never any chance of it being normally distributed.  It is an angle!  It only takes values on $[-\\pi, \\pi)$.",2010-10-01 00:06:22.0,352.0,
4241,3198,1,@csgillespie: I was just amazed how quickly people jump in and answer!,2010-10-01 00:24:23.0,521.0,
4242,3224,0,"Oh neat, I didn't know plot could do that.",2010-10-01 00:31:50.0,251.0,
4243,3224,1,"Thanks! In examples like plot(sin, -pi, 2*pi), is there a default spacing for sampling the range in domain?",2010-10-01 01:16:11.0,1005.0,
4244,3224,1,The default is 101 points. See help(plot.function).,2010-10-01 01:21:02.0,159.0,
4245,3216,1,P(Y/X $\\le$ q) = P(Y $\\le$ qX) is not correct if X is a normal r.v. - X *can* be negative too.,2010-10-01 02:38:42.0,1112.0,
4246,3216,0,"@ronaf: actually, since $X$ and $Y$ are the side lengths of a physical triangle, we should *not* have negative $X$!",2010-10-01 04:26:34.0,795.0,
4247,3216,0,"d'accordo shabbychef! - but you can't eat your cake and have it. if X and Y are normal r.v.s. they will be negative occasionally. perhaps one solution is to think of the triangle as pointing down into the third quadrant if X > 0 but Y < 0 [etc.]  in any case, one can still consider the behavior of arctan(X/Y) - which will then range in ($-\\frac{\\pi}{2}, \\frac{\\pi}{2}$).",2010-10-01 04:49:12.0,1112.0,
4248,3218,0,(+1) and thanks for the references.,2010-10-01 06:50:47.0,930.0,
4250,3170,0,"Thanks John, that makes sense - I'll wrap that later and possibly republish it (with credit).  Thanks again :)    Tal",2010-10-01 08:27:29.0,253.0,
4251,3196,0,Thanks whuber - the power of google searching  :),2010-10-01 08:28:40.0,253.0,
4252,3202,0,"I second chl's recommendation to use some kind of penalized regression (e.g., the Lasso).",2010-10-01 09:28:34.0,1352.0,
4253,3202,7,"@chl: I'm unhappy with recommending stepwise predictor selection. Usually, this is based on p-values (""exclude a predictor with p>.15, include it if p<.05"") and leads to biased estimates and bad predictive performance (Whittingham et al., 2006, Why do we still use stepwise modelling in ecology and behaviour? J Anim Ecol, 75, 1182-1189). However, AIC-based stepwise approaches have the same weakness - Frank Harrell discussed this in a post to R-help on Mon, 09 Aug 2010 16:34:19 -0500 (CDT) in the thread ""Logistic Regression in R (SAS -like output)"".",2010-10-01 09:40:13.0,1352.0,
4254,3202,0,Is there any way to include links in comments?,2010-10-01 09:40:33.0,1352.0,
4255,3202,0,"@Stephan +1 Thanks for this. I know Frank Harrell's point of view. His book is a ""salvation"" for biostatistics, as well as the more recent one by EW Steyerberg. This is why I referred to his work at the end of my response, assuming the interested reader would look for a more thorough explanation. My initial thought was just to point to different ways of approaching the problem. As whuber nicely pointed it out, there's room for improvement in my response, esp. with cross-validation and conceptual issues around predictive modeling.",2010-10-01 09:49:30.0,930.0,
4256,3202,0,"@Stephan Now the problem is that often we want to keep some variables in our model (whatever their p-values), as @whuber said; this is why I suggested to look at the `penalized` package from J Goeman, because it allows to penalize only a subset of the covariates. From my experience, I found results from Lasso/Ridge regression a little bit difficult to explain to an external audience, esp. when data don't really called for shrinkage/regularization (e.g. ""ideal"" case with 500 subjects and 10 variables, no collinearity or mediation issues). So when we can keep things simple without overfitting...",2010-10-01 10:07:33.0,930.0,
4257,3230,0,"I was thinking of confidence intervals, but the question is not clear to me because there seems to be different outcomes each time (but may I don't understand the question) which would prevent from pooling anything at all.",2010-10-01 10:56:55.0,930.0,
4258,2818,0,"@Thylacoleo Thanks for the links (esp. the genetic related one). So, what Stata command would you recommend?",2010-10-01 11:07:58.0,930.0,
4259,2818,0,The question concerns Cox regression. I'll post an expmple (above) provided with Stata 11.,2010-10-01 12:37:53.0,521.0,
4260,2818,0,"@Thylacoleo Ok, this was just to be sure that the `stcox` command was the correct approach.",2010-10-01 12:39:56.0,930.0,
4261,2749,1,Also as a note to the poster you can not estimate the shared frailty in SPSS as in Thylacoleo's example. You can only include covariates in the Cox regression (either time varying or static).,2010-10-01 14:54:45.0,1036.0,
4262,2818,0,"@Thylacoleo, do you know if you included fixed effects for all the matched groups (ie dummy variables) would you get the same covariate coefficients when you define shared frailty?",2010-10-01 14:57:50.0,1036.0,
4263,3239,0,I had come accross cointegration a few years back - but it did seem terribly complicated to me (I didn't understand it!). I was hoping there would be a less theoretical (i.e. more practical) solution ...,2010-10-01 15:08:16.0,1216.0,
4264,3239,3,"The Engle-Granger method is not especially complicated: you just take the residuals of a regression between the two series and determine if it has a unit root.  This is certainly practical: it's used regularly for a broad spectrum of problems.  That said, I imagine that any answer to your question will require some statistical knowledge (for instance, you should understand things like stationarity, independence, etc.)...",2010-10-01 15:19:35.0,5.0,
4265,3214,0,I'm a bit confused what the goal is as well. Is there a reason examining a plot of each of the 5 hazard functions (and/or their confidence intervals) is insufficient? Do you need a test statistic to state where the curves intersect? As chl suggested in a comment to Thylacoleo's answer pooling seems inappropriate with different outcomes.,2010-10-01 15:29:21.0,1036.0,
4266,3216,2,"@ronaf: That's the right idea.  If one uses signed side lengths and also considers the angle as a real value (rather than its value modulo $2\\pi$), there is no inconsistency with normality in either case.  Your point about the inequality possibly being wrong is excellent.  All I can do in response is to claim that the equation is an excellent approximation under the assumptions made because the chance of X or Y being negative is negligible.",2010-10-01 16:01:38.0,919.0,
4267,3239,0,is there a better way to do this than to test all pair-wise series for co-integration (with the same ideal in mind to cluster series together?) Also wouldn't this suggestion be dependent on the fact that the series themselves are integrated at the onset?,2010-10-01 16:34:55.0,1036.0,
4268,3239,0,"@Andy: I'm sure that there is a better way, and I look forward to hearing about it.  This is a pretty basic approach.",2010-10-01 16:38:29.0,5.0,
4269,3238,1,"You may also be interested in the responses to this question, http://stats.stackexchange.com/q/2777/1036",2010-10-01 16:39:38.0,1036.0,
4270,3217,0,"Assigning a best answer can be difficult when there's several great suggestions, but I feel this is the right direction, keeping ""succinct"" in mind. I will give ggplot2 a try, and take a look at maps, GeoXp, and Mondrian. Thanks for the idea of graphing spatially!",2010-10-01 17:34:52.0,1118.0,
4271,3244,0,Could you link to the spreadsheet Mike Lawrence provided?,2010-10-01 17:43:53.0,1036.0,
4272,3215,0,"Thanks for both your excellent answers, which (as far as I can tell with my limited stats expertise) are both intuitive and sound.",2010-10-01 18:03:21.0,77.0,
4274,3246,0,"I second this. There are many examples of something that looked like a power law, but when examined a little more rigorously turned out not to be....and no, the high R^2 on the chart is not enough.",2010-10-01 18:32:38.0,247.0,
4275,2888,0,"Thanks for the additional link. Still for me the problem is with the small $n$ and the heterogenous predictors. It seems to me that the $n\\ll p$ case is now increasingly well-studied in genetics, neuroimaging studies, or when we can assume an exponential relationship between $n$ and $p$, but at the moment I never found any evidence of the relevance or predictive power of boosting in the particular study I presented. I am currently running MC simulations to see how RFs and sparse regression perform in this case. I'll let you know all of any progress in this direction.",2010-10-01 19:08:27.0,930.0,
4276,3244,0,Here is the URL https://spreadsheets.google.com/ccc?key=0Ap2N_aeyRMGHdHJxUnVNeEl5VGtvY1RVLVc5UjU4Vmc&hl=en#gid=0 It was related to his question http://stats.stackexchange.com/questions/2956/have-i-computed-these-likelihood-ratios-correctly,2010-10-01 19:13:11.0,1329.0,
4277,2525,1,"Great recommendation, thank you -- I got a copy recently based on this and it really is quite good.",2010-10-01 19:14:28.0,251.0,
4278,2525,0,I'm glad to hear someone else appreciates this book!,2010-10-01 20:14:08.0,919.0,
4279,3198,0,Thanks for the answer.  I accepted this because it included all the newbie stuff about p values and rejecting.,2010-10-01 20:22:05.0,1456.0,
4280,3236,1,"Don't you mean `mean(replicate(N, system.time(f(...))[3]), trim = 0.05)` ?",2010-10-01 20:29:55.0,46.0,
4281,3236,0,Yup. Thanks for catching that.,2010-10-01 20:31:00.0,334.0,
4282,3248,0,thanks for your feedback.  This is the exact type of comments I was interested in.  You have really leveraged the sharing and importing component of Google docs.  Good for you.  I'll read your material to learn more about it.,2010-10-01 20:43:16.0,1329.0,
4283,3246,0,"""So you think..."" is an excellent reference.  Points 1-6 (out of 7) directly address the question posed here.",2010-10-01 20:45:26.0,919.0,
4284,3236,1,Thinko.  I like it.,2010-10-01 20:56:48.0,5.0,
4285,3241,1,"Chris, regular clustering won't cut it. You either have to acknowledge that a series is highly correlated with it's own past by putting each $y_{1,t}$ as a dimension of it's own (i.e. resulting in N*T dimensions) or you trow all the dimensions together, but then (given the high correlation inside a series) you will always end up with a single cluster. Also most clustering methods are ill-suited/advised for highly correlated variables (there is a more or less bending assumption of spherical clusters).",2010-10-01 21:01:15.0,603.0,
4286,3239,0,"> i can't suggest anything else, but cointegration is both very fragile ('parametric assumptions' gone wild series) in practice and ill suited for the task at hands: at each step, it amounts to doing hierarchical clustering, at most merging two series unto one (the co-integrated mean).",2010-10-01 21:09:58.0,603.0,
4287,3246,0,"But a power-law *distribution* isn't the same thing as fitting a power law relationship between two separate variables. I'd assumed the question was about the latter, though i'm not certain.",2010-10-01 21:39:59.0,449.0,
4288,3251,2,"Actually, the arima function in R will not fit your model (1). arima() does regression with ARIMA errors and your equation (1) is an ARMAX model.",2010-10-01 23:56:20.0,159.0,
4289,3255,0,"Hi Kwak and Rob.  Thanks for looking at this.  I wanted to use exponential smoothing because this is what I'm more familiar with.
I'm thinking that I need to learn on how to use the ARIMA framework.  Could you recommend a good book that would help me to learn enough about the ARIMA framework to apply such a dummy variable approach?

I have Bowerman's ""Forecasting, Time Series, and Regression"" and Levenbach ""Forecasting: Practice and Process for Demand Management"", which I used to learn about exponential smoothing.   I don't know if these go in sufficient detail for what I'd need.
Thanks!",2010-10-02 00:25:27.0,1479.0,
4290,1011,0,"excellent, +1 from me.",2010-10-02 00:58:03.0,438.0,
4291,3246,0,"Non-expert's question: apart from ""robustness"", are there other reasons why one should check goodness-of-fit with Kolmogorov-Smirnov instead of $\\chi^2$ in this case?",2010-10-02 02:02:56.0,830.0,
4293,3246,2,"@J.M.: not really, chi-square is sensitive to binning and tail fluctuations complicate that.  I think even with the KS, they reweigh the statistic for extremal points, and there's some discussion of other tests.  @onestop: I assumed the other way, and on re-reading, you could be right.  I'm not really sure ..",2010-10-02 05:51:33.0,251.0,
4294,3251,0,"Rob:> i've edited equation one. Can you point to a source where the difference(s) between armax and regression with arima errors are explain (or alternatively provide an intuitive explanation). Also, would you know of a R package that implements ARMAX models ? Thanks in advance.",2010-10-02 07:46:42.0,603.0,
4295,3254,0,+1 Thanks for the link (and the retag). I'll look into this direction.,2010-10-02 08:05:26.0,930.0,
4296,2818,0,"@Andy W. No as frailty is a the equivalent of a random effect in survival analysis. So the cathetar model is equivalent to a mixed model with both fixed and random coefficients. To show this I'll add the dummy variable example above. Perhaps it could be interpreted as an ""order effect"" - second infection versus first.",2010-10-02 08:11:37.0,521.0,
4297,3248,0,"Dear Gaetan, I am delighted by your response - thank you for the kind words.  Best,  Tal.",2010-10-02 08:17:52.0,253.0,
4300,3228,3,"You mean, notepad++ with the use of npptor :)",2010-10-02 08:33:16.0,253.0,
4302,2818,0,@Thylacoleo (+1) Thanks for all of this. It looks definitively better with your annotated Stata example.,2010-10-02 08:37:50.0,930.0,
4303,3240,0,mod-tip: post the second part as a comment to the Dirk's answer.,2010-10-02 09:28:27.0,88.0,
4304,3247,0,"@Gaetan Aside from my response, I gave my +1 to the question because I think it is very relevant for debating about statistical practice and project management.",2010-10-02 09:35:43.0,930.0,
4307,3260,0,zenna:> do you have a set of pre-identified break points ? is the dates of points were 'interesting changes across different datasets' did occur ?,2010-10-02 13:06:49.0,603.0,
4308,2818,0,Yes Thank you very much Thylacoleo,2010-10-02 13:14:49.0,1036.0,
4309,3260,0,"no, I have to find them also.  They are (normally) local minima within each dataset, the problem is there are lots of local minima which are not 'interesting points' which is why I am trying to combine multiple datasets to find the meaningful local minima.",2010-10-02 13:23:29.0,809.0,
4310,3236,2,"If the f() call is long then it's fine.  However, if the f() call is short then any timing call overhead will likely be increasing error measurement.  With a single call for system.time() over many repetitions of f() one gets to divide out the error the call until it's some infinitesimal value (and it returns faster).",2010-10-02 16:07:29.0,601.0,
4311,2400,0,Here's something interesting I found about divergences recently -- each step of Belief Propagation can be viewed as a Bregman Projection http://www.ece.drexel.edu/walsh/Walsh_TIT_10.pdf,2010-10-02 17:05:35.0,511.0,
4312,3257,0,"This looks like a good, tractable starting point. thanks for the links.",2010-10-02 17:17:38.0,1216.0,
4313,3235,3,"Just for the record, since I'm clearly far too late to change the course of this question: this is the kind of issue that I think is best suited for StackOverflow.",2010-10-02 17:22:02.0,71.0,
4314,3264,0,"Good leads, thanks chl :)",2010-10-02 17:48:35.0,253.0,
4315,3264,0,"I agree that permutation tests and other explicit manifestations of randomness can be quite educational.  This suggests showing dynamic simulations to the class, so they can watch the permutations being done and see the effects on the statistics.  Just to tweak you a little bit (apropos a different thread): one of the best tools available for that is...Excel!  (It helps that the students will have access to this and be familiar with it, unlike a better platform like Mathematica.)",2010-10-02 18:13:52.0,919.0,
4316,3264,1,"@whuber Thanks. Even before using any software, I like discussing Phillip Goud example (updated in my answer) and let them do the calculation by hand. Then, I think any software will do the job, provided they feel involved and do it themselves.",2010-10-02 18:27:20.0,930.0,
4317,3260,0,"Statistics works by training. The idea is that you need to first identify (by yourself) a set of points satisfying what you are looking for, then try to train a statistical procedure to do this job for you.",2010-10-02 18:51:32.0,603.0,
4318,3266,0,"Thanks for the link. What about imputation when data are not missing at random? It seems to me the problem lies in the fact that there is a complete block of measurements that is missing, and we cannot assume MAR or MCAR, nor are these data missing by design (in which case ML estimation yields unbiased parameters estimates).",2010-10-02 19:10:27.0,930.0,
4319,3008,1,"@Farrel - here's a very short script, which assumes [0,1]-uniformly distributed covariates, an OR of 2 between the first and third quartile of the covariate and standard normal noise, leading to power .34 for n=100. I'd play around with this to see how sensitive everything is to my assumptions: runs <- 1000; nn <- 100; set.seed(2010);
detections <- replicate(n=runs,expr={covariate <- runif(nn);
outcome <- runif(nn)<1/(1+exp(-2*log(2)*covariate+rnorm(nn)));
summary(glm(outcome~covariate,family=""binomial""))$coefficients[""covariate"",""Pr(>|z|)""] < .05})
cat(""Power:"",sum(detections)/runs,""\\n"")",2010-10-02 20:04:27.0,1352.0,
4320,3008,1,"You can attach your code as a pastie (http://pastebin.com/) or a Gist (http://gist.github.com/) if you feel it's more convenient, and link back to it in your comment.",2010-10-02 20:20:43.0,930.0,
4321,3266,1,"@chi MAR is exactly the sort of assumption that Manski's approach is meant to avoid. Perhaps a simple example will make it clearer. This is straight out of the paper I linked above. Suppose you want to estimate the average of some function of a variable y, E[g(y)]. Let z=1 for people with y observed, and z=0 otherwise. Also, suppose g is bounded between g0 and g1. Then you know that E[g(y)|z=1]P(z=1) + g0 P(z=0) < E[g(y)] < E[g(y)|z=1]P(z=1) + g1 P(z=0). E[g(y)|z=1] and P(z) can be estimated as the sample mean of the observed y, and the portion of the sample with y observed.",2010-10-02 20:23:52.0,1229.0,
4322,3266,0,"Thanks for the explanation. I have no access to Manski's paper, but I will try to get it at work. (I have not vote left, so I'll +1 your response ASAP)",2010-10-02 20:31:52.0,930.0,
4323,3008,0,"@chl: +1, thanks a lot! Here's the gist: http://gist.github.com/607968",2010-10-02 20:41:40.0,1352.0,
4324,3255,0,Bowerman O'Connell and Koehler is quite good for introducing ARIMA models but I don't think it includes ARIMA with covariates. You could try my 1998 textbook which covers ARIMA modelling and regression with ARIMA errors at an introductory level. See http://robjhyndman.com/forecasting/ for details.,2010-10-02 21:41:39.0,159.0,
4325,3251,0,"A first order ARMAX model with one covariate is
y_t = a + bx_t + cy_{t-1} + e_t
where e_t is iid zero mean.
The corresponding regression with ARIMA error is
y_t = a + bx_t + n_t where n_t = phi*n_{t-1}+z_t
and z_t is iid zero mean.",2010-10-02 21:44:55.0,159.0,
4326,3266,0,"@chi This ungated paper by Kline and Santos might also be useful. It focuses on quantile regression, but has references to similar papers about other models. http://www.econ.berkeley.edu/~pkline/papers/missing4.pdf",2010-10-02 22:25:22.0,1229.0,
4328,3251,0,"Thanks Rob, but i'm still a bit confused with the n_t in 'y_t = a + bx_t + n_t where n_t = phi*n_{t-1}+z_t and z_t is iid zero mean'. It seems that 'n_t' follows a moving average process (i.e. not autoregressive). Can you confirm that a regression with ARMA errors is actually a 'MAX' model (moving average + eXogeneous) ? Also, does it mean that regressions with ARIMA errors cannot be fitted by OLS ?",2010-10-02 23:02:18.0,603.0,
4330,3251,0,"Also, I get exactly the same estimates when fitting 'arima(y,order= c(1, 0, 0),xreg=x)' from package stats and 'arimax(y,order= c(1, 0, 0),xreg=x)' (using the arimax function in package TSA). Does that mean that arimax() does not fit ARIMAX models ?",2010-10-02 23:26:31.0,603.0,
4332,3251,1,"@kwak. First, n_t = phi*n_{t-1} + z_t is AR(1). A moving average process of order 1 would be n_t = theta*z_{t-1} + z_t. Second, an regression with MA errors is equivalent to a MAX model. But once you add AR terms in the error process, there is no equivalence between the two classes. Third, the arimax() function in TSA fits transfer function models, a special case of which is a regression with ARIMA errors. It does not fit ARIMAX models. I might write a blog post about this as it is hard to find the various model classes compared and discussed anywhere.",2010-10-03 03:38:12.0,159.0,
4333,3274,0,"one such $f$ is based on EM, as described in Little & Rubin (I was somewhat put off by their treatment of regression, but perhaps I should revisit).",2010-10-03 04:19:38.0,795.0,
4334,3276,0,Similar question at http://stats.stackexchange.com/questions/2957/ols-is-blue-but-what-if-i-dont-care-about-unbiasedness-and-linearity/2961#2961. A nice review paper on the topic is http://webee.technion.ac.il/Sites/People/YoninaEldar/Download/67j-04490210.pdf,2010-10-03 07:30:40.0,352.0,
4335,3261,0,What is $\\epsilon$?,2010-10-03 07:49:16.0,352.0,
4337,2270,0,"(+1) Nice review, thanks for sharing.",2010-10-03 10:20:19.0,930.0,
4338,3143,0,"Certainly, Sir :)",2010-10-03 11:04:56.0,1439.0,
4339,3251,0,"@ Rob:> Ok, thanks for all the clarifications.",2010-10-03 11:05:01.0,603.0,
4340,3247,0,A comment for the downvote would be greatly appreciated.,2010-10-03 12:25:48.0,930.0,
4341,3278,0,What a wonderful synopsis!,2010-10-03 12:57:04.0,919.0,
4342,3282,0,Nice book!,2010-10-03 13:20:11.0,930.0,
4343,2749,0,"@Andy Thanks for the tips. In fact, I don't really use SPSS (only from time to time for Factor Analysis, and to exchange with colleagues), but it might be of interest for SPSS users. (my +1)",2010-10-03 13:23:25.0,930.0,
4344,3282,0,Thank you! Any other materials are welcome.,2010-10-03 15:08:02.0,1250.0,
4345,3278,0,Thanks @chl this is a great description (merci bien pour le link egalement. Le premier link as une bonne explication des lignes et de collones). I have one more question. What would you say about the relationship between people with red hair and people with green eyes?,2010-10-03 18:13:26.0,776.0,
4346,3278,1,"@Brandon The 1st axis is an axis of ""dominance"" (light -> dark) for both modalities, but we can also see that the 1st axis opposes blue and green eyes to brown and hazel eyes (their coordinates are of opposite signs), and red hair/green eye combination--which is quite uncommon--contribute mostly to the 2nd factor axis. As this axis only explains 9.5% of the total inertia, it is rather difficult to draw firm conclusions (esp. wrt. genetic hypotheses).",2010-10-03 18:35:01.0,930.0,
4347,3278,1,"@Brandon Two further references (in english this time): the PBIL course (http://j.mp/cHZT7X) and Michael Friendly's resources (http://j.mp/cYHyVn + `vcd` and `vcdExtra` R packages, the latter including a nice vignette).",2010-10-03 18:37:19.0,930.0,
4348,1470,0,(+1) Good references! Thanks.,2010-10-03 19:26:09.0,930.0,
4349,3278,0,"When you refer to modalities, are you referring to ""hair colour"" and ""eye colour""? Also, using the ""ca"" package, I see clearly how inertia for the dimensions is stated but I'm not clear on which numbers you used to identify that red hair/green eyes contributed to the 2nd factor. Are you looking at the ""cor"" column (high/low) where k=2?",2010-10-03 19:29:22.0,776.0,
4350,3278,1,"@Brandon Yes, one modality = one category for your variable. For your 2nd question, `cor` is the squared correlation with the axis, and `ctr` is the contribution (it has to be divided by 10 to be read as a %). So ""red hair"" contributes 55.1% of the inertia of the 2nd axis. In a certain sense I found the FactoMineR output more ""intuitive"" (`CA(tab, graph=FALSE)$row$contrib` gives you directly the %).",2010-10-03 19:52:17.0,930.0,
4351,3283,0,"Thanks for the help!  I do have different probabilities for each game, which makes this method inefficient for calculating outcome probabilities.  However, I have come up with a way to enumerate all outcomes without having to check all 3^10 possibilities by using dynamic programming!",2010-10-03 19:57:04.0,,Kenny
4352,1470,0,This post has also useful references related to RCTs: http://j.mp/bAgr1B.,2010-10-03 20:06:52.0,930.0,
4353,3278,0,Here's another example for follow-up. http://stats.stackexchange.com/questions/3287/interpreting-2d-correspondence-analysis-plots-part-ii,2010-10-03 21:24:49.0,776.0,
4354,3286,2,"please, read the highest graded answer (i.e. mbq's) here: http://stats.stackexchange.com/questions/1001/spearmans-correlation-coefficient-to-compare-distributions",2010-10-03 21:33:34.0,603.0,
4355,2247,0,could you provide a link to what you feel is the best tutorial that you've seen using R and survival?,2010-10-03 21:45:56.0,776.0,
4356,3261,0,$\\epsilon$ is a zero-mean error term.,2010-10-04 01:49:59.0,795.0,
4357,3274,0,It seems like imputation would not work for covariance estimation.,2010-10-04 01:51:07.0,795.0,
4358,3283,0,If you're enumerating all outcomes you're looking at all 3^10 possibilities.  There's no free lunch here ;-).,2010-10-04 02:05:43.0,919.0,
4359,618,7,"A colleague of mine looked at the data for this in the post-2000 period, and found that the relationship held fairly well 'out-of-sample', which is even more disturbing...",2010-10-04 04:12:47.0,795.0,
4360,3283,0,"You're right, I'm not actually enumerating all individual game outcomes, just overall outcomes AFTER 10 games.  I'm looking at the possible outcomes for only one category, and then adding the new possible outcomes with each new additional category (one at a time).  With this method, you're able to find the probability of all (W,L,T) outcomes after X games in polynomial time: O(n^3) :-)",2010-10-04 05:09:30.0,,Kenny
4361,3236,0,"@John: Thanks but I don't quite get what you said. I am still wondering which is better, repeating f() inside or outside system.time()?",2010-10-04 05:59:47.0,1005.0,
4362,3284,1,There's also an example of using Mfuzz in my [tutorial paper](http://www.mas.ncl.ac.uk/~ncsg3/microarray/),2010-10-04 08:51:19.0,8.0,
4363,3294,2,You should probably make your post a community wiki since there isn't a correct answer.,2010-10-04 09:09:16.0,8.0,
4364,3294,1,I've just converted it.,2010-10-04 09:14:39.0,88.0,
4365,3236,0,"Every call to the system.time() command has some variable time it takes to call that causes some amount of measurement error. This is a small amount.  But what if f() is a very brief call?  Then this error can be conflated with the time taken to call f().  So, when you call f() 1e5 times inside a single system.time() call the error gets divided down into 1e5 chunks.  When you call system.time() for every f() it's impact could be meaningful if time for f() is small.  Of course, if all you need is relative timing it doesn't much matter.",2010-10-04 10:13:52.0,601.0,
4366,3236,0,"Oh, and the second part is that it would be faster to just call system.call() once.",2010-10-04 10:14:36.0,601.0,
4367,3288,2,"To make it clearer, here you will rather need Kolomogorov-Smirnov distance than KS-test.",2010-10-04 10:22:05.0,88.0,
4368,3286,0,Wikipedia has some sort of overview of distribution distances here: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_distance#Other_probability-distance_measures,2010-10-04 10:22:48.0,88.0,
4370,3251,2,I've tried to summarise the various models at http://robjhyndman.com/researchtips/arimax/,2010-10-04 12:41:45.0,159.0,
4371,3267,0,+1.  Absolutely agree with stressing the real-world relevance of stats and focussing on fundamental concepts.,2010-10-04 12:53:34.0,266.0,
4373,3247,0,"@chl: although I didn't downvote this answer, I think I understand why one would downvote it. The information you have provided is correct, very very important and think-provoking. HOWEVER, most of it (except for the last two paragraphs) do not answer the question. Ideally, one would write this large disclaimer elsewhere and give a link to it.",2010-10-04 13:23:51.0,1496.0,
4374,3247,0,"@chl: despite what I said in my comment, I love your answer and up-vote it",2010-10-04 13:24:35.0,1496.0,
4375,3283,0,Are you sure it's not O(3^n)?,2010-10-04 13:34:50.0,919.0,
4376,3289,0,What is the statistical motivation for this question?  (It seems like it would be more appropriate to pose it on a math forum.),2010-10-04 13:37:06.0,919.0,
4378,3247,0,"@bgbg Thanks for your comment. Maybe I didn't answer the CW question. However, I never intended to give a purely provocative answer. The OP asked about potential ""bugs and flaws"" in GDocs: I provide illustrations about what I know from Excel, acknowledging the fact I don't know how it would translate to GDocs. I also understand part of the question as ""what are the benefits of using GDocs for data analysis"", and I just gave some arguments against the use of spreadsheet for large scale projects, or analysis at the bleeding edge (still, I acknowledged at the beginning that this would be biased).",2010-10-04 14:21:58.0,930.0,
4380,3293,0,"@kwak Well, some savings for @shabbychef! I found Izenman's book more an applied textbook featuring some methods not covered in ESL (e.g. correspondence analysis), but you're right they are pretty written in the same spirit one each other. (I'll update my answer)",2010-10-04 15:11:03.0,930.0,
4381,3303,0,(+1) Thx for the link. I like the PGP framework too.,2010-10-04 15:53:07.0,930.0,
4382,3296,0,Could you indicate what classification method(s) is/are used?,2010-10-04 16:56:11.0,930.0,
4383,3288,0,Bonus marks for explaining how to find out stuff for myself.,2010-10-04 17:03:03.0,5141.0,
4384,3295,0,". Yes, but one of the difficulties I had was in extracting the clusters from the heatmap.2 object. Is there an easy way of extracting the clusters? I am aware of the `cutree` command which can be used to extract clusters from the heatmap.2 object.",2010-10-04 17:33:26.0,1307.0,
4385,3311,0,"This is just a recap' of what I read so far. Obviously, *I will not accept my own answer*. Any other thoughts would be much appreciated.",2010-10-04 17:58:01.0,930.0,
4386,3296,0,@chl: see update to the question,2010-10-04 18:52:28.0,1496.0,
4387,3310,0,"Thank you ""Extreme value theory"" was what I was looking for!",2010-10-04 18:59:45.0,253.0,
4388,3308,0,"when I say they are misclassified, I mean that when I test different alternative models on the training set, all those samples are misclassified by all or most of those models",2010-10-04 19:00:36.0,1496.0,
4389,3293,0,"> i agree with you btw (i own both books). On the other hand, if budget is a binding constraints, you might want to diversify.",2010-10-04 19:27:55.0,603.0,
4391,3293,0,@kwak For sure :) Add your must-have references! (thx for the undelete),2010-10-04 19:58:53.0,930.0,
4393,3159,0,another funny website: http://www.visualizing.org/,2010-10-04 20:15:24.0,930.0,
4394,3314,1,"Thanks for the reference.  It's always fun to see early work (even when it was published in one's own lifetime, LOL!). The results are simple because they combine two simple results: (1) we know the pdf of the Normal distribution (which allows us to give a name to its cdf, the Gaussian integral,which has no closed form evaluation).  (2) the standard expressions for distributions of the order statistics of any pdf.  Thus you do obtain ""exact"" expressions for the order statistics, but in they end they must be polynomials in the Gaussian integral. (Computing them in 1961 wasn't easy.)",2010-10-04 20:18:24.0,919.0,
4395,3317,0,"Cont'd:                                                          2. Cook R.J. and farewell V.T. Multiplicity considerations in the design and analysis of clinical trials. Journal of the Royal Statistical Society, Series A 1996; Vol. 159, No. 1 : 93-110",2010-10-04 20:41:43.0,1501.0,
4396,3317,0,"Thank you for your comments, Brenden, especially the last one about prediction vs. causal explanation.  And welcome to the site!  I hope to see many more of your contributions in the future.",2010-10-04 20:44:50.0,919.0,
4397,3317,0,"Cont'd:                                                           3. Rothman K.J. No adjustments are needed for multiple comparisons. Epidemiology 1990; Vol. 1, No. 1 : 43-46            4. Marshall J.R. Data dredging and noteworthiness. Epidemiology 1990; Vol. 1, No. 1 : 5-7                                        5. Greenland S. and Robins J.M. Empirical-Bayes adjustments for multiple comparisons are sometimes useful. Epidemiology 1991;
Vol. 2, No. 4 : 244-251",2010-10-04 20:45:48.0,1501.0,
4398,3314,0,"@whuber:> Yes. the link will probably be less useful than the Gumbel approximation (except perhaps for the <strike> min and max <\\Strike> *largest* and *smallest* draw) but i cited the paper nonetheless because, as you said, it has this 'head in the cloud' cachet about it :)",2010-10-04 20:59:04.0,603.0,
4399,3295,1,@suncoolsu: I've updated my answer. Does that help?,2010-10-04 21:13:41.0,8.0,
4400,3317,0,(+1) You may be interested in the following thread: http://stats.stackexchange.com/questions/3252/how-to-cope-with-exploratory-data-analysis-and-data-dredging-in-small-sample-stud. It seems we share a lot of links in common :-),2010-10-04 21:14:57.0,930.0,
4402,3319,0,"Nice tips, thanks.",2010-10-04 21:43:57.0,930.0,
4405,3322,1,"Wow, starting right at me.  Thanks.",2010-10-04 23:18:37.0,569.0,
4406,3289,0,"@whuber: i was hoping that some clever chap, such as you, would have some insights about how to approach this. btw - there is a longish list of related problems on the RHS of the screen, to many of which your question could be applied.",2010-10-04 23:50:36.0,1112.0,
4407,3278,1,"@chl: wow, for someone who knows nothing about CCA or the ""French way"", this was a great read!  Many thanks.  I also found this with some googling that might be of interest: http://www-stat.stanford.edu/~susan/papers/dfc.pdf",2010-10-05 00:53:11.0,251.0,
4408,3324,0,"From your question you may be reading the code slightly wrong. The syntax is using the differences as the ""instruments"" to estimate the lag of the dependent variable.",2010-10-05 01:39:55.0,1036.0,
4410,3326,0,"Thank you for answering, many voted, but not so many answered =) I have been looking at Multi Hypothesis Tracking for a while now. I will check out your suggestions, have not heard of two of them!",2010-10-05 07:47:02.0,1411.0,
4411,3331,2,you might get a few ideas from an earlier question on clustering individual longitudinal data trajectories http://stats.stackexchange.com/questions/2777/modelling-longitudinal-data-where-the-effect-of-time-varies-in-functional-form-be,2010-10-05 07:52:45.0,183.0,
4412,3285,1,"The `timecourse` package isn't really for determining clusters, rather it's for calculating which genes are differentially expressed.",2010-10-05 09:41:49.0,8.0,
4413,3285,0,"@csgillespie (+1) Thanks. I thought it might be used to isolate genes with varying temporal profiles across biological conditions, or as a first step before using a clustering procedure (in fact, I was thinking of `kml` but I'm not really an expert in that domain).",2010-10-05 09:53:59.0,930.0,
4414,3285,0,"You are correct in that you would tend use to isolate interesting genes before any clustering - basically thin down your list of genes. I suppose it does perform clustering of a sort, i.e.differentially expressed vs non-differentially expressed.",2010-10-05 10:09:48.0,8.0,
4415,3278,1,"@ars (+1) Thanks for the link (didn't know about this monograph, it looks interesting). My best recommendations for recent developments are actually ALL papers from Jan de Leeuw and these two books: *Multiple Correspondence Analysis And Related Methods* from Greenacre, and *Geometric Data Analysis: From Correspondence Analysis to Structured Data Analysis* from Le Roux & Rouanet (the french way).",2010-10-05 10:46:46.0,930.0,
4416,2635,1,... and maybe a better topic?,2010-10-05 10:52:27.0,88.0,
4417,3331,1,@Jeromy Anglin Thanks for the link.  Did you have any luck with `kml`?,2010-10-05 12:24:07.0,179.0,
4418,3334,0,Sometimes I just want to thwack myself upside the head.  Excellent suggestion.,2010-10-05 12:28:18.0,1499.0,
4420,3342,0,see http://stackoverflow.com/ for R question,2010-10-05 13:04:33.0,1154.0,
4421,3342,7,@pbneau R questions are acceptable on here.  See http://meta.stats.stackexchange.com/questions/252/should-we-allow-more-computing-questions and the FAQ.  See also the recent discussion: http://meta.stats.stackexchange.com/questions/474/printhello-world-n,2010-10-05 13:10:23.0,5.0,
4422,3341,0,"interesting ! there are testing images to try it, exercices to learn how to use it...",2010-10-05 13:16:52.0,223.0,
4423,3343,4,I did not know about this. Now I can manipulate the `h$count` vector to my nefarious purposes. Marvelous. Thanks,2010-10-05 13:21:57.0,5141.0,
4424,3339,1,"@csgillepsie: there is catch here. The answers about comparing distributions assume the draws from the distributions are iid. From the tone of the questions (..""mapped to line chart"") this may not be the case in Tristan's application. That would invalidate all the solutions (k-s, chi-2,...) proposed.",2010-10-05 13:26:26.0,603.0,
4425,3339,0,"@kwak: Very good spot and something that I missed. Just to clarify it invalidates the *k-s* and *chi-2* solutions, not all **all** the answers given, i.e. my answer and the answer by Joris still stand.",2010-10-05 13:31:59.0,8.0,
4426,3339,0,@kwak @csgillepsie: yeah just to clarify the data i'm dealing with is not iid at all.,2010-10-05 13:40:58.0,1505.0,
4427,3334,0,I also just thought of using the `kmeans` function in R.  I really shouldn't ask questions between midnight and 4am.,2010-10-05 13:44:33.0,1499.0,
4428,127,0,I agree with @PeterR.,2010-10-05 13:49:57.0,1499.0,
4431,3347,0,"Unfortunately, the Valencia meetings are no more. The last one was this year! Instead, we're just going to have general ISBA meetings.",2010-10-05 16:23:44.0,8.0,
4432,3345,0,Thanks Kwak. I have ordered the first book from Amazon. I could still do with a few lines to get me started though ...,2010-10-05 16:36:42.0,1216.0,
4434,3347,0,Oh!  I didn't know.  Crap.,2010-10-05 17:08:35.0,1499.0,
4435,3347,0,yeah Dr. Bernardo has apparently told others that he is too old to take care of the valencia conference. He wants young guns to take care of it now.,2010-10-05 18:08:48.0,1307.0,
4436,3353,0,Thanks for the note. I updated my question to include the priors on beta_0 and tau; I had omitted these for simplicity,2010-10-05 21:07:04.0,1381.0,
4438,3355,0,"Sorry, M. Tibbits updated the answer later, but my comments are still valid for David's question.",2010-10-05 21:59:18.0,1307.0,
4439,3327,0,"If I have two possible models to fit to my data, the sine wave as described in my original question and a LMS straight line fit, could I simply compare the average squared deviation from the true data values of the sine wave with the residuals of the LMS fit line and then choose the model with the lower overall value on the grounds that this model exhibits a more accurate fit to the data? If so, would it also be valid to perhaps split the data into halves and do the same with each half separately, using the same sine wave/LMS fits to see how each model may be improving/getting worse with time?",2010-10-05 22:24:15.0,226.0,
4440,3283,0,Yes I'm sure :-),2010-10-06 02:11:04.0,,Kenny
4441,3283,0,(see answer below for clarification),2010-10-06 02:24:10.0,,Kenny
4442,3357,0,"Thank you for the clarification.  The point is that at each stage you only have to consider the probability distribution over (W,L,T) and not how it was derived.",2010-10-06 02:44:23.0,919.0,
4443,3331,0,"I've had a quick look, but for the moment I'm using a customised cluster analysis based on selected features of the individual time series (e.g., mean, initial, final, variability, presence of abrupt changes, etc.).",2010-10-06 02:59:14.0,183.0,
4444,3327,0,"I'm not sure.  My suggestion was to use a Least Squares metric, but I wasn't saying to run linear regression.  You might check out [Periodic Regression](http://www.google.com/search?client=ubuntu&channel=fs&q=Periodic+Regression&ie=utf-8&oe=utf-8).",2010-10-06 03:41:21.0,1499.0,
4445,3327,0,"As to your other question, could you cut the data in half, I would be very cautious in doing so -- because that would double the minimum frequency you could consider.  I think you may end up needing to look at Fourier coefficients (take an [FFT](http://en.wikipedia.org/wiki/Fast_Fourier_transform) or a [DCT](http://en.wikipedia.org/wiki/Discrete_cosine_transform) and regress on them?!? -- __Not sure__).  Or perhaps periodic regression as mentioned above.",2010-10-06 03:47:50.0,1499.0,
4446,2611,0,"@chl: I can send you the tables with the results when I'm done, but I won't be inventing anything new really. So far I'm just planning to compare MI under a two-level imputation model (R package pan) to MI under a simple normal model (ignoring the two-level structure, R package norm) and listwise deletion. Under different sample sizes, values of the variance component etc. This should be enough for the seminar (I'm a PhD student), but not exactly groundbreaking. If you have any ideas on how to ""jazz up"" the simulation study, i'd love to hear.",2010-10-06 07:37:47.0,1266.0,
4447,2611,1,"One other thing: i'm not sure that a proper analytical solution to this problem even exists. I've looked at some additional literature, but this problem is elegantly looked over everywhere. I've also noticed that yucel&demirtas (in the article i mentioned, page 798) write:“These multiply imputed datasets were used to estimate the model […] using the R package lme4 leading to 10 sets of (beta, se(beta)), (sigma_b, se(sigma_b)) which were then combined using the MI combining rules defined by Rubin.”",2010-10-06 07:40:59.0,1266.0,
4448,2611,0,"It seems they used some kind of shortcut to estimate the SE of the variance component (which is, of course, inappropriate, since the CI is asymmetrical) and then applied the classic formula.",2010-10-06 07:47:05.0,1266.0,
4449,3311,0,"Thanks for accepting my answer chi, though your own reference list is much better and more recent. I really should have thought of a couple of them myself as I've got them on my hard drive, and may have even read parts of them...",2010-10-06 08:18:51.0,449.0,
4450,2611,0,"Ok, thx for that. Can you put your comments into an answer so that it can be voted?",2010-10-06 09:25:40.0,930.0,
4452,3360,1,"For record keeping: The DOI of the ""The distribution of product of independent beta random variables with application to multivariate analysis"" is 10.1007/BF02480942.",2010-10-06 13:59:29.0,1512.0,
4453,3360,1,The other one is 10.1137/0118065 :),2010-10-06 14:00:17.0,1512.0,
4454,3358,0,"I appreciate you came back to share your experience with this problem. Unfortunately, I have no real solution but maybe other suggestions will come up.",2010-10-06 14:30:17.0,930.0,
4455,3072,0,"Thanks, but MATLAB isn't free!!",2010-10-06 15:33:25.0,1024.0,
4456,3051,0,@J.M - I hadn't! Thank you! I'm about to see how it works.,2010-10-06 15:36:57.0,1024.0,
4457,3051,0,@Shane - Yes! I'm sorry that wasn't clear. The slide is the number of positions/indices you move to start computing the next window of averages. So rather than the next window starting after the end of the last there is some overlap when the slide is smaller than your window size. The idea is to smooth out the data points a bit.,2010-10-06 15:39:42.0,1024.0,
4458,3362,0,"I find the question interesting. However, it would help if you could briefly summarize the idea in the paper so that users need not download and read the paper to answer your question. I think it would help a lot if your question is self-contained to get quick and quality answers.",2010-10-06 15:46:05.0,,user28
4459,3054,0,"Thanks! Especially for noting the last value as zero assumption, I hadn't considered that. I definitely care about that last window!!",2010-10-06 15:59:54.0,1024.0,
4460,203,3,"You may be interested in this recent article published in PARE, *Five-Point Likert Items: t test versus Mann-Whitney-Wilcoxon*, http://j.mp/biLWrA.",2010-10-06 17:05:16.0,930.0,
4461,3072,0,"@T-Burns: octave is free, however; also R is close enough to Matlab that this code can easily be translated. In fact, @mbq did that..",2010-10-06 17:24:20.0,795.0,
4462,3361,0,The last page alone is worth $1.20! maybe we can wedge that into @Carlos Accioly's knapsack by buying one of the books used..,2010-10-06 17:26:47.0,795.0,
4463,3361,1,The last page comes from this article: http://www.math.wm.edu/~leemis/2008amstat.pdf.  I've posted a condensed version with links for footnotes here: http://www.johndcook.com/distribution_chart.html,2010-10-06 17:28:55.0,319.0,
4464,3054,0,This is perfect! Thanks!!,2010-10-06 17:51:30.0,1024.0,
4465,1618,0,"@Srikant (+1) Interesting links, thanks.",2010-10-06 19:17:45.0,930.0,
4466,3348,0,"This looks very useful! Just to be clear (since I'm not fluent in R): this calculates spatial correlation, but weights more highly correlations between things that are closer together. Is that correct?",2010-10-06 19:44:02.0,900.0,
4467,3348,1,"Precisely. It uses the inverse distance between the pixels (as measured in pixels) to weight the measure of spatial autocorrelation. Note that I generated a grayscale image, but you could similarly apply this to a color image either treating the colors separately or taking some combined score.",2010-10-06 19:46:05.0,1499.0,
4468,3368,0,Please consider adding some references to this question (e.g. for the Nyquist frequency).,2010-10-06 19:57:33.0,5.0,
4469,3362,0,"Good suggestion.  I'm not sure I understand the method well enough yet to summarize how it works accurately... in the mean time, I've excerpted the brief summary from the R implementation.",2010-10-06 20:46:30.0,644.0,
4470,3356,0,"Hi Andy. I don't know stata code. That is why i do not mention the code snipped in my answer, which has to be understood as a response to the part of the question that is formulated in english.",2010-10-06 21:17:38.0,603.0,
4471,3324,0,lara: could you edit your question to explain in plain terms the meaning of the stata code snipped ?,2010-10-06 21:19:16.0,603.0,
4472,3372,0,Kaelin:> do you mean whether there exists 'on the fly' method for computing summary stats such as median and quartiles ? If this is what you want i could give you links to papers detailing them. You could also give more details about the platforms you are working on as efficient GNU implementation of these methods likely exists in R.,2010-10-06 21:24:31.0,603.0,
4473,3349,0,"Thanks, Srikant! Sorry, I somehow missed your comment earlier. The upper cluster is really just a spike right at the ceiling - there's no variability there except for the long stretch of uniformity that links it to the lower distribution, which is basically as you describe.  It'll take me some time to parse out your answer (especially since I'm stuck in IE and can't see the LaTeX properly right now), but I really appreciate your dedication to this odd little question.",2010-10-06 23:09:06.0,71.0,
4474,3064,0,"Note: I apparently missed the deadline to actually award the bounty, so I'm setting up another so that I can properly reward Srikant for his help. More answers are always welcome, but the bounty is for him.",2010-10-06 23:13:29.0,71.0,
4475,3372,0,"@kwak: Yes, that sounds like what I am looking for. I would greatly appreciate those links. :-) I am working on Mac OS X… I can use R for post-processing data, but can't link GPL code into my company's product for the usual reasons.",2010-10-06 23:20:49.0,1515.0,
4476,731,0,Me too. +1 for the paper.,2010-10-06 23:29:24.0,1515.0,
4477,3331,0,Is this a duplicate? http://stats.stackexchange.com/questions/3238/time-series-clustering-in-r,2010-10-07 01:32:55.0,159.0,
4478,3238,1,And this one: http://stats.stackexchange.com/questions/3331/is-it-possible-to-do-time-series-clustering-based-on-curve-shape,2010-10-07 01:33:23.0,159.0,
4479,3368,0,What do you mean by `I have 3 time series'? Do you have 1 time series that has been divided it into 3 pieces? Or do you have 3 devices all independently measuring the same `signal' at the `same' time?,2010-10-07 01:55:12.0,352.0,
4480,3356,0,"@kwak - I was not criticizing your post, I agree with everything you said. I was simply wondering if there was some logic as to why someone would use the differences as instruments that I was unaware of. I can't imagine any situation in which the differences would meet any of the requirements for such a procedure.",2010-10-07 02:04:20.0,1036.0,
4482,3356,0,"Hi Andy:> i didn't take you're comment as a critic. Your post is highlighting a key aspect of the question that neither Rob nor I (admittedly) understood. If anything, it illustrates the importance of collaboration.",2010-10-07 02:49:50.0,603.0,
4483,3336,0,Thank you for those references.,2010-10-07 03:00:35.0,1036.0,
4484,3378,0,"Wow, really interesting approach. I didn't know about that, and I will keep it in mind. Unfortunately, in this case it will not work, since I have really restrictive requirements from the point of view of memory usage, so M should be really small, and I guess there would be too much precision loss.",2010-10-07 05:26:10.0,667.0,
4485,3331,0,"@Rob This question doesn't seem to assume irregular time intervals, but indeed they are close one each other (I didn't remind of the other question at the time of my writings).",2010-10-07 06:11:08.0,930.0,
4486,3365,0,"@Chi Thanks! This works, but means I have to plot each item individually (and combine them with par). What I was hoping for was to simply obtain the same result by using par(mfrow=c(3,3)) and simply plot(fit1, 'some edits to 'main='"") which gives me all the plots (9 in my case).",2010-10-07 06:14:54.0,913.0,
4488,3365,0,"@Tormod I'm afraid you can't. Look at the R code for that function (just type `plot.grm` at the R prompt or `edit(plot.grm)`) near line 52 and 91: Dimitri R. automatically prefixes the title if missing and append ""Item"".",2010-10-07 06:25:55.0,930.0,
4489,3365,0,"@Chi Thanks again! Still, your solution works fine so I am still able to achieve what I want, albeit with a few more lines of code.",2010-10-07 06:31:47.0,913.0,
4490,3374,2,"Note that you will get the same results with `principal(attitude, 2, rotate=""none"")` from the `psych` package and that Kayser's rule (ev > 1) is not the most recommended way to test for dimensionality (it overestimates the number of factors).",2010-10-07 06:36:59.0,930.0,
4491,3365,0,@Tormod The same applies for the `eRm` package. It's difficult to provide users with both a friendly and generic interface :(,2010-10-07 06:40:03.0,930.0,
4493,3363,1,"Thanks for your very detailed answer chi.  I've got `kml` running on my data, but as you suggested it is clustering mostly based on magnitude rather than curve shape, so I'm trying a few pre-processing steps to see if I can improve matters.  The work by Sangalli et al. looks very promising for what I want to do - I cannot find an implementation of their approach however.  I probably do not have time to create my own implementation of their work for this project, hwoever.  Are you aware of any FOSS implementations?",2010-10-07 06:45:18.0,179.0,
4494,3363,0,"@fmark No OSS implementation to my knowledge (the work is quite recent, though); they use k-means and k-medoids which are both available in R. To my opinion, the most critical parts are to generate template curves and implement the warping function. For that, you could find additional infos by looking at morphometry/procruste analysis, or lookup the code of the Matlab PACE toolbox (but this should be full of EM or things like that). My best recommendation would be: Ask the author for any free of charge implementation of their algorithm.",2010-10-07 06:53:49.0,930.0,
4495,3363,2,I'll report back if I get an affirmative :)  Their paper [k-mean alignment for curve clustering](http://mox.polimi.it/it/progetti/pubblicazioni/quaderni/13-2008.pdf) has some more implementation details that also might be useful to someone wanting to do this themselves.,2010-10-07 07:10:19.0,179.0,
4496,3376,0,+1: nice answer. Thanks.,2010-10-07 08:16:54.0,8.0,
4497,3247,0,"@chi: I can see why someone would downvote this, but I think your answer is a valid response.  A list of circumstances when using spreadsheets is not a good idea is a handy addition to a discussion of the advantages of using them.",2010-10-07 08:27:38.0,266.0,
4498,3376,1,+1 Right; I was still in dark ages of making approximation from histogram.,2010-10-07 08:50:21.0,88.0,
4499,3368,0,"Shane, by references do you mean tagging? If so, it seems I'm not allowed to add new tags as I'm a newbie.",2010-10-07 10:08:32.0,1513.0,
4500,3368,0,"No, the three time series are not divided from one. Although independently sampled, they are strongly correlated with each other.",2010-10-07 10:15:30.0,1513.0,
4501,3373,1,"Really appreciate your help! Using your cell tower as an example, my scenario is more like the following: three cell phones at three equally-spaced locations receive signal from the same cell tower. Would this be considered ""in phase""? Thanks again!",2010-10-07 10:33:02.0,1513.0,
4503,3373,1,"I'm not sure.  If you look at your data, does the first sample collected in each of the three time series correspond to the same point in time in the underlying process?  If so, then they're ""in phase"".  Or is there a lag between when one series is collected and the other two are collected?  If there is a lag, then they would be out of phase. There's a cute picture on [this Wikipedia](http://en.wikipedia.org/wiki/Phase_%28waves%29) page which might provide some insight.",2010-10-07 11:28:20.0,1499.0,
4504,3381,7,What's wrong with conventional kernel density estimation?,2010-10-07 12:08:17.0,449.0,
4505,3375,0,"Thanks Srikant, yes I think that summarizes the method well.  The examples I've seen have applied the method to an aggregate dependent variables like GDP or cigarette sales per capita (estimated from tax revenues).  Is there anything special I'd need to do when using a survey-based dependent variable instead?  I guess add confidence bands to my time-series plots??  Then do some power estimates based on what divergence between treated and control groups I wouldn't want to miss by chance, then extrapolate/guesstimate how many control-groups responses I need to form useful synthetic control??",2010-10-07 12:53:56.0,644.0,
4506,3375,0,Are there other things I'd need to do to take into account the fact that my dependent variable is an estimate?,2010-10-07 12:55:49.0,644.0,
4507,3383,0,Can you distinguish what pre-treatment weight is assigned to each different treatment group? (i.e. I know these 16 observations are pre-treatment weights for treatment A). Also were treatments randomly assigned?,2010-10-07 13:03:32.0,1036.0,
4508,3383,0,"The 16 observations are 16 fishes in a tank. The treatment is given to the tank. So the 16 fishes in the tank were weighed before and after treatment. But as the fishes weren't marked, it's impossible to link the weights to a specific individual.",2010-10-07 13:08:06.0,1124.0,
4509,3385,0,"I've been thinking about that as well, but the initial mean weight differs significantly between tanks, and using all t=0 observations as one control makes for a very unbalanced design. On top of that, the hypothesis is formulated in difference between treatments, which cannot be formally tested using a control as reference group. The multi-level framework doesn't help me either for the same reason.",2010-10-07 13:55:38.0,1124.0,
4510,3385,0,"I don't see why the unbalanced design is a problem, but the mean weight difference is obviously a problem with my suggestion. I would still think the multi-level framework would allow you to assess differences between treatments somehow (I do not know how offhand though). The only other thing I can think of would be to simply graph each control/comparison like this response did (minus the connecting lines) http://stats.stackexchange.com/questions/2067/follow-up-in-a-mixed-within-between-anova-plot-estimated-ses-or-actual-ses/2138#2138 , although I imagine this is not entirely satisfactory.",2010-10-07 14:10:36.0,1036.0,
4511,3374,3,"Yes, I know psych principal wraps this up.  My purpose was to show what SPSS ""factor analysis"" was doing when using the principal components extraction method.  I agree that the eigenvalue rule is a poor way to select the number of factors.  But, that is exactly what SPSS does by default and this was what I was demonstrating.",2010-10-07 14:21:18.0,485.0,
4512,3383,1,"Your issues with independence of observations go beyond repeated measurements.  Your 16 fish within a tank are not independent and the treatment was applied to the tank.  You do not have 16 independent pre and post observations.  In fact, you have 6 observations assigned to one of 6 treatments.  Any analysis that uses the fish as the analytical unit will grossly overestimate your degrees of freedom.  At very least, you need to account for the nested structure and the intra-tank correlations.  Needless to say, you need to increase your N, which is 6 right now (see pseudo-replication).",2010-10-07 14:41:28.0,485.0,
4513,3382,0,well to console you i have just started out and was just looking around some. i have no interest in getting in your way (: its just something that has been bugging me for a while. there should be some kind of normalization possible with sequences,2010-10-07 15:08:36.0,1516.0,
4514,3383,1,"@Brett : I am aware of the fact that my observations aren't independent. But I don't agree with your statement that N=6. That would mean that a clinical study with 4 hospitals involved would have N=4, which obviously doesn't make sense. It's impossible to give fish in the same tank different treatments, as the treatment is in the water. The 16 fish are independent, but the measurements at t=0 and t=1 aren't. I correct for this by calculating the df using Satterthwaite and then divide them by 2, which gives me a df of appx. 14 for the t-tests.",2010-10-07 15:09:33.0,1124.0,
4515,3375,0,"@heather Why do you call your DV as an estimate? If you call it as an estimate because you are taking the mean/sum of your likert scales then I do not see how that matters conceptually. As far as power is concerned, that would be dependent on the model/estimation specifics and I am unable to offer any suggestions on that issue. Perhaps, you can just email the authors for some suggestions or do a simulation to see how much sample size you need for desired power?",2010-10-07 15:11:03.0,,user28
4516,3383,0,"In your hospital example, it depends on how the patients are randomized.  If the hospitals are assigned to the treatments, then your N is the number of hospitals.  If patients are randomly assigned within hospitals, then your N is the number of patients.  Again, I refer you to the literature on pseudoreplication which you'll find easily.  Once you say ""the treatment is given to the tank"" you are done.  Level of treatment implies appropriate level of analysis.  Here is the original article http://www.masterenbiodiversidad.org/docs/asig3/Hurlbert_1984_Pseudoreplication.pdf There's plenty more.",2010-10-07 15:31:17.0,485.0,
4517,3388,1,"@gaetan I do not understand the remark about a single column vs multiple columns. Are you suggesting that categorical variables should be coded as 1, 2, 3 etc in a single column instead of using dummy variables? I am not sure that makes sense to me as you are then imposing an implicit constraint that the difference in the effect on dv between leve1s 1 and 2 is the same as the difference in the effect on dv between levels 2 and 3. Perhaps, I am missing something.",2010-10-07 16:07:12.0,,user28
4518,3375,0,"Great, Srikant, thanks.  The fact that you don't think it matters conceptually answers it for me!  I wanted confirmation of that I guess.",2010-10-07 16:13:46.0,644.0,
4519,3378,0,"@gianluca: it sounds like you have 1. a lot of data, 2. limited memory resources, 3. high precision requirements. I can see why this problem is freaking you out! Perhaps, as mentioned by @kwak, you can compute some other measure of spread: MAD, IQR, standard deviation. All of those have approaches which might work for your problem.",2010-10-07 16:22:27.0,795.0,
4520,3383,0,"@Brett Magill - I completely disagree with the statement ""Level of treatment implies appropriate level of analysis"". Your units of analysis will be determined both by the question at hand and restrictions on your data. Now I agree that this design causes nesting complications, but it doesn't make it unreasonable to make assumptions about the independence of units within treatment groups and make adjustments to degrees of freedom accordingly. Your statement is essentially refuting the methodology of multi-level models and that entire body of work (as well as any non-experimental study.)",2010-10-07 16:45:27.0,1036.0,
4521,3383,0,"Notice, I said implies rather than dictates or controls.  Also notice the mention that you need to at least account for intra-tank correlation--a nod to a multi-level approach.  In fact, in this case, this is exactly what multi-level models will do--penalize the d.f. according to the degree of intra-class correlation, effectively adjusting the N downward to account for the structure.  By the way, I've got slides from George Casella's Experimental Design session at ASA that talk about this specific problem--randomizing tanks rather than fish--as an example of pseudo-replication.",2010-10-07 17:00:37.0,485.0,
4523,3383,0,"If you don't want to read the original article and the ensuing literature that I cited previously, take a look at this 1.5 page writeup from the statistical consulting unit at Cornell. It's pretty easy to follow. http://cscu.cornell.edu/news/statnews/stnews75.pdf",2010-10-07 17:24:08.0,485.0,
4525,3388,1,"@Gaetan I am not sure I follow you. How exactly does XLStat transform the 'text' values of cold, mild or hot into numerical values for the purpose of estimation? If there is a method that will let you estimate the effects of categorical variables without using dummy variables surely that should be independent of the software you use as there should be some underlying conceptual/model based logic.",2010-10-07 17:35:04.0,,user28
4526,3391,0,Thanks for the link!,2010-10-07 17:48:50.0,930.0,
4527,3378,0,"gianluca:> Give us more quantitative idea about the size of memory, arrays and accuracy you want. It may well be that your question will be best answered @ stackoverflow though.",2010-10-07 18:28:03.0,603.0,
4528,3388,0,"@Gaetan I don't follow your point unless you consider that your ordinal variable is treated as a continuous one (this might make sense sometimes, although we clearly assume that the variable can inherit the property of an interval scale as pointed by @Skrikant). Usually, a variable with $k$ levels is represented in the design matrix as $k-1$ columns, and I think this is quite independent of the software used (surely, XLStat takes care of constructing the correct design matrix as R, SPSS or Stata does).",2010-10-07 18:42:08.0,930.0,
4530,3388,0,"@Srikant.  XLStat in its demo used an example a model where the dependent variable was probability of renewing a subscription.  And, one categorical variable within a single column had 6 different age ranges of subscribers.  Using a maximum likelihood algorithm, it can interpret each specific age range as a separate data set equivalent to a separate dummy variable in its own column.  When you choose that specific column, you just have to state it is a ""qualitative"" variable (instead of a ""quantitative"" one).  From everyone comments, I gather this is not something you can code in SPSS.",2010-10-07 19:01:26.0,1329.0,
4531,3382,0,"@tarrash Nah, this fourth point is rather too fresh to be tested enough. And as I wrote, normalization is either in aligning or in getting into spectra.",2010-10-07 19:10:21.0,88.0,
4532,3388,1,"@Gatean Ok, in this case, the same can be done in SPSS (you have the choice between numerical/ordinal/nominal for each variable) -- then, the design matrix is constructed accordingly.",2010-10-07 19:21:50.0,930.0,
4533,3388,2,"@Gaetan @chl To summarize my understanding: The features of SPSS and XLStat whereby you can specify the measurement scale (nominal, ordinal etc) decreases the data file size. However, in both instances, the software uses the correct coding scheme (e.g., expand a nominal variable with J categories into J-1 dummy variables) as part of the estimation process in the background. Would that be a fair assessment of the situation?",2010-10-07 19:32:54.0,,user28
4534,3388,0,@Skrikant It seems you are summarizing the situation much better than me!,2010-10-07 19:49:18.0,930.0,
4535,3397,1,(+1) You are nicely echoing the discussion on Medstats about the need to keep a record of data edit and analysis (http://j.mp/dAyGGY)! Thx.,2010-10-07 20:01:03.0,930.0,
4536,3383,0,"@Brett : I did read the article and I understand what you're getting at, but this is not an ecological experiment. Following Hurlbert, each fish should have its own tank, and the water in these tanks should not come from the same source. But the experiment done here is not equivalent to the ecological studies he describes. Plus, this approach is impossible if you can't collect water in 192 different lakes... Furthermore, his paper is not a statistical result, but a logical -and in a number of cases valid- argument that not everybody agrees upon.",2010-10-07 20:11:23.0,1124.0,
4537,3383,0,"Ok, how about Casella in hist Statistical Design of Experiments Book.  Here's a link the relevant page in Google books.  http://books.google.com/books?id=sqnbSUtryVAC&pg=PA5&lpg=PA5&dq=casella+pseudoreplication&source=bl&ots=634YgjM_h7&sig=hf8C_vWd03hELis6bC-nxyltTWA&hl=en&ei=3ymuTOeDJdT-nAfis4DuBQ&sa=X&oi=book_result&ct=result&resnum=2&ved=0CBYQ6AEwAQ#v=onepage&q&f=false",2010-10-07 20:14:30.0,485.0,
4538,3396,0,Thanks for your help!!! Do you know if anything of the sort exists for Matlab by any chance? I am not that familiar with R..,2010-10-07 20:15:00.0,,BobJones
4539,3397,0,"Saving your ""work and blind alleys"" isn't any harder to do with Excel than with R.  It's just a matter of actually doing it.  The main problem with Excel is related to its strength: it's all too easy to change something inadvertently.  But for EDA--the focus of the OP--we rarely if ever save everything we do.  EDA, after all, is supposed to be *interactive.*",2010-10-07 20:18:51.0,919.0,
4540,3383,0,"@Brett : As I said, it is a point of view. One can easily argument that every fish has its own physiology. Biologically spoken, adding the weights of all fish in 1 tank doesn't even make sense, as you have males and females, different age groups and the likes. If I would have done the experiment, I would have marked the fish so it would be a truly repeated design. But having 4 seperate tanks filled with exactly the same water and given exactly the same treatment, does not mean for me that I suddenly have more degrees of freedom. It merely means I have split up 1 tank in 4.",2010-10-07 20:19:45.0,1124.0,
4541,3396,0,"@BobJones: I am unfamiliar with MatLab but quite familiar with a lot of the geostatistical software out there; I haven't run across anything specifically mentioning MatLab.  It's not hard to create an empirical variogram, though: it involves binning the squared differences of values associated with all distinct pairs of (X,Y) points, summarizing the values by bin, and graphing those summaries.  It's so easy, though, to learn to import a simple dataset (your X, Y, Z values) and call an R routine that you could be up and running quickly if you want.",2010-10-07 20:25:34.0,919.0,
4542,3383,0,"@Brett : the main idea behind Hurlbert, is that you can't distinguish between the effect of ""tank"" and the effect of ""treatment"" theoretically spoken. Practically, it's proven in the lab that the used procedures do not cause a ""tank"" effect beyond natural variation between fish. Hence... edit : for the record, I do appreciate your input, it makes me think about the setup and it gives me something to tell the lab people not to repeat this setup any more in their life :-)",2010-10-07 20:31:03.0,1124.0,
4543,3398,7,@whuber A nice and handy overview of pros and cons!,2010-10-07 20:32:22.0,930.0,
4544,3388,0,"@Skrikant, you are summarizing the situation correctly.  I may have confused everyone with an earlier comment whereby I used ""nominal"" incorrectly.  I thought this adjective related to real numbers so to speak.  I realize that nominal means just a label which is what it should be when dealing with qualitative categorical variables.",2010-10-07 20:36:32.0,1329.0,
4545,3394,0,"I like the nuance about ""different levels of discussion.""",2010-10-07 20:39:50.0,919.0,
4546,3398,3,"+1 nice and balanced.  I especially like the point about ""immediacy of interacting directly"" which I think is Excel's (or really, the spreadsheet's) biggest selling point.  Declarative programming for the masses -- which explains why some people think that 80% of the world's business logic is written in Excel (worth pointing out to programmers and statisticians who argue about R v SAS or Java v C++, etc).",2010-10-07 20:42:44.0,251.0,
4547,3386,2,"This is just a partial answer: even when you create the dummies explicitly (rather than using the software's implicit capabilities), keep them together in all analyses.  In particular, they should all enter together and all leave together in a stepwise regression, with the p-value computed appropriately for the total number of variables involved.  (This is Hosmer & Lemeshow's recommendation, anyway, and it makes a lot of sense.)",2010-10-07 20:53:03.0,919.0,
4549,3398,1,I heard that Microsoft hired some numerical analysts several years ago to fix the broken functions in Excel.  Do you know whether the problems with Excel are still there in the 2007 or 2010 versions?,2010-10-08 00:44:30.0,319.0,
4550,3398,0,"John, the McCullough and Heiser paper that Carlos references addresses Excel 2007.  I will not adopt Excel 2010 for various reasons, so I haven't had the opportunity to test it.",2010-10-08 02:29:18.0,919.0,
4551,3399,1,Thanks for the additional points and for sharing your perspective.,2010-10-08 02:31:07.0,919.0,
4552,3403,4,"@ars: Everything is correct and nicely stated.  But one thing seems to be missing: the standard deviation of the ""best approximation"" j/n depends on the *true* proportion of redheads, not the estimated one.  The problem, of course, is that we don't know the true proportion.  But the fact remains that the standard error does not actually equal the standard deviation of the approximation except when the estimate happens to be exactly correct.  I know you don't need reminding of this subtlety, nor will most readers, but it's rather relevant to the original question.",2010-10-08 02:39:29.0,919.0,
4553,3405,4,"This is far and away the aspect of Excel that infuriates me the most.  Data storage needs explicit data types, not formatting.",2010-10-08 02:45:13.0,71.0,
4554,3404,0,I fully agree with you.  An observational study may be good to uncover some associations that in turn one can test using a much more rigorous framework (randomized trial as you suggest).,2010-10-08 03:42:40.0,1329.0,
4555,3405,2,"Actually, this is something about MS software in general that annoys me: it changes your input into what it believes you actually meant, and you usually don't even see it happening.",2010-10-08 04:17:49.0,666.0,
4556,3405,0,@csgillespie (+1) Good to hear of Excel from this perspective!,2010-10-08 06:15:51.0,930.0,
4557,3403,0,"@whuber: good point, thanks for noting that.",2010-10-08 08:47:32.0,251.0,
4558,3397,1,"it is possible to keep a reproducable record of your methods if you do it in VB, but the GUI focus of Excel doesn't encourage that behaviour.",2010-10-08 10:06:19.0,229.0,
4559,3381,0,"Correct me, conventional KDE uses *all* the input data points for *each* estimate(x) -- quite unpractical for many points. I'm looking for ways to combine a) get some nearby points (a general problem), b) weight the values from this small sample",2010-10-08 10:54:06.0,557.0,
4560,3396,0,Ok cool. I have played around with R a bit so maybe I'll try to combine both somehow. Thanks again for your help.,2010-10-08 12:17:07.0,,BobJones
4561,3412,0,"Your models are not nested, what would be the rationale for using an LRT between the two?",2010-10-08 12:52:42.0,930.0,
4563,3405,2,"My favorite error occurred when Excel used to quietly truncate fields during export to other formats.  In a file of pesticide concentrations in soil, it converted a value of 1,000,050 (extraordinarily toxic) to 50 (almost inconsequential) by clipping off the initial digit!",2010-10-08 13:04:25.0,919.0,
4566,3414,0,Thank you! Do you know any Matlab / Mathematica based packages?,2010-10-08 14:12:08.0,1250.0,
4567,3414,0,"No, I'm afraid not.  But R is fully open source so all the algorithms above will be very transparent if you want to re-implement.",2010-10-08 14:20:03.0,5.0,
4568,3416,1,I was thinking that this wouldn't be appropriate because the two would actually be highly correlated.  As the center is far away so will the near tend to be.,2010-10-08 14:34:55.0,601.0,
4569,3416,0,@John Good point.,2010-10-08 14:41:46.0,,user28
4570,3414,1,and if you google for matlab hurst exponent you'll find several.,2010-10-08 14:42:09.0,247.0,
4571,3414,0,First result from google: [Hurst Exponential](http://www.mathworks.com/matlabcentral/fileexchange/9842),2010-10-08 14:59:39.0,1499.0,
4572,3416,0,I think your point is good as well... I'm actually not sure it matters.  I know it's high but under 0.8... still analyzable.,2010-10-08 15:03:32.0,601.0,
4573,3417,1,"I would probably add, that the difference between regions in terms of occaisonal smokers is largely restricted to the second axis of variation and as we can see in the stats this is such as small component as to not really be worth interpreting. The structure in the data is really the contrast between BC and the other regions in terms of daily smokers and former smokers.",2010-10-08 16:25:56.0,1390.0,
4574,3403,0,"@whuber: This clarification left me a bit confused. Given a $j$ and an $n$, what would be the standard error, described by $j$ and $n$? (In contrast to being dependent on the *true* proportion of redheads, which we can't know.)",2010-10-08 16:42:05.0,5793.0,
4575,3403,2,"@cool-RR: ars is correct about the standard error.  The point is that the standard error itself is an estimate of how accurate the statistic j/n estimates the true proportion.  For example, suppose 10% of all people are redheads.  Then in many cases it can happen that j=0 when n=10.  You would obtain an SE of Sqrt(0(1-0)/10) = 0.  This obviously underestimates the actual precision of your statistic p = j/n = 0/10.  The true precision is Sqrt(0.10(1-0.90)/n), even though you don't know that!",2010-10-08 17:19:14.0,919.0,
4576,3411,0,Thanks for the insight!,2010-10-08 17:20:40.0,1499.0,
4577,3396,1,@BobJones: You can see http://www.mathworks.com/matlabcentral/fileexchange/20355 for variograms or http://mgstat.sourceforge.net/ for a full-out matlab geostatistics toolkit,2010-10-08 17:29:22.0,900.0,
4578,3395,0,I asked a similar question that you might find useful: http://stats.stackexchange.com/questions/3199/clustering-of-a-matrix-homogeneity-measurement,2010-10-08 17:32:34.0,900.0,
4579,3396,0,Cool thanks Xodarap!! Appreciate it!,2010-10-08 18:21:20.0,,BobJones
4580,3396,0,@Xodarap: many thanks for sharing that tip.,2010-10-08 20:11:54.0,919.0,
4581,153,2,"John Tukey wrote otherwise (back in 1960) in a monograph ""Data Analysis and Behavioral Science"" (published in Collected Works v. III).  One result he obtained is that if you're getting better than about 10% test-retest agreement, your scale isn't narrow enough!",2010-10-08 20:17:35.0,919.0,
4582,663,1,"Michael Lavine is a clear lecturer and thinker, so I have little doubt his book is worth a look.",2010-10-08 20:26:34.0,919.0,
4583,3403,0,"Again: I am interested in what I *can* know, not in what I *can't* know. Let's take your example where $j=0$ and $n=10$. The most likely proportion of redheads is 0%, but there's good chance it's 2% or 5% or 10%. So my question is: Given that $j=0$ and $n=10$, what is the probability distribution function of the proportion of redheads, *from the information that I know, not the information that I don't know?*",2010-10-08 20:50:22.0,5793.0,
4585,3422,0,Thanks for the clarification.,2010-10-08 21:11:35.0,930.0,
4586,3403,1,"@cool-RR: for small samples, use the Agresti-Coull interval specified in the Wikipedia link on confidence intervals.  Based on your observations, you will obtain a 95% interval for estimate.  Then, what you will know, based on what you observed, is inherent in the definition of a 95% CI.",2010-10-08 21:26:40.0,251.0,
4587,3424,0,"Yeah, I thought about using a log scale. Unfortunately, I don't think our selected charting package (highcharts) supports it. Also, the data is such that even with a log scale it's still pretty extreme. I guess I was hoping there'd be some obscure viz type that I haven't heard of. :)",2010-10-08 21:31:18.0,1531.0,
4588,3368,0,@user1513 By references I mean links to more information.,2010-10-08 21:31:34.0,5.0,
4589,3424,1,"@sprugman I wouldn't accept this just yet.  There are plenty of other possible answers.  But I do think that scaling the data so that it's more linear between small and large values would be a good approach (you don't have to use a log scale, it can be something else...).  And scaling can be done on the data before it is plotted, so you can do that with any package.",2010-10-08 21:33:22.0,5.0,
4590,3424,0,"ok, shane, you talked me out of accepting your answer. Thanks. :)",2010-10-08 21:43:10.0,1531.0,
4591,3296,0,It is not clear to me: are the independent (i.e. right hand side) variable continuous or discrete ?,2010-10-08 22:02:54.0,603.0,
4592,3427,8,"+1. Also, in case it's helpful, the term to google when comparing strings is ""edit distance"": http://en.wikipedia.org/wiki/Edit_distance",2010-10-08 22:05:39.0,251.0,
4593,3427,0,"@ars:> thanks, that's a handy list to feed unto a R search engine and see what comes out!",2010-10-08 22:09:46.0,603.0,
4594,3424,3,"+1, log scale sounds good to me -- @sprugman: couldn't you just preprocess your data and log-transform it before dispatching it to your charting routine?",2010-10-08 22:22:45.0,251.0,
4595,3432,0,"> thanks, first time i've eared of this idea.",2010-10-09 00:26:53.0,603.0,
4596,663,2,"I'm teaching out that book this semester.  It may be a good *statistics* book, but I'm having second thoughts regarding its light probability content.",2010-10-09 01:21:48.0,319.0,
4597,3425,1,"Tal, could you show us some of the data? Or tell us a little bit more about how the strings do or do not match? There are a number of options here most of which are dependent on how you match it up visually.",2010-10-09 02:28:06.0,776.0,
4598,3422,0,"I didn't specify that I had been fitting with REML = FALSE.  I'm still in a bit of a quandry though... The AIC gives me a measurement of the whole likelihood including the random effects.  That's a large component. And of course, the AIC's are extremely unlikely to be exactly the same.  Therefore, it seems unwise to just select the larger value with out some analytic way of telling how much larger it is.",2010-10-09 07:49:55.0,601.0,
4599,3422,0,"@John This lecture highlights interesting point about REML vs. ML and AIC (and points to what you said, John), http://j.mp/bhUVNt. Bolker's review of GLMM is also worth to take a look at: http://j.mp/cAepqA.",2010-10-09 08:29:22.0,930.0,
4600,3436,2,"Agree with last para. The Gaussian kernel uses all the data points but other commonly-used kernels (Epanechnikov, biweight, cosine, Parzen, triangular, ...) are finite-width, i.e. they are defined to be zero outside a finite interval. R's density() function defaults to Gaussian but Stata's -kdensity- command defaults to Epanechnikov.",2010-10-09 09:38:31.0,449.0,
4601,3427,0,+1 for RecordLinkage.,2010-10-09 12:01:21.0,88.0,
4602,3425,0,"Hi Brandon, I will now update my question with some data.  Thanks",2010-10-09 13:12:14.0,253.0,
4603,3433,0,Hi Brandon - I added a sample of the data.  Thanks!,2010-10-09 13:17:59.0,253.0,
4604,3440,0,(I have version 7.) I have no problem loading the Statistics package. But what's the function in there called? Because I get the impression that this `Quantile` line will do the calculation manually instead of using a formula.,2010-10-09 14:13:25.0,5793.0,
4605,346,1,@Srikant:> it's a pretty active area of research in statistics :) The solution closest to the lower theoretical bounds in terms of storage involve some pretty clever probability constructs as well. All in all i was surprised when i first looked unto it a couple of month ago; there is more stats here than meets the eye.,2010-10-09 14:21:03.0,603.0,
4606,3439,1,"Quantile *vs.* percentile (it's merely a matter of terminology), http://j.mp/dsYz9z.",2010-10-09 14:22:20.0,930.0,
4607,3440,0,"Evaluate it with symbolic parameters (i.e. don't assign values to `mu`, `sigma`, and `q`); you should get an expression involving the inverse error function.",2010-10-09 14:24:37.0,830.0,
4608,3425,2,"Hi Tal:> Given that these seems to be typo-free scientific names, i would try the Levenshtein metric first (in the context of a 92-by-55 distance matrix) and see how it comes out.",2010-10-09 14:31:27.0,603.0,
4609,3439,1,"While we are in, in R Wald-adjusted CIs (e.g. Agresti-Coull) are available in the `PropCIs` package. Wilson's method is the default in `Hmisc::binconf` (as suggested by Agresti and Coull).",2010-10-09 14:36:02.0,930.0,
4610,3424,1,"I agree with @ars -- log the data first, then plot and adjust the axis labels.",2010-10-09 15:41:05.0,88.0,
4611,3444,0,Thanks for adding a real illustration with ppf.,2010-10-09 16:22:52.0,930.0,
4612,2371,0,"Looks interesting, thanks.  I have similar problem to the OP trying penalized + coxph on a big data set, wonder if this helps.",2010-10-09 16:48:11.0,251.0,
4613,3445,3,Could you clarify whether the Ms Hudson to whom you refer is Kate Hudson or Jennifer Hudson??,2010-10-09 17:16:13.0,449.0,
4614,3445,0,"@onestop This is a software for population genetics, http://j.mp/a75Z4Y",2010-10-09 18:07:00.0,930.0,
4615,3447,0,"Ouaouh, this is again an JIT answer :)",2010-10-09 18:15:32.0,930.0,
4616,3443,0,Thanks for adding the original reference.,2010-10-09 18:24:43.0,930.0,
4617,3448,0,"@chl: I just noticed, funny. :)",2010-10-09 18:24:49.0,251.0,
4618,3450,0,Good that you cite the whole book from Pearl.,2010-10-09 19:26:33.0,930.0,
4619,1957,0,"I like this ""original"" approach (wrt. other entries). Still, I always find difficult to explain why bootstrap works in practice...",2010-10-09 19:38:21.0,930.0,
4620,3412,0,restated things as per your comment,2010-10-09 19:56:16.0,601.0,
4621,3427,2,Levenshtein edit distance is implemented as part of the base package via agrep(),2010-10-09 20:05:57.0,776.0,
4622,3427,0,"@Brandon:> thanks, i couldn't find it....",2010-10-09 20:11:02.0,603.0,
4623,3453,0,"@ccgillespie:> i think my question may have been poorly worded. In the package i see (GPUtools, magma) double precision seems to be used as standard (with the loss of performance you describe). I was wondering why single precision is not offered as an option.",2010-10-09 20:46:34.0,603.0,
4624,3453,0,@kwak: The double precision values must be converted to single precision by the wrapper. The wrapper was just trying to be helpful.,2010-10-09 20:51:10.0,8.0,
4625,3453,0,"@ccgillespie:> yes, but it seems the wrapper comes with  performance costs exceeding the factor 2 you cite (again, correct me if i'm wrong on this) and in some cases no tangible benefits (i can think of many application in stat were SP FP arithmetics would be okay). I was wondering whether it makes sense to ask for an option to switch off said wrapper.",2010-10-09 20:54:58.0,603.0,
4626,3453,2,"@kwak: Glancing at the GPUtools help file, it seems that `useSingle=TRUE` seems to be the default in the functions.  Am I missing something here?",2010-10-09 20:56:27.0,251.0,
4627,3453,0,"@csgillespie: Remember, until relatively recently most nvidia cards simply **couldn't** do double precision computation. The factor of 2 hit is what I observed using raw C/CUDA code. Having a python/R wrapper may make this worst.",2010-10-09 21:08:08.0,8.0,
4628,3453,0,"@ars:> argh. I didn't have a NVdia cart at home, so i didn't bother to read the manual. The white papers (and the GPU+R site) are all about double precision arithmetic. Would you be so kind to post you comment as a response so i can close the question ?",2010-10-09 22:36:36.0,603.0,
4629,3381,6,"If you use a kernel with a finite support (e.g., the quadratic Epanechnikov kernel), then only nearby points contribute anything to a kde. Even with a Gaussian kernel, the contribution of points more than 3 bandwidths from x is negligible. I would definitely use kde rather than your proposed approach.",2010-10-09 22:39:23.0,159.0,
4630,3453,0,"@kwak: ah, got it.  I posted a separate answer.",2010-10-09 22:57:15.0,251.0,
4631,3428,0,Nice paper and suggestion -- thanks.,2010-10-10 00:11:19.0,251.0,
4632,3453,0,"As noted above, the Fermi cards support double precision.  The GTX 2xx series also support double precision -- but the Fermi cards GTX 4xx have approximated twice the double precision performance of the 2xx series.  __However__, the Tesla 20xx product is four times faster still.  Here is a [discussion](http://www.vizworld.com/2010/04/geforce-gtx-480-18supthsup-double-precision-performance/) about the crippled double perf. of GTX 480.  [Specs for Tesla 20xx](http://www.nvidia.com/object/product_tesla_C2050_C2070_us.html) (515 GFlops DP Perf)",2010-10-10 00:12:41.0,1499.0,
4633,3453,0,"[Specs for GTX 480](http://www.nvidia.com/object/product_geforce_gtx_480_us.html) -- 168 GFlops Peak DP Perf  [Specs for GTX 285](http://www.nvidia.com/object/product_geforce_gtx_285_us.html) -- 88.5 GFlops Peak DP Perf.  The performance numbers are not listed on the nvidia website for the GTX products, I found them [here.](http://techreport.com/articles.x/18682)",2010-10-10 00:18:10.0,1499.0,
4634,3439,0,Thanks for the comments chl,2010-10-10 04:11:21.0,253.0,
4635,3454,0,"@kwak: I find the answer above helpful, but it really doesn't answer the question posed - ""is single precision so bad?"" Perhaps you should reword your question?",2010-10-10 07:57:45.0,8.0,
4636,3433,0,Hi Brandon - your solution worked great - thank you.,2010-10-10 08:23:07.0,253.0,
4637,3435,0,Thank you Andy - I will have a look at it in the future.,2010-10-10 08:23:27.0,253.0,
4638,3432,0,Thank you Whuber - I never heard of it.,2010-10-10 08:23:43.0,253.0,
4639,3455,0,"Can you elaborate a bit? It seems some iterative algorithm (matrix invert, QR decomposition) seem to work well. I'm also curious as to whether the inaccuracy of SP becomes more of a problem for operations involving larger arrays.",2010-10-10 08:23:43.0,603.0,
4640,3427,0,Great answer Kwak - I will have a look at it in the future!,2010-10-10 08:24:00.0,253.0,
4641,3427,0,"Personally, I feel that this is a more complete answer to Tal's question. +1 for pointing our RecordLinkage - I'll definitely have to try that out.",2010-10-10 08:40:15.0,776.0,
4642,3433,0,+1 for the link to the previous question on the subject in S.E. (thaks for the pointer to agrep()).,2010-10-10 08:50:58.0,603.0,
4643,3296,0,@kwak the variables are continuous (fixed the question),2010-10-10 09:43:57.0,1496.0,
4645,3463,2,Are the time series both stationnary ? www.econ.ohio-state.edu/dejong/note1.pdf,2010-10-10 11:54:40.0,603.0,
4646,3461,0,"+1 for kernlab, really makes life easier.",2010-10-10 13:20:27.0,88.0,
4647,3458,5,"Don't bother with neural networks, this is an obsolete technology.",2010-10-10 13:22:11.0,88.0,
4648,3466,0,"When you say ""condition"", do you mean group assignment?",2010-10-10 13:49:45.0,561.0,
4649,3464,6,"I would have given almost the same answer but here it is and well stated, too.  Let me add just two things based on experience.  First, I have found it's almost always worthwhile re-running any analysis I possibly can: it serves to check my understanding and, more often than one might expect, it exposes errors in the paper itself.  Second, it's essential to locate the key references and to find references of your own by searching the Web for phrases in the paper.  A substantial number of contributions recently are (auto-)plagiarisms or bald attempts to get another paper out of old work.",2010-10-10 15:28:07.0,919.0,
4651,3446,2,"I confess to being mystified by this, despite struggling several times to make sense of it: is there a question here?  ""So bad"" is vague and has no referent.  What exactly are you seeking to understand or find out?",2010-10-10 16:19:55.0,919.0,
4652,3462,0,(+1) Nice concrete recommendation! I was about to suggest the use of the `mvoutlier` package (which relies on mahalanobis distance and displays outlying obs. in an interactive display).,2010-10-10 16:27:41.0,930.0,
4655,3465,2,"kernlab also uses libsvm for the optimization, so there isn't a big difference in that sense (although it is much more flexible).",2010-10-10 16:44:26.0,5.0,
4662,3461,0,"@Tal Here is a fair (or I think so) review of SVM *vs.* RFs: A comprehensive comparison of random forests and support vector machines for microarray-based cancer classification, http://j.mp/ab7U8V. I also prefer `kernlab` to `e1071`.",2010-10-10 19:25:28.0,930.0,
4663,3307,1,"Can you elaborate a little bit about the purpose/context of this particular analysis, or why do you seek an alternative measure of association?",2010-10-10 19:33:06.0,930.0,
4665,3461,0,@chl: Thanks for the reference.,2010-10-10 21:00:12.0,5.0,
4666,3459,2,+1 Great answer.  I also agree with the caret recommendation.,2010-10-10 21:00:48.0,5.0,
4670,3455,0,"There are two parts to it: 1) What does the data represent? 2) How do you process the data?  If you're looking at thousands of points of data from a medical study, single precision would likely be plenty for quantifying patient wellness, and I doubt you would ever need double.  Geometry, on the other hand, could require either single or double precision depending on your scale & zoom.  Calculating the trajectory of a probe to Saturn would always require doubles, as even small errors could drastically effect the result.  You need to look at the data and decide what your tolerances are.",2010-10-10 22:55:50.0,1539.0,
4672,3466,1,@propofol: yes. apologies if my language is not clear.,2010-10-11 00:47:50.0,183.0,
4677,3464,0,"I've added an additional question. If it's not too much hassle, would you update your answer?",2010-10-11 08:29:16.0,8.0,
4678,3478,0,"I've added an additional question. If it's not too much hassle, would you update your answer?",2010-10-11 08:29:52.0,8.0,
4679,3455,0,It will depend on the numerical stability of the algorithm you are using and how well-conditioned the problem is. Remember that double precision gives you access to smaller numbers as well as larger ones.,2010-10-11 11:18:17.0,229.0,
4680,3462,0,"@chl:> actually, i wonder whether your solution would not ultimately be better. I think you should post it, either for the benefit of bgbg or that a future readers (though it is mentioned in the paper i cited). The main point, in my opinion, is to outline the fact that in this setting (binary dependent, continuous independent) it is quiet satisfactory to look for the outliers on the design space.",2010-10-11 11:43:49.0,603.0,
4681,3462,0,I would be equally happy with this second option added to your own response.,2010-10-11 12:01:08.0,930.0,
4682,3448,0,"Thanks for your detailed answer!  Actually, I do not know how to convert my binary data into genotype data? Do you have any idea about that?",2010-10-11 12:05:59.0,,jacki
4683,3448,0,"@jacki Well, actually it is not very clear to me what's the repeated lines of binary value mean: Does it stand for an haplotype block (1=mutant, 0=ancestral allele) on different chromosomes, simulated for different individuals (each new line starting with //), because in this case the haplotype are of varying size... Is it possible to compute haplotype frequencies easily from your software? Maybe I need to reread some papers and come back to you, but if you have any further information (how do we read the output in terms of haplotype, allele marker, etc.), pls update your original question.",2010-10-11 12:28:49.0,930.0,
4685,3454,0,"@csgellespie: you are totally correct. I will reword this question so as it can be used by future readers. Indeed, the wording was particularly poor.",2010-10-11 14:01:49.0,603.0,
4686,3446,0,"@Whuber:> My question was poorly worded. It probably was due to it being borne out of ignorance: i had read some white papers on use of GPU , (although, unfortunately it turns out, not the R command reference of GPUtools) and could not understand why all the tests were carried out in DP. I will re-phrase the question (and the title).",2010-10-11 14:02:30.0,603.0,
4689,3474,1,"@kwak I'm not sure why changing the title was necessary...the ""homework"" tag should be sufficient.",2010-10-11 14:10:03.0,5.0,
4690,3474,0,@Shane:> ok changed back to the original.,2010-10-11 15:18:48.0,603.0,
4691,3464,1,"@csgillespie: I guess I'm too early in my career to answer that, as I probably do not get asked to review as many papers as someone with more experience then me. I think @whuber answer makes a lot of sense though.",2010-10-11 16:03:25.0,582.0,
4693,3381,0,"@Rob, thanks; I'll try to clarify the question, and split off ""adaptive histograms"" as a separate question.",2010-10-11 16:40:54.0,557.0,
4694,3448,0,"@chl:  I am trying to do programming in R for haplotype frequencies, for number of segregating sites, etc. 
          SNPs
Ind1   0 0 0 0 1
Ind2   1 0 1 0 0
Ind3   0 1 0 1 0
Ind4   1 1 1 0 0

In above data, each column-wise is SNP and row-wise is individual(Ind1, Ind2,...). As I said before it was simulated on whole genome. The position is normalize random variable from uniform distribution [0,1]. 
Thanks for your help!",2010-10-11 17:28:14.0,,jacki
4695,3448,0,"@jacki If I read it correctly, 00001 means 5 markers/alleles with one variant, and you have $n$ such vectors with varying locations for SNP (it was not clear `ms` manual)?",2010-10-11 17:36:18.0,930.0,
4696,3487,0,"I've heard about World Programming System (WPS), http://j.mp/9GjesM. Don't know how comparable it is, though. There was an old GNU project, called Dap (http://j.mp/955DqP), but it never reaches a mature state. In biomedical research, SAS is still considered as THE reference software, although FDA is on her way to (progressively) accept R-based analyses for clinical trials.",2010-10-11 17:57:20.0,930.0,
4697,3487,0,Interesting.  I've never heard of WPS.  I'll check it out - it mentions the ability to read SAS datasets...,2010-10-11 18:01:22.0,1499.0,
4698,3491,0,"Thanks for your answer. Yes, the distribution does represent the scores for the population that should be rejection by my biometric system but are instead authenticated. However, I'm not sure how to choose my threshold in this case, so I need to determine that before I can proceed.",2010-10-11 18:04:16.0,1224.0,
4699,3483,0,excluding bounty and +15 rep for accepted answer :),2010-10-11 18:05:25.0,930.0,
4700,3491,2,"I'd bet that the threshold would be a design parameter set by the client.  Basically it will come down to which is more important for a specific client Type I or Type II error (probably Type II - Falsely accepting someone who should've been denied access).  With a knowledge of what the client wants, you could then state that under a specific threshold, the probability of falsely admitting someone who should be denied access is 1 / 10,000,000 or something.",2010-10-11 18:09:49.0,1499.0,
4701,3490,0,"Is getting certified actually gold?  I've heard lots of mixed reports about the impact of technical certifications in general, but I don't know how that applies to SAS.  Can you expand on that a little bit?",2010-10-11 18:09:59.0,71.0,
4702,3478,1,"Interestingly, most researchers in genetics studies are encouraged or pleased (it depends on the review) to make data available. I also remind of @csgillepsie nice answer about *reproducible research*, http://stats.stackexchange.com/questions/1980/complete-substantive-examples-of-reproducible-research-using-r/1993#1993",2010-10-11 18:10:25.0,930.0,
4703,3491,0,"I could however envision a client who must employ a certain biometric authentication device due to government law or otherwise -- and said individuals might hate when their scanner goes wonky and won't give anyone access -- hence, they might care more about Type I error - not admitting someone who __should be allowed__ (because they probably have six other security measures and this one is only a deterrent for *wandering eyes...*",2010-10-11 18:12:16.0,1499.0,
4704,3490,1,"I've also heard mixed reports -- I taught the SAS class at PSU for 4 semesters, had a bunch of student report that companies were happy that they'd simply had exposure to SAS.  Several students mentioned being drilled on *which* Procs they knew during interviews, but mostly the feeling I got was that employer want a familiarity with SAS and accept that you'll need to learn specific procedures or methods after you've been hired.  Within the programming community, employer will sometimes view Certs as a red flag -- a waste of time, or a book knowledge over practical/experience-oriented info.",2010-10-11 18:20:29.0,1499.0,
4705,3490,1,Here's a link to a certification discussion over on [Not Programming Related](http://programmers.stackexchange.com/questions/44/are-certifications-worth-it) -- also a Stack Exchange website.,2010-10-11 18:21:03.0,1499.0,
4706,3491,0,"So, does the threshold need to be the minimum score that needs to be obtained in order to be authenticated?",2010-10-11 18:53:02.0,1224.0,
4707,3491,0,"I think so, but this is highly dependent on your implementation.",2010-10-11 18:56:44.0,1499.0,
4709,3483,1,@chl of all the people here you should be the least concerned about voting enough!,2010-10-11 20:06:08.0,919.0,
4710,1151,0,I upvoted a long ago but re-reading your answer it remind me that I always liked *Plane Answers to Complex Questions* from Christensen (http://j.mp/atRp9w).,2010-10-11 21:14:48.0,930.0,
4711,3491,1,"@rohanbk Did you look at Signal Detection Theory or ROC curve analysis (http://j.mp/b49wDl in French, http://j.mp/aTjobH in English)? Determining the cut-off is exactly finding the best compromise between sensitivity and 1-specificity, as M. Tibbits said.",2010-10-11 21:23:54.0,930.0,
4712,3455,0,"Not necessarily smaller or larger numbers; remember, we're dealing with floating point.  Rather, it lets you use larger and smaller numbers in relation to each other, while preserving the significant digits.",2010-10-11 21:54:47.0,1539.0,
4713,3491,0,Excellent link @chl.  My brain balked on that one.,2010-10-11 21:57:56.0,1499.0,
4714,3481,0,"Ahh, it's sort of coming back to me from multivariable calculus. I remember doing problems like that. How do I find the radius as a function of the remaining variable? It still seems like I'm going to have some sort of monster integral left over.",2010-10-11 22:13:13.0,1545.0,
4715,3494,0,"Hey Shankar, try the `hopach package` in Bioconductor. It gives you an estimate of the number of clusters in your group. Also, you should try `pvclust package` in R/BioC, it gives you the ""statistically significant"" clusters.",2010-10-11 22:20:25.0,1307.0,
4716,3481,4,"Let the remaining variable be $y$.  Then $x^2 \\le 1-y^2$ describes the region over which you have to integrate.  Evidently the radius equals $\\sqrt{1 - y^2}$, whence the cross-sectional area equals $\\pi (1 - y^2)/2.$  That's a pretty simple formula :-).  (Remember, the theme here is geometry, not calculus...)",2010-10-11 22:29:45.0,919.0,
4717,3481,0,"Oh, right. That crossed my mind, but it seemed too simple. I guess I was determined for it to be complicated. Thanks!",2010-10-11 22:40:12.0,1545.0,
4718,3481,0,I forgot to ask: how does c figure into this?,2010-10-11 22:49:48.0,1545.0,
4719,3448,0,@chl: Now it is clear to you? How we can covert this binary data into genotype data?,2010-10-12 00:42:20.0,,jacki
4720,1151,0,"@chl: cool, definitely going to check it out then.  :)",2010-10-12 02:30:51.0,251.0,
4721,3483,1,@chl: you set a high bar in every way! :) Maybe our first polystats project should be to set up some scripts to maintain and update a set of charts like these: http://meta.stats.stackexchange.com/questions/314/vote-early-vote-often/317#317,2010-10-12 02:44:02.0,251.0,
4722,3490,0,"I suppose my comment was stated with little backing evidence, and in retrospect, cannot be backed fully. I have seen a few jobs in the San Francisco Bay Area that, in the qualifications section, ask that the individual be familiar with using a data analytics package, and add that SAS Certification is recommended or highly recommended. It's very likely they are instead just looking for some way for you to show you are comfortable with SAS and not just putting it on your resume because it is ""one of the most common requirement is experience of SAS"".",2010-10-12 05:52:42.0,1118.0,
4723,3506,0,"I am sorry chl, I think we were almost answering this question together at the same time. Basically, you and me are pointing to the same fact.",2010-10-12 07:52:46.0,1307.0,
4724,3505,0,"Hi @suncoolsu, there are n=6 replicates in each of the 12 treatments. It was set up as an RCBD with three blocks with n=2 reps/treatment/block because there were three trays with for 24 samples each, but there was no block effect so I dropped it from my analysis. Sorry for the omission",2010-10-12 08:21:33.0,1381.0,
4725,3506,0,"@chi The effect of A was expected, while the effects of B and A*B were both main hypotheses. And I am going to show the effect sizes in a figure. Thank you for reminding me not to make the error of accepting Ho.",2010-10-12 08:32:55.0,1381.0,
4726,3505,0,"Hello @David, I will be a little uncomfortable with saying - ""there was no block effect"". Please see my updated reply for your answer.",2010-10-12 08:43:29.0,1307.0,
4727,3481,2,"In my opinion, Whuber's answer deserves to be upvoted for two reasons. First it answers the question asked, second as a model for how we could in the future handle (explicitly stated) homework questions: this type of answers actually contributes to the learning process and could be a better policy with respect to homework question than that adopted at MO/SO.",2010-10-12 08:50:32.0,603.0,
4728,3478,0,"@chl: yes, making data available very much depends on the discipline, and I would love to see more of this in ""mainstream"" psychology - I just can't recall having seen a single instance of a psych paper that actually did give out the data.",2010-10-12 09:42:04.0,1352.0,
4729,3504,4,"In papers, when interaction effects aren't significant I still mention in passing that I looked for them to stop reviewers asking about interaction effects. This is obviously different from ""accepting H_0"" and making a big thing about no effect.",2010-10-12 10:56:00.0,8.0,
4730,3513,0,"@wok Of note, the scikit.learn package also offers efficient implementation in Python for this kind of stuff.",2010-10-12 12:03:50.0,930.0,
4731,3461,2,"@chl I don't like this paper while it made from the SVM learning perspective -- making one repetition of a stochastic algorithm (RF) is just a junk; also appendix 2 shows how bad it may be to apply SVM workflow to RF. Yet I agree that almost always SVM  can be tuned to outperform RF because of the kernel trick (which plain RF does not have, while it doesn't mean it can't have it in general), but with exponentially growing optimization effort.",2010-10-12 12:13:13.0,88.0,
4732,3510,2,+1 Very good point.,2010-10-12 12:18:14.0,88.0,
4733,3515,0,"Ok, let me correct my question. How do you know if a binary variable follows the binomial distribution?",2010-10-12 12:42:15.0,,Roger
4734,3481,0,"@Jarrod: you can use c to determine the normalization constant for the marginal density.  Alternatively, forget c, derive the functional form of the marginal (that is, up to a constant factor), then integrate the marginal to find its normalizing constant.",2010-10-12 12:45:48.0,919.0,
4735,3515,1,"See http://en.wikipedia.org/wiki/Binomial_test . Talking as a moderator now: so can you please make a new, good question? You'll get a better answer and the wiki spirit of this site will be held.",2010-10-12 12:49:40.0,88.0,
4736,3461,0,"@mbq Indeed, this a good point.",2010-10-12 13:03:05.0,930.0,
4737,3481,0,@Kwak: Thanks for pointing out there is a policy concerning homework questions!  I continued this discussion on meta at http://meta.stats.stackexchange.com/q/12/919 .,2010-10-12 13:12:55.0,919.0,
4738,3515,0,Sure. Let me do that.,2010-10-12 13:14:50.0,,Roger
4739,3514,3,Since you have made a new question I think you should accept mbq's answer and link to your new question.,2010-10-12 13:19:58.0,1036.0,
4740,3517,1,(+1) The second point is very interesting.,2010-10-12 13:38:58.0,930.0,
4741,2739,0,"@Henrik Yes, as a base of rule-based classifiers.",2010-10-12 13:41:15.0,88.0,
4742,3514,0,vote to close.  (but I'm below 500r),2010-10-12 13:47:57.0,1499.0,
4743,3508,0,+1 Wooooooowwww  (It had to be 15 characters long...),2010-10-12 13:52:21.0,1499.0,
4744,3496,0,"I suspect that the answer is domain specific (i.e., specific to spam filters). If you can calculate the components P(A|B) etc then you should be able to calculate the simpler P(A) as you stated. Or, perhaps the answer is related to pedagogy so that readers understand the relationship between P(A) and its decomposition in terms of P(A|B), P(B) etc.",2010-10-12 14:06:35.0,,user28
4745,3519,2,R is based on Scheme but is pretty good in pretending to the beginners that it is rather C based.,2010-10-12 14:31:24.0,88.0,
4746,3519,2,"> I will not post this as an answer because there are many people more knowledgeable in these things than me. However, i  think that concerning speed you have to distinguish between R base and the mix good programmer/great packages. A good programmer can leverage some the tools in cran such as multicore, GPUtools&magma (soon but not yet usefull for MCMC), Rcpp,... to make a pretty fast code. I don't think matlab central has anything comparable to offer.",2010-10-12 14:56:49.0,603.0,
4747,3519,0,@kwak -- I probably should've posted my answer as a comment too.  Sorry about that.,2010-10-12 15:02:41.0,1499.0,
4748,3521,0,+1: very practical answer with good explanation.,2010-10-12 15:06:28.0,603.0,
4749,3505,0,"Hi @suncoolsu, Thank you again for your response. re: 1. This is an interesting point, I will look at Venable's reference. re: 2. The blocking and replication within blocks was based on the fact that the trays held 24 samples, so I blocked them in case there was any effect of tray positon or time of measurement. re: 3. Thanks for again for reminding me not to incorrectly interpret my findings, e.g. by saying that there was no block effect. It is a bad habit. re: 5. I should have clarified that by 12 treatments, I meant the 12 permutations of three levels of A and four of B.",2010-10-12 15:28:00.0,1381.0,
4750,3519,2,"MT, that's crazy - your answer is great.  If anything, kwak should make his a proper answer, too.  Let those more knowledgeable vote or answer as they see fit.",2010-10-12 15:33:50.0,71.0,
4752,3523,3,(+1) The exchangeability assumption is also at the heart of permutation tests.,2010-10-12 15:53:46.0,930.0,
4754,3517,1,"@chl: Yes, it occurred to me as I was writing about the CLT that there could be problems with the variances converging to zero, for then the Lindeberg condition could be violated.  It's easy to see why the resulting limit distribution might be non-normal, for if (for example) the chance of observing a 1 approaches zero sufficiently rapidly, then the distribution of the mean might remain strongly skewed and never get close to normal.",2010-10-12 16:00:11.0,919.0,
4755,3521,0,"+1 for C++; while it is quite easy to embed C/C++ to R I often wrap my codes and run them inside R -- then it is nicer to pass parameters, do live visualisation and obviously analyse results without thinking of output file format.",2010-10-12 16:42:55.0,88.0,
4756,3521,0,"well put; MC will eventually require one to move to C/C++. I do not have enough experience in R to comment, but have had a number of headaches using C/C++ with Matlab due to different versions of shared object libraries (under Linux) being pulled in by the Matlab executable than what I want to link with my code.",2010-10-12 16:54:29.0,795.0,
4757,3525,2,"I completely agree @Henrik.  If you're concerned about profiling, Matlab has an **excellent** profiling tool (even back in version 7.1!!).  Rprof on the otherhand leaves a lot to be desired.  I end up profiling by executing each command several times in a for loop and comparing the `system.time` difference amongst different versions.  [Here is an interesting case study](http://jeromyanglim.blogspot.com/2010/02/case-study-in-optimising-code-in-r.html)",2010-10-12 16:57:24.0,1499.0,
4758,3463,0,"@kwak: No, the series are both NOT stationary.",2010-10-12 17:49:19.0,1216.0,
4760,3503,0,"Hello Gaten, Thank you for looking into this. I am still not sure that I understand the logic. The full sample beta.hat is and estimate of the true beta. My sample.mean.beta.hat is an estimate of beta.hat is it not? Is the CLT argument that both beta.hat and sample.mean.beta.hat converge towards each other?",2010-10-12 18:24:59.0,,Joseph Rickert
4761,3507,0,"Hello suncoolsu. Yes, I think that the exchangeability assumption is crucial. Thank you for pointing that out. Do you know of any results on rates of convergence?",2010-10-12 18:29:47.0,,Joseph Rickert
4762,304,2,"A test of normality is usually too severe.  Often it suffices to obtain symmetrically distributed residuals.  (In practice, residuals tend to have strongly peaked distributions, partly as an artifact of estimation I suspect, and therefore will test out as ""significantly"" non-normal no matter how one re-expresses the data.)",2010-10-12 19:02:17.0,919.0,
4763,301,1,"@cgillespie: Concentrations, yes; but age?  That is strange.",2010-10-12 19:02:52.0,919.0,
4764,3526,1,Perhaps [this discussion](http://groups.google.co.bw/group/sci.stat.edu/browse_thread/thread/c247a85a83c5ae00) is relevant as -- it isn't often used because it is very conservative (much like [Scheffe's Method](http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method))?,2010-10-12 19:06:18.0,1499.0,
4765,3530,2,+1 Great answer/summation.  You're on a roll today.,2010-10-12 19:23:57.0,5.0,
4766,3495,0,"This does not give the exact answer, for the following reason: $\\sqrt{\\sum_i x_i} \\ne \\sum_i \\sqrt{x_i}$. You are computing the former, while the OP wants the latter.",2010-10-12 19:45:10.0,795.0,
4767,3512,0,"Thanks. I am reading this and thinking about it. However, this would be a huge modification of the current algorithm.",2010-10-12 19:53:06.0,1351.0,
4768,3513,0,The coordinate descent algorithm is interesting. Thanks. Still thinking about it.,2010-10-12 19:53:46.0,1351.0,
4769,301,0,"@whuber: I suppose it's very data dependent, but the data sets I used, you would see a big difference between a 10 and 18 yr old, but a small difference between a 20 and 28 yr old. Even for young children the difference between a 0-1 yr old isn't the same as the difference between a 1-2.",2010-10-12 20:00:38.0,8.0,
4770,3507,1,"Hello Joseph, as with most (_not in general_) results in classical statistics: parametric bootstrap converges around rates $n^{\\frac{1}{2}}$, where$n$ is the sample size. In your case $n$ corresponds to $M$, as you are taking average of $M$ bootstrap estimates. This is equivalent to the asymptotic normality (or _CLT_). The assumptions and the details of the result can be found in: Hall, P. 1988. Rate of Convergence in Bootstrap Approximations. Annals of Probability.",2010-10-12 20:02:07.0,1307.0,
4771,3505,0,"Hello @David, I understand your point of blocking on trays to remove the variation because of them. This is perfectly justified and you should block if you can. But what I am trying to point out is: as you said, there are 2 replicates for a particular block and treatment level (say: tray 1 had two replicates of first level for trt A) IMHO is _not correct_. If you really want to block for positions as well, probably a more efficient design like **latin square** or **split plot** designs may be the way to go. The choice of specific design depends on your resources, your parameter of intrst. etc.",2010-10-12 20:14:35.0,1307.0,
4772,3481,0,"In case anyone was dying to know if there was a way to solve this with calculus, you can instead recognize that $x = \\sqrt{1-y^2}$ is of the form $u = \\sqrt{a-b^2}$, which has the special integral $\\int udx = \\frac{1}{2}(xu + a^2 arcsin \\frac{x}{a})$, which gives you the same result as wheber's method when integrated from $-\\sqrt{1-y^2}$ to $\\sqrt{1-y^2}$. My professor discussed this in class today, but I prefer wheber's method for obvious reasons.",2010-10-12 20:19:28.0,1545.0,
4773,3507,0,"Addendum: when I say $n^{1/2}$, I mean error goes to zero with that rate $O(n^{-1/2})$.",2010-10-12 20:32:46.0,1307.0,
4774,551,0,Very good summary Prof. Cookson! I like it a lot.,2010-10-12 20:34:57.0,1307.0,
4775,3523,3,"Given the question of the when & why of exchangeability, chl's pointer to permutation tests may merit a few additional words. Permutation tests are a nonparametric technique used when normality and similar assumptions are untenable - instead one uses the much weaker ""null assumption"" of exchangeability, approximates the distribution of a test statistic under this null assumption (by permuting) and looks whether the actually observed test statistic is extreme compared to this null distribution. There is an accessible book by P. Good, ""Permutation, Parametric, and Bootstrap Tests of Hypotheses"".",2010-10-12 20:43:34.0,1352.0,
4776,3527,3,"I like the part about ""My current working practice is to use R to prototype and use C when I need an extra boost of speed."" - it sounds like the job description of my poor unworthy self and the C++ developers in the next office... and I think it really captures basically *any* situation involving R, C/C++ and an issue in statistical computation.",2010-10-12 20:48:37.0,1352.0,
4777,3523,0,"@Stephan I like this book! Still, exchangeability is weaker than independence...",2010-10-12 21:01:42.0,930.0,
4778,3481,0,"@Jarrod: Thank you for the follow-up.  If the purpose of your course is to learn applications of calculus, then your professor's solution has pedagogical merit.  If it's to learn about probability, then seeing multiple solutions--including some relatively simple ones--can help you identify the key *probability* ideas and learn them better.  (The calculus stuff might then just be a distraction...)",2010-10-12 21:36:44.0,919.0,
4779,301,0,"Yes it will be data dependent: your ability to conduct an insightful and effective analysis is the ultimate arbiter of this issue, not my preconceptions.  I was just trying to envision situations where age as an *independent* variable would merit such a strong transformation.  Some strange things will happen with newborns, too ;-).",2010-10-12 21:40:22.0,919.0,
4780,3507,0,Hello suncoolsu. Thank you for the reference. I very much appreciate it. I'll do my homework.,2010-10-12 21:47:05.0,,Joseph Rickert
4782,3526,2,"Surely you mean ""after rejecting the null"" not ""after failing to reject the null""? And it seems there's only one L in 'Marascuilo' (NIST's error, not yours):

Leonard A. Marascuilo. Large-sample multiple comparisons. Psychological Bulletin, 1966; 65(5): 280-290. http://dx.doi.org/10.1037/h0023189.",2010-10-12 21:58:37.0,449.0,
4783,3526,0,@onestop Thanks. I corrected the typo.,2010-10-12 22:39:40.0,,user28
4784,3536,0,Thanks for the pointers to R packages/functions. I will take a look at them.,2010-10-12 22:40:13.0,,user28
4785,3495,0,"I agree that the method is inexact.  However, I disagree with your diagnosis of the inexactness.  Welford's method for calculating variance, which does not even contain a sqrt, has a similar error.  However, as `n` gets large, the `error/n` gets vanishingly small, suprisingly quickly.",2010-10-12 23:06:18.0,179.0,
4786,3495,0,"Welford's method has no sqrt because it is computing the variance, not the standard deviation. By taking the sqrt, it seems like you are estimating the standard deviation, not the mean absolute deviation. am I missing something?",2010-10-12 23:33:53.0,795.0,
4787,3529,0,"+1. Josh, that's a pretty complete reference sample -- thanks!",2010-10-13 00:06:21.0,251.0,
4788,3541,0,"@Shankar, It is not prudent to relate causality and gene expression of microarray data. It is close to saying ""correlation $\\Rightarrow$ causation""",2010-10-13 00:19:06.0,1307.0,
4790,3542,0,+1 Great question; will be interested to see the answers.,2010-10-13 02:00:42.0,5.0,
4791,3542,1,"@Jeromy, I don't know if Andrew Gelman will be happy with the question, but definitely for small tables this question needs to be addressed. +1",2010-10-13 02:04:15.0,1307.0,
4792,3523,0,"Thanks to both of you, a very important component and right on the mark.",2010-10-13 03:11:32.0,1108.0,
4793,3495,0,"@shabbychef Each iteration of Welfords is calculating the contribution of the new datapoint to the absolute deviation, squared.  So I take the square root of each contribution squared to get back to the absolute deviance.  You might note, for example, that I take the square root of the delta before I add it to the deviance sum, rather than afterward as in the case of the standard deviation.",2010-10-13 05:07:59.0,179.0,
4794,3542,2,@suncoolsu I suppose any good resource on table design should talk about the pros and cons of tables versus graphics.,2010-10-13 05:25:13.0,183.0,
4795,3542,1,"@Jeromy Just to point to the `apsrtable` R package which offers an alternative display of Tables, compared to `xtable`, and `reporttools` described in the JSS, http://j.mp/97GXWV",2010-10-13 06:17:35.0,930.0,
4796,3537,0,"I think it would be excellent if you could give more detail (see suncoolsu's answer). Furthermore, with design, do you mean how to analyze the data?",2010-10-13 08:59:28.0,442.0,
4797,3521,0,tibbits: How do you generate RN's when using openMP?,2010-10-13 09:05:34.0,8.0,
4798,3463,0,"Here: http://stats.stackexchange.com/questions/1881/analysis-of-cross-correlation-between-point-processes I was proposing a Monte Carlo approach to determine confidence limits. The idea was to do this for two point processes, but I guess it could be easily adapted for your situation.",2010-10-13 10:53:44.0,582.0,
4800,3538,0,"Hi all,
Thanks for you answers!

1) I want check the effect of treatment(control, stress) with time (week1, week2,...,week12) and with varieties (var1,var2,...,var12).

2) difference b/w replicates is because of source available
											3)Some description of data is given below:

All Citrus plants were grown in pots with same soil and all condition same. Then only' Stress' labelled plants were treated with chemical 'NO3' and other plants 'Control' were untreated.													
 Chlorophyll content (Data given) was recorded at day 1 and then each week for exactly the same plants.",2010-10-13 11:27:00.0,,jacki
4802,3521,0,"Right now, I'm not.  The most expensive parts of my MCMC algorithms are computing several likelihoods so I try to bunch them together best as possible and compute them in parallel.  But all of the setup, RN generation (for proposals), are done on a single cpu core.  For parallel RNGs, I'd start with [DC](http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/DC/dc.html) for the Mersenne Twister -- but I've never personally used it beyond a trivial translation to CUDA for GPUs (more as an exercise really).",2010-10-13 11:35:31.0,1499.0,
4803,3546,3,"Three additional references:  1.  [Beyond Kappa: A review of interrater agreement measures](http://onlinelibrary.wiley.com/doi/10.2307/3315487/abstract) by Mousumi Banerjee, Michelle Capozzoli, Laura McSweeney, & Debajyoti Sinha  2.  [Interrater reliability and agreement of performance ratings: A methodological comparison](http://www.springerlink.com/content/76887531n77808n4/) by John W. Fleenor, Julie B. Fleenor & William F. Grossnickle",2010-10-13 12:28:39.0,1499.0,
4804,3546,0,3. [Statistical methods for assessing measurement error (reliability) in variables relevant to sports medicine.](http://www.ncbi.nlm.nih.gov/pubmed/9820922) by Atkinson G & Nevill AM.  The first reference is specific to ordinal data and discusses other measures beyond kappa for ordinal data.  The second and third are specific to interval data.,2010-10-13 12:30:37.0,1499.0,
4805,3555,2,+1:> faily good way to make it intuitive.,2010-10-13 13:17:08.0,603.0,
4806,3546,0,"(+1) Much Thanks M. Tibbits! I generally provide a lot of references and examples during my lectures in psychometrics, including the first you cited, but I didn't know the two others.",2010-10-13 13:22:58.0,930.0,
4807,3555,3,(+1) Very nice example!,2010-10-13 13:25:22.0,930.0,
4809,2467,0,"Could you confirm that ""classification"" is not meant as ""cluster analysis"" (which is called *classification* in French), that is do you really seek to apply a supervised method following your FA?",2010-10-13 13:59:10.0,930.0,
4810,3544,1,Would you mind explaining why such procedure should be preferred?,2010-10-13 14:20:44.0,930.0,
4811,3538,0,"@Jacki, Your experimental design looks like a special case of Split plot design, but to decide the special case, I need extra details. Can you please represent your design inform of a picture (see my update) ? Currently, I don't see how you use the varieties? Is the difference between varieties of interest or not?",2010-10-13 15:11:30.0,1307.0,
4812,3558,2,"This one is *much* older than xkcd, LOL!  Did you read J.M.'s contribution (August 9)?  Same joke, different animal.",2010-10-13 15:21:18.0,919.0,
4814,3559,2,"Srikant's answer is certainly a good one (and I have nothing to contribute), but you might want to wait a bit longer than 24 minutes to mark a best answer to see if anybody else has any ideas.",2010-10-13 17:05:07.0,71.0,
4815,3495,2,"I see the problem; Welfords obscures the problem with this method: the online estimate of the mean is being used instead of the final estimate of the mean. While Welford's method is exact (up to roundoff) for variance, this method is not. The problem is *not* due to the `sqrt` imprecision. It is because it uses the running mean estimate. To see when this will break, try `xs <- sort(rnorm(n.testitems))`  When I try this with your code (after fixing it to return `a.dev / n`), I get relative errors on the order of 9%-16%. So this method is not permutation invariant, which could cause havoc...",2010-10-13 17:05:59.0,795.0,
4818,3558,0,http://stats.stackexchange.com/questions/1337/statistics-jokes/1436#1436,2010-10-13 17:34:34.0,88.0,
4821,1388,4,I do not have enough reputation to downvote this!,2010-10-13 17:41:41.0,1077.0,
4822,3560,5,"A ""perfect fit"" is so far from being attainable in any realistic logistic regression that it seems unfair to use it as a reference or a standard.",2010-10-13 17:47:46.0,919.0,
4823,3565,6,Linear transformations (like these) never change correlation coefficients.  The point to standardization is to improve the conditioning of the normal matrix.,2010-10-13 17:57:00.0,919.0,
4824,3563,0,"Nice idea.  (It doesn't generalize well to categorical predictors, though.)  I suspect that many other strategies could be interpreted from this point of view, too.  For example, selecting a subset of $k$ of the predictors could be interpreted as approximating a basis for the span of the $k$ largest eigenvectors in a PCA.",2010-10-13 17:59:45.0,919.0,
4825,3565,0,"Yes, I agree. As my last line says: _this is not a remedy_",2010-10-13 18:02:21.0,1307.0,
4826,3565,0,"Sorry I see, I said reduce correlation. Its edited now.",2010-10-13 18:03:00.0,1307.0,
4827,3563,0,"In an explanatory approach, then you have to interpret how your linear combination(s) of the $p$ variables relate to the outcome, and this might sometimes be tricky.",2010-10-13 18:05:59.0,930.0,
4828,3565,0,"Standardizing the variables will not affect the correlations among the independent variables and will not ""reduce the effect of correlation"" in any way that I can think of with respect to this problem.",2010-10-13 18:15:24.0,485.0,
4829,3567,1,"(+1) Now, the problem is that the OP didn't indicate how many variables enter the model, because in case they are numerous it might be better to do both shrinkage and variable selection, through e.g. the *elasticnet* criterion (which is combination of Lasso and Ridge penalties).",2010-10-13 18:22:26.0,930.0,
4830,3565,2,"@Brett, a typical example where standardization helps is _Polynomial Regression_. It is always recommended to standardize the regressors. Standardizing doesn't change the correlation matrix, but makes the var cov matrix (which is now the correl matrix) well behaved (called conditioning by @whuber pointing to the condition number of the matrix, IMHO).",2010-10-13 18:23:23.0,1307.0,
4831,3565,0,"Agreed.  Centering is useful when entering higher order terms, like polynomial or interaction terms.  That doesn't seem to be the case here and will not otherwise help with the problem of correlated predictors.",2010-10-13 18:28:56.0,485.0,
4832,3562,0,"(+1) I was initially thinking of expanding my response (that came just after yours), but definitely your answer is self-sufficient.",2010-10-13 18:31:14.0,930.0,
4833,3560,1,"@whuber True but you could use the standard to compare the relative performance of two competing models. Your points of low R^2 in your answer and its implications are good points but if you *have* (e.g., reviewers demand it etc) to use some form of R^2 then Nagelkerke is preferable.",2010-10-13 18:32:23.0,,user28
4834,3560,1,"@Skridant Yes, still the problem of reviewers that want to see $R^2$ and Bonferroni correction everywhere...",2010-10-13 19:32:18.0,930.0,
4835,3560,0,"@Srikant, @chl: A cynical reading of this thread would suggest just picking the largest R^2 among all those the software reports ;-).",2010-10-13 19:44:32.0,919.0,
4836,3562,0,@chl I wondered why your response wasn't as encyclopedic as usual :-).,2010-10-13 19:45:10.0,919.0,
4837,3560,2,@chl Offering push-back to reviewers/clients is of course necessary but sometimes we have to be pragmatic as well. If readers do not mis-interpret low R^2 as lack of adequate model performance then the issues raised by @whuber will be mitigated to some extent.,2010-10-13 19:46:09.0,,user28
4838,3561,1,"I'm sorry see @Suncoolsu's answer was deleted.  It and the comments that followed clarified a difference between multicollinearity and ill conditioning.  Also, in a comment Suncoolsu pointed out how preliminary standardization can help with polynomial regression.  If it happened to reappear I would vote it up ;-).",2010-10-13 19:49:11.0,919.0,
4840,3573,0,I second this answer... and this really *is* my area (or at least I tell myself it is).,2010-10-13 20:03:36.0,1352.0,
4841,3564,0,"Could you tell us a little more about your problem? Why exactly do you want to use KDE? I happen to be rather active in the area of forecasting & replenishment, and I have never seen anyone use KDE. As onestop below notes, at first glance one would much rather use some kind of time series analysis method.",2010-10-13 20:05:24.0,1352.0,
4842,3573,0,"I've retagged the Q to add 'time-series'. Turns out there's a max of 5 tags, so i took out 'pdf'.",2010-10-13 20:10:29.0,449.0,
4843,1377,9,Was his name Ronald Coase? http://en.wikipedia.org/wiki/Ronald_Coase#Quotes,2010-10-13 20:26:15.0,449.0,
4844,3565,0,+1 for the conversation this one inspired.,2010-10-13 20:33:12.0,919.0,
4845,3563,0,"@chl Good point.  But since the principal components are linear combinations, it's straightforward (although sometimes a bit of a pain) to compose the fitted regression model (=one linear transformation) with the projection onto the components (=another linear transformation) to obtain an interpretable *linear* model involving all the original variables.  This is somewhat akin to orthogonalization techniques.  Note, too, that Srikant's latest proposals (sum or average the regressors) essentially approximate the principal eigenvector yet induce similar explanatory difficulties.",2010-10-13 20:37:38.0,919.0,
4846,3569,1,"Completely agreed, +1.  But the characterization of PCA as a ""mathematical trick"" unfairly disparages it, IMHO.  If you agree (I'm not sure you do) that summing or averaging groups of regressors, as Srikant suggests, would be acceptable, then PCA should be just as acceptable and it usually improves the fit.  Moreover, the principal components can provide insight into which groups of predictors are correlated and how they correlate: that's an excellent tool for the thinking you are advocating.",2010-10-13 20:40:44.0,919.0,
4847,3570,1,Thanks for pointing out that paper; somehow I missed it (and it appeared when I was in the middle of a big logistic regression project!).,2010-10-13 20:44:17.0,919.0,
4848,3561,0,"@Ηλίας : The product is likely to be unstable in many applications.  It can be plagued by many zeros if the individual regressors have some zeros; its absolute value is likely to have strong positive skew, giving rise to some high-leverage points; it might amplify outlying data, especially simultaneous outliers, further adding to their leverage.  It might be rather difficult to interpret, too, especially if the regressors already are re-expressions of the original variables (like logs or roots).",2010-10-13 20:47:57.0,919.0,
4849,3576,0,+1.  Beat me by 22 seconds... :-),2010-10-13 20:51:25.0,919.0,
4850,3576,0,That's a relief.  You've been fast on the draw recently.  :),2010-10-13 20:55:16.0,5.0,
4851,3569,1,"@whuber, I see and agree with your point, and I don't want to disparage PCA, so definitely +1. I just wanted to point out that blindly using PCA without looking at and thinking about the underlying problem (which no one here is advocating) would leave me with a bad feeling...",2010-10-13 21:22:18.0,1352.0,
4852,3571,1,"This makes sense if the regressors are all measured on the same scale. In psychology, various subscales are often measured on different scales (and still correlated), so a weighted sum or average (which is really the same here) would be appropriate. And of course, one could view PCA as providing just this kind of weighting by calculating axes of maximum variance.",2010-10-13 21:24:28.0,1352.0,
4853,3564,0,Another question: how does a Gaussian *distribution* enter into KDE? Are you sure you are not looking at a Gaussian *kernel*?,2010-10-13 21:27:12.0,1352.0,
4854,3573,0,"I suspect you are right. Maybe I should recast my question ""How do I know I have a problem where KDE can help""?",2010-10-13 21:33:43.0,1574.0,
4855,3564,0,"Mostly because I read KDE generalizes histograms, and our current program essentially buckets sales and calculates an average, which seemed like it was related to me.  

I probably meant Gaussian kernel ... apologies, I'm not up on the terminology.",2010-10-13 21:35:06.0,1574.0,
4857,3572,0,My initial idea was to add take into account the pairwise  interaction of the regressors,2010-10-13 22:17:12.0,1077.0,
4858,3538,0,"Thanks for you detailed answer and help!
The design you made is fine. Yes I am also interested to see the differences between varieties.
Do you know how to do in SPSS?",2010-10-13 22:40:26.0,,jacki
4862,3565,0,I deleted it because I didn't want to confuse people with wrong answer. Probably the moderators brought it up again.,2010-10-14 00:14:45.0,1307.0,
4863,3538,0,"@Jacki, you would do this in SPSS using the mixed model options. I am sorry, but this is the end of my knowledge in SPSS (not a regular user :-( ). I can point out some good reference for R or SAS, in case you are interested.",2010-10-14 00:17:12.0,1307.0,
4864,3538,0,"Also, please modify your question above so that we can see the full detail. The comment space is too small for such discussions.",2010-10-14 00:39:47.0,1307.0,
4865,3556,0,"Can you give alittle more context as to what you mean by ""what works"" or the particular goals of your project at hand. I've used them for visualizing spatial point processes but I doubt that is what you had in mind when asking this question.",2010-10-14 01:11:10.0,1036.0,
4867,3582,3,Loess is a variable kernel REGRESSION method. The question asked about variable kernel DENSITY estimation.,2010-10-14 02:04:15.0,159.0,
4868,2103,0,@Srikant: Done.  Thank you for the excellent guidance.,2010-10-14 02:31:45.0,919.0,
4869,3503,0,"@Joseph.  I am not sure I understand your comment.  We just use a slightly different syntax.  I don't know what beta.hat means.  My point was that a greater sample N will give you greater statistical significance (lower standard error, higher t stat, lower p value) on all regression coefficients within a single run.  Meanwhile, the greater number of iterations M will give you greater statistical significance for the Mean of each specific coefficients across all iterations.  They are two different things.",2010-10-14 03:34:22.0,1329.0,
4870,3495,0,"Ah, well spotted :)  I guess the utility of this algorithm will depend on the dataset then...",2010-10-14 04:07:56.0,179.0,
4871,3585,5,"This model assumes the response is an additive function of traveling to each place, which is highly unlikely.  It can still be made to work by including interaction terms.  A full set of all possible interactions might be needed (beyond just the two-way interactions).  (That would be mathematically identical to providing a separate dummy for each possible combination of destinations.)",2010-10-14 04:59:39.0,919.0,
4872,3563,0,"@whuber Yes, I agree with both of your points. I extensively used PLS regression and CCA, so in this case we have to deal with linear combinations on both side (st. a max. covariance or correlation criteria); with a large number of predictors, interpreting the canonical vectors is painful, so we merely look at the most contributing variables. Now, I can imagine that there is not so much predictors so that all of your arguments (@Stephan, @Mike) make sense.",2010-10-14 05:56:18.0,930.0,
4873,3574,3,"The original HL $\\chi^2$ GoF test is not very powerful for it depends on categorizing the continuous predictor scale into an arbitrary number of groups; H & L proposed to consider decile, but obviously it depends on the sample size, and under some circumstances (e.g. IRT models) you often have very few people at one or both end of the scale such that cutoffs are unevenly spaced. See A comparison of goodness-of-fit tests for the logistic regression model, Stat. Med. 1997 16(9):965, http://j.mp/aV2W6I",2010-10-14 06:36:24.0,930.0,
4874,3582,0,"Oops, you're right. Misread the question.",2010-10-14 06:56:25.0,1569.0,
4875,3585,4,Better have a *lot* of data if you use all interactions (15 parameters) rather than just the main effects (4 parameters)...,2010-10-14 06:56:40.0,1352.0,
4876,3587,0,"I'm using the riskratio function from the EpiTool package, manual attached here:
http://bm2.genes.nig.ac.jp/RGM2/R_current/library/epitools/man/riskratio.html

I'm still digesting your answer, thanks again!",2010-10-14 07:01:20.0,588.0,
4877,3589,0,"This does not sound like you have observations (or ""cases"") on which you observe both an X and a Y realization. How do you find out which X is associated to which Y?",2010-10-14 07:01:47.0,1352.0,
4879,3589,1,I suppose I forgot to mention this. X and Y are stock prices. Company X has been public for a much shorter time period than Y. I wanted to tell how correlated the prices of X and Y are. I could definitely get a correlation for the period of time that X and Y both exist. I wanted to know if knowing the stock prices for several extra years of Y that X did not exist yielded me any additional information.,2010-10-14 07:10:00.0,1118.0,
4880,3574,0,"Thanks chi, that's a useful ref, though your j.mp link took me to a BiblioInserm login prompt. Here's a doi-based link:
http://dx.doi.org/10.1002/(SICI)1097-0258(19970515)16:9<965::AID-SIM509>3.0.CO;2-O",2010-10-14 07:20:21.0,449.0,
4881,3558,0,I just noted that I got it via xkcd forum. I didn't mean this is its origin. Thank you for pointing out the deer hunting skit. Also funny. :),2010-10-14 07:25:31.0,144.0,
4882,3586,0,"I think the condition of 5 is for the actual cell count and I see that one of the cell has cell count = 3. Further, the p-value calculated by the ChiSq test is based on pchisq( .. , lower.tail=F), if I understand the code correctly. Therefore, the p-value is based on one sided test, where as the CI is two sided, hence the discrepancy.",2010-10-14 07:25:56.0,1307.0,
4883,3574,0,Sorry for the incorrect link... I seem to remember Frank Harrell's `Design` package features the alternative H&L 1 df test.,2010-10-14 07:31:47.0,930.0,
4884,3586,0,"I'll investigate on the ""one-sided/two-sided"" issue, but for condition of 5, I think this should be referred to the EXPECTED FREQUENCY as from http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test: ""Expected Cell Count – Adequate expected cell counts. Some require 5 or more, and others require 10 or more. A common rule is 5 or more in all cells of a 2-by-2 table, and 5 or more in 80% of cells in larger tables, but no cells with zero expected count. When this assumption is not met, Yates' correction is applied.""",2010-10-14 07:53:58.0,588.0,
4885,3591,0,"actually I have used the small-sample adjustment, and it then CI and Chi-Square fits perfectly, but how can I justify that it is a ""small sample""?",2010-10-14 07:55:09.0,588.0,
4886,3534,0,Thanks for the Windows warning.,2010-10-14 08:10:10.0,8.0,
4887,3535,0,Geyer's slides are really helpful. Thanks.,2010-10-14 08:10:51.0,8.0,
4888,3593,0,(+1) Both responses provide a handy summary of possible discrepancies between all three statistics.,2010-10-14 08:11:47.0,930.0,
4889,3589,1,"@Christopher I'd recommend that you update your question to reflect your above comment. Also, for correlation to be meaningful, more than just equal dimensions are required; the actual measurements have to come from the same cases, which in your case is presumably the same time points.",2010-10-14 08:13:20.0,183.0,
4890,3591,2,"The asymptotics (and power) depends much more on the total number of positives than the overall total (strictly, the total number of the whichever outcome is rarer). Your total number of positives is 14. That's pretty small. 'Expected value 5 or more in all cells' is only a rule of thumb. 

I'm not sure what the small sample adjustment used by epitools is doing exactly (can't find any explanation in the documentation), but it's certainly something more sophisticated than Yates' correction.",2010-10-14 08:22:41.0,449.0,
4892,2697,2,What about the eigenvectors & eigenvalues?,2010-10-14 08:29:12.0,1077.0,
4893,3492,1,"I think it's wrong to say that R would not be helpfull in most corporate settings.  Knowing R won't really help you learn SAS if that's what you're stuck on, but that's like saying you need a hammer when faced with a box of screws.....",2010-10-14 08:34:30.0,114.0,
4894,3589,1,I second Jeromy's comment on updating the question...,2010-10-14 08:53:30.0,1352.0,
4895,1441,0,"Since we cannot migrate, closed.",2010-10-14 09:00:32.0,88.0,
4896,3589,0,Another question: you mention that X and Y have the same number of columns. Would that be one each? Or do you have multiple series for both X and Y (prices at different stock exchanges or some such)?,2010-10-14 09:00:46.0,1352.0,
4897,3565,0,"@suncoolsu: yes, standardization helps in polynomial regression - but I would go the whole way and transform the data into appropriate polynomial basis functions. Numerical people have been thinking a lot about stuff like this.",2010-10-14 09:04:30.0,1352.0,
4899,1462,3,Consider accepting some answer.,2010-10-14 09:10:46.0,88.0,
4900,3591,0,"@lokheart In fact, the ""expected counts greater than 5"" has been shown to be somewhat too stringent, at least in the case of the Pearson $\\chi^2$ test, see Chi-squared and Fisher-Irwin tests of two-by-two tables with small sample recommendations, Stat. Med. 2007 26:3661, and Campbell's webpage, http://bit.ly/dbPfOO.",2010-10-14 09:18:44.0,930.0,
4901,2467,0,Yes i think to cluster analysis rather than classifcation.,2010-10-14 09:55:37.0,1154.0,
4902,3591,0,"Thanks for the ref chl. Wald CIs have worse finite-sample properties than the Pearson chi-square test though, so it may well not be stringent enough when it comes to appropriateness of Wald CIs.",2010-10-14 10:12:59.0,449.0,
4903,3595,5,"Can you pls be a bit more specific? What do you actually want to data mine and how do you plan to do it? I have used R to analyze similar size records as yours, and it wasn't a bad experience at all.",2010-10-14 10:31:53.0,1307.0,
4906,3585,0,"@whuber and @Stephen, Thanks for the responses, and I agree completely with each of you. I personally would be ok with the main effects dummy variable approach if multiple responses weren't all that common, which may not be a tenable assumption given the original posters concerns. I would maybe propose other designs if the original poster was interested in the risk of travelling to A vs B (such as some type of matching procedure). And I agree additive risk does not make sense except if some selection bias is occurring.",2010-10-14 12:31:35.0,1036.0,
4907,3566,0,"could you elaborate on the distinction between ""goodness of fit"" and strength of association or predictive ability?",2010-10-14 13:12:04.0,1036.0,
4908,3492,0,"Let me rephrase: I don't think R will help anyone get a SAS programming position in most corporate settings, primarily because I doubt most people in corporate SAS shops even know what R is.",2010-10-14 13:20:59.0,71.0,
4909,3511,0,Still could not get a better convergence by adapting IRLS. :'(,2010-10-14 13:29:45.0,1351.0,
4910,3594,0,"For basic financial time series references, you can see my answer here: http://stats.stackexchange.com/questions/328/resources-for-learning-about-the-statistical-analysis-of-financial-data/329#329.  The Tsay text is one of the most popular.",2010-10-14 13:34:08.0,5.0,
4911,3566,0,"@Andy Thanks for pointing that. I realize afterwards that my first sentence does not sound well indeed. I'll update my answer, pls let me know if this ok with you.",2010-10-14 14:00:07.0,930.0,
4912,3605,0,Clarification: I didn't mention GARCH to deal with the missing data problem (which of course would not make sense) - but to improve on a simple calculation of correlation between the time series at times where both exist.,2010-10-14 14:12:55.0,1352.0,
4913,2095,0,"And what if x=12 and y=1/2? Then your answer will have  F(12, 1/2)=12+1/2-1=23/2.",2010-10-14 14:15:44.0,247.0,
4914,3599,0,"> *in that values of B above a threshold will change C* Do you know that threeshold in advance or does it need to be estimated as well ? Also, when you write B*t do you actually mean $B_t$ ?",2010-10-14 14:20:12.0,603.0,
4915,3603,0,"How did you deduce that B and C are correlated?  They might be, but this does not seem to be necessarily the case.  In particular, if B rarely exceeds that threshold, B and C might be approximately independent.",2010-10-14 14:23:22.0,919.0,
4916,3605,0,@Stephan: OK.  I mentioned it mainly to show I wasn't ignoring you!,2010-10-14 14:24:38.0,919.0,
4917,3603,0,"@whuber There is no issue at all if B rarely exceeds the threshold. We may as well assume that they are independent and use standard logistic models. I was implicitly assuming that there is some correlation between B and C given that there are threshold effects, the fact that changes in C impact B and that B and C are continuous variables.",2010-10-14 14:30:54.0,,user28
4918,3588,0,"Thanks Kwak. ""... for Gaussian distributed random variables""; would you know of newer work for ""clumpy"" distributions ?",2010-10-14 14:38:07.0,557.0,
4919,2095,0,@PeterR good point! Fixed the error.,2010-10-14 14:44:31.0,,user28
4920,3505,0,"Hi @suncoolsu. Your points are good but, as the experiment is complete, the design is fixed.",2010-10-14 14:46:21.0,1381.0,
4921,3603,0,"@Srikant You're right.  The concern lies with intermediate situations.  In those cases the correlation is probably not great enough to warrant special measures to deal with correlated regressors.  Moreover, although we speak of ""correlation"" the relationship likely is not linear, so a lot of care is needed in dealing with it.",2010-10-14 14:48:29.0,919.0,
4922,3505,0,"Re the first point (1) - I read Venables' article that you referenced about main effects being an artifact of experimental design (the levels chosen of each factor) when interactions are present (p. 13), but I believe my study is one of the 'rare and special' exceptions, because the main effect of A on Y is very large, i.e., Y is an order of magnitude different at each level of A, while any effect of B would have been on the order of a 10-50% change or so; of course the point is moot since the interaction effect was not significant.",2010-10-14 14:54:22.0,1381.0,
4923,3505,0,"""Three solid gold (significance) stars on the main effects will do very nicely, thank you, and if there are a few little stars here and there on the interactions, so much the better!"" -Venables 2000 [N.B. said in jest, but this attitude is so pervasive in research, peer review, and graduate training is so great I thought it would be edifying to include it here.",2010-10-14 15:05:54.0,1381.0,
4924,3597,0,@chl: Have you yet found an effective parallel computing solution for 64-bit R?  When I last looked (late this summer) the only non-commercial ones appeared to work only in 32-bit R.,2010-10-14 15:07:42.0,919.0,
4925,3582,0,"@Rob, excuse my naive questions: if varying kernel width is (sometimes) good for local regression / Kernel smoothing, why is it bad for density estimation ? Isn't density estimation a case of f() estimation for f() == density() ?",2010-10-14 15:13:04.0,557.0,
4926,3582,0,"@Hong Ooi, how many points in what Ndim have you used ? Thanks",2010-10-14 15:15:06.0,557.0,
4927,3589,0,"Stephan: Thanks for editing my question to reflect the new information. I actually have a few columns, because I have date, opening price, highest price of the day, lowest price, and closing value.",2010-10-14 15:28:23.0,1118.0,
4928,3605,0,"Thank you, whuber. This is in line with what I was looking for. I don't think the backcasting will be of much use (or feasibility) to add a couple extra weeks of X when the mutual time frame between X and Y is about 16 years already.",2010-10-14 15:36:47.0,1118.0,
4929,3503,0,"@Joseph, using your language.  I am not sure that the CLT argument suggests that both beta.hat and sample.mean.beta.hat will converge towards each other.  But, that their respective distributions of outcome (defined by their standard error around the mean) will be normally distributed.  I think the two beta.hat(s) will converge towards each other simply because they will each become more firmed up or statistically significant as you use greater N and greater M.",2010-10-14 15:49:41.0,1329.0,
4930,730,8,"And this is an actual quote, as opposed to something ""attributed to"" Box. It appears, e.g., in Box & Draper (1987), *Empirical model-building and response surfaces*, Wiley, on page 424. Yes, I did go and look it up before using it in a paper.",2010-10-14 15:53:20.0,1352.0,
4933,3605,1,"@Christopher: !! With 16 years (of daily closings?) you have enough data not only to find a correlation, but also to explore how it has been changing over time.  (This I believe is the spirit of @Stephan Kolassa's reply.)",2010-10-14 16:08:41.0,919.0,
4934,3495,0,"@fmark nevertheless, your method seems to work reasonably well when the data are i.i.d. I suspect, however, that using Welford's trick somewhat obscures the approach, and that it is unnecessary for this approximation. The simplified version would then have `a.dev <- a.dev + abs(x - mean)` as the update.",2010-10-14 16:41:04.0,795.0,
4935,3597,1,"@whuber Nope. I had to switch to 64 bits last year to manage large genetic data sets, but the statistical models we used do not call for parallelization (as far as I know). I thought there was an OpenMP binding for R but did not investigate this further. I know Revolution Analytics have made effort in this sense (http://j.mp/d7dFb5), but still in 32 bits (this is probably what you referred to). I found R/parallel (http://www.rparallel.org/) in the meantime, but I don't know how reliable/mature it is.",2010-10-14 16:47:50.0,930.0,
4936,555,18,"ANOVA can be seen as ""syntactic sugar"" for a special subgroup of linear regression models. ANOVA is regularly used by researchers who are not statisticians by training. They are now ""institutionalized"" and its hard to convert them back to using the more general representation ;-)",2010-10-14 16:52:51.0,1307.0,
4937,3560,0,"@Skridant There are alternative measures of prediction performance that makes more sense or are more intuitive (e.g. ROC area, Somers D) and that can be reported together with pseudo $R^2$.",2010-10-14 16:55:48.0,930.0,
4938,3597,0,@chl I tried them all but couldn't get any of them to work.,2010-10-14 17:53:29.0,919.0,
4939,825,0,"@chl: Thanks for bumping this up.  In fact, I checked out all the non-commercial references from this thread shortly after it appeared but couldn't find anything that works on Win 7 x64.",2010-10-14 18:07:11.0,919.0,
4940,3588,0,@Denis:> 'Clumpy'=?concentrated=?with narrower tails than the gaussian?,2010-10-14 18:28:07.0,603.0,
4942,3607,0,That is quite analogous to what I´m looking into. I´ll look into their work. Thx for the input.,2010-10-14 19:05:22.0,1291.0,
4943,3599,0,Unfortunately we do not know the threshold and the threshold itself might not be constant over time.,2010-10-14 19:09:03.0,1291.0,
4944,3612,0,@chi: Thanks a lot. I'll look the papers. Would you please comment the first question? Is hazard variable always time?,2010-10-14 19:12:54.0,1586.0,
4945,3615,0,"@Srikant Excellent answer, but does that function resolve to something arithmetic (ie: not recursive)?",2010-10-14 20:06:46.0,1456.0,
4946,3615,0,"@C. Ross Unfortunately I do not think so. But, I suspect that the recursion should not be that hard as long as are dealing with reasonably small n and small s. You could just build-up a lookup table and use that repeatedly as needed.",2010-10-14 20:10:30.0,,user28
4947,3616,0,"What do you mean by ""different""?",2010-10-14 20:21:50.0,5.0,
4948,3616,0,"What do you mean with ""5-10 data points *per data set*""?",2010-10-14 20:25:09.0,1352.0,
4949,3612,0,"@yuk Not necessarily, as suggested by @whuber. I have in mind another application of Cox regression dealing with the treatment of systematic pattern of missing responses in educational testing, as it arises when a student has not enough time to complete the test (missing responses might then be considered as right-censored) -- in this case, this is item ordering that is considered as the time scale. I'll look at the original paper (although I think this was also the subject of a PhD).",2010-10-14 20:43:01.0,930.0,
4950,3582,0,@Denis. Great question. Can you please add it as a proper question on the site and we'll see what answers people can come up with.,2010-10-14 20:58:14.0,159.0,
4951,3605,0,I agree. Using techniques to figure out what values X would've taken prior to its IPO seems prone to error. I might also question the relevance of data that's 16 years old to predict modern trends.,2010-10-14 22:20:43.0,1118.0,
4952,3618,0,Will that mathematica code work with wolfram alpha?,2010-10-14 23:56:42.0,,user28
4953,3616,0,"I think he has a collection of several time series, each one with 5-10 observations.",2010-10-15 01:07:37.0,159.0,
4954,3613,0,"+1, thanks for pointing out the NADA package.  I noticed it makes it easier to handle left-censored data through the survival package -- is left-censored a common scenario with environmental data?",2010-10-15 02:11:07.0,251.0,
4955,3612,0,"+1.  There are other papers, but I'm not sure they're necessarily better; I think Chalise does a pretty good job summing up the situation.",2010-10-15 02:13:50.0,251.0,
4958,3613,0,"@whuber: Thank you for the comment, NADA package looks very interesting.",2010-10-15 05:23:24.0,1586.0,
4959,3613,0,@Andy: Thanks for the links. I think its worth to be an answer. I'd upvote.,2010-10-15 05:25:07.0,1586.0,
4960,3619,0,"Do you have success with it? If yes, for what kind of application?",2010-10-15 05:50:47.0,930.0,
4961,3588,0,"I'm no expert, but like ""data set clumpiness"" in the paper Lang et al., ""Insights on fast Kernel Density Estimation algorithms"", 2004, 8p",2010-10-15 10:15:12.0,557.0,
4962,3582,0,"@Rob, which part, can you help me formulate ? along the lines ""Is adaptive kernel width good for both kernel smoothing and KDE ?""",2010-10-15 10:15:36.0,557.0,
4964,3629,0,"Ok, now it's clear. Many thanks for your prompt reply.",2010-10-15 11:07:23.0,1219.0,
4965,3613,0,"@Yuk, per your request I made my comment into an answer, and @whuber thanks for your example.",2010-10-15 12:03:07.0,1036.0,
4967,3592,0,"Just as a note from what I can gather time varying in this context does not make sense, unless the criteria used to classify individuals in either before or after is time varying itself (in which case I would say you want those characteristics themselves as the covariates not the classification).",2010-10-15 12:50:18.0,1036.0,
4968,3566,0,Thanks for the update and it does clarify the distinction.,2010-10-15 13:24:09.0,1036.0,
4969,3613,0,"@ars: Yes, left censoring is characteristic of environmental data (and is a key concern of chemometrics in general).  It's a tricky and interesting problem.  Among the reasons are (1) the censoring limits are themselves determined by statistical estimates (through a calibration process), (2) censoring can occur in multiple ways--as limits of detection, limits of quantification, or ""reporting limits"", (3) the thresholds often vary in response to covariates (""matrix interferences"") that can be strongly correlated with the original censored values, (4) data often are lognormally distributed.",2010-10-15 14:11:52.0,919.0,
4970,3618,0,"@Srikant I didn't even try it because usually alpha does not process Mathematica code correctly except for simple expressions.  However, see what happens when you type ""Expand[(x + x^2 + x^3 + x^4 + x^5 + x^6)^3]"" !  ( http://www.wolframalpha.com/ )",2010-10-15 14:15:17.0,919.0,
4971,3632,0,"Did you really mean to include ""logistic"" in ""use logistic regression to estimate x""?",2010-10-15 14:18:30.0,919.0,
4972,3626,1,"+1 for noting the nonlinearity.  But your use of the phase ""expected proportion"" seems to assume the data are a random sample of the distribution over which one is averaging and that's often not the case.",2010-10-15 14:22:41.0,919.0,
4973,3623,0,"Over what probability distribution do you intend to form an average, David?",2010-10-15 14:23:52.0,919.0,
4974,3618,1,That works. I tried your earlier version but could not make any sense of the output.,2010-10-15 14:24:16.0,,user28
4975,3632,0,"I think a bit more context (e.g., other variables, what kind of data you have, dependent and independent variables etc) would help someone to give a useful answer.",2010-10-15 14:28:43.0,,user28
4976,3619,0,"Yes, RHIPE is great. Some of my friends use it to analyze internet traffic data. One of their aims is to model break-in attempts. Data is huge in such cases, petabytes is common!",2010-10-15 14:37:59.0,1307.0,
4977,3592,0,"@ Andy W: yes, you're right. Maybe this was why I was uncomfortable with the idea of time varying covariates.",2010-10-15 14:42:27.0,1573.0,
4978,3581,0,Thanks for the links. I actually requested your book from my local university. :),2010-10-15 14:43:19.0,1574.0,
4979,3604,0,"onestop, that's a good idea! Definitively valid with a clear conclusion to be drawn from the results. I don't have the final data until now, but hopefully I will have enough fluctuation between classes. If not, I will have a very small sample size in groups (c) and (d) and inference will be difficult. But that's the best strategy so far... :)",2010-10-15 14:47:09.0,1573.0,
4980,3616,0,"I still think that this question is nearly impossible to answer without understanding what ""different"" means...",2010-10-15 15:30:42.0,5.0,
4981,3383,0,What sort of variance correction were you thinking about?  Any error in estimate as a consequence of using the sample mean will result in increased variance in the difference scores.  So isn't the variance already adjusted for?  If anything it is going to be high.,2010-10-15 16:05:43.0,196.0,
4982,3636,1,"What are trying to do with the data, Robert?  How do you intend to interpret the standard deviation?",2010-10-15 16:12:47.0,919.0,
4984,3641,0,What's wrong with the moving average?,2010-10-15 16:55:19.0,,user28
4985,3641,0,"What's a ""period""?  You can always add variables into your model (e.g. to prevent it going to unrealistic values).",2010-10-15 16:56:25.0,5.0,
4986,3641,0,@Shane - each period is 10 years. Age will be the only variable i have for those years,2010-10-15 16:57:40.0,59.0,
4987,3641,0,"I understand that age is the only variable that you will have in your data, but you can add additional factors based on priori information (e.g. that population can never be negative).  This is especially true if you model this using a Bayesian approach.",2010-10-15 16:59:47.0,5.0,
4988,3641,0,"@Shane - I'm not that advanced yet as to do Bayesian analysis on the data set, but I'm a fast learner :)",2010-10-15 17:01:48.0,59.0,
4989,3641,0,@dassouki: Suppose that an average adult (say between 25 - 40 yrs) gives birth to 2 kids (something you can get from census). Then you can extrapolate using some assumptions: (a) Percentage of adults who have kids (say 60%) etc. Then you can perform a rolling lagged forecast using the number of kids born in 1930s who would be adults in 1960s etc Does that make sense?,2010-10-15 17:12:13.0,,user28
4990,3641,0,@Sikrant Vadali - Changed image,2010-10-15 17:25:44.0,59.0,
4991,3641,0,@Shane - changed Image,2010-10-15 17:26:20.0,59.0,
4992,3643,0,a. I need to estimate the variance and the variance of the Cauchy distribution is not defined.,2010-10-15 17:26:44.0,1381.0,
4993,3643,0,"b. If I understand your second point, yes, I could assume that y-1 ~ N(mu, sigma), but I still need to calculate mu and sigma from the summary statistics given for y; also, I've chosen not to consider distributions with values < 0 for variables only defined > 0 (even though in many of the cases p(X<0 | X~N(mu,s)) -> 0 )",2010-10-15 17:32:32.0,1381.0,
4994,3640,0,should I post the Update question about calculating the variance on random draws from the Cauchy as a separate question?,2010-10-15 17:39:48.0,1381.0,
4995,3644,0,"Thanks for the reference, that is where I found the Haaya 1975 reference and the equations in my question, although I'd appreciate reassurance that the equations are appropriate for my problem.",2010-10-15 17:43:55.0,1381.0,
4996,3636,0,Is the data normally distributed and i.i.d.?,2010-10-15 17:52:45.0,5.0,
4997,3643,0,Doesn't the Cauchy apply for zero mean normals?,2010-10-15 17:57:04.0,251.0,
4998,3644,0,"Taking a quick look at Haaya, it seems that they're concerned with obtaining a Normal approximation for the ratio and use simulations to determine when that applies (using the coefficient of variation, cv).  Does the cv in your case meet the criteria?  If so, the approximations apply.",2010-10-15 17:58:56.0,251.0,
4999,3613,0,"@whuber: Thanks, I usually encounter right censoring or left truncation and rarely anything else, so it's interesting to hear about other domains.",2010-10-15 18:02:18.0,251.0,
5000,3643,0,@ars You are correct. The cauchy then may be of limited use.,2010-10-15 18:15:34.0,,user28
5001,3643,0,"Ars: Yes, I believe the Cauchy result requires zero means. But that still means that at least in that special case, the variance that David is trying to estimate DOES NOT EXIST.",2010-10-15 18:15:35.0,319.0,
5002,3643,0,@David: Simply invert y and compute the sample mean and sample standard deviation and use those as estimates of mu and sigma. A normal may approximate y^-1 well if sigma is relatively small.,2010-10-15 18:17:24.0,,user28
5003,3643,0,"@John: true, good point; I missed David's first comment.",2010-10-15 18:25:26.0,251.0,
5005,3634,1,What is the reason for a non-parametric approach?,2010-10-15 19:01:32.0,930.0,
5006,3356,0,+1.  Didn't see all this earlier - Thanks for noting the issue as well as the crash/mini course on stata notation.  I took your first comment to imply the interpretation was mistaken and answered in a very general sense.  I'm glad you were more persistent and that kwak figured it out.,2010-10-15 19:14:58.0,251.0,
5007,3336,0,"+1 mystery solved, nicely done. :)",2010-10-15 19:15:43.0,251.0,
5008,3644,1,@David: use Marsaglia 1965 instead as updated in the answer.,2010-10-15 19:18:47.0,251.0,
5009,3638,0,"I seem to remember that Median Polish can be used for a two-way layout; how does it extend to 5 factors, including nesting?",2010-10-15 19:19:14.0,930.0,
5010,3643,0,"@Srikant I can't compute the sample standard deviation since I don't have the raw data... although I could do the calculation on simulated data sets, and take the average of these simulations.",2010-10-15 19:27:39.0,1381.0,
5011,3616,0,My applogies for the poorly worded question.  By different I mean whether over the course of the time series (rather than at individual points) there is a difference between two treatment groups.  There would be inter-subject variation (which i guess would need to be accounted for) as well as inter-group variation (which is what I am interested in).,2010-10-15 19:32:41.0,1327.0,
5012,3646,2,"It's unclear how ""regression"" could be invoked in a *univariate* dataset.  Also, exactly what distinction are you suggesting between the two bulleted methods?",2010-10-15 19:35:50.0,919.0,
5013,3638,1,@chl Median polish can work with as many factors as you might care to handle.  Tukey does some three-way examples (by hand!).,2010-10-15 19:37:15.0,919.0,
5014,3646,0,"I accept that the question might make very little sense - I admit I'm confused! In terms of regression, I imagine that you would input the formula for the normal distribution into some application that can handle non-linear regression, with the mean and variance being parameters of that formula. You would then let the application attempt to derive the values for those parameters that maximise the fit of your data to the formula. So I see that as a completely different approach than just using the sample mean/variance as estimates.",2010-10-15 19:42:59.0,1598.0,
5015,3641,1,@Srikant: more detailed data of birth rates (by women grouped into five year age ranges) are readily available for exactly the purpose you propose.  It's best to obtain these data for the particular state in question rather than using nationwide averages.,2010-10-15 19:44:52.0,919.0,
5016,3647,0,"This is not a US state forecast. I can get the gender, race, income, birth/death data, for the last 10 years only, would that be enough?",2010-10-15 19:48:50.0,59.0,
5017,3646,0,"if you have no predictor variables, there would be no way to fit a non-linear model, or any model other than 'data has a mean and variance'",2010-10-15 19:53:16.0,1381.0,
5018,3647,0,"@dassouki: sorry, I saw the ""statewide"" on the graphic's title and presumed (incorrectly) that it referred to a US state.  The birth/death data for the last 10 years would be excellent, because it will be a reasonably good predictor of birth/death rates into the near future.",2010-10-15 19:58:14.0,919.0,
5019,3648,0,"Thanks David, I suppose I was thinking of a more elaborate non-linear model, e.g. Y = (whatever it would be if the data is in normally distributed, including things like pi and the two parameters of interest, the mean and variance) + epsilon ? Sorry again if this is meaningless!",2010-10-15 20:01:04.0,1598.0,
5020,3648,1,"Nice example.  It might be helpful to clarify what that regression model is.  You have an intercept only regression--so you have no predictor variables.  This is precisely equivalent to a one-sample t-test for difference from 0. More simply in R: x<-rnorm(100); t.test(x); summary(lm(x~1)).  P values, SEs, estimates, intervals, etc. will all be the same.",2010-10-15 20:01:18.0,485.0,
5021,3646,1,"@Bio.X2Y: one of the points your respondents are making is that one ""fits...a normal model"" to the data by estimating the mean and standard deviation.  These become the mean and sd of the fitted normal distribution.  The other point they are making is that ""regression"" in your case means performing a least-squares fit of a constant to the data (""no covariates"") and that's exactly the same as using the usual mean and sample standard deviation estimates.  So the short answer is, both approaches are reasonable and they're the same.",2010-10-15 20:02:51.0,919.0,
5022,3648,1,I think there's a typo and you invert `se` and `sd`; BTW your `mse` (I assume it is mean square error) is actually the SS.,2010-10-15 20:03:24.0,930.0,
5023,3646,0,"OK, am I right in thinking that the model you're suggesting is a linear one with no predictors, where the error is normally distributed? Isn't that different from a model where you use two predictors for mean and variance, regardless of the error?",2010-10-15 20:09:13.0,1598.0,
5024,3646,0,"@Bio.X2Y I think you make a confusion between (a) estimating the mean and variance from an observed sample (which is strictly an univariate problem as @whuber said), while being happy with an underlying gaussian assumption, and (b) check how well your distribution fit a theoretical distribution, with unknown mean and variance. Neither of these cases call for a modeling approach. Maybe @David can update his response for (b), otherwise the first one was already suggested to you: just use the arithmetic mean (which is an unbiased estimator) and SD (I let others discuss the denominator issue).",2010-10-15 20:11:50.0,930.0,
5025,3646,0,"thanks chl. Yes, I think I understand the (a) option (and the n-1 vs n in the denominator), and my confusion lies with the (b) option. I suppose I don't understand why (b) *doesn't* call for a modeling approach - surely since the theoretical distribution has an unknown mean and variance, these have to be 'estimated' before a fit can be established. So the output to (b) would be something like ""mean =1, sd=2, r^2=0.94""? Thanks again",2010-10-15 20:19:53.0,1598.0,
5026,3648,0,"@ chi, Thanks for pointing out my error - I think its fixed",2010-10-15 20:21:03.0,1381.0,
5027,3644,0,Thanks for the reference @ars,2010-10-15 20:23:35.0,1381.0,
5028,3646,0,"@Bio.X2Y No, checking how an empirical distribution departs from an hypothetical/theoretical distribution is not the same than modeling a relationship between an outcome and a potential explanatory variable, or simply fit a regression with only an intercept. In the latest case, it is even better to simply use a Quantile-Quantile plot (observed vs. gaussian) to check if your data follows an expected normal distribution (as is the assumed distribution of the residuals in your LM).",2010-10-15 20:27:11.0,930.0,
5029,3646,0,"ok, thanks again chl, looks like I have a lot more background reading to do!",2010-10-15 20:28:50.0,1598.0,
5030,3651,0,I could concede distributional assumptions for all of the variables. I will update my question.,2010-10-15 20:34:14.0,1036.0,
5031,3651,0,"@Skrikant For the 1st part: Wouldn't it be simply an hypergeometric distribution (and at the limitng case, a binomial)?",2010-10-15 20:39:29.0,930.0,
5032,3651,0,@chl True. As the total size of the urn is fixed. Answering a question too fast is not without its perils. :-),2010-10-15 20:44:05.0,,user28
5034,3655,0,"This approach would work if we assume that our CI are of the form 
$\\hat{\\beta}\\pm Z_{\\alpha}SE$. Unfortunately, sometimes asymmetric CI may be more sensible, for example the CI for a binomial proportion when it's close to 0. In that case pooling the SE like this may not help.",2010-10-15 20:52:17.0,1600.0,
5035,1642,0,@Thomas Andrew Gelman discussed type I and II errors before introducing S and M errors. I think this response is a valid and interesting one (wtr. other well-founded answers) since it allows to go beyond the traditional decision theory framework. I've upvoted this response.,2010-10-15 20:56:49.0,930.0,
5038,3655,0,@user1600 Good point.,2010-10-15 22:14:37.0,,user28
5039,3651,0,"@Srikant: ""Any"" deviation?",2010-10-15 22:58:50.0,919.0,
5040,3651,0,@whuber I meant some sort of chi-squared analysis implicitly. If you see the revisions you will see that I started off the answer with that idea. Would it help if I mention that explicitly?,2010-10-15 23:02:18.0,,user28
5041,3651,0,"@Srikant: I think it would help, given the elementary nature of the question.  If you like, add something about alternative tests, too.  For example, I suspect a one-sided KS test might be more powerful for the alternative suggested by the OP (""higher frequencies of purple balls are disproportionately drawn"").",2010-10-15 23:21:14.0,919.0,
5042,3651,1,@whuber Reg chi-square tests: Done. I feel that any mention of KS test and alternatives to chi-square should be added to your answer instead of mine as the idea is yours.,2010-10-16 00:15:09.0,,user28
5043,3657,0,Thank you so much for your response!,2010-10-16 02:37:15.0,834.0,
5044,3643,0,"The variance is infinite for any normal distribution in the denominator, not just those with zero means. Similarly, the mean is undefined for any normal distribution in the denominator.",2010-10-16 03:53:38.0,159.0,
5045,3582,0,"@Denis. I meant the question you already asked, viz., ""If variable kernel widths are often good for kernel regression, why are they generally not good for kernel density estimation?"" I would post it myself, but then you would miss out on the rep points.",2010-10-16 03:56:40.0,159.0,
5046,3632,1,"re: ""use logistic regression to estimate x"".  Logistic regression is not applicable to estimating income (x).  It is for binary dependent variables only.",2010-10-16 04:58:01.0,919.0,
5047,825,0,"whuber, the solution I present works with win 7 and is non commercial (read the post I linked to for details).  It is bundled with a commercial environment but it can be separated from it (as my post shows how).  And the code itself is GPL...",2010-10-16 08:44:26.0,253.0,
5048,3596,0,"beautiful link, thanks Jeromy.",2010-10-16 08:45:48.0,253.0,
5049,2952,0,(My pleasure chl - thank you :) ),2010-10-16 08:47:09.0,253.0,
5050,3660,0,Great... I wasn't aware of that!,2010-10-16 08:47:16.0,930.0,
5051,3272,0,"Wonderful, Wonderful(!), answer!  Thank you Drury, I'll go through it a few more times to see what I can introduce to my teachings.",2010-10-16 08:55:07.0,253.0,
5052,3664,1,"(+1) Thanks for mentioning the BA plot. You might be interested in an earlier response I made on a similar topic, http://stats.stackexchange.com/questions/527/what-ways-are-there-to-show-two-analytical-methods-are-equivalent/2834.",2010-10-16 08:55:35.0,930.0,
5054,3597,0,"@Whuber: are you on windows or a *nix box (mac, linux,...)",2010-10-16 09:07:21.0,603.0,
5055,3508,1,"The enclosed pictures were my first attempt at using Asymptote (http://j.mp/c8XUGq) instead of Metapost :-) Very sad idea, but I can share the code if you like.",2010-10-16 09:10:05.0,930.0,
5056,3508,0,That's impressive; please share.  :),2010-10-16 10:09:15.0,251.0,
5057,3508,3,"@ars Here it is (as Gist): http://gist.github.com/629642, http://gist.github.com/629644, http://gist.github.com/629645.",2010-10-16 10:20:15.0,930.0,
5058,3664,0,"Thanks chl, that was before my time here.",2010-10-16 11:56:45.0,449.0,
5059,3590,0,@Seb Thanks for that additional information!,2010-10-16 13:31:18.0,930.0,
5060,3597,0,@kwak: Win 7.  (Thanks for asking.),2010-10-16 14:15:15.0,919.0,
5061,3666,0,"(+1) I can imagine that with a 1400+ pages book the authors offer several chapters to AN(C)OVA :) BTW, there are SAS and Stata code for most of the chapters on UCLA, http://www.ats.ucla.edu/stat/sas/examples/alsm/",2010-10-16 16:18:15.0,930.0,
5062,3508,0,thank you!,2010-10-16 17:52:28.0,251.0,
5063,3657,0,"Hello Sarah, you should close this question if you think it is answered.",2010-10-16 21:14:50.0,1307.0,
5064,3664,0,"onestop and chi - thanks so much for the detailed explanations an dthe refs. I will try to go through the analyses as you described, and if I have more questions, I might pester you again - thanks again for the fast reply!",2010-10-16 21:56:14.0,1603.0,
5065,3666,0,"Indeed, there are several chapters. I want to say that about half the book is dedicated to AN(C)OVA, while the first half is regression, so that's about 700 pages of analysis of variance. There are parts of the text (block designs, nested designs) that I felt were incredibly boring, and could've used some more work, but the regression sections were great.",2010-10-17 00:08:13.0,1118.0,
5066,3655,0,"This answer could be applied to any two distributions, it is just that the product of normals is a normal, giving a nice solution. MCMC simulation could be used with pairs of distributions without a closed form solution, using a Bayesian approach with one sample being the prior and the other the likelihood.",2010-10-17 04:38:32.0,1381.0,
5067,3671,2,"The use of circular graphics for tabular data with Circos has been evoked here too, http://stats.stackexchange.com/questions/3158/what-is-this-type-of-circular-link-visualization-called/3159. My opinion is that this is a good way to reduce large symmetric tables, but it is less useful for summary tables with p-values and the like.",2010-10-17 07:46:22.0,930.0,
5068,422,6,"I found Statistics in a Nutshell to be seriously flawed in terms of wrong/missing figures, mistakes in formulas, bad explanations and the book doesn't even have tables for critical values. This is especially bad at places where the authors write ""and since the critical value for this is foo, this is significant"", leaving the reader totally unclear about where this foo value comes from. The book does have a good intro section but should be edited eventually to make it good. Just look at the errata page for the book and be stunned at all the errors.",2010-10-17 09:07:29.0,1048.0,
5069,3597,0,"@Whuber:> there is a problem with winbox: it's not POSIX compliant, as a consequence there is no high prec. timing. This makes it difficult to do things like parallel generation of pseudo random numbers or organisation of tasks. *nix boxes are POSIX compliant. In *nix boxes all cores (as well as the GPU if it's an nvdia) can be easily accessed from within R thru packages such as multicore, gputools, magma,... (which you install as regular packages).",2010-10-17 10:40:55.0,603.0,
5070,3597,2,I would advise you to install ubuntu (google 'download ubuntu') and to run your windows-only apps from within ubuntu via virtualbox (http://www.youtube.com/watch?v=KXgKnd-u2R4). R and latex editors run on ubuntu like a charm.,2010-10-17 10:41:39.0,603.0,
5071,3665,0,"Thanks. ""You should always aim to solve the problem at hand directly, rather than use a more general method and post-process the result"" - I was wondering if you have any intuitive/mathematical justification of the statement? Will be especially helpful with the current context.",2010-10-17 11:14:29.0,994.0,
5072,3672,1,"A bit more details could be helpful; how they are obtained, are branches weighted?",2010-10-17 11:47:41.0,88.0,
5073,3672,0,Let me give you an example. I wrote an algo to cluster variables. Clustering can be represented by a dendrogram. This dendrogram changes over time (based on time series). The structure of the dendrogram slowly evolves and I am look for a measure to describe it (sort of descriptive statistics for dendrograms).,2010-10-17 11:59:04.0,1250.0,
5075,3597,0,@kwak: Many thanks for the suggestions and the information.,2010-10-17 14:44:25.0,919.0,
5076,3674,2,"In R there is the `profdpm` package, and an overview of available indices (incl. Fowlkes & Mallows's $B_k$) to compare HCs is available here, http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.6189&rep=rep1&type=pdf.",2010-10-17 16:09:08.0,930.0,
5078,3683,0,Is there an approachable textbook you could recommend that discusses how and why Manhattan Distances are useful in statistics?,2010-10-17 20:44:18.0,1515.0,
5079,3684,1,"Of course, for discrete distributions, including distributions of finite support (like those in question here), the cf is just the probability generating function evaluated at x = exp(i t), making it a more complicated way of encoding the same information.",2010-10-17 21:24:03.0,919.0,
5080,3626,0,"Argh, yes, I realised this after some guys at work raised some objections about the categorical variables (interestingly, they didn't complain about the continuous variables!). The problem is even more complicated than I outlined, so I need to think about things a bit more.",2010-10-17 21:51:02.0,1144.0,
5081,3683,1,"@Kaelin: Unfortunately I can't think of a text which discusses this in particular.  I can tell you that the L1 distance is preferred since it's less sensitive to outliers.  It's also related to distances between empirical distributions in probability theory ( L1 is twice the ""total variation distance"": http://en.wikipedia.org/wiki/Total_variation_distance ).",2010-10-17 22:00:39.0,251.0,
5082,3687,0,"+1. Though I'm not sure an analysis book, even if it is Rudin, is ""approachable"".  ;-)",2010-10-18 00:35:59.0,251.0,
5086,3640,0,"david - since your variables are all positive, why do you want to fuss with $\\mu = 0$?  btw - in your simulation, you seem to be generating variables per.c and per.n that are independent. is that correct - and if so, is that what you want?",2010-10-18 02:25:53.0,1112.0,
5087,3684,2,"@whuber: As you say, the cf, mgf, and pgf are more-or-less the same and easily transformable into one another, however Mathematica has a cf builtin that works with all the probability distributions it knows about, whereas it doesn't have a pgf builtin. This makes the Mathematica code for working with sums (and differences) of dice using cfs particularly elegant to construct, regardless of the complexity of dice expression as I hope I demonstrated above. Plus, it doesn't hurt to know how cfs, FTs, convolutions, and cross-correlations can help solve problems like this.",2010-10-18 02:38:02.0,,A. N. Other
5088,3643,0,"@rob - how do you define the variance of Y = 1/X when X is normal, as EY is undefined?",2010-10-18 03:26:29.0,1112.0,
5089,3684,1,"@Elisha: Good points, all of them.  I guess what I wonder about the most is whether your ten or so lines of Mathematica code are really more ""elegant"" or efficient than the single line I proposed earlier (or the even shorter line Srikant fed to Wolfram Alpha).  I suspect the internal manipulations with characteristic functions are more arduous than the simple convolutions needed to multiply polynomials.  Certainly the latter are easier to implement in most other software environments, as Glen_b's answer indicates.  The advantage of your approach is its greater generality.",2010-10-18 03:35:44.0,919.0,
5090,3687,0,"@ars: Yes, but I don't know of any one that really is.  That's why I pointed out the two Wikipedia articles.",2010-10-18 03:37:59.0,919.0,
5091,3643,0,"@ronaf. Good point. E[Y^2] is infinite whenever X is normal. The same goes for E[Y^m] for any even m. If m is odd, the result is undefined. Is that better?",2010-10-18 03:57:29.0,159.0,
5092,3685,0,"I have also wondered about this problem, but (unfortunately) haven't found any convincing answers yet. I think there is no solution. There are R/BioC packages like `hopack` (and others) which can estimate the number of clusters, but that doesn't answer your question.",2010-10-18 04:02:09.0,1307.0,
5093,3687,0,"I know, I liked it -- it's the right recommendation to make in case the OP wants to dig deeper.",2010-10-18 04:12:20.0,251.0,
5094,3618,1,"@Srikant: Expand[Sum[x^i,{i,1,6}]^3] also works in WolframAlpha",2010-10-18 07:04:52.0,,A. N. Other
5096,3696,0,"Great answer, thanks. As I now used the solution or ars I will accept his as the accepted answer.",2010-10-18 09:21:51.0,190.0,
5097,3697,0,"+1 Yup, this is the way it is mostly done.",2010-10-18 09:44:20.0,88.0,
5098,3698,0,"Hmm, that's what I get too: |O-E|/sqrt(E) = |17-16.5|/16.5 = 0.123. Can you add a link to the paper if it's available on the web?",2010-10-18 09:57:08.0,449.0,
5100,3698,0,"Unfortunately it isn't. I have collected comparable data and wish to compare my findings with the publication. I have found that the R function prop.test gives results that approximately agree with the paper, but I wanted to understand how the author arrives at his exact numbers.",2010-10-18 10:11:34.0,1614.0,
5101,3698,0,I've contacted the author. Perhaps he can shed light on the problem.,2010-10-18 11:27:15.0,1614.0,
5102,3636,0,@whuber - the std dev would give me a number that tells me how much the temperature is varying.,2010-10-18 12:17:21.0,1595.0,
5103,3636,0,"@Shane - It's a temperature reading.  How would I know if it is normally distributed?  It should be i.i.d., but I'm just a lowly engineer trying to implement something management wants.",2010-10-18 12:19:56.0,1595.0,
5104,3698,0,Are these paired proportions or two independent samples?,2010-10-18 12:32:14.0,930.0,
5106,3701,0,"That was my understanding and like I say, the prop.test function of R gives the same direction of results. I just wondered if there was something that I was doing wrong to not get the z=.08.",2010-10-18 12:55:33.0,1614.0,
5109,3690,0,"Thanks for the answer! However, I'm interested in modeling individuals within groups where group-level effects are difficult to quantify or anomalous and can't easily be included in future predictions. To alter your example a little, suppose we're predicting minute usage for individual customers over the next month using the last three month's of data. Suppose that while customer age is a significant predictor of usage, a local concert last month made young users unusually likely to make phone calls. The 'age' coefficient may be unnaturally large and skew predictions. Does this make sense?",2010-10-18 13:20:46.0,1611.0,
5110,3702,2,"Could you try formating your data so that we can get a clearer idea of what you want. For example, I don't see an 8x11 matrix in your example.",2010-10-18 13:52:17.0,8.0,
5111,3702,1,"More of a SO question, I think.",2010-10-18 14:07:00.0,144.0,
5112,3702,0,"Thanks for quick reply, I am trying format matrix in this windows but quit difficult. I describe my matrix in word: the 8x11 matrix comprise of 8 rows corresponding to ages (1, 1.25,1.5,1.75,2,3,4,5) and 11 columns (first column name ""age"", then boy01...boy05, girl01...girl05) the values of matrix is the height of 5 boys and 5 girls.",2010-10-18 14:07:04.0,1615.0,
5113,3669,0,Thanks. I will pose another question as I am not a stats expert and am not sure my question was clear.,2010-10-18 14:11:12.0,834.0,
5114,3702,0,"I do not know how to present the height of group boy as ""$hgtm"", group girl as ""hgtf"" and age column as ""$age"" as display above.",2010-10-18 14:15:34.0,1615.0,
5115,3690,0,@danpelota That is an example of 'interactions'. I will edit the answer to clarify.,2010-10-18 14:16:28.0,,user28
5116,3703,0,Thank you very much for your kind help.,2010-10-18 14:23:39.0,1615.0,
5117,3702,0,"At this point, I'm a little confused about the data structure too.  Maybe describe what you're trying to do?  You have a set of boys and girls, along with their respective heights and ages?  Are the heights measured at different ages?",2010-10-18 14:29:14.0,5.0,
5118,3673,1,"Thanks chl. I think the problem is that many of the answers were too good - e.g. a search for ""patient cluster analysis"" in Google scholar returns 650,000 results. So rather than saying ""here are some good directions to go in,"" I wanted a more discrete bibliography.",2010-10-18 14:39:03.0,900.0,
5119,3702,1,"As it hasn't been mentioned before, there are *introductory manuals* that come with R. The *Introduction* will teach you the basics of data structure use (and more); the *Data Import/Export* is helpful for data transfer questions.",2010-10-18 14:44:16.0,334.0,
5120,3704,1,"Please explain why you're not satisfied with suncoolsu's answer to your previous version of the same question http://stats.stackexchange.com/q/3653/449.
Also please indicate what software the code in your question is for, and add an appropriate tag. That way, someone with experience in that software package might be able to suggest explicit code.",2010-10-18 14:52:05.0,449.0,
5121,3657,0,"Hi - Thanks again for your answer. I forgot to mention that I am using Stata. When I add two coefficients together (using the output from Stata), can I also just add the standard errors? If so, then I should be able to obtain the standard errors by dividing the sum of the coefficients by the sum of the standard errors. Do you agree? Thanks again.",2010-10-18 15:13:34.0,834.0,
5122,3704,0,Sorry for posting a second question. For some reason I can't figure out how to edit my question. I'm new to the site so please bear with me. And thanks again to those who have provided answers so far!,2010-10-18 15:15:52.0,834.0,
5123,3708,0,What are the geographic units and do you expect them to have any geographic dependency (i.e. spatial autocorrelation?) Although we would always like to have more data you have a reasonable amount of observations to project estimates.,2010-10-18 15:28:59.0,1036.0,
5125,3707,0,Since their is some confusion over what we are seeing can you state how you standardized the distributions and why you standardized the distributions? Also some greater context as to your motivation might be nice although not necessary.,2010-10-18 15:33:28.0,1036.0,
5126,3708,0,"The 100 regions belong to the same country, so I expect that they are correlated in time and in space (in fact I began with a correlation clustering exercise that shows that they are correlated)...",2010-10-18 15:35:34.0,1443.0,
5127,3657,0,"Sarah, In Stata, use the 'lincom' function. Suppose you have variables var1 and var2 and want to add 3 times the coefficient on var1 and 2 times the coefficient on var2. Type 'lincom 3 * var1 + 2 * var2'. This gives the standard error and confidence interval for this estimate.",2010-10-18 15:42:56.0,401.0,
5128,3708,0,"What is the frequency (i.e. is it weekly,monthly, quarterly, annual data) ?",2010-10-18 15:44:28.0,603.0,
5129,3711,0,"I'm not sure this is right. U^n corresponds to multiplying the *same* random value by itself n times. dassouki's procedure will multiply n *different* random values. As an example, the product distribution of two standard normal r.v.'s is *not* chi-squared on 2 d.f : http://dx.doi.org/10.1137%2F0118065",2010-10-18 15:45:34.0,449.0,
5130,3708,0,"Annual data, so I have only 9 points per region...",2010-10-18 15:45:52.0,1443.0,
5131,3706,1,I did not downvote but you should see the other thread as to why this is not ok.,2010-10-18 15:47:05.0,,user28
5132,3704,1,You can edit the question by clicking 'edit' which appears below the tags for your question. If some aspect of suncoolsu's answer is not clear you should ask for clarification via commenting to the answer. There is a 'add comment' link below every answer.,2010-10-18 15:49:16.0,,user28
5133,3711,0,@onestop hmm your are right but I think the reasoning carries over to the general case. I will think about it and fix the answer.,2010-10-18 15:50:32.0,,user28
5134,3712,0,"Thanks so much Charlie. This is very useful. I didn't know about the lincom command so this will save me lots of time. However, if I want to get the impact of treatment on the outcome for quintile 2 should i not add treat + treatXquin2 (rather than adding quintile2 + treatXquin2)?",2010-10-18 15:53:39.0,834.0,
5135,3704,0,Thanks Srikant. I appreciate your help.,2010-10-18 15:57:39.0,834.0,
5136,3711,0,"@Srikant: The conclusion about convergence to a delta function isn't terribly useful or insightful.  (Analogously, if one weren't to adopt the right standardization of sums of random variables, the CLT would vanish into thin air: you would often just conclude that the sums ""converge"" to an improper uniform distribution over all the reals!)  A better way to investigate such phenomena is to seek some standardization that assures convergence to something non-trivial if that is at all possible.",2010-10-18 16:03:29.0,919.0,
5137,3705,0,Many thanks for your nice commends,2010-10-18 16:04:56.0,1615.0,
5140,3711,2,@Srikant: The distribution of the product of n uniforms is the exponential of a (negated) Gamma(n) variate.,2010-10-18 16:08:16.0,919.0,
5142,3711,0,@whuber You are correct about standardization but I did not interpret the question that way (or at least it was not clear from the question context that the OP wanted to know the behavior of the product of different standardized uniform random variates. Thanks for the pointer on the product of n uniforms. I will update the answer to reflect that info.,2010-10-18 16:14:29.0,,user28
5144,3712,0,Charlie - I'm not sure I understand why you multiply treatXquin2 by 1 for yes. If that is the case then how do I obtain the estimate for the untreated (i.e. treat==0)? It doesn't seem to make sense that I would multiply treatXquin2 by 0. Any help would be much appreciated!,2010-10-18 16:17:59.0,834.0,
5145,3647,0,I just discovered that the data I have covers 2 periods in the last 10 years: 2 counts at -5 and -10 years,2010-10-18 16:42:34.0,59.0,
5146,3640,0,"no, I don't want to fuss with $\\mu$ = 0; these variables are generally treated as independent, and covariance data is rarely available. Since C is fairly constant, independence is a reasonable assumption.",2010-10-18 16:47:19.0,1381.0,
5148,3705,2,Another point in favor of the second layout is that it is consistent with good database practices. The data normalization advantages of databases and a knowledge of SQL is very useful if used appropriately.,2010-10-18 17:10:43.0,,user28
5150,3658,1,"Thanks for the response. Don't feel obligated to update anything (as it is not your job to teach me mathematics), but I do not have any clue how you ""integrate over the prior distributions"". Does it involve just calculating all possible combinations since my distributions are uniform?",2010-10-18 18:10:49.0,1036.0,
5151,3705,0,"@Srikant: Thanks, I've updated my post to include.",2010-10-18 18:20:47.0,485.0,
5152,3711,0,"@Srikant: Yes, there's always opportunity for multiple interpretations.  I took the OP's careful investigation, including the standardization and graphing, as an indication of a deeper and more sophisticated perception of the phenomenon than just being something ""getting to zero"" or converging to a delta distribution.  The ultimate arbiter of the question's intent is its proposer, of course.",2010-10-18 18:25:21.0,919.0,
5154,3658,0,"Thank you again, that certainly answers my question.",2010-10-18 18:44:12.0,1036.0,
5155,3715,1,As I understand the [Agresti-Coull](http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Agresti-Coull_Interval) procedure is used to generate approximate confidence intervals for the binomial proportion confidence interval. I am not sure I see the relationship to Bayes theorem. Could you provide some context and your goals within the context to understand how the bayes arises in the context of the Agresti-Coull method?,2010-10-18 18:55:43.0,,user28
5157,3717,2,"@Andy Perhaps, spatial dependencies exist because employers tend to co-locate in certain geographical areas which has spill-over effects to neighboring areas? For example, during the IT bust of 2000 I am sure several regions around the Silicon valley would have had high unemployment rates but perhaps not in Detroit (which is dominated by the auto industry).",2010-10-18 18:58:43.0,,user28
5159,3717,0,"@Srikant you are right. I was thinking about it some more and spatial externalities are much more likely to occur in economic data than in crime data I am used to working with. Although I would still be skeptical with a lag of a year, it deserves more attention than my answer would suggest.",2010-10-18 19:02:11.0,1036.0,
5161,3717,0,"The effects that Srikant has highlighted cannot be ignored to my mind, these need to be taken into account...",2010-10-18 19:10:58.0,1443.0,
5163,3717,0,"@Srilant:> what you see is correct, and J. Hamilton has done some work on economic contagion between US states. But these sorts of model need more than 9 observations per cross section to be estimated. For this types of situation, i recommend the A/B estimator (see my answer) since it has been designed, *precisely* for these types of situation (small **T** large **N**).",2010-10-18 19:21:28.0,603.0,
5164,3715,0,@cool-RR: How exactly are you estimating those three probabilities?,2010-10-18 19:50:26.0,919.0,
5166,2214,0,"My understanding is that you need a finite mean to for a population L-*moment* to be defined, but not for the estimands corresponding to all other L-estimators. For example, the sample median is an L-estimator, though it isn't an L-moment.",2010-10-18 20:59:37.0,449.0,
5167,3715,0,"@whuber: For $P(A)$ I check how many of the messages in my finite sample have $A$, and divide that by the size of the sample. For $P(B|A)$ I check how many of the messages that have $A$ also have $B$, and divide that by the number of messages that have $A$. etc.",2010-10-18 21:01:36.0,5793.0,
5168,3715,1,"@cool-RR: why don't you just estimate P(A|B) directly, then, as the fraction of B samples that are also A?",2010-10-18 21:03:13.0,919.0,
5170,3715,0,"@Srikant: Agresti-Coull lets you draw conclusion about the true proportion from a small finite sample, and gives you a confidence interval with an answer. I have a similar situation: I have a small finite sample and I want to get the value of $P(A|B)$ along with a confidence interval. To give more context: I'm developing something like a Bayesian spam filter, and this is the probability that a message is spam given that it has a certain word.",2010-10-18 21:05:01.0,5793.0,
5171,3725,10,"VAR is unbiased for any distribution with a finite variance, not just for the normal distribution.",2010-10-18 21:08:53.0,159.0,
5173,3717,4,I wouldn't use a univariate time series method with 9 observations.,2010-10-18 21:12:58.0,159.0,
5174,3673,0,"(+1) Thanks for the precision. It's easier to edit your original question and ask for dedicated references, otherwise some of us may be confused; without any upvote we cannot know whether responses were useful or not, nor improve them. Good luck anyway!",2010-10-18 21:19:10.0,930.0,
5175,3725,1,Thanks Rob.  That condition was indeed unnecessary.,2010-10-18 21:41:27.0,919.0,
5176,3720,0,"Given that the unemployment rate must be strictly positive, I guess, for simulation purposes, I could model the log returns for example. Is this reasonable?",2010-10-18 21:49:28.0,1443.0,
5177,3724,0,"@onestop The ""statistical-bias"" tag is somewhat confusing here.",2010-10-18 21:57:23.0,930.0,
5179,3715,0,"Wow @whuber, you blew my mind. I have no idea why I've been doing this. I got the impression this is what people do on Bayesian spam filters, so that's what I've been doing. ($P(A|B)$ is the probability a message is spam given that it contains a certain word, for example 'cheese'.) Now I tried to simply calculate $P(A|B)$ directly, and it seems to work, so I don't even need the answer to this question. Unless I'm missing something. Maybe you have an idea why people use Bayes' Theorem in Bayesian spam filtering when filtering by a single word like this?",2010-10-18 22:04:44.0,5793.0,
5180,3712,0,"Sorry, I misread your question. Add treat + treatXquin2 for the treatment impact for those in quintile 2. I have updated my answer.",2010-10-18 22:05:24.0,401.0,
5181,3724,0,"Perhaps you're right. The guidelines suggest updating tags in light of the the answers as well as the original question, so I was wondering if there was a tag for 'unbiased estimator' or 'unbiasedness', but there wasn't, so i used 'statistical bias' on the grounds that separate tags for flip sides of the same coin seem a bit unecessary, and the guidelines favor reuse of existing tags over creation of new ones. I'm about to fall sleep but I'll take another look at this in the morning.",2010-10-18 22:13:36.0,449.0,
5182,3706,0,"Sorry, I downvoted this answer to indicate I disagreed, but that was an inapproriate and lazy way of doing so, for which I apologise. I'm afraid it's now too late to undo my vote, however - the software won't allow it, for reasons I don't entirely understand. I've just upvoted your question in an attempt to balance things out, though I realise two wrongs don't make a right.",2010-10-18 22:21:23.0,449.0,
5183,3725,0,"Right, whatever variance mean when the distribution is not close to normal ;-)",2010-10-18 22:26:47.0,88.0,
5184,3724,0,I changed statistical-bias to unbiased-estimator. Does that work?,2010-10-19 00:14:47.0,,user28
5185,3731,1,No. Pearson's correlation does NOT assume normality. It is an estimate of the correlation between any two continuous random variables and is a consistent estimator under relatively general conditions. Even tests based on Pearson's correlation do not require normality if the samples are large enough because of the CLT.,2010-10-19 01:46:20.0,159.0,
5186,3731,2,"I am under the impression that Pearson is defined as long as the underlying distributions have finite variances and covariances. So, normality is *not* required. If the underlying distributions are not normal then the test-statistic may have a different distribution but that is a secondary issue and not relevant to the question at hand. Is that not so?",2010-10-19 01:47:13.0,,user28
5187,3731,0,"@Rob, @Srikant: True, I was thinking of significance testing.",2010-10-19 01:59:28.0,251.0,
5188,2214,0,Somehow my thumb bumped this dad-blamed touchpad and I voted down on this question?  Now it won't let me un-downvote unless the question is edited?  Would somebody please consider changing a comma somewhere so I can fix it?,2010-10-19 02:07:40.0,1108.0,
5189,3734,3,"Good question, although my immediate reaction, backed up by my limited experience of teaching this, is that the CLT isn't initially at all intuitive to most people. If anything, it's counter-intuitive!",2010-10-19 02:39:52.0,449.0,
5190,3008,0,Great code but there is a problem. I am not as smart as you are. I need it broken down stepwise. I take it is runs the number of simulations? What is nn? Is it the number of subjects in the study? Then I see you created a distribution of covariates and made them determine a yes or a no depending on a threshold.,2010-10-19 03:03:51.0,104.0,
5191,3643,0,"@rob + @david  if the variables in the ratio are *asymptotically* normal [with a non-zero mean in the denominator] and also consistent, the ratio is also asymptotically normal - as the usual delta method shows. perhaps that will render moot the discussion about non-existence of moments.",2010-10-19 03:15:10.0,1112.0,
5192,3643,0,"@ronaf. Actually, that doesn't help. It is possible to have asymptotic normality as well as having non-existent moments. Asymptotics with probability distributions can have weird properties.",2010-10-19 03:16:36.0,159.0,
5193,3734,0,@onestop AMEN! staring at the binomial distribution with *p* = 1/2 as n increases does show the CLT is lurking - but the intuition for it has always escaped me.,2010-10-19 03:18:40.0,1112.0,
5194,3725,2,"@mbq to some, the variance is a moment of inertia. there is also chebyshev's inequality, that applies to any distribution. it also has an interpretation similar to that for the normal for any location/scale family with a finite second moment - like the logistic, for example. [the pdfs need not be symmetric, tho.]",2010-10-19 03:34:14.0,1112.0,
5195,3643,0,"@rob - i have in mind the ratio of two sample means. even tho the actual moments may be infinite or undefined, there is an asymptotic mean and asymptotic variance that go with the limiting normal distribution [again assuming the denominator is not consistently estimating zero]. it is often the parameters of the asymptotic distribution that are relevant for analyses of the data [judging by the way in which many statistical analyses are carried out these days]. in that case, the non-existence of actual moments is a side issue [or a quibble?].",2010-10-19 03:44:15.0,1112.0,
5196,3725,0,"altho, as whuber points out, there are two excel functions for variance, if my recollection is correct, there is only one for covariance - and it divides by n, not n-1. [i haven't checked the latest version of excel, tho. can someone tell us if that is still the case?]",2010-10-19 03:56:00.0,1112.0,
5197,3643,0,"@ronaf. I don't really follow your last comment. The asymptotic normal distribution will have a well-defined mean and variance. But they are not the same as the true asymptotic mean and variance which are undefined whenever the denominator has a non-zero density at 0. In practice, this may not matter. Imagine, for example, if the denominator is N(100,1). Then the sample values of the ratio will behave nicely with very high probability.",2010-10-19 04:26:18.0,159.0,
5198,3720,0,"@kwak, could you provide a link to the Hamilton work you talked about in a comment to my answer. Also do you know of any work that used the A/B estimator and included spatial effects in the models?",2010-10-19 04:40:45.0,1036.0,
5201,3731,0,"@Srikant: I'm not sure it's a ""secondary issue"". You can compute anything after all -- it's the inference that matters. @Rob: your ""if"" qualifier is key here -- it seems to me that's central to this question. We can justify a whole lot with asymptotic hand waving; exceptions matter.",2010-10-19 05:54:39.0,251.0,
5202,3718,2,"Excel's RAND() is the worst pseudo RNG I have ever measured.  (I applied the DieHard tests to it about ten years ago.)  It's not even equidistributed!  Nevertheless, it's OK for this kind of investigation.",2010-10-19 05:55:05.0,919.0,
5204,3739,0,"the distribution is given by a few parameter already with your definition ? what is the difference for the application you havde in mind, between sampling things directly and using a transfomation after sampling ?",2010-10-19 06:20:05.0,223.0,
5205,3713,1,Can you try to give more specific description of what you want to cluster? or is it just a state of the art in clustering that you need?,2010-10-19 06:23:19.0,223.0,
5206,3702,0,"I don't see the question here. It seems to be ""how to load that type of data"" as suggested by Dirk, but the data seems already here. I vote for close.",2010-10-19 06:28:35.0,223.0,
5207,3734,0,Similar question with some nice ideas: http://stats.stackexchange.com/questions/643/how-do-you-convey-the-beauty-of-the-central-limit-theorem-to-a-non-statistician,2010-10-19 06:42:56.0,88.0,
5209,832,0,"Looks like it could be very nice, but I just tried installing and running it in Stata 11.1 (i.e. the latest version) and it keeps giving me an r(3000) error upon clicking ""Done"" in the dialog, even if i type -version 6: clt-.",2010-10-19 06:49:38.0,449.0,
5211,3720,0,@Andy W:see edited answer above.,2010-10-19 07:02:19.0,603.0,
5212,3720,0,"@Teucer: certainly. There are some facilities in the plm package to do these transformations (i.e. log, difference, dlog,...) in a single command-line: look for the **dynformula()** function.",2010-10-19 07:04:29.0,603.0,
5213,3720,0,"@kwak Actually what you propose is very close to what I have in mind: I was using the package lme4 to model the log returns. However, if I well understood, the difference lies in the estimator: lme4 uses REML whereas with plm you can use GMM. Is that correct? If so, why GMM is in this case superior to REML? Btw do you think that one can use the package heavy which is fitting with heavy tailed distributions (e.g. Student t)?",2010-10-19 07:08:34.0,1443.0,
5214,3745,0,"Unless I'm misleading, the second moment are not exactly identical between the two.",2010-10-19 07:18:07.0,930.0,
5216,3720,0,"@Teucer. The difference between the two approaches is discussed in the vignette to the plm package. I would think that gmm estimation would be more efficient in your case, because one can assume that the innovations to the unemployment rate have a well behaved Gaussian distribution (i.e. as aggregation of a large number of small shocks). This assumption also explain why i don't understand the need to use the **heavy** package.",2010-10-19 07:20:34.0,603.0,
5217,3724,0,"Fine with me. I've just added this new tag to a few other old posts - what do you think? Also does the ""statistical bias"" tag still have a place? If so, when would you use it? Is this what the ""tag wiki"" system is meant to be for? I'm afraid I haven't worked out if or how i can edit that. Maybe we should take this to meta...",2010-10-19 07:24:00.0,449.0,
5220,3745,0,"@chi - I meant the usual parameterisation of the logit-normal is in terms of the mean and SD of the related normal dist, not that the mean and SD of the logit-normal are the same as that of the related normal. Sorry if that was, or still is, unclear - i'm editing in a hurry as i should really have left for work by now!",2010-10-19 07:29:31.0,449.0,
5222,3733,7,"Pearson's $\\rho$ does not assume normality, but is only an exhaustive measure of association if the joint distribution is multivariate normal. Given the confusion this distinction elicits, you might want to add it to your answer.",2010-10-19 07:42:26.0,603.0,
5223,3744,1,"I'm also a big fan of Kendall's tau. Pearson is far too sensitive to influential points/outliers for my taste, and while Spearman doesn't suffer from this problem, I personally find Kendall easier to understand, interpret and explain than Spearman. Of course, your mileage may vary.",2010-10-19 07:44:51.0,1352.0,
5224,3733,0,@kwak. Good point. I'll update the answer.,2010-10-19 07:45:38.0,159.0,
5226,3717,1,"Yes, simple methods usually work better than complex ones (sometimes to a surprising extent). However, in this case univariate methods would throw away 99% of the data - whenever we forecast for one geography, we disregard all the other geographies. And I would definitely expect some kind of panel data model to be better than a univariate one. Not so much because employers co-locate, but because regulatory, tax, central bank and other external factors will be common and largely similar drivers of unemployment for *all* geographies.",2010-10-19 07:49:56.0,1352.0,
5227,3740,0,"Try the plots with a N(0,10^2) (i.e., 10 is standard deviation). It will not appear to be normal anymore.",2010-10-19 08:21:24.0,,user28
5228,3747,0,"Thanks @Stephan Kolassa, this will already help towards finding a solution.",2010-10-19 08:39:35.0,1623.0,
5229,3720,0,"@kwak On using heavy: I believe that the estimates are  more robust with heavy tailed innovations (my measurements are imperfect), but I might be wrong. Another reason was that I did a correlation clustering and looked at the distribution of log returns per year for each cluster, a student t distribution fits not too bad. But maybe it is irrelevant here. I have some questions: now assume I have more points, let's say about 25 points per region, would you still use GMM or REML for estimation? Where is the threshold? For a small sample is the asymptotic efficiency relevant?",2010-10-19 09:12:09.0,1443.0,
5230,3740,0,"@Srikant I agree, this is an answer to second point of the question.",2010-10-19 09:14:24.0,88.0,
5231,3720,0,"@Teucer:> *Another reason was that I did a correlation clustering and looked at the distribution of log returns per year for each cluster, a student t distribution fits not too bad.* this does not in itself justifies using a $t$ distribution: mixes of Gaussian distribution with varying variances, for instance, will also converge to a fat tailed distribution.",2010-10-19 09:32:10.0,603.0,
5232,3720,0,"@Teucer:> * For a small sample is the asymptotic efficiency relevant?* I think there is a slight misunderstanding here. If your assumptions on the distribution of the residuals are correct, then asymptotic efficiency is a measure of mean accuracy of your estimates (the 'asymptotic' here refers to the relative average precision over a large number of estimation instances).",2010-10-19 09:35:33.0,603.0,
5233,3720,0,"@Teucer:> *now assume I have more points, let's say about 25 points per region, would you still use GMM or REML for estimation? Where is the threshold?* At some point, the balance indeed tilts. And for larger sample sizes the gains in efficiency do not outweighs the costs in (statistical **AND** computational) complexity of the GMM approach over the REML one. The exact point depends also to the extend to which your data conforms to the working assumptions underlying each model (REML does not,imho, have less requirements, just different ones)....",2010-10-19 09:39:28.0,603.0,
5234,3697,2,See http://stackoverflow.com/questions/2492947/boxplot-in-r-showing-the-mean for solutions using R,2010-10-19 09:40:03.0,229.0,
5235,3720,0,"@Teucer:> It's certainly a good sign if the two approaches do not lead to wildly different results (in the hypothetical that this would not be the case, one should explain why)",2010-10-19 09:40:57.0,603.0,
5237,3582,0,"@Rob:> given that three days have elapsed, i think for the benefit of the wider community, you should consider posting this as a separate question.",2010-10-19 10:55:46.0,603.0,
5238,3724,0,"@Skrikant In fact, I didn't think another tag was needed at all, but yours looks better.",2010-10-19 11:22:42.0,930.0,
5240,3582,0,@kwak. OK. I've posted something at http://stats.stackexchange.com/questions/3752/,2010-10-19 11:35:49.0,159.0,
5241,3732,0,"(+1) Thanks for the link! You may be interested in this related question, http://stats.stackexchange.com/questions/3200/is-adjusting-p-values-in-a-multiple-regression-for-multiple-comparisons-a-good-id. Feel free to contribute.",2010-10-19 11:37:43.0,930.0,
5242,3720,2,"@kwak, the spatial effect you suggest is what is referred to as local Geary's C, Global formula, http://en.wikipedia.org/wiki/Geary's_C, (or here is a link for the local version  http://www.passagesoftware.net/webhelp/Introduction.htm#Local_Geary_s_c.htm ) you could also consider local Moran's I, http://en.wikipedia.org/wiki/Indicators_of_spatial_association",2010-10-19 11:41:37.0,1036.0,
5243,3731,0,"@ars,@Srikant. Even with small samples, you can still do inference on correlations, but not using the asymptotic normality result.",2010-10-19 11:41:55.0,159.0,
5244,3746,1,"This answer is similar enough to help you with the averaging of the points, http://stats.stackexchange.com/questions/2493/managing-error-with-gps-routes-theoretical-framework/2497#2497, it is simple to incorporate weights in that framework. I would think you would be able to use some simple heuristics to identify outliers, but that doesn't preclude you from taking a more empirical approach like Stephan suggested.",2010-10-19 11:52:04.0,1036.0,
5245,3752,1,"You wrote ""On the other hand, variable kernels are usually thought to lead to poor estimators in kernel density estimation"", what is the part of the paper you mention that makes you believe that ? I have plenty of references that go in the other derection, see for example the references mentioned in this paper: http://arxiv.org/PS_cache/arxiv/pdf/1009/1009.1016v1.pdf",2010-10-19 12:37:01.0,223.0,
5246,3752,1,"The abstract of Terrell and Scott summarises it nicely: ""Nearest neighbor estimators in all versions perform poorly in one and two dimensions"". They only seem to find much advantage in multivariate density estimation.",2010-10-19 12:50:45.0,159.0,
5247,3720,0,"also to note how you define k is only limited to your imagination. There is currently no consensus on what is proper or improper, although many people suggest you try to optimize it like you suggested.",2010-10-19 13:05:20.0,1036.0,
5248,3755,0,"I'm running Windows 7 64-bit on Quad CPU 2.67 GHz, 4GB RAM",2010-10-19 13:07:04.0,315.0,
5249,3732,0,"@chl, I don't think I can add anything to the already excellent answers for that question. I actual think Brendan's response is very poignant because I suspect the original poster is really interested in causal inference not solely prediction based on the context of the question.",2010-10-19 13:12:58.0,1036.0,
5250,3720,0,"@kwak @Andy thx for all the explanations. Just another question: now let's assume that I can further aggregate my regions on some broader regions (synthetic, using correlation clustering, or economic) would it make sens to compute the average  $\\bar{u}_{−it}$ on these regions?",2010-10-19 13:13:00.0,1443.0,
5251,3752,1,"""Nearest neighbor"" is not the only variable kernel. The papers I mention use other tool such as Lepskii's algorithm. I'll read the AOS paper but as the performences of nearest neighbor should decrease with the dimension, I found it strange that increasing the dimension gives advantages to a ""very non-parametric"" estimator (If we admit constant bandwidth is less non parametric than varying bandwith). In this type of situation, the evaluation case that is used often determine the results ...",2010-10-19 13:13:58.0,223.0,
5252,3720,0,"@teucer, your asking if it would make sense to use that spatial neighborhood average as a predictor right? That is close to what local Moran's I does, it makes no difference if you choose neighbors based on theoretical reasons or empirical ones. I can probably guess why kwak initially suggested using the spatial differences as opposed to the average, but I will let kwak clarify.",2010-10-19 13:22:16.0,1036.0,
5255,500,1,"You may want to read this answer to a seperate question and see why adjusting p-values in such a manner may not be the best solution, http://stats.stackexchange.com/questions/3200/is-adjusting-p-values-in-a-multiple-regression-for-multiple-comparisons-a-good-id/3317#3317",2010-10-19 14:02:05.0,1036.0,
5256,3731,0,"@Rob: Sure, but it seems this is where one should advocate Spearman's method over Pearson's.  For example suppose small samples where X is normal but Y isn't -- you can compare the two on even terms with ranking methods such as Spearman's.  Using Pearson's requires more work, for example, finding an appropriate transformation.",2010-10-19 14:18:59.0,251.0,
5257,3759,0,"Hey @Jonathan; machine learning is on-topic here (in fact, the machine learning proposal was merged with this one): http://meta.stats.stackexchange.com/questions/492/are-the-machine-learning-questions-on-topic",2010-10-19 14:32:58.0,5.0,
5258,3759,0,"thanks Shane, wasn't clear on that",2010-10-19 14:38:52.0,1127.0,
5259,3720,0,"@kwak, I also think the random component to the error term is a very good idea. There are many logical situations in which you wouldn't expect Beta(s) to be the same in different regions. I remember an example that New York City probably influences its neighbors, but the neighbors of NYC are less likely to influence it.",2010-10-19 14:40:56.0,1036.0,
5260,3720,0,"@Teucer:> As Andy W said, any distance measure from which you can obtain a matrix of pairwise distances (geographic, economic as well as those coming from a clustering algorithm and there variations) are certainly to be tried. @Andy W: i don't advise Teucer to directly use the local average as a predictor because of the risk of correlation between the component of the residuals accounting for heterogeneity in the $\\beta_s$'s and the local average. Is this what you had in mind ?",2010-10-19 14:57:52.0,603.0,
5261,3720,0,@Andy W:> that (NYC) is a very good intuitive example. Worth using as an illustration.,2010-10-19 14:58:55.0,603.0,
5262,3713,1,I don't have an immediate application in mind.  I'm just interested in a general approach to choosing a clustering method and measure of similarity.,2010-10-19 15:02:42.0,485.0,
5264,3720,1,"@kwak, that was actually not the reason I imagined (I was thinking more along the lines of stationary estimates of Beta(s)). Local Moran's I does scale the average (ie it Z scores the average of the neighbors based on global mean and variance), but I think your concern is still legitimate. Is that a big deal though if your only interested in prediction?",2010-10-19 15:16:44.0,1036.0,
5265,3732,0,"Yes, I was thinking of his answer. I have initiated a reflexion on data dredging issue (not exactly about model/variable selection issues or causal inference), but so far receive few responses. If you like to add your own ideas, it would be interesting: http://stats.stackexchange.com/questions/3252/how-to-cope-with-exploratory-data-analysis-and-data-dredging-in-small-sample-stud",2010-10-19 15:38:37.0,930.0,
5266,3760,0,+1 for linking to the full essay rather than one of the all-too-common oversimplifications - it's not long and is still relevant today and well worth reading in full. Hill's criteria for causation are about much more than just pyramid / piling up of evidence though.,2010-10-19 15:39:25.0,449.0,
5267,3762,2,"@Shadi How many discrete values do you have? Are they really to be considered to be ordinal, if they lie between 0 and 1?",2010-10-19 15:42:12.0,930.0,
5268,3754,5,Why do you insist on crossposting here and on SO?,2010-10-19 15:46:11.0,334.0,
5269,3762,0,"They are continuous, every float value between 0 and 1. Is there any incoherence between ordinal data and [0 1] interval? Thanks.",2010-10-19 15:49:16.0,1564.0,
5270,3758,0,Can you add the name of a function/package you use?,2010-10-19 15:51:59.0,88.0,
5271,3762,1,For me this would rather be considered an interval scale: http://en.wikipedia.org/wiki/Interval_scale#Interval_scale,2010-10-19 15:54:27.0,442.0,
5272,65,1,"As far as I remember, I was taught the 'sample' calculation in GCSE maths and science (age 14-16) and the distinction between populations and samples and their associated variance measures was covered (though not in depth) at A-level (age 16-18).  So I'm not sure this is a simple UK/US difference.",2010-10-19 15:55:26.0,266.0,
5273,3720,0,"@Andy W:> *(ie it Z scores the average of the neighbors based on global mean and variance)* As you said, i don't think this alleviate my concern. The main issue for the use of RE is that we need $E(x_{it})\\approx E(x_{jt})$ for any $i \\neq j$. this most likely holds true if x_{it} is $u_{it}-\\bar{u}_{-it}$ (or as Teucer suggested, $\\log(u_{it})-\\log(u_{-it})$) but it most certainly does not hold when x_{it} is $\\bar{u}_{it}$ because it could be that $\\bar{u}_{jt}\\neq\\bar{u}_{it}$. At any rate, this hypothesis has to be tested (Haussman test).",2010-10-19 15:57:22.0,603.0,
5274,3762,0,"Yeah, I believe you are right. So you mean I cannot use ordinal measures for interval data, right? I corrected my question. I am looking forward to your guidances. Thanks.",2010-10-19 15:59:20.0,1564.0,
5275,3758,0,"Thanks @mbq! I have tried to figure out which one provides F-test, but no success...",2010-10-19 16:08:22.0,930.0,
5276,3762,0,"When you stay between 0 and 1, does that include the endpoints 0 and 1 themselves or exclude them?",2010-10-19 16:10:34.0,449.0,
5277,3720,0,"@Andy W:> *Is that a big deal though if your only interested in prediction?* Can you post this as a separate question on the main board? It is a very important point, one which should interest many future readers.",2010-10-19 16:11:11.0,603.0,
5278,3762,0,the interval includes 0 and 1.,2010-10-19 16:14:19.0,1564.0,
5279,3752,0,"@Robin Girard:> * found it strange that increasing the dimension gives advantages to a ""very non-parametric"" estimator (If we admit constant bandwidth is more non parametric than varying bandwith)* is there a typo in this sentence ? Otherwise you would seem to agree with the authors, at least on an intuitive level. Thanks to confirm/correct.",2010-10-19 16:18:05.0,603.0,
5280,3740,0,You will run into trouble when you use any normal distribution with substantially nonzero mean.  The logistic transformation is almost linear for values near zero but becomes strongly nonlinear for more extreme values.  The resulting distribution will be strongly skewed; a normal approximation will be poor.  This explains why your answer is effective for the particular distribution proposed by the OP (so I upvoted it and hope others do too) but also puts constraints on its generalization to similar looking problems.,2010-10-19 16:21:24.0,919.0,
5281,3739,0,"To sample y, just sample x from a Normal(0, 0.2) distribution and compute y = e^x/(e^x+1).  You can use (0, 0.2) as the parameters: by means of this formula they completely determine the distribution.",2010-10-19 16:25:44.0,919.0,
5282,3762,0,"@Shadi So, I suggested you change your title accordingly.",2010-10-19 16:26:15.0,930.0,
5283,3762,0,"You can use any ordinal measures on interval data. But, you should not do so. You should use measures for interval data as they use more information are more powerful, etc as measures for ordinal data. Measurements are itself ordinally scaled (from low to high): nominal, ordinal, interval (and ratio). It is generally a good idea to use measures of that level of measurement that one has as these are the most powerful/informative ones.",2010-10-19 16:29:20.0,442.0,
5284,3767,2,"No you can't use correlation to assess the reliability of the measurements or the inter-rater agreement, even for two series of measurement. The correlation computed from two raters will remain the same even if you add some arbitrary value to the 2nd rater's assessments, while the agreement ICC will decrease and correctly reflects that there is a rater-effect.",2010-10-19 16:39:48.0,930.0,
5285,3740,0,"@whuber I agree, but I won't run into trouble, rather see that in such case it is not normal. This answer is just to promote trying as a technique, not to imply that resulting distribution will be always normal.",2010-10-19 16:40:21.0,88.0,
5286,3767,0,"No, it is for more than two measures.",2010-10-19 16:40:42.0,1564.0,
5288,3005,0,"@kwak Thanks for the fruitful exchange, I think I'm going to work throughout the LARS algorithm to try to connect the two approaches.",2010-10-19 16:42:48.0,930.0,
5289,3767,1,"@Shadi Yes, I understand your design; I just take as an illustration @Henrik's POV with 2 series; the same line of reasoning applies with $k$ series of measurement.",2010-10-19 16:44:22.0,930.0,
5290,3582,0,Thanks @Rob @Kwak -- I got hung up noodling KDE variants. (Is 3 days a statisticians' limit :),2010-10-19 16:47:15.0,557.0,
5291,3769,0,"@ars (+1) Many thanks! I missed this package... Always better to look at the code directly, I appreciate.",2010-10-19 16:48:22.0,930.0,
5292,3740,1,"Thank you for sharing your philosophy.  I agree: it's good to help people learn to answer their own questions.  Empirical investigation (""trying""), though, is usually best when informed by theoretical considerations to indicate the limits of its applicability.  In this case you're safe with the Q-Q plot, because it is so sensitive to deviations from normality and you used a largish sample size of about a thousand, but the histogram alone can be deceiving.  (Almost *any* bell-shaped histogram ""looks"" normal!)",2010-10-19 16:50:36.0,919.0,
5293,3766,0,"If I want to explain the data in each set, it can be: 0.98, 0.01, 0.5, ... which shows 'sound1' and 'sound2' are very similar (0.98), 'sound1' and 'sound3' are much different (0.01) and so on. In this case, do you believe I can use ICC or Pearson is better?",2010-10-19 16:59:40.0,1564.0,
5294,3766,0,"@Shadi Did you read the thread I pointed to? You cannot use correlation-based criteria. So, I would better advice you to rely on the ICC unless someone has a better idea to cope with bounded values. For me it's not a problem as you're likely to end-up with similar results than with any other most complicated method. Still I agree with others than it makes sense not to use ANOVA or mixed-effects models with inappropriate link function in certain cases.",2010-10-19 17:00:42.0,930.0,
5295,3770,0,That seems to be it. I will go away and think about it so more. Many thanks all.,2010-10-19 17:51:08.0,1614.0,
5297,3720,0,@Teucer:> I'm sorry i do not understand your last message. But you can certainly edit your question.,2010-10-19 17:56:29.0,603.0,
5298,3766,0,"Yes, I have studied them. Thank you. Actually I am completely new to this concept. I did not understand almost 80% of the thread. I wanted to use ICC in Matlab, but at first I should know the meaning of 'type', 'alpha' and 'r0' to know what value is the best for my purpose. Do you know any quick way to get some information? Thanks for your guidance.",2010-10-19 18:17:20.0,1564.0,
5299,3720,0,"@kwak I think it will help me a lot if you can edit your answer with the model formulas and the R code: I have read the vignette of plm several times, but I do not understand the model specification! Thanks in advance for the effort...",2010-10-19 18:20:25.0,1443.0,
5300,3773,2,"Fisher's exact test and Pearson's chi-squares test the null hypothesis that all 5 methods are equally effective against the alternative that at least 1 is better than the others. The p-values tell me that the null is rejected. So, if I want to find out which methods is actually better than the others won't I have to do 10 pairwise comparisons?",2010-10-19 18:26:53.0,1558.0,
5301,3720,0,"I would have used the following specification with lme4: fm <- lmer(lu~lu1+ls+(1|R),data=df) where $lu=log(u)$, $lu1=lag(log(u),1)$, $ls=log(\\bar{u})$ and R is the region factor. Is this the model you have in mind? How can I specify with pgmm the following model lu~lu1+R+(lu1|R) for example?",2010-10-19 18:29:07.0,1443.0,
5302,3720,0,"@Teucer:> i will do this, but not tonight ;<",2010-10-19 18:43:19.0,603.0,
5303,3775,0,"That's a much better answer than mine! I failed to read the question properly I'm afraid (Step 3 in particular). I thought of deleting my answer, but I stand by the greater interpretability of a Bayesian approach is it's really the ranking that's of interest.",2010-10-19 19:09:55.0,449.0,
5305,3775,0,Just to make sure I understand correctly- The indicator that tracks the relative difference between method 4 and 5 will be updated whenever we see a difference that is greater than 0.21.,2010-10-19 19:21:08.0,1558.0,
5307,3769,0,"@chl: I was looking at the code and about to look up the formula, when I saw your answer come in. Looks like we started from different ends and met in the middle -- making a nice complement of answers. :)  (Already upvoted you.)",2010-10-19 19:23:28.0,251.0,
5308,3769,0,@ars I have to turn back to Zar's textbook because I didn't find the function though I've heard of the alternative statistic :),2010-10-19 19:25:16.0,930.0,
5309,3775,0,"@sxv Yes, that's right.  (Well, I actually used greater than or equal.  Ties do happen.  I think including equality among the significant results is the correct thing to do, because we're evaluating the probability that differences *this large or larger* can occur by chance.)",2010-10-19 19:26:17.0,919.0,
5310,3752,0,@kwak thanks to notice that! this is a typo: I wanted to say constant bandwidth is less NP ... I can't modify my comment :( sorry about that.,2010-10-19 19:27:55.0,223.0,
5311,3766,0,"@Shadi Oups, sorry, I didn't think of language issue. I've updated my response. HTH",2010-10-19 19:42:50.0,930.0,
5313,3721,0,"I need just one number for all of the judges, not a matrix that shows the pairwise correlations of judges. for example: pearson(a,b,c)=0.53     Is it possible?   Thanks.",2010-10-19 21:39:52.0,1564.0,
5315,3680,0,"Is that the ""Riemann manifold Langevin"" paper? Do integrate Fisher information at some point?",2010-10-19 21:51:45.0,511.0,
5316,3721,0,"@shadi Correlation is a bivariate relationship in the sense that it tells you the relationship between two sets of variables. Thus, calculating pearson(a,b,c) where I presume a,b,c are different judges is not possible.",2010-10-19 21:52:18.0,,user28
5319,3731,0,"@ars. You can just use Monte Carlo methods or a bootstrap. Not much work in that, just computation.",2010-10-19 22:24:18.0,159.0,
5320,3752,0,@robin. I've edited your comment,2010-10-19 22:25:56.0,159.0,
5321,3766,0,"Dear chl, Thanks for your great guides. I'm not sure if I explained my data well or not. when you asked : ""how many discrete data do you have"", did you mean something like ""partner1"" and ""partner2"" in (uvm.edu/~dhowell/StatPages/More_Stuff/icc/icc.html)? If so, I should say just 1. I try to explain my data better. I have, for example, 100 different sounds. I want to know how similar each two sounds are. I used a program to do that for me. It generated some (4851) numbers. I also used 5 other programs to do the same thing. Now, I want to know how close the results of the 6 programs are",2010-10-19 22:43:28.0,1564.0,
5322,3731,2,"@Rob: Yes, we can always come up with workarounds to make things work out roughly the same.  Simply to avoid Spearman's method -- which most non-statisticians can handle with a standard command.  I guess my advice remains to use Spearman's method for small samples where normality is questionable.  Not sure if that's in dispute here or not.",2010-10-19 23:43:16.0,251.0,
5323,3731,1,"@ars. I would use Spearman's if I was interested in monotonic rather than linear association, or if there were outliers or high levels of skewness. I would use Pearson's for linear relationships provided there are no outliers. I don't think the sample size is relevant in making the choice.",2010-10-20 00:32:52.0,159.0,
5324,3779,0,Should it not be 'pick k tiles *without* replacement'? Very interesting question.,2010-10-20 00:39:34.0,,user28
5325,3769,0,"sorry for the late post, but yes, i am using agricolae package.  friedman.test doesn't work on my data as it is returning an error message ""your data is not an unreplicated design"" to that effect.       but why are there 2 p-value f's?  am i right that the first is for the the treatment and the second for the block?",2010-10-20 00:59:09.0,1627.0,
5326,3731,2,"@Rob: OK, thanks for the discussion.  I agree with the first part, but doubt the last, and would include that size only plays a role because normal asymptotics don't apply.  For example, Kowalski 1972 has a pretty good survey of the history around this, and concludes that the Pearson's correlation is not as robust as thought.  See: http://www.jstor.org/pss/2346598",2010-10-20 01:00:48.0,251.0,
5327,3769,0,"@kathy: see chl's answer for an explanation of what the two mean -- the first value you see is $F_r$ and the ""value f"" is $F_{obs}$ in his answer.  Also, it's possible you're passing the arguments in the wrong order to the friedman.test method; it should be (data, group, block).",2010-10-20 01:08:07.0,251.0,
5328,3769,0,"aw, thanks ars, now i get it, its the adjusted value...mmmm that's why i'm wondering why its so close to the chisqr value",2010-10-20 01:16:45.0,1627.0,
5329,3779,0,oops. indeed it should.,2010-10-20 01:58:11.0,795.0,
5330,1332,0,"I think it's a great one (I think it goes well with - ""no models are true, some  are useful"")",2010-10-20 02:27:35.0,253.0,
5331,3783,0,great!  thank you so much,2010-10-20 02:57:18.0,1627.0,
5332,3780,0,"The quick and dirty approach may not be so quick! The dictionary may contain 100,000 words, and the search for a match of the given tiles could be a coding disaster.",2010-10-20 04:26:51.0,795.0,
5333,189,2,"This would be the approach when the CDF is only approximated empirically. It gives lousy estimates of the PDF, though.",2010-10-20 05:13:19.0,795.0,
5334,3766,0,"@Shadi So you have 6 ""raters"" or ""methods"" that are assessing 100 objects similarity. How do you explain that there are 4851 measurements: Are there replicate measurements for each object (sound)? The coding (ordered or discrete, continuous) is for the measurements. In your case, I consider it as reflecting a continuous scale of similarity on [0,1].",2010-10-20 06:30:14.0,930.0,
5335,3780,0,@shabbychef This is something well done to suit spell checkers.  See for instance http://www.n3labs.com/pdf/lexicon-squeeze.pdf,2010-10-20 07:06:49.0,88.0,
5336,3781,0,*How* have you simulation the datasets?,2010-10-20 07:07:33.0,449.0,
5337,3767,0,@chl You are right! I will keep this post to remind everyone that this is wrong.,2010-10-20 08:21:23.0,442.0,
5338,3788,0,"IMHO lag auto-correlation makes sense only if you have an ""time-seris like"" ordering otherwise it doesn't make a lot of sense. Also, independence => correlation = 0, but not the other way.",2010-10-20 08:46:25.0,1307.0,
5339,3790,0,"I am sorry, I am just  a first year math student. Could you please provide/recommend a link/book/paper that describes how the relationship was derived?",2010-10-20 08:52:00.0,1636.0,
5340,3788,0,"Why do you want to know if the data are independent? If you are fitting a model the model might assume the **errors** are IID, not the data. Can you provide more context - either as a comment or (better) by editing your Question (see the edit link beneath the tags)?",2010-10-20 08:57:34.0,1390.0,
5341,3780,0,"@shabbychef Reg monte-carlo- if the dictionary is sorted a match should be fairly quick no? In any case, the direct approach that I outlined earlier was flawed. I fixed it. The problem in my earlier solution was that the same word can be formed multiple ways (e.g., 'bat', 'b*t' etc).",2010-10-20 09:14:00.0,,user28
5342,3792,0,Your first point remind me of the *run test* (at least for simple design).,2010-10-20 09:31:13.0,930.0,
5343,3790,3,"@Sara I think it dates back to Karl Pearson, which uses this empirical relationship for his ""Pearson mode skewness"". Aside from this, you may find interesting this online article, http://j.mp/aWymCv.",2010-10-20 09:42:06.0,930.0,
5344,3780,1,"@shabbychef On further reflection, I agree with you that the monte carlo approach will not work. One issue is that you need to figure out which words you can actually form with the k tiles and the second one is that you can form multiple words with the k tiles. Calculating these combinations from k tiles is probably not that easy.",2010-10-20 09:49:17.0,,user28
5346,3795,0,"Out of interest, what did you find was the problem with GraphViz? Not flexible enough? In what way(s)?",2010-10-20 10:50:18.0,449.0,
5347,3792,0,"G. Jay Kerns:> Thanks, i've edited my post to adress your comment.",2010-10-20 11:02:06.0,603.0,
5348,3790,0,Thank you chl and kwak for the link and answer you have provided. I will study them.,2010-10-20 11:13:39.0,1636.0,
5350,3794,0,"So, in excel, if I wanted to look at a time frame of 365 days...then I'd add 365 columns along side the ID table?",2010-10-20 11:35:00.0,1641.0,
5351,3792,0,"OK, I've deleted my earlier comment.",2010-10-20 11:42:30.0,1108.0,
5353,2732,0,"+1, I second all your recommendations and have only one *very* minor quibble: there is no ""c"" in Frank Harrell's name... He's American and not French, after all ;-) I'll wait for another 317 rep and edit your answer...",2010-10-20 11:50:29.0,1352.0,
5354,2732,0,Oups... thanks Stephan :),2010-10-20 11:56:24.0,930.0,
5356,3799,0,"Given my limited experience, I cannot give an exact answer. However, I believe that you can use a panel data (because you consider in your example variations within individuals and between individuals) approach with logit. Maybe others can elaborate on this...",2010-10-20 12:14:46.0,1443.0,
5357,3799,0,"Your small example is very useful, but I assume your real dataset is larger. How much larger, i.e. (roughly) how big are your real *N* and *k*?",2010-10-20 12:25:25.0,449.0,
5358,3794,0,@Alex Yes. I suppose you'd need one of the more recent versions of Excel that overcomes the old column limit. And the algorithm would be more elegant in something like R.,2010-10-20 12:29:56.0,183.0,
5359,3588,0,"@Denis:> i would say it makes the problem worst (i.e. NN kernel should work better on less clumpy data). I have an intuitive explanation but it won't fit here, plus you may want to asks this out on the main board as a separate question (linking to this one) to have additional opinions.",2010-10-20 13:37:36.0,603.0,
5360,3712,0,Thanks Charlie - Is it possible to get the estimates separately for treated and untreated? The lincom command only gives me the difference between treated and untreated.,2010-10-20 13:52:24.0,834.0,
5361,3798,0,"@chl: thanks!  I remember plspm being announced on the semnet list -- for some reason PLS isn't as big on this side of the Atlantic, not sure why.  plotSEMM looks really interesting, can't wait to play with it.",2010-10-20 13:57:35.0,251.0,
5362,3802,1,(+1) Thanks for linking with OpenMx! Really great package that has replaced Mx on my Mac now.,2010-10-20 14:09:25.0,930.0,
5363,3690,0,"Thanks Srikant. Since the group-level effects to which I'm referring often unique, complex, and erratic, I'm not sure I'll be able to quantify them with interaction terms. However, your answer has led me to approach my problem from a different angle and refine my question. I'll restate in more formal terms once I've more clearly defined the problem.",2010-10-20 14:30:45.0,1611.0,
5364,3766,0,"yes, as you said I have 100 objects and if I want to know the similarity between each two sound, it will be (99*98)/2. (s1,s2)(s1,s3)(s1,s4)...(s1,s100)(s2,s3)(s2,s4)...(s2,s100)...(s99,s100).  and I know that (s2,s1)=(s1,s2) so I do not calculate that. so, I will have 4851 measurements for each method(each method calculates the similarity of all the sounds pairwise).and totally I have 6 methods. So, I have 6*4851 data in general. It is a continuous scale of similarity on [0 1]. Thanks.",2010-10-20 14:46:21.0,1564.0,
5365,1251,3,"That's a good answer. In addition, I'd suggest horizontally jittering the points, so they don't overlap, especially if you have more points per group than this. In ggplot2, the geom_jitter() will do that.",2010-10-20 15:00:21.0,6.0,
5366,3805,0,How many percentiles are included in your real data? I hope it's more than in your example!,2010-10-20 15:04:43.0,449.0,
5367,3793,2,What's the reason for the vote to close?  How does this problem not relate to statistical analysis?,2010-10-20 15:31:37.0,919.0,
5368,3794,0,@Jeromy Your idea is a good one but the implementation can be greatly improved :-).  I posted an explanation.,2010-10-20 15:33:30.0,919.0,
5369,3803,0,"@whuber The tree is a neat idea (upvote for that idea) but would it not require lot of memory? I guess it depends on how diverse the dictionary is but I am guessing a reasonably diverse dictionary would require many trees For example, the 'b' tree would start with the letter 'b' instead of 'a' for all those words which do not have 'a' in them. Similarly, the 'c' tree would start with the letter 'c' for those words which do not have 'a' and 'b' but have 'c'. My proposed direct approach seems simpler as it requires a one-time traversal of all the words in the dictionary, no?",2010-10-20 15:49:09.0,,user28
5370,3803,1,"@Srikant: The tree would likely require far less RAM than caching the entire dictionary to begin with.  Are you really concerned about a few megabytes of RAM, anyway?  BTW, there's only one tree, not many: they are all rooted at the empty word.  Your approach, as I have understood it, requires multiple searches of the dictionary (up to 7! of them) on *every iteration*, making it impracticable as @shabbychef fears.  It would help if you could elaborate on the algorithm you have in mind where you write ""see if you can form a word"": that hides a lot of important details!",2010-10-20 16:09:27.0,919.0,
5371,3803,0,@whuber: I realized the fact that there is only one tree after I posted my comment. Reg my approach- I agree that my monte carlo proposal is fuzzy and your answer fleshes out how one can actually implement monte carlo in this setting. I actually meant that the *direct approach* (see my answer) may actually be simpler as that approach requires a one-time operation on the dictionary unlike a monte carlo which requires several thousands of iterations on the tree. Just wondering on the relative merits of the approaches.,2010-10-20 16:15:11.0,,user28
5372,3803,0,"@Srikant I refrained from commenting on your direct approach because it I suspect it gets the wrong answers.  It does not appear to account for the dictionary structure: that is, the subset relationships among words.  For instance, would your formula get the correct answer of zero for all dictionaries that contain all the possible one-letter words?",2010-10-20 16:23:00.0,919.0,
5373,3780,0,"@Srikant I'm confused.  What exactly do you mean by ""the sth word from the alphabet""?  Alphabets contain only letters, not words.",2010-10-20 16:23:50.0,919.0,
5374,3780,0,@whuber Sorry. I meant that m_a counts the number of 'a's needed by the s^th word. I corrected the text.,2010-10-20 16:26:40.0,,user28
5375,3803,0,"@whuber hmmm good point. Perhaps, I am answering the wrong question!",2010-10-20 16:30:03.0,,user28
5376,3803,0,"@whuber re your bet that I could conduct this study in seconds: you overestimate my programming skills! My intuition about MC methods is they should be easy to implement: ""let the computer do the work."" If MC is going to take a lot of work, maybe the exact approach is better. In fact, it seems like once the dictionary is in tree form (+1, BTW), you are a long way towards counting the subsets of letters from the bag that form legitimate words...",2010-10-20 16:34:59.0,795.0,
5377,3780,1,"@Srikant Thanks.  Your formula seems to assume you have to use all k letters to form the word, but I don't think that's what the OP is asking.  (That's not how Scrabble is played, anyway.)  With that implicit assumption, you're on the right track but you need to modify the algorithm: you mustn't repeat the calculation for words in the dictionary that are permutations of each other.  For example, you mustn't subtract both t_{stop} and t_{post} in your formula.  (This is an easy modification to implement.)",2010-10-20 16:35:04.0,919.0,
5378,3769,0,"@kathy, you're welcome; glad to help. :)  BTW, you should accept @chl's answer since he noted the adjustment (click the check mark next to his answer).",2010-10-20 16:38:15.0,251.0,
5379,3803,0,"@shabbychef That's an excellent point.  I suspect the computation is not easy.  The leaves of my tree have the property that (a) each can be permuted into at least one word but (b) no proper subset can be permuted into a word.  We can readily count the number of k-letter sets that contain a given leaf, but we're going to double- (and triple- and quadruple-...) count things that way.  E.g., in English ""a"" is a leaf and so is ""ept"", but ""aaeptxz"" contains *both*.  We could apply PIE (principle of inclusion-exclusion) but it would ferociously complicated to do, I think.",2010-10-20 16:45:10.0,919.0,
5380,3805,0,"It depends on the factor I'm using.  For body weight, I'm actually using more percentiles.  But for something like dust ingestion rates, which has less observed measurements, I am using as few as four percentiles.",2010-10-20 16:45:33.0,1645.0,
5381,3810,0,What do you mean by *power*? If you wish to bring them to a common scale then you can use a linear transformation to convert all scales to a common scale (say 5 point). Likert scales are interval scales and hence linear transformations do not 'destroy' the properties of the scale. Is this what you want/meant?,2010-10-20 16:46:47.0,,user28
5382,3798,0,"@chl: btw, I meant to add that it's shame PLS isn't more noted here, since there seems to be a lot of exciting stuff happening around it, especially with tools being developed (e.g. SmartPLS in addition to plspm).  I read some of Wold's work a while back and some of his ideas are only just being realized (e.g. ""having a conversation with your data"").  I really need to set aside some time to explore it more.",2010-10-20 16:47:53.0,251.0,
5383,3810,1,The usual way to accomplish this is to do the PCA on the correlation matrix instead of the covariance matrix.,2010-10-20 16:52:19.0,919.0,
5384,3766,0,Could you please tell me if you still believe that the best one is Intra-class correlation? Thanks a lot.,2010-10-20 16:54:31.0,1564.0,
5385,3766,0,"@Shadi Huh, that makes a difference; because here you might also be interested in assessing whether your 6 similarity matrix (and not six series of measurement) present some form a variance attributable to the raters. Provided you consider a stacked version of your pairwise similarity (a long vector), the ICC remain applicable; otherwise there exist methods to assess the comparability of (dis)similarity matrix but I feel this should be clarified either in your question or best, in a new question (and mods could close this one) so that others may contribute. Let me ask the mods first.",2010-10-20 17:02:52.0,930.0,
5386,3804,0,"Hello Harlan, can your details be translated in mathematical notation? It would be very helpful to understand the details (for me).",2010-10-20 17:07:40.0,1307.0,
5387,3798,0,"@ars Do you want a list of recommend readings? I also worked with Arthur Tenenhaus who submitted a nice paper with his father (yes, Michel Tenenhaus) to Psychometrika: They are unifying all two-block methods (PCA, CCA, PLS, inter-battery, etc.) thanks to a very neat rewrite of the argmax constraint. I've been playing myself with penalized PLS/CCA (L1/L2) in genomics, but I feel it will bring more interesting on my biomedical data.",2010-10-20 17:14:12.0,930.0,
5388,3801,0,> you could add a link to Efron's bootstrap paper**s**. It's as good a place as any to start.,2010-10-20 17:22:40.0,603.0,
5389,3780,0,"@whuber you are correct; I will edit the question: the words are of any length up to $k$. This is how Scrabble is played, but the question should be self contained.",2010-10-20 17:24:30.0,795.0,
5390,3762,2,"@Shadi Following my latest comment, I asked for closing this question so that you can reformulate a new one by adding precision on your design, especially the fact that you actually have 6 similarity matrices instead of 6 series of measurement. This way, others may provide useful insights into this question. You can still link to this question, but I really feel it call for a new thread with your added clarifications so that everyone can contribute.",2010-10-20 17:34:06.0,930.0,
5392,3809,0,"But that's two parameters to set for one dependent variable! I want to keep the underlying beta-binomial structure in $prior_1$, and just update it, perhaps by shifting the mean without changing the variance, to give $prior_2$. And I want to do it in a principled way, as I only 20% trust that scalar anyway...",2010-10-20 17:57:19.0,6.0,
5393,3806,0,(+1) Very nice answer.,2010-10-20 18:00:50.0,930.0,
5394,3801,0,"@kwak Ok, yet it is easier to put a link to Wikipedia http://en.wikipedia.org/wiki/Bootstrapping_(statistics)#References",2010-10-20 18:06:27.0,88.0,
5395,3804,0,"added some notation, hope it helps clarify!",2010-10-20 18:16:37.0,6.0,
5396,3799,0,"N and k can be huge, but computational power is not a problem.",2010-10-20 18:37:28.0,1643.0,
5397,3809,0,@harlan See edited answer.,2010-10-20 18:42:22.0,,user28
5399,3812,0,Thanks for the pointers to optimal scaling.,2010-10-20 18:53:35.0,919.0,
5400,3761,0,"Thanks, you are right that FNN falls into the category of neuro-fuzzy systems.  The links you gave are helpful.  I think I will contact the author of the paper to see if there are closer implementations.",2010-10-20 18:58:56.0,1127.0,
5401,1251,0,@Harlan: I agree. Although if I had many more points I would probably use a boxplot.,2010-10-20 19:00:39.0,8.0,
5402,3813,0,or *biclustering* (+1).,2010-10-20 19:09:37.0,930.0,
5405,3814,0,Does it extend to justifications received in response to an initial review (where minor and/or major revisions were asked)?,2010-10-20 19:14:34.0,930.0,
5406,3814,0,"@chl: Yes, why not.",2010-10-20 19:19:20.0,8.0,
5410,3813,0,That is what I was looking for. Name of the problem helps a lot. Thanks,2010-10-20 19:35:59.0,1643.0,
5411,3816,1,"+1 for me. This frustrates me, especially when they cite the wrong thing and I've provided the relevant details on how to cite the packages",2010-10-20 19:36:52.0,1390.0,
5412,3818,7,"Huh... You should add a little bit of contextual information here, e.g. at least specify your working hypotheses.",2010-10-20 19:39:55.0,930.0,
5414,3818,3,Definitely needs more information.,2010-10-20 19:41:23.0,5.0,
5415,3800,0,"Thanks for info about Netflix, shame on me that I haven't heard about it earlier :/",2010-10-20 19:44:24.0,1643.0,
5416,3821,1,s.d. is shift invariant.,2010-10-20 19:53:44.0,603.0,
5417,3821,0,Thanks Srikant.  Do you have an idea on how to define the transformation I gave in my example?,2010-10-20 19:54:01.0,253.0,
5418,3821,1,@Tal Y = X + 1?,2010-10-20 19:55:00.0,,user28
5419,3817,3,I'm a little curious about the stepwise regression bullet. What makes stepwise regression so bad? Is it the data dredging and multiple comparisons issue?,2010-10-20 19:56:12.0,1118.0,
5420,3813,0,"+1 Wow, it has a name...",2010-10-20 20:02:05.0,88.0,
5421,3817,13,"The problem is that stepwise procedures completely invalidate all the assumptions and preconditions for ""normal"" inferential statistics based on p values, which are then badly biased (downwards towards being ""more significant""). So basically, the answer is ""yes"", with the caveat that one could in principle correct for all these multiple comparisons (but which I have never seen done). I believe strongly that this is the single most important reason why I see so much research in psychology that cannot be replicated - which in turn leads to a huge waste of resources.",2010-10-20 20:04:48.0,1352.0,
5422,3821,2,You missed a=-1 ;-),2010-10-20 20:07:16.0,88.0,
5423,3820,0,"Actually, you cannot change *one* variable and still keep the same standard deviation. In your example, after all, you are not changing *one* variable but *three*.",2010-10-20 20:08:41.0,1352.0,
5424,3822,2,"Still asking for a Matlab solution? In this case, I let you add this tag. And feel free to add or delete the edits I made if they don't exactly reflect your design.",2010-10-20 20:10:20.0,930.0,
5425,3821,2,"The standard deviation will not change if you simply reorder your data, but this does not seem to be too helpful here...",2010-10-20 20:10:42.0,1352.0,
5426,3821,0,@mbq Good point!,2010-10-20 20:13:28.0,,user28
5427,3821,2,"@Srikant: if we expand our search from linear transformations to analytic transformations (i.e., transformations that can be represented by their Taylor series), I think one could prove that all higher order terms need to vanish for the standard deviation to be unchanged. That is, the only analytic transformation that preserves sd would be adding or subtracting a constant to all values. Might be a nice exercise for statistics undergrads ;-)",2010-10-20 20:14:54.0,1352.0,
5428,3821,0,@stephan I was actually thinking of the same idea but it seemed too much tex typing so I abandoned the idea.,2010-10-20 20:16:31.0,,user28
5429,3818,1,And a question mark... fixed.,2010-10-20 20:18:51.0,88.0,
5430,3822,0,thanks for editing.,2010-10-20 20:20:16.0,1564.0,
5431,3817,8,"@Stephan: I agree, stepwise is a bad idea.  Though, while they may have not made it to psych methods yet, but there are a variety of selection procedures that adjust for bias related to overfitting by adjusting estimates and standard errors.  This is not typically thought of as an issue of multiple comparisons. They are known as shrinkage methods.  See my response in this thread <http://stats.stackexchange.com/questions/499/when-can-you-use-data-based-criteria-to-specify-a-regression-model> and Harrell's ""Regression Modeling Strategies"" or Tibshirani on the lasso.",2010-10-20 20:34:42.0,485.0,
5432,3820,0,"@Stephan I understand the question as referring to an univariate distribution, hence one variable with multiple observations. Am I missing something?",2010-10-20 20:34:50.0,930.0,
5433,3820,1,"@chl - I'd say it's more probable that *I* am missing something. Tal asks about ""changing one *value*"", and I should have written about ""values"", not ""variables"" in my comment, where ""value"" to me sounds much like ""observation"".",2010-10-20 20:38:47.0,1352.0,
5434,3817,4,"@Brett Magill: +1 on that, and yes, I know about shrinkage and the lasso. Now all I need is some way to convince psychologists that these make sense... but people have been fighting with very limited success just to get psychologists to report confidence intervals, so I'm not too optimistic about psychologists' accepting shrinkage in the next twenty years.",2010-10-20 20:42:34.0,1352.0,
5435,3820,1,@Stephen In the example he changed 2 to 5: only one value was altered.,2010-10-20 20:47:54.0,919.0,
5436,3821,1,"@Stephen Not true for analytic transformations or even the most general transformations.  Apply *any* transformation, such as a Box-Cox transformation.  Determine the new variance.  If it exists and is nonzero, rescale the result to match the original variance.",2010-10-20 20:50:20.0,919.0,
5437,3820,1,"@Tal I changed ""roles"" to ""rules"" in your question and hope that was what you intended.",2010-10-20 20:57:13.0,919.0,
5438,3820,0,"@chl It's univariate all right but I take it that he is asking about the *observations,* not the variable.",2010-10-20 20:58:03.0,919.0,
5439,3820,0,"@whuber I understood it in the same way (or I think so): we have a series of observations (= observed numerical values for a given variable), is there a way to keep the same SD by altering one or more of these values?",2010-10-20 21:08:38.0,930.0,
5440,3820,0,"@chl We take it in the same way.  Although it is based on a different interpretation, Srikant's answer is nevertheless an interesting response.  As the comments afterwards show, though, it doesn't lead to very interesting solutions: there are too many ways one can change a random variable while preserving its variance.",2010-10-20 21:13:51.0,919.0,
5441,3823,17,"Reminds me that in my early days as a referee i spent *far* too long reviewing a statistical paper that was eventually rejected by that particular journal, but the other referees and I suggested a more useful application for the method, and I also sketched an algebraic proof to replace an unsatisfactory simulation study in the manuscript. The authors have since got two published papers out of it. I'm not *annoyed* by that, but an acknowledgement such as ""we thank referees of an earlier version of the paper for helpful comments"" would have been good manners.",2010-10-20 21:23:38.0,449.0,
5442,3820,0,"@whuber: 'Interesting' is subjective. I believe that both interpretations are equally valid, interesting in their own right.",2010-10-20 21:44:50.0,,user28
5443,3823,0,"@onestop Yes, I can imagine how disappointing such a situation might be...",2010-10-20 21:56:10.0,930.0,
5445,3812,0,Thanks to all of you for your very good comments and suggestions.,2010-10-20 22:08:02.0,1647.0,
5446,3712,0,"Estimates of what? Using a version of the lincom statement above, you get the treatment effect for each quintile (the estimate of the treatment effect for the first quintile is just the coefficient on treatment).",2010-10-20 22:08:24.0,401.0,
5447,3813,0,"@mbq don't feel bad, I only learned the name a week ago!",2010-10-20 22:40:01.0,795.0,
5449,3792,0,"My background is civil/hydraulic engineering, i try to understand statistical discussions :)",2010-10-21 00:13:10.0,1637.0,
5450,3798,0,"@chl: wow, you keep good company; very cool!  I would really appreciate some references, thanks!",2010-10-21 01:02:09.0,251.0,
5451,3831,0,"the velocity components (3D) belong to a turbulent flow. Although the trend follows a sinusoidal pattern, there are high-frequency variations. in my case the trend follows 1/12hr frequency but measurement frequency is 1 Hz, i.e. if at time t speed is 20cm/s, speed at t+1 the speed might have any value saying 20+-5 cm/s. Hence, essentially i cannot predict the next value in a turbulent field.",2010-10-21 01:11:11.0,1637.0,
5452,3831,0,could you please explain more about checking ID condition?,2010-10-21 01:11:48.0,1637.0,
5454,3831,0,"Well, I do not know enough about your context to give a sensible comment but you have to model the velocity vector as a time series of some sort with an additive error term which you assume is iid. You estimate the model and test if the residuals are iid. Perhaps, you could ask another question with some context of the data and your goals are as far as data analysis is concerned. Perhaps, the issue of iid or not is not that relevant given what you want to achieve?",2010-10-21 01:39:33.0,,user28
5455,3828,1,+1 Log-linear sounds good given the plentiful requirements! :)  I'd also recommend Agresti's text on categorical data analysis.,2010-10-21 03:24:56.0,251.0,
5457,3823,0,"@onestop, I hear you brother :-)",2010-10-21 03:51:03.0,1307.0,
5458,3809,0,"@Srikant, a (hypothetical) Bayesian will have strong disagreements with your answer. She would have done something like this: prior  $\\propto f(\\alpha_1,\\beta_1|-) \\alpha + f(\\alpha_2,\\beta_2|-) (1-\\alpha)$ and then put prior on $\\alpha$. However, your answer will be a little less flexible than the Bayesian's answer.",2010-10-21 04:16:41.0,1307.0,
5459,3823,19,"A few weeks ago I was given a paper to review and found that 85% of it had been published in another journal...by the same authors.  That, too, is still considered plagiarism.  For the last several years I have routinely submitted chunks of papers--especially abstracts, introductions, and conclusions--to Web search engines *before* doing any review.  I want to be sure the work is original before I invest any time in reading it.",2010-10-21 04:40:42.0,919.0,
5460,3794,0,@whuber I kept my answer pretty software-neutral. It's good to have the Excel implementation.,2010-10-21 05:03:27.0,183.0,
5461,3794,1,"@Jeromy I see what you mean.  I was responding to your comment in answer to Alex's question.  Of greater interest to me is the idea that Excel can be used as a prototype for commands in R, Mathematica, (and even APL if anyone remembers it), provided you use its array-oriented procedures.  Excel itself is pretty bad as a statistical tool but good for rapid prototyping because it's easy to see and correct one's mistakes.",2010-10-21 05:09:15.0,919.0,
5463,3817,8,"I'd also argue that in psychology maximising prediction is not typically the theoretical aim, yet stepwise regression is all about maximising prediction, albeit in a quasi-parsimonious way. Thus, there is typically a disconnect between procedure and question.",2010-10-21 06:28:49.0,183.0,
5464,3794,0,@whuber good point,2010-10-21 06:47:18.0,183.0,
5465,3779,0,"As far as I remember Scrabble does not allow one letter words, so at least that part of the problem is solved ;)",2010-10-21 07:07:57.0,582.0,
5466,3821,0,"To Whuber - ""rescale the result to match the original variance"" - here's a thought, thanks!",2010-10-21 07:11:07.0,253.0,
5467,3820,0,"Thanks everyone for the replies.  I meant ""observations"" indeed.  I'm honored to be able to converse with all of you through here.",2010-10-21 07:13:41.0,253.0,
5468,3825,0,Amazingly detailed answer Whuber - thank you very much!  This obviously leaves open the questions of non additive transformations - but for my curiosity needs - I am satisfied.  Thanks again!,2010-10-21 07:16:47.0,253.0,
5469,3833,0,"I like this. I would add that (1) the OP is likely to be interested in studying asymmetrical relationships between all three variables, so that a SEM approach would make sense; proceeding this way would also (2) allow to account for specific measurement error at the level of the scales.",2010-10-21 07:27:34.0,930.0,
5470,3818,0,"(1) presumably you have three groups of variables and within each group, you have multiple variables (e.g., perhaps you have the Big 5 factors of personality within the personality group); (2) calling your variables categorical makes people think of 'unordered categorical' when the variables are probably best treated as numeric. Thus, in addition to providing more information, you may wish to change the title of your question.",2010-10-21 08:14:28.0,183.0,
5472,3827,0,"+1 esp as you gave a link, and hence traceability and attribution!",2010-10-21 08:43:51.0,449.0,
5475,3827,0,@onestop Any idea for the difference between the two series of results?,2010-10-21 09:58:02.0,930.0,
5477,3840,1,I would suggest you to work on clarity of the question; this helps in getting answers.,2010-10-21 10:56:34.0,88.0,
5478,3840,0,"@mbq, i have updated to my post",2010-10-21 11:15:01.0,1655.0,
5479,3795,0,"@onestop, with graphviz I was not able to draw an arrow to the center of a line without ""cracking"" the arrow that is pointed to. See my question at stackoverflow http://stackoverflow.com/questions/3718025/graphviz-dot-how-to-insert-arrows-from-a-node-to-center-of-an-arrow",2010-10-21 11:33:50.0,767.0,
5480,3847,0,Thanks for the ggplotized version :),2010-10-21 13:02:54.0,930.0,
5481,3689,0,"One of the best ways to improve a forecast is to average different forecasts (from different models, or from different experts or both...). You may want to look around on forecastingprinciples.com or look through past issues of the International Journal of Forecasting.",2010-10-21 13:02:54.0,1352.0,
5482,3488,0,"Other posters have noted that stationarity is important here. If both series have a linear upwards trend (one kind of nonstationarity), they will be correlated - but all the correlation may be due to the common trend, which may or may not be what we are interested in.",2010-10-21 13:06:39.0,1352.0,
5483,3841,0,"I am dealing with a similar problem, I guess you can use `pgmm` from **plm** package but as your response variable is binary I don't know exactly how to do it. Maybe others can elaborate... (And yes you are right: my understanding is whenever you have an endogenous variable, in this case the lagged value, you can't use REML to estimate because it is biased, so you need to use GMM.)",2010-10-21 13:08:39.0,1443.0,
5484,3846,0,"I'm sorry, it does not. I don't know how 2(k-1)sum(vi(di-kvi)) can become sum(di-kvi)^2 after k was estimated. Obviously, I have failed to grasp the significance of the constraint imposed. Please explain the significance of the constraint's role in solving the equation...Thank you.",2010-10-21 13:21:25.0,1636.0,
5485,3818,2,I'm inclined to close this question if further information isn't provided. At present it's impossible to see what is being asked.,2010-10-21 13:43:57.0,8.0,
5486,3791,0,"hi S.  yes we did transformed the data, then we standardize against the largest value, then transformed the standardized value",2010-10-21 13:49:28.0,1627.0,
5487,1966,0,"your experience -- has run on 1k / 100k points, in 1d / 2d / 3d -- would also be useful.",2010-10-21 14:16:47.0,557.0,
5489,3791,0,but was your design a Randomized Complete Block Design?,2010-10-21 14:32:09.0,1307.0,
5490,3845,0,I haven't used this package extensively myself. I think individual effect is indeed what you want. Two step will yield smaller standard errors (see the paper i linked to in my original answer). The number of instruments should be smaller than the number of years-1 you have. Try 2:9 (i think you have 9 years per region). You could also start by using the gretl implementation (which is simpler to use) and come back to R whence you have mastered the model a bit.,2010-10-21 14:33:34.0,603.0,
5493,3825,0,"@Tal I hope you're satisfied, because I can't do better than this: *every* transformation is additive!  Consider replacing x_i by arbitrary z_i; just define y_i = z_i - x_i.",2010-10-21 14:53:51.0,919.0,
5494,3807,0,Thanks @whuber!  That's really helpful and I'm going to try it out.,2010-10-21 14:54:07.0,1641.0,
5496,3793,0,"@whuber, i don't understand the comment just above this one...it sounds like its in reply to something (that I don't see.)    (I also couldn't figure out how to contact you directly to ask that...)",2010-10-21 14:59:33.0,1641.0,
5497,3793,0,"Members with sufficiently high reputation have the option to ""close"" a question.  This is typically done when a question is vague or off-topic.  It takes five votes to close, but before all five votes are in they are anonymous.  In this case somebody cast a vote shortly after your question appeared.  It's considered polite to explain such votes and to provide constructive suggestions for making the question acceptable, but that courtesy was not offered in this case.",2010-10-21 15:04:07.0,919.0,
5498,3845,0,"@kwak but the model specification is correct, isn't it? Btw, why the numbers of instruments should be smaller than years-1? If you look the example of **plm** vignette with EmplUK data set (p.23) they use 2:99 (I don't really understand why)?  I came across the following formula for number of instruments (T-1)*(T-2)/2+(T-3)...I'll have a look at gretl, thx!",2010-10-21 15:08:19.0,1443.0,
5500,3849,3,You will likely get excellent and authoritative answers to this question on the stata list (http://www.stata.com/statalist/ ).  It is THE place to go with issues about how Stata works.,2010-10-21 15:11:22.0,919.0,
5501,3849,1,i would agree with @whuber but no harm in asking here either. The question is on-topic as far as this site is concerned.,2010-10-21 15:15:20.0,,user28
5502,3809,0,"@suncoolsu Sure you can do that as well. However, if you choose the prior for $\\alpha$ to be very tight around 0.8 then your suggestion essentially collapses to mine. However, I agree imposing a prior on $\\alpha$ is a bit more flexible than assuming that it is 0.8.",2010-10-21 15:29:53.0,,user28
5503,3725,0,"@ronaf Allegedly Excel 2010 now has both, COVARIANCE.P and COVARIANCE.S.  However, the documentation is so dismal one cannot really be sure!  (In the older versions if you know about covariance at all you're probably knowledgeable enough to be able to multiply the result by n/(n-1) in the formula.)  For more info visit http://office.microsoft.com/en-us/excel-help/statistical-functions-reference-HP010342920.aspx",2010-10-21 16:36:14.0,919.0,
5504,3779,1,"@nico good point, but I think this is only for mid-game. 1 letter words either don't require one to play a letter, or would allow one to place a single letter anywhere on the board, both clearly unacceptable. However, I was thinking of the opening move. In fact, the question can be compactly stated, for those familiar with Scrabble, as ""what is the probability that the first player will have to pass?""",2010-10-21 16:43:55.0,795.0,
5506,3856,1,what type of data are you working with? From your examples I assume you are dealing with time series?,2010-10-21 17:57:52.0,582.0,
5507,3827,1,"@chi Oh dear, I think -sizefx- contains a coding error! It's ignoring the values of one variable for which the other variable is missing (the problem is with the use of -marksample-). I'll contact the author. I've never used it before myself (i'm relieved to say...)",2010-10-21 19:06:52.0,449.0,
5508,3827,0,"You also make a good point that it would be better if -sizefx- supported a ""by()"" option. I'm also not sure why it's referring to Cohen's d as Hedges' g. The latter really involves a more complex bias correction. All in all, i'd advise against using -sizefx- for the time being!",2010-10-21 19:21:36.0,449.0,
5509,3864,0,"Agreed that things need to be user friendly.  Since people get very protective over their working practices, any changes must make people's lives easier or they'll fail.",2010-10-21 20:13:18.0,478.0,
5510,3863,3,Absolutely agreed on the version control.  I use it; as do a substantial proportion of the developers and statisticians.  (I'd like to see 100% adoption but that's another pipe-dream for now.)  The hard bit is getting non-techies to use it.  Any ideas appreciated.,2010-10-21 20:18:11.0,478.0,
5511,3793,0,"Your question makes sense, from a statistical POV. I don't understand the vote to close, given that there are some very basic questions for which browsing online help would have suffice (and is educative)... and those questions were never voted down or voted to be closed. Anyway, that's community life. However, may I also remind you that you can also upvote the question you accepted as a the correct one.",2010-10-21 20:48:56.0,930.0,
5512,3827,0,"@onestop (+1) Good to hear that you spotted the problem (not enough Stata experience myself). Anyway, the distinction between Cohen's d and Hedges's g is rather confusing, even on Wikipedia (I admit this is not really a reference, though). In addition to not supporting a `by()` option, no CIs are computed (we'd need to rely on a non-central $t$ distribution, or use some kind of bootstrap approach).",2010-10-21 21:02:58.0,930.0,
5513,3862,1,Excellent links.  I think two important messages for me to pass on are: we need more automated data checking and I need to start explaining about separating data entry and data presentation.,2010-10-21 21:06:04.0,478.0,
5514,3853,2,"I can understand that reading online help might seem a tiresome activity at first sight (still, it's very educative and often help to capitalize knowledge on a particular software), but could you at least accept answer(s) you find helpful for your ongoing R activities?",2010-10-21 21:09:55.0,930.0,
5515,3863,2,"@Richie Cotton: I don't know why, but version control seems to be a difficult concept for non-techies to grasp. People continue to just make some changes to a file, rename it and send it via email. How I hate those ""PaperDraftCorrectedByJohnRevision3RewroteByLeslie-NewVersion3.doc"" files...",2010-10-21 21:14:05.0,582.0,
5517,3868,1,"Have you tried dropping that zero from the analysis?  I don't think zero is a part of the gamma distribution.  Also: are you really sure it's zero, and not just very, very small?",2010-10-21 23:57:17.0,71.0,
5518,3846,0,"""The idea is the following insight: Suppose that we choose k such that the above constraint is satisfied then it immediately follows that:
                                          ∑i(di−lvi)2=∑i(d−kvi)2+(k−l)2∑iv2i                                                      ""                                                                Could you please show the intermediate steps? I really cannot see how that equation follows immediately from satisfying the constraint. Sorry for the inconvenience caused.",2010-10-22 00:36:17.0,1636.0,
5519,3252,0,I'm not quite sure why the size of your sample matters. Can you offer anymore specific reasoning as to why you think it is different for small n than it is for big n?,2010-10-22 03:39:10.0,1036.0,
5520,1116,0,"Although this is the only Paul Allison text I have read, I would like to say various other texts of his have come highly recommended for me to read. Even if you don't use SAS (I have in the past so I was familiar with the code, although when I read this book I had completed migrated to other software) it is a really excellent book on survival analysis.",2010-10-22 03:47:03.0,1036.0,
5521,3650,0,"Both of the answers by Srikant and whuber were equally informative in helping me answer this question. I wish I could give both an accepted checkmark, but I chose whubers response simply for the added context in helping to demonstrate how I would integrate given my prior distributions.",2010-10-22 05:00:40.0,1036.0,
5522,3651,0,"@Srikant, I assume it would be inappropriate to use the observed 100 realizations for my prior distributions in this context? Hence the need for ""priors"" to begin with.",2010-10-22 05:02:33.0,1036.0,
5523,3252,1,"@Andy Because then it becomes very difficult to consider an holdout sample and/or class imbalance with very limited sample size ($13<n<25$) generally yields larger classification error rate when applying CV; some individuals might be considered as outliers when studying bivariate distributions; and measures gathered on instruments with their own measurement error are less reliable (small $n$, large $\\sigma$). In a certain sense, it is sometimes difficult to disentangle an unexpected relationship from an artifact.",2010-10-22 05:29:38.0,930.0,
5524,3876,1,Should one set lmer's REML argument to FALSE when generating those models since they'll eventually be compared using the anova() function?,2010-10-22 05:47:06.0,364.0,
5525,3875,1,IMHO t-test may not be a good idea here,2010-10-22 06:52:40.0,1307.0,
5526,3876,6,"When comparing models using likelihood-ratio tests, you can compare different *random effects* structures using REML (restricted/residual maximum likelihood, as above), but you must use ML (maximum likelihood) to compare different *fixed effect* models.",2010-10-22 07:36:08.0,449.0,
5527,3872,0,"How are you quantifying the results? Numbers of events, or continuous measurements (on a scale with a true zero, i hope) or...?",2010-10-22 07:39:48.0,449.0,
5528,3877,0,"@onestop (+1) Nice follow-up. My understanding of the question was ""how to calculate an SMD in the case of a logistic regression?"". Maybe worth clarifying that this is indeed the case.",2010-10-22 07:57:54.0,930.0,
5529,3871,0,"Thanks Rob and Srikant for your explanation. I understand independent events now. Just out of curiosity, if I were to draw the Venn Diagram of aforementioned events, are there any characteristics that I can look for to reliably identify independent events?",2010-10-22 08:15:53.0,1636.0,
5530,3848,0,"Thanks for your input. Will test this approach and will update. Also, do you think, i can use Kolmogorov-Smirnov Test in my case. Please see the plot, http://img254.imageshack.us/img254/9645/25971376.gif",2010-10-22 08:40:08.0,1655.0,
5531,3846,0,"Thank you very much, Srikant.",2010-10-22 09:07:18.0,1636.0,
5532,3871,0,"@Sara No, You cannot. Venn diagrams will only help you write down the formulas for the various possible relationships between the events (e.g., $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$. But, the diagram itself will not tell you anything about the probabilities of the events under consideration.",2010-10-22 09:34:38.0,,user28
5533,3871,0,@Srikant: Noted. Thank you so much for your assistance.,2010-10-22 09:44:16.0,1636.0,
5534,3847,0,"I hope you don't mind, but I added the output of your commands.",2010-10-22 10:16:36.0,8.0,
5535,3847,0,@csgillespie no problem :),2010-10-22 10:52:37.0,1443.0,
5536,3252,0,"I think I can understand that sentiment if what your interested in is solely classification. I think for causal inference the problems with data snooping are the same (i.e. the problems aren't solved by increased power to identify relationships). I'll try to formulate this opinion into an answer. I may ask a question on the main forum in the meantime about the use of cross-validation for causal inference, as I have not come across any work in my field that does this.",2010-10-22 12:48:13.0,1036.0,
5537,3876,0,"@onestop: Ah! Good to know, thanks!",2010-10-22 12:56:16.0,364.0,
5538,3833,0,Thank You Jeromy for clearing the concept. I am really grateful. Can you please explain PCA.,2010-10-22 13:12:10.0,1649.0,
5540,3252,1,"@Andy Thanks. Hopefully, your question will receive a lot of interesting answers.",2010-10-22 13:40:25.0,930.0,
5541,3865,0,"Thanks @Andy W, looks relevant but Wiley want $$. ""Nature of my data"": messing about with Kdtree variants / KNN; as I said, looking for an overview.",2010-10-22 13:45:08.0,557.0,
5542,3891,0,depends on which package list ? http://www.stat.ucl.ac.be/ISdidactique/Rhelp/doc/html/packages.html,2010-10-22 14:15:05.0,603.0,
5544,3891,0,@kwak Your link must be to an outdated mirror.,2010-10-22 14:17:41.0,5.0,
5546,3877,0,"Thanks, these answers are incredibly helpful.  To clarify (hopefully), I intend to calculate the Cohen's d statistic for the difference in mean values of the predictions generated by a logit model.  I suspect that there are simpler ways to estimate the effect size of a logit model, but I want to compare results to a meta-analysis reporting Cohen's d.  Thanks again.",2010-10-22 14:28:42.0,,Fred Vars
5547,3891,0,> Ran/build on latest R (here on ubuntu). Thanks.,2010-10-22 14:30:59.0,603.0,
5548,3865,0,"@Denis, I have added in a link to the pdf. Often times if you search for the publication on Google Scholar a version is posted free to the public. The supplementary material I posted is free as well, and the R code would be insightful even without access to the paper. The description ""messing about with Kdtree variants / KNN"" is not specific enough to be helpful. Based on that I imagine what I quoted is not that useful unless you are explicitly interested in geographic data.",2010-10-22 14:31:23.0,1036.0,
5549,3886,0,"> Can you add a link (paper, explanation, hypothesis,...) to the denominator used in the first one ? (i.e. your $\\hat{\\sigma}_1$) - thanks.",2010-10-22 14:34:13.0,603.0,
5550,3720,0,"@kwak, When I get the time I will post the question on the role of regression model assumptions when one is solely interested in prediction. Feel free to post it yourself in the meantime though if you would like.",2010-10-22 14:44:15.0,1036.0,
5551,3884,0,Some very interesting points here.  Convincing people to simplify and standardise their spreadsheets is likely to be more successful than getting them to abandon them.  Also I had no idea that version control could integrate with Excel.  Nice to know.,2010-10-22 14:46:21.0,478.0,
5552,3885,2,"Don't these become unbiased only in conjunction with additional distributional assumptions?  For example, the relationship between the range and the SD *has* to depend on the shape of the distribution.  In that context, it's worth observing that many more unbiased estimators of SD have been studied, such as those based on linear combinations of quantiles and ranks.",2010-10-22 15:06:59.0,919.0,
5553,3872,0,"@pom I hope you can clarify this query.  In addition to the issues raised by @onestop, your modification of both ""samples"" and ""models"" by the word ""independent"" makes one wonder about your precise meaning and your synonymous use of ""ratio"" and ""difference"" in the same sentence raises questions about what you mean by those words.  One possible interpretation is that each sample consists of a set of ratios; another interpretation is that you are estimating some kind of statistic in each sample, taking the ratio of those two numbers, and want to compare it to some standard value (such as 1.0).",2010-10-22 15:13:55.0,919.0,
5554,3871,1,"@Sara, if you look at a probability density function http://en.wikipedia.org/wiki/Probability_density_function of the two events fxy then you will get a rectangular distribution if they are independent.",2010-10-22 15:18:51.0,1673.0,
5555,3877,1,"Thanks Fred. I've looked up some old refs and an alternative way to convert a log-odds ratio (i.e. the coef for a binary variable in a logistic regression model) to an effect size comparable to Cohen's d is to multiply the logOR by √3/π. See:

Hasselblad V, Hedges LV.  Meta-analysis of screening and diagnostic tests.  Psychological Bulletin 1995; 117:167-178. http://content.apa.org/journals/bul/117/1/167

Chinn S.  A simple method for converting an odds ratio to effect size for use in meta-analysis.  Stat Med 2000;19:3127-3131. http://www3.interscience.wiley.com/cgi-bin/abstract/75500445",2010-10-22 15:22:06.0,449.0,
5556,3894,1,That wouldn't measure dispersion (no dimensionless value possibly could): it's a measure of *shape* for a non-negative variable.  The range itself is a (poor) measure of dispersion.,2010-10-22 15:23:43.0,919.0,
5557,3894,0,@whuber: range is a difference between max and min values.,2010-10-22 15:25:08.0,219.0,
5558,3894,2,"I think everyone knows what the range is, but there are two possible areas in which it can be applied: to a *sample* or to a *distribution*.  In the latter case the range is not often used because the range is often infinite.  Thus I assumed you were asking about a sample.  Regardless, in either case the ratio of the range to a central statistic cannot measure dispersion.",2010-10-22 15:30:30.0,919.0,
5559,3896,1,"That's a reasonable guess.  It still does not measure dispersion,though!",2010-10-22 15:31:17.0,919.0,
5560,3868,3,"@Matt Dropping a zero is usually not a good idea unless the dataset is so large that losing an extreme value will make no difference in the analysis.  Your implicit suggestion of treating it as very small is good, but then the result can be sensitive to the value chosen.  One could check for that in various ways.  A more formal approach is to treat the zero as a left-censored value (censored perhaps at the second lowest value) and use methods that handle censored or interval-valued data (which includes ML).",2010-10-22 15:35:35.0,919.0,
5561,3871,0,@Kortuk: Thank you for the link. I will study it.,2010-10-22 15:37:27.0,1636.0,
5563,3070,0,"another oddity of Matlab: `std(randn(1))` returns `0`, not `nan`. This makes utterly no sense.",2010-10-22 16:08:29.0,795.0,
5564,3803,0,"@whuber I really want to see the PIE solution, but it looks like MC would have to suffice.",2010-10-22 16:10:59.0,795.0,
5565,3865,0,Thanks again @Andy W. Does my wish for *both* analysis and synthesis make sense ?,2010-10-22 16:12:44.0,557.0,
5566,3871,1,"@Sara, the wiki is not perfect for the multivariable case. There are better sources. but a prob density function of 2 variable can be very interesting.",2010-10-22 16:15:21.0,1673.0,
5567,3779,0,"@nico Thank you for that clarification.  Theoretically a similar issue pertains in dictionaries containing all possible two-letter combinations as words: when that's the case, any hand of 2 or more letters automatically contains a word.  @shabbychef's comment about mid-game shows how irrelevant the original question is to most of Scrabble, because in mid-game you have available an array of word parts (prefixes, suffixes, and even middle sections) in addition to the 7 letters in your hand.  This greatly increases the chances of being able to make a word.",2010-10-22 16:29:30.0,919.0,
5568,3897,1,With one df the range is not even an approximation--it's directly proportional to the SD :-).,2010-10-22 16:31:05.0,919.0,
5569,3865,0,"@Denis, yes your distinction between analysis and synthesis make sense given your description in the question (although I would use the word ""simulate"" instead of synthesize). I'm not sure if reducing clustering to a single coefficient for simulation will be all that insightful, and the geographic coefficients for clustering will likely be harder to implement in higher dimensional space and would probably take custom coding. The more refined a question the more applicable answers you will receive are.",2010-10-22 16:33:14.0,1036.0,
5570,3873,0,"This is such a nice summary!  I am grateful for your effort in sharing it with us.  I can't help upvoting it despite your appeals not to :-).  (I don't think it will change your rep, anyway, and there is a lot of merit to having it appear close to the original question if it does get substantially upvoted.)",2010-10-22 16:38:54.0,919.0,
5571,3887,0,Python isn't widespread in our organisation but it looks like an interesting project.  I'll see if I can pinch some ideas on how things should be done from their documentation.,2010-10-22 17:02:17.0,478.0,
5572,3829,0,Agree -- struggling to understand what the author(s) meant before even evaluating the scientific content is really annoying.,2010-10-22 17:04:53.0,1355.0,
5573,3880,0,I don't really understand why this option was added to `hist()` (`labels=T`) and not `barplot()`.,2010-10-22 18:07:58.0,930.0,
5574,3883,0,(+1) This is the hard way :) I like it.,2010-10-22 18:08:58.0,930.0,
5575,3877,0,"@Fred Still, it is not clear (sorry for that!) if your model includes a single predictor or more. In the latter case, SMDs are good summary statistics but they are not adjusted for other covariates of interest. @onestop I think there's a typo and you meant ""the coef relating a predictor to the binary outcome"" instead of ""the coef for a binary variable..."".",2010-10-22 18:18:45.0,930.0,
5576,3689,0,"Thanks Stephan, I'll check it out.",2010-10-22 19:33:57.0,1611.0,
5577,3902,1,The easy way to find out is to feed it some simple datasets having ties!,2010-10-22 21:12:33.0,919.0,
5578,3891,1,That mirror was last updated in October 2003. It's better to use http://cran.r-project.org/web/packages/,2010-10-23 00:10:01.0,159.0,
5579,3904,0,+1 It will take me some time to digest your answer. I admit that asking for an intuition for the CLT within the constraints I imposed may be nearly impossible.,2010-10-23 01:42:43.0,,user28
5580,3895,0,"""Orthogonal regression"" is another thing one might encounter when looking for methods to deal with contaminated abscissas and ordinates.",2010-10-23 02:31:14.0,830.0,
5581,3833,0,"@Neelam no problems. PCA = Principal Components Analysis, a data reduction technique: see this excellent tutorial by Lindsay I Smith: http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf",2010-10-23 02:40:22.0,183.0,
5582,3907,0,I think the answers to this question will be helpful to you. http://stats.stackexchange.com/q/3611/1036,2010-10-23 02:43:04.0,1036.0,
5583,3907,1,"Have you seen the August issue of The American Statistician?  The article ""Two Pitfalls in Survival Analyses of Time-Dependent Exposure"" covers exactly this issue.  Abstract at  http://pubs.amstat.org/doi/abs/10.1198/tast.2010.08259",2010-10-23 02:44:43.0,919.0,
5585,2700,1,Good call on the Lindsay I Smith manuscript - just read it today; very helpful.,2010-10-23 05:58:21.0,901.0,
5586,3885,0,"Yes, indeed. Under the normality assumption.",2010-10-23 07:21:27.0,339.0,
5587,3907,0,"@whuber Sorry, I can't see why that AmStat article is relevant. Where is the time-*dependent* exposure in the question?",2010-10-23 07:25:01.0,449.0,
5588,3877,1,"@chl Yes, thanks for the correction. Was a bit more than a typo to be honest, more of a thinko. This conversion requires some assumptions, of course - i think they amount to requiring that the predictor should have (approx) the same variance in those with and w/o the outcome after a transformation, if required, to make the distribution of the predictor approximately normal/logistic (strictly logistic, but the two are hard to distinguish without a huge dataset).",2010-10-23 07:38:39.0,449.0,
5590,3886,0,"1. Quality Control and Industrial Statistics - Duncan
2. Statistical Quality Control - Grant, Leavenworth
3. Introduction to Statistical Quality Control - Montgomery",2010-10-23 08:51:34.0,339.0,
5591,3912,0,"Why do you say hypothesis tests are often meaningless for odds ratios, any more than any other effect estimate? I'd stress instead that confidence intervals are more useful than standard errors for odds ratios and other estimates with asymmetric sampling distributions in finite samples.",2010-10-23 08:53:17.0,449.0,
5592,3914,0,Thanks. But what about frequentist confidence intervals specifically? Are there any circumstances at all where they would be relevant?,2010-10-23 09:33:52.0,1393.0,
5593,3895,0,+1 This is yet a niche only in statistics; more complex least squares methods (not only adding X variability but also different penalties for points based on error approximations) are common in experimental physics; ROOT framework has dozens of such.,2010-10-23 11:38:36.0,88.0,
5594,1280,2,http://imgur.com/0dsVC.gif,2010-10-23 11:59:06.0,830.0,
5595,3912,0,"@onestop Well, I was partly thinking of what you say about ""asymmetric sampling distributions..."" (and it seems I was not so clear), but also of the fact that in epidemiological studies we are generally most interested in CIs (that is, how precise is our estimate) than HT.",2010-10-23 12:33:33.0,930.0,
5596,3918,2,"@Srikant. No! This is how classical CIs bite. Let's assume for simplicity that the amount of cereal filled in a box is normal with mean $\\mu$ and variance $\\sigma^2$. The confidence interval of $\\mu$ is based on its *sampling* distribution which is different. A particular CI may be way off due to sampling errors and then it will have no relation to how the machine performs. If you were to repeatedly sample and repeatedly form CIs then 95% of them would be right, but that is no consolation.",2010-10-23 13:44:59.0,1393.0,
5597,3920,0,"(+1) `psych::alpha` does a better job than `score.items`, thanks for adding that. I never remember the one I should use!",2010-10-23 13:53:50.0,930.0,
5598,3918,1,"@Jyotirmoy Of course, a specific CI may be way-off. In other words, there is a 5% chance that the CI does not contain the true value. Nevertheless, the interpretation I gave is consistent with how CIs are actually constructed. We imagine using the method repeatedly and construct the CI such that the probability that the observed CI contains the true value is 0.95. Notice that my answer does not say anything about the probability of where the true value actually lies as that is a statement that can only be made with credible intervals and not confidence intervals.",2010-10-23 14:03:22.0,,user28
5599,3918,1,"@Jyotirmoy Lower/Upper bounds for a $(100-\\alpha)$% CI of an observed mean are constructed under $H_0$, where the sampling distribution of a mean (or a difference of means) is the one you assumed depending on your sample ($t$ or $z$ distribution). I found Srikant's answer correct, and his interpretation doesn't seem to go beyond the experiment that was framed. CIs are random variables.",2010-10-23 14:09:23.0,930.0,
5600,3918,0,"@Srikant. I perhaps misunderstood ""method=machine"" in the answer. I thought you were saying that 95% of all boxes coming out of the assembly line would have weights within the 95% confidence interval derived from a particular sample of the boxes.",2010-10-23 14:43:15.0,1393.0,
5601,3914,0,"I believe having different priors is a non issue (at least from the objective Bayesian point of view), if it happens that you have different knowledge about the situation at hand. We meed to see the priors as a way of casting our a priori information. I know that it is not simple...",2010-10-23 14:57:15.0,1443.0,
5602,3914,0,"@Jyotirmoy About bayesian vs. frequentist approaches, interesting points were made here: http://stats.stackexchange.com/questions/1611/do-working-statisticians-care-about-the-difference-between-frequentist-and-bayesi/3370#3370",2010-10-23 15:23:13.0,930.0,
5603,3925,3,(+1) This remind me of the `pairs.panels` function in the `psych` package by W Revelle.,2010-10-23 20:03:15.0,930.0,
5607,3924,0,I'm missing something: you have your dataset so you compute its mean *m* and standard deviation *s*.  The value at *z* standard deviations from the mean therefore equals m + z*s for any *z* you like.  Where is the problem?,2010-10-23 21:19:14.0,919.0,
5609,3927,0,"I've been looking at my data, and I misrepresented the question a little bit.",2010-10-23 21:47:35.0,,Jane
5610,3927,0,@Jane If the added context makes my answer irrelevant I will delete my answer or perhaps edit to fit your context better.,2010-10-23 21:49:08.0,,user28
5611,3924,0,"@whuber Perhaps, the problem is that we cannot assume that the distribution is normal? It seems to me that the values are bounded from below (at 0) and from above (at 255). The normal may not make sense unless the standard deviation is very small and the mean far from the the boundary points. @Dan Could you please clarify?",2010-10-23 21:51:59.0,,user28
5612,3925,0,"Interesting. I did come across that code, but never knew it also existed in the psych package.  I am sure it inspired me in someway when I wrote that post (I should add this to the credits on the post...)",2010-10-23 22:11:25.0,253.0,
5613,3924,1,"@Srikant That's a reasonable guess.  But nothing in the current statement requires assuming any kind of distribution at all.  Perhaps the question really is about how to *interpret* values of the form m + z*s?  Regardless, although the question makes sense, it requires some amplification in order to elicit an answer that would be genuinely helpful.",2010-10-23 22:26:49.0,919.0,
5614,3929,1,@Jane Please add the above information to your question using the 'edit' link below the tags and delete this 'answer' as it really does not answer your question.,2010-10-23 22:26:57.0,,user28
5615,3914,0,@chl. Thank you!,2010-10-24 05:13:12.0,1393.0,
5616,3909,1,"So you don't think repeatability in effect estimates can be useful? Although you are not alone in your conception of what proof of causality is , I think it is quite narrow. We will never be able to indefinately prove a causal relationship, even with an experiment, absent all the evidence in the universe. Hence in my opinion the goal is to proffer evidence that whatever relationship we estimate is as close to the truth given the information we do know. Given that don't you think repeatability in prediction from a training set to a hold out sample could be a useful check on inferences made?",2010-10-24 05:48:17.0,1036.0,
5617,3909,0,"I appreciate your comments as well, and I completely agree that inferences are heavily dependent on logic and the research design.",2010-10-24 05:50:03.0,1036.0,
5618,3917,0,"Thank you for the reference. So say you are not concerned about model selection, could cross validating the effect estimates of the training data set to the hold out dataset be useful?",2010-10-24 05:54:54.0,1036.0,
5619,3934,0,"Hi Michael, I feel that some combination of your answer and Whuber one is good, removing the second moment part :)  ,  Thanks)",2010-10-24 07:13:20.0,253.0,
5620,3932,0,"Thank you Whuber.  I have to teach the students with the n-1 correction, so dividing in n alone is not an option.  As written before me, to mention the connection to the second moment is not an option.  Although to mention how the mean was already estimated thereby leaving us with less ""data"" for the sd - that's important.  Regarding the bias of the sd - I remembered encountering it - thanks for driving that point home.  Best, Tal",2010-10-24 07:15:34.0,253.0,
5621,3922,1,"BTW, A question about *Graphing Likert scale responses* just came across Andrew Gelman's weblog yesterday :) http://j.mp/aBm8mZ",2010-10-24 08:46:37.0,930.0,
5623,3924,0,"In most cases the values will be nearer the high range (ie. nearer 255). The SD can also vary quite a lot unfortunately. As I mentioned in my post, I'm completely new to stats, so the formula you gave whuber might be what I want. I'll need to play around with various datasets to see if it works or not.",2010-10-24 09:37:32.0,1683.0,
5625,3941,0,That's a nice answer :),2010-10-24 11:19:21.0,253.0,
5626,3942,0,"Thanks onestop - I didn't think there would be an answer from that direction.  Any way to sum-up the intuition, or is that not likely to be possible?  Cheers, Tal",2010-10-24 11:20:45.0,253.0,
5627,3942,0,"I couldn't do so myself, but a book reviewer summarised the approach in a paragraph in Amer. Stat. in 1993: http://www.jstor.org/stable/2684984. 

I'm not sure it's really practical to use this approach with your students unless you adopt it for the entire course though.",2010-10-24 12:10:18.0,449.0,
5629,3909,1,"Andy, I've edited my post to address your comments.  Also, I don't mean to suggest that causal inference cannot be done outside of the context of a designed experiment.  Nonetheless, it is more difficult and less certain in observational studies and we shouldn't look to model building procedures to help us with that problem.  Rather, we should try to better understand the problems for which we are attempting to understand causal relationships.",2010-10-24 14:53:16.0,485.0,
5630,3934,1,"Let's not confuse ""intuitive"" with ""nontechnical"".",2010-10-24 15:38:31.0,919.0,
5631,3932,1,"@Tal I was writing in your language, not that of your students, because I am confident you are fully capable of translating it into whatever you know will reach them.  In other words, I interpreted ""intuitive"" in your question to mean intuitive to *you*.",2010-10-24 15:40:16.0,919.0,
5632,3929,0,"@Srikan Vadali For some reason the site wont let me edit my own question, but it does give me an edit button for the question, so there seems to be something strange going on... Sorry about that.",2010-10-24 15:40:58.0,,Jane
5634,3210,0,"I knew of no freeware GWR implementation either until I came across this website recently, http://www.ecoevol.ufg.br/sam/#Graphics . I wouldn't be surprised if others exist though.",2010-10-24 16:03:18.0,1036.0,
5637,3954,3,(+1) Great that you add some pictures to illustrate the whole thing!,2010-10-24 17:54:28.0,930.0,
5645,3958,0,"Your question is great, unfortunately it's very similar to a previous (very highly rated) [question](http://stats.stackexchange.com/questions/3). Perhaps you could change your question to just ask about websites?",2010-10-24 21:04:13.0,8.0,
5646,3210,0,@Andy Thank you!  That's a real find.  This software looks like it reproduces GeoDa's capabilities with the addition of logistic regression and GWR.  I look forward to exploring it.,2010-10-24 21:09:43.0,919.0,
5647,3798,1,"@ars So, I'd like to suggest the following papers from Father & Son: http://j.mp/dvEDgb, http://j.mp/csD1Yf, http://j.mp/dkEHq5.",2010-10-24 21:11:56.0,930.0,
5648,3932,0,"Hi Whuber.  Thank you for the vote of confidence :).  The loose of the degree of freedom for the estimation of the expectancy is one that I was thinking of using in class.  The problem is that the concept of ""degrees of freedom"" by itself is one that needs knowledge/intuition.  But combining it with some of the other answers given in this thread will be useful (to me, and I hope others in the future).  Best, Tal",2010-10-24 21:12:58.0,253.0,
5649,3958,0,"@csgillespie Oh yes, I forgot about this one, sorry!",2010-10-24 21:18:38.0,930.0,
5650,3940,2,Thanks. I think it would be handy to have built-in functions (with min and max paraemters ala the unif family). It's a bit ugly to have to add function definitions into scripts just to use the discrete uniform distributions the way you would use other standard distributions. The builtin functions also deal with error handling (for example - if parameters aren't integers) and are optimized for speed.,2010-10-25 00:47:13.0,,Joseph Hsieh
5651,3917,0,"It could be, but I'd say that you're basically doing bootstrapping (or some variation thereof) at that point.",2010-10-25 01:46:59.0,303.0,
5652,3940,0,+1 for the implementation of the PDF and CDF! :),2010-10-25 06:46:56.0,582.0,
5655,3939,2,"This isn't correct for the edge cases. To see this, try running the following command: `table(round(runif(10000, min=0, max=2)))` It's clearly not discrete uniform.",2010-10-25 08:30:36.0,8.0,
5656,3939,0,"@csgillespie: nicely spotted, I updated my answer :)",2010-10-25 08:32:44.0,582.0,
5659,3948,1,+1 (although it deserves +3 at least) - very thorough and comprehensive. Made the whole issue much clearer for me. Thanks!,2010-10-25 11:20:54.0,22.0,
5660,3953,0,Very useful. Thanks!,2010-10-25 11:26:25.0,22.0,
5662,3951,0,"Thanks for the input. Using poly() to compare different polynomial transforms of grade was also suggested to me this weekend on the R-SIG-mixed-models list, and I think this solves things. I've developed code that automates the process that will be included in the next version of my R package ""ez"".",2010-10-25 12:05:46.0,364.0,
5663,3967,3,"Just to be sure, are you talking about ST as found in clinical trials, e.g. http://en.wikipedia.org/wiki/Sequential_analysis?",2010-10-25 14:07:41.0,930.0,
5664,3931,2,"I'd like to quote this zinger from the book *Numerical Recipes*: ""...if the difference between $n$ and $n-1$ ever matters to you, then you are probably up to no good anyway - e.g., trying to substantiate a questionable hypothesis with marginal data.""",2010-10-25 14:09:30.0,830.0,
5665,3888,0,"I don't think a ""ggplot"" tag will be very useful here (maybe mods would have a different opinion).",2010-10-25 14:16:02.0,930.0,
5667,3917,0,"I agree, I and think there are other things regularly done that reflect this same sort of logic (such as subset specificity tests or non-equivalent dependent variables). I simply posed the question because I imagined more formal treatments existed.",2010-10-25 15:06:40.0,1036.0,
5668,3909,0,"I agree with pretty much everything you say, except that issues of accuracy and repeatability are essential to make correct inferences in the face of doubt. I can give experts the benefit of the doubt that they are building logical models. Where I am concerned is repeatability of the findings in many observational contexts. Although I agree repeatability does not necessarily account for confounding influences that are best dealt with in experimental settings.",2010-10-25 15:11:45.0,1036.0,
5669,3970,0,"The reason I posed the question is because we don't have to think of cross validation as solely a way to evaluate a models predictive ability. It is not uncommon to be concerned that a models results (and hence inferences made) are artifacts for many potential reasons. Hence we want to examine the robustness of the findings, and I figured cross validation could be a useful context to examine the robustness of the results.",2010-10-25 15:18:49.0,1036.0,
5670,3940,0,"Nice answer.  And for the quantiles we can do something like qdu <- function(p, k) ifelse(p <= 0 | p > 1, return(""undefined""), ceiling(p*k))",2010-10-25 15:24:32.0,1108.0,
5671,3965,2,"Although these statements make no sense as given, they make perfect sense--and lead to an intriguing question--provided ""probability distribution"" is replaced by ""mean"" throughout.  Is this perhaps what you intend to ask?  Also, is this ""certain numerical range"" specified in advance of the analysis or are you asking how to construct an interval that has a given chance of enclosing at least one of the [means]?",2010-10-25 15:35:36.0,919.0,
5673,3924,1,"@Dan It would also help if you could tell us why you are doing this. As an aside: doing something like m + z*s may very well end with a upper bound of greater than 255 (depending on your values for m, z and s). Some sense of what you want to achieve (i.e., your goals) would help.",2010-10-25 15:53:13.0,,user28
5674,3884,2,"Concerning the advice not to store redundant variables: this is appropriate for RDBMSes but I would like to suggest that the opposite should be encouraged for spreadsheets.  The latter are so error-prone that mechanisms to detect and correct errors are invaluable.  One of the best consists of redundant information, such as computed fields and statistical summaries.  For example, if column C is the ratio of columns A and B, then an error in a single column in any given row can be detected and usually fixed.",2010-10-25 15:55:25.0,919.0,
5676,3971,0,"I had suspected there was a 'trivial' algorithm, which generated the identity permutation with probability $r$, and was uniform on all permutations with probability $1-r$, for the case $r > 0$. It looks like that is not exactly correct, and your ""uninteresting"" solution fixes my error.",2010-10-25 16:29:06.0,795.0,
5677,3884,1,"@whuber : that is what we check in the data control step. You can use that extra column to check fast, but you shouldn't keep it in the final sheet. Formulas in spreadsheets are horror, and the larger the spreadsheet, the more difficult to get the data out of it. Plus, in the case of Excel you'll be fighting the differences between .xls and .xlsx anyway. Be sure that a decision of a manager to update Microsoft Office can break tons of code if you rely heavily on excel files. So: save as csv, and keep these csv files as small as possible.",2010-10-25 16:33:03.0,1124.0,
5678,3974,2,"What do you mean by 'trivariate'? It would also help if you could to the extent possible use equations to illustrate your situation. Do you mean that you observe $Y$ where $Y = X + Z$, $X \\sim Exp(\\lambda)$ and $Z \\sim N(\\mu,\\sigma^2)$?",2010-10-25 16:47:16.0,,user28
5679,3884,0,"After spending a significant part of the last 24 years of my career coping with data transmitted in spreadsheets and managing substantial databases, I must respectfully disagree.  There is no such thing as ""control"" over spreadsheets (whether .xls, .xlsx, .wks, .wb*, etc) or even csv files.  The presence of redundant information in such files--even when they're available only in printed form--has many times resurrected some fairly large databases (100k+ records).  Every time this happens I (and my clients) have been grateful for the redundancies.",2010-10-25 17:14:21.0,919.0,
5680,3975,1,"(+1) Interesting! Which software do you use? Just a remark: There's no indication about absolute values for % or counts, so this seems to allow only a relative interpretation.",2010-10-25 17:45:40.0,930.0,
5682,3975,0,"Sorry, I didn't read your last sentence (the x-axis is invisible). I'll try another remark: Any chance to get the NA counts visible in the centered view (i.e. distinguish them from neutral)?",2010-10-25 18:11:11.0,930.0,
5683,3971,0,"@shabbychef You have a fine intuition: picking a permutation uniformly at random gives an expected correlation of zero. To achieve an expectation of r > 0, then, select the identity with probability r and otherwise select any uniformly random permutation (*including* the identity) with probability 1-r. A similar approach (using the reversal of ranks) will yield any expectation down to the minimum possible value for the data (which depends on the data, not just on how many there are). These solutions don't yield the most uniform distributions overall, though: one permutation clearly is favored.",2010-10-25 18:30:48.0,919.0,
5684,3974,0,"Sorry to use a non-standard term.  The data are multivariate with three variables: R,G,B.  I have the joint J(R,G,B) and I can compute the marginals.  The form of the (univariate) marginals is the Y you defined.",2010-10-25 18:31:11.0,1704.0,
5685,3976,0,"I know that copulas capture advanced correlation structures; I don't know much else about them.  Unfortunately, I don't think they would be useful to me (for understanding the relationships in the data), nor to scientific colleagues (who would have to have some intuition/experience with copulas to understand my conclusions).",2010-10-25 18:32:23.0,1704.0,
5686,3974,1,"It would also help if you clarify a bit more what you mean by ""understand this data"". Is there a specific question(s) you are trying to answer or is this just exploratory data analysis? I would suggest editing the question to incorporate your comment and make the question as precise as possible so that useful answers can be given.",2010-10-25 18:36:45.0,,user28
5687,3970,0,sorry for the misinterpretation.,2010-10-25 18:52:46.0,1307.0,
5688,3924,0,"Unfortunately it's not that easy to explain why I want it without writing a huge article worth of text. The upper bound going greater than 255 isn't a problem. It's the lower bound I need.
I think the m+z*s pretty much answers my question. Thankyou for that. If you post it as an answer, I'll mark it as the answer. Unfortunately I don't have the rep to upvote.",2010-10-25 19:02:44.0,1683.0,
5689,3967,0,"Yes. There are quite a few variants of sequential testing, including sequential t-tests, but none are used in basic research. I don't see any impediment to their use.",2010-10-25 19:19:09.0,1679.0,
5690,3971,0,"@whuber yes, I had mistakenly thought that I could also achieve any negative $r$ by this trivial method, by 'reversing' the indices with probability $|r|$. I guess I was tacitly assuming the vector $X$ was symmetric about $0$. The trivial algorithm is unsatisfying, though, as you suspected.",2010-10-25 19:25:01.0,795.0,
5691,3974,0,"I will try improve the question.  I need to give it a bit of thought.  -If- the data were multivariate normal (and, as follows, normal in the marginals), I would compute r and be done with it.  So, maybe I have two options:  (1) transform the data so they are normal or (2) compute r (via resampling) to assess its validity.  As far as ""understanding the data"":  ideally, I would want to characterize the conditional distributions (not necessarily by closed form equations) ... that is, I'd like to know how the variables affect each other.",2010-10-25 19:29:34.0,1704.0,
5692,3971,0,"@shabbychef I believe these permutation methods tend to be used more with Spearman (rank) correlation coefficients, where all values between -1 and 1 are attainable.",2010-10-25 20:15:36.0,919.0,
5694,3961,1,"@kwak Concerning your deleted response: I see vaguely that there is some kind of limited permuting going on until somehow a correlation coefficient below 0.5 (e.g.) is attained, but I cannot quite connect that with the conditions of the original question.  Are you saying you just keep applying ""small"" random permutations until you obtain a correlation within a targeted range of correlations?  If that's so, I think it's a really creative idea but some analysis is needed to show it can actually work.  Regardless, thanks for sharing it (however briefly!).",2010-10-25 20:25:53.0,919.0,
5695,3977,3,"I don't think that this is on-topic, but I will wait for others to vote to close it.",2010-10-25 20:29:24.0,5.0,
5696,3977,2,Plenty of answers are at http://www.google.com/search?q=abbreviate+millions .,2010-10-25 20:40:08.0,919.0,
5697,3884,0,"@whuber : We do data control with extra scripts, looking for the impossible values/outliers/odd cases. That's what I mean with the data control step. This is industry standard btw in companies like SGS and others doing analyses of clinical trials etc. The redundant information that is needed is kept in seperate databases. If one of them fails, the other is needed for the resurrection. In case you don't have a decent backup system, that is...",2010-10-25 20:56:58.0,1124.0,
5698,3971,0,"@whuber: you read my mind! A solution to this problem could be used to generate a bootstrap sample from a vector with a fixed level of Spearman correlation to any other vector of the same length (modulo degeneracies in that other vector.) In that case, it suffices to think of $X_i = i$ up to location and scaling shifts.",2010-10-25 21:02:29.0,795.0,
5700,3979,1,"I cannot think of any situation in which molar can be confused with ""millions"". Millions is not generally used by itself, you usually write, for instance, 2M€, 2Mb or 2MΩ. Moreover, you use Molar next to a chemical name, es. CaCl2 0.5M, so there is no ambiguity.",2010-10-25 21:44:36.0,582.0,
5701,3951,0,(+1) and thanks for the reproducible example.,2010-10-25 21:49:27.0,930.0,
5702,3975,0,(+1) - very nice.,2010-10-25 22:13:36.0,253.0,
5703,3975,0,"@chl Thanks. I use JMP, which I get paid to work on. The first one is a stacked bar chart with positive and negative values, which should be possible in lots of tools. NA counts could be done different ways (at one end, split over both ends, in the middle, separate column) and none seems obviously better for most situations.",2010-10-25 23:55:45.0,1191.0,
5704,3970,0,"No need for apologies. I'm the one suggesting something apparently fringe, and cross validation is apparently always used in the context you suggest.",2010-10-26 00:07:07.0,1036.0,
5705,3955,1,"I'm surprised you have not received any answers yet (although it is an obviously popular question). Since each of your four bullets are worth a question in and of themselves, how about limiting this question to one of them and post the others in separate questions. They are all really excellent questions that take distinct approaches, and all four are worthwhile on their own.",2010-10-26 00:11:17.0,1036.0,
5706,3924,0,"@whuber See Dan's comment. Perhaps, you should post your suggestion as an answer for him to accept it. FYI.",2010-10-26 05:02:12.0,,user28
5707,3982,1,Might this be more likely to be find an answer at http://stackoverflow.com/?,2010-10-26 06:26:20.0,449.0,
5708,2032,1,"This Q has reappeared on the 'active questions' page as it was 'poked' by the 'Community' background process, one of whose tasks is ""Randomly poke old unanswered questions every hour so they get some attention"". I don't think this question *deserves* any more attention. It's community wiki, it received only one up-vote, got a reasonable answer on Aug 26 but the user who asked it was last seen on Aug 24, so it seems unlikely that user is ever going to accept an answer. I suggest it should be closed by a moderator.",2010-10-26 06:40:40.0,449.0,
5710,3876,0,"Shouldn't time be a random effect since the heart rate measurements are samples taken during the surgery? 

If this is the case, would the following fit make sense (since I'm still reading up on the lmer function and have not quite understood the syntax)?

lmer(heart.rate~treatment+(1|id)+(1+time),data=heart)",2010-10-26 08:10:34.0,1663.0,
5712,3975,0,Thanks. I was hesitating between JMP and Systat.,2010-10-26 08:20:41.0,930.0,
5713,3371,4,"Just to say that my answer may actually look a little bit off-topic since this question has been merged with another one, http://stats.stackexchange.com/questions/3369/difference-between-fa-and-pca (I initially answer to the latter).",2010-10-26 09:16:23.0,930.0,
5715,3986,0,"Hi chl,Thanksfor the reply..I admit that I got some information from your answer..Let me put my understanding and then you can comment please. (1) I get a hint that P values can go down if your sample size is big...--Is that so ?? To my understanding p values can only show whether or not ur null hypothesis is rejected. (2) I understand now that I need to see difference in AIC values with intercept only and with covariates. I suppose when we say that we want lower AIC we mean for the same dataset. I am getting character character left in my comment so will comment again once you answer please,",2010-10-26 09:48:39.0,1763.0,
5716,3986,1,"@ayush (1) the test statistics (e.g. Wald) depend on the sample size (the standard error decrease with increasing sample size, and you're likely to get lower p-values with a larger sample). (2) yes, although AIC may be used to compare non-nested models, here I was thinking of it as a way to compare different models of increasing complexity.",2010-10-26 10:35:32.0,930.0,
5717,3965,0,first of all thanks for pointing out that my question is imprecise. i still need to to a lot practising for that. i modified the question accordingly. bear in mind what i want to do evolves along with what i probably need :),2010-10-26 10:53:03.0,1516.0,
5718,3986,0,thanks again..I get the essence of the p value now. Some 5mins back I ran a model which is giving me p values below .05 for all the variables but AIC of 28238.407 with intercept only and with covariates 21507.933. I also have a case in which AIC is 16035.xy with intercept only and with covariates 4234.xy. What is your opinion comparing two cases ? Please note that the second model had different variables 25 var while first had 20. so second though had more variables ( 25 comparison to 20) had lower AIC. Though p values werent .05 for all. Please suggest..more to ask after this..Thanks.,2010-10-26 11:33:26.0,1763.0,
5719,3980,0,"I think we need greater clarity with respect to the timing of interventions and the measurements. If I am reading your question correctly the timing is: A, B, {k1 successes in n1 trials}, B, C, D, {k2 successes in n2 trials}, {k3 successes in n3 trials} etc. I also assume that there are no other treatments beyond the 5 treatments listed. Is the above correct?",2010-10-26 12:43:51.0,,user28
5720,3955,0,"Hi Andy, thanks a lot for your answer. I'll try to rephrase it and focus on a single issue at a time. My initial thought was making it as broad as possible (and avoid repeating the same question afterwards) but given the lack of answers, it seems it is too general for someone to answer.",2010-10-26 12:56:23.0,1694.0,
5721,3981,0,"(It was Fred, I just like dumping my reputation into bounties every time I break 500, apparently...)",2010-10-26 13:22:06.0,71.0,
5722,3986,0,"@ayush It's difficult to answer about model quality without knowing how variables were selected. The gap in AIC between a model including only an intercept and some covariates gives you an indication about the ""explanatory power"" of those predictors (the residual deviance seems to decrease by a larger extent in the 2nd case you showed, and AIC penalizes for the # parameters as I said in my response). It's by no means a full answer about the relevance of these predictors. I'd suggest you to ask for a more specific question (IMO), e.g. about *variable selection in GLMs* for your specific study.",2010-10-26 13:29:07.0,930.0,
5723,3982,0,"I agree, this has nothing to do with statistical analysis as stated.",2010-10-26 13:29:23.0,1036.0,
5724,3993,0,"I will add a more detailed comment later, but yes what I am suggesting is a method of cross validation that is concerned entirely with internal validity.",2010-10-26 13:44:57.0,1036.0,
5725,3876,1,"The term '(time|id)' on the random effects side tells the function to fit different (linear) slopes for each person. So you can have time as both a fixed effect and a random effect, but they mean different things. Have a look at the sleepstudy example in Douglas Bates' book: http://lme4.r-forge.r-project.org/book/Ch4.pdf",2010-10-26 13:47:09.0,966.0,
5726,3982,3,"IMO, it does have to do with visualization (which is on-topic), so I'm open to allowing it.  That being said, I'm sure that you will have a hard time getting an answer either here or SO for this question: your best bet is the protovis mailing list.",2010-10-26 13:50:17.0,5.0,
5727,3982,2,"@Saneef It would be helpful if you could provide a data sample so this is reproducible.  Either make geoPopList available, or else use a smaller data same that can be posted here.",2010-10-26 13:51:44.0,5.0,
5728,560,1,The link to the paper doesn't work for me. Should it be http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.1675 ?,2010-10-26 14:46:11.0,461.0,
5729,3982,1,"@AndyW Thanks for a flag, but I agree that it is on topic.",2010-10-26 14:52:52.0,88.0,
5730,3955,0,I edited the question to expose the two main issues I'm mostly interested in. I chose to put them in the same question given their tight relationship.,2010-10-26 15:03:35.0,1694.0,
5731,3993,1,"@chl: Can you explain your statement in the first paragraph on internal v. external validity? In the tradition that I am familiar with: **internal validity** refers to the ability to assert cause and effect relationships among the variables within the particular sample; **external validity** is about the ability to generalize from a sample to other persons, places, and times.  Traditionally, cross-validation is about the latter and thus by the above definition about external validity, whereas you state that it is about internal validity. Did I misunderstand your statement?",2010-10-26 15:08:34.0,485.0,
5732,3907,0,Is the case that the gene change took place for all participants *before* they enrolled in the registry? The other possibility is that gene changes took place at some random time before/after enrollment but you simply do not know when it took place. Can you clarify this issue?,2010-10-26 15:12:07.0,,user28
5733,3993,1,"@Brett I was thinking of CV as a statistical technique to avoid overfitting or to provide a measure of prediction accuracy on the working sample (hence not necessiraly as a dedicated tool to demonstrate internal validity). I was not very clear, thanks or pointing that. I agree that this is then used to generalize over the sample at hand, but there I think it has nothing to do with causal inference (CV doesn't prove anything about causal links as modelled on the working sample). I share your view on external validity, but to demonstrate it we need other samples, no?",2010-10-26 15:21:36.0,930.0,
5734,3993,1,"You might clarify that first paragraph.  I think you're trying to say that CV doesn't do internal validity.  That's a matter for other processes.  But, if we've got good internal validity for other reasons, whatever that might be, CV will help estimate that effect more accurately across persons, places, and times--i.e. improve external validity. I still cannot think of any way that CV would help us to make causal claims about relationships between variables--the internal validity question itself--only to help generalize an established causal relationship.",2010-10-26 15:28:48.0,485.0,
5735,3924,0,"@Srikant I am happy Dan has found something useful in these comments (and that you have helped him in that).  However, I am reluctant to post an answer to a question I just don't understand!  Remember, too--I think I'm recalling this correctly--it is this site's policy not to go to extraordinary efforts to help someone formulate their question, so I'm even more reluctant to restate what I think the question might be asking and then answer the restated question.  Whom would that serve?  Others should feel free to jump in and do this if they want some easy points.",2010-10-26 15:53:42.0,919.0,
5736,3999,0,"As a hypothetical example, suppose the categories are named ""A"" through ""E"".  Could an object simultaneously have values of 10 for A.Low, 20 for B.High, 50 for B.Medium, 15 for B.Low, and 5 for C.High (and zeros in all other subcategories)?  Or are you treating all five categories independently so that, e.g., the object could have 100 in A.Medium, 90 in B.High and 10 in B.Medium, 50 in C.Low, 10 in E.High (and zeros in the other 10 subcategories)?  In the former case you would seek a single index; in the latter, five index values (one for each of A, B, ..., E).",2010-10-26 15:58:40.0,919.0,
5737,3971,1,"@shabbychef Then you might want to give some thought to achieving a desired *variance* in the correlation coefficients that will be generated.  You can include that within the constraints and, provided the desired variance is neither too great nor too small, you should still have a nonempty feasible set.  For this application it might be wise to achieve as uniform a distribution as possible.  Alternatively, you can impose a desired variance by means of a penalty term in the objective function, seeking a balance between uniformity and achieving the target values of variance and expectation.",2010-10-26 16:16:45.0,919.0,
5738,3980,0,"Sorry, I think I did not communicate clearly.  Case 1 had k_11 successes in n_11 trials in one month, then A and B were applied to Case 1, and then it had k_12 successes and n_12 trials in the following month.",2010-10-26 16:47:19.0,,george s
5739,3980,0,"Followup - Case 2 had k_21 successes in n_21 trials in one month, then B,C and D were applied to Case 2, and then it had k_22 successes and n_22 trials in the following month.",2010-10-26 16:48:31.0,,george s
5740,3993,1,"@Brett I think your comments to this question are very pertinent and sum up some of the issues very nicely. I doubt it will help any of the confusion between internal and external validity at this point, but chl's genetic epidemiology example is actually a problem of internal validity not external validity (except for between dataset heterogeneity (or population substructure), but that IMO is of less concern than internal validity in these examples).",2010-10-26 16:52:18.0,1036.0,
5741,3881,2,You must first figure out that barplots are made by barplot function... this is not that easy when you don't know that.,2010-10-26 17:24:39.0,88.0,
5742,3694,0,+1 for the J example.,2010-10-26 17:26:34.0,919.0,
5743,3907,0,"@Srikant Vadali They where born with these genes. There was not change in genes :-) This is the root of the problem/question. I have not seen this spelled out well but I assume the starting time could be birth. But it cannot be unrelated to the outcome event? But what if age is part counted is used as an exogenous variable, I not clear on how this effect the estimation. Does this help. If so I will edit the original question to include this.",2010-10-26 18:06:14.0,1189.0,
5744,3981,0,"@Srikant, how did u manage to get 1., 2., 3. numbering working. I, for the life of me, could not figure it out!",2010-10-26 18:17:25.0,1307.0,
5745,3981,1,"Magic! The secret is: Type a space at the start of the following paras: ""Apart from..."" and ""There is a R package..."".",2010-10-26 18:20:39.0,,user28
5746,3907,0,"I do not see any issues in using 'age' as an independent variable while using birth time as the starting time. Think of the following analogy: Suppose that you are investigating the variation in salary increases and you have a hypothesis that the higher someone's income is the lower is that person's salary. In such a situation, the 'income variable' will appear on both sides of your model: In 'Percent salary increase' income will appear as part of the dependent variable and income will again appear as an independent variable in the model.",2010-10-26 18:25:59.0,,user28
5747,3981,0,@Srikant! Thanks! ...,2010-10-26 18:52:41.0,1307.0,
5748,3999,0,"Thanks for you answer, yes an object could have A.low = 20, A.medium = 40, B.High = 40 and C.High = 5. All others would be zero. So yes, I am seeking for a single index.",2010-10-26 19:07:43.0,791.0,
5749,3999,1,"OK, just to be clear, because this is an important detail: is there a natural hierarchy to the categories and subcategories?  Could one assume that the ordering is A.High A.Medium A.Low B.High ... E.Low (or something comparable)? (If so, is there anything at all to prevent us from thinking of these 15 classifications as forming a single *ordinal* variable?) Also, it would help to clarify what the numerical values are intended to mean. *E.g.*, should they be interpreted as fuzzy degrees of membership, as probability of membership, as combined results of multiple measurements, or something else?",2010-10-26 19:24:23.0,919.0,
5750,3976,2,"@Mark In light of the revised question, I agree that looking at a copula-based description is a good suggestion.  There's nothing terribly difficult about copulas: because you have the marginals, you can apply the probability integral transform to make the three variables (R, G, B) uniform on [0,1].  A copula merely describes their multivariate distribution in these terms.  In effect, it eliminates the parameters from the marginals to allow you to focus on describing their multivariate dependencies.  It affords the non-parametric approach you're asking for.",2010-10-26 19:31:53.0,919.0,
5751,3987,0,"Basic biomedical researchers do interim analyses all the time, they just don't declare them because they don't even know that it matters! I have surveyed researchers at a national congress and found that more than 50% did not know that the control of error rates from Student's t-test is dependent on a pre-determined fixed sample size. You can see evidence of that in the sometimes erratically varying sample sizes used.",2010-10-26 19:44:16.0,1679.0,
5752,3999,0,"For example: if I am interested in the overall urbanization of a state, my five categories would be: roads, residential areas, industrial areas, airports and intense agriculture. Each of the categories is divided into 3 subcategories (low, mid, high) with the percentage of area falling into these categories. Now I would like to express the degree of urbanization in one number without loosing to much detail. Maybe I should get rid of the subcategories? Thanks a lot for your effort.",2010-10-26 19:44:48.0,791.0,
5753,3987,0,Some of the disadvantages that come from the complexities of sequential designs come specifically in the design of the analyses rather than in their implementation. Perhaps we could have a set of pre-canned designs for small sample basic experiments.,2010-10-26 19:49:06.0,1679.0,
5754,3993,2,"Brett's definition between internal and external validity is accurate, but for our purposes it will help to define it in different terms. External validity is only concerned with the sample and how that sample relates to other populations. Internal validity is concerned with various aspects about the effects estimated and the constructs used to estimate those effects.",2010-10-26 20:06:13.0,1036.0,
5755,4002,0,"""However, if you use the starting time as the time at which they enter the registry....bias your conclusions."" Thanks for your answer. I agree with your answer I am trying to convince others that using time of enrollment is wrong as it is not related to outcome or genetics. Age could be the endog which is equivalent to starting the survival interval at birth.  Do you have a reference that would indicate that using time of enrollment is wrong.",2010-10-26 20:06:26.0,1189.0,
5756,3998,0,"Thanks Shane, I have not had any esperience analysing timeseries i tried briefly dxts = as.xts(as.POSIXct(ae$date_time)) but had no luck any pointers?",2010-10-26 20:14:48.0,1716.0,
5757,3993,0,"@Andy @Brett I fully agree with Brett's definitions. My point was that CV won't help confirming an hypothesized conceptual model, this one contributing to internal validity. But I must admit that it wasn't clear in my original answer. I'll update it when I get back to my computer.",2010-10-26 20:31:10.0,930.0,
5758,3976,0,"Thanks for sticking with it through the reformulation.  I will look into copulas.  Will copulas allow me to compare marginals and conditionals from different Js:  $J_1$, $J_2$, ...  I might also wish to compare $R_1|G_1$ (from $J_1$) and $R_2|G_2$ in some way.  The different joints are measurements on individuals in experimental conditions.  Some of the measurements are repeated measurements of the same individual).",2010-10-26 20:31:47.0,1704.0,
5760,3976,0,"@Mark Copulas will be useful for identifying, creating, and comparing models of joint distribution, but I don't think they will help in formal tests of hypothesis concerning those models.  Consider asking another question in which you reveal more about the nature of the data and how they are collected, because that ought to play an important role in the modeling and choice of analytical techniques.",2010-10-26 21:00:40.0,919.0,
5761,4003,0,"Many thanks for this detailed answer, it makes a lot of things clearer and I think I will follow your suggestion regarding PCA.",2010-10-26 21:06:12.0,791.0,
5763,4002,2,"@Vincent At risk of being repetitive, I would refer you again to the TAS article (""Two Pitfalls in Survival Analyses..."") because the authors address *precisely* this issue.  They use ""multistate modeling"" to examine ""specific types of survival bias such as *length bias* and *time-dependent bias*.""  You should be particularly concerned about length bias, which is a form of sample selection bias.  (What about people who died before they had a chance to ever make it into your study?)  This article, as is typical for TAS, has a good bibliography for further research.",2010-10-26 21:33:35.0,919.0,
5764,3907,0,@onestop See my comment to Srikant's answer.,2010-10-26 21:34:12.0,919.0,
5765,3976,0,"Will do.  It's always difficult to get the right abstraction of a problem.  These two questions also seem to get at distribution free, multivariate analysis:  http://stats.stackexchange.com/questions/4/assessing-the-significance-of-differences-in-distributions and http://stats.stackexchange.com/questions/1927/is-there-an-accepted-definition-for-the-median-of-a-sample-on-the-plane-or-highe",2010-10-26 21:35:25.0,1704.0,
5766,3976,0,"I'm reading over some of the terminology regarding copulas.  I'm missing a ""process"".  For example:  with an empirical joint and empirical marginals and a distribution for the marginals, what do I do?  It seems that I need a transformation function to take my marginals to uniform distributions and then a ""copula function"" (term?) to combine those to for a distribution which (hopefully?) models the joint.  It may turn out that I'm more interested in the pairwise marginals (RG, RB, GB) than I am in the full joint.",2010-10-26 21:39:59.0,1704.0,
5767,3876,0,"Dear Matt, I just came across the sleepstudy example yesterday. I will read it up. Thanks!",2010-10-27 00:42:06.0,1663.0,
5768,4002,0,"@Vincent No, I do not know of any reference. Perhaps, the reference pointed out by whuber may help. Alternatively, you can perform a simulation study and demonstrate what the right choice is. By the way, if people enroll roughly at the same age then the issue is not relevant as enrollment time and age will be synonymous. The issue of incorrect estimates becomes more prominent as enrollment times vary a lot vis-a-vis age. Or, at least that is my intuition.",2010-10-27 00:55:49.0,,user28
5769,3643,0,"@rob - if i catch your drift, i think we are on the same page regarding actual vs asymptotic parameters. we seem to agree that the latter are what really matter.",2010-10-27 03:01:12.0,1112.0,
5770,4010,1,"You say this sequence 'may represent the correlation between a couple of measurements'. Do you have the data for the measurements? If there are only a couple of measurements, how can you derive a sequence of *N* correlations from them?",2010-10-27 06:48:08.0,449.0,
5771,3992,0,"I'm using Mathematica for developing the codes I need, how can I use what I downloaded from robustbase website?",2010-10-27 08:24:29.0,1637.0,
5772,4006,0,"(+1) Thanks for reiterating the need to read the R vignette. Nothing's missing there. And for a more thorough understanding of it, all preprint/draft papers that sit on Jan's website, http://gifi.stat.ucla.edu/",2010-10-27 09:28:46.0,930.0,
5774,4006,0,"@chl Yeach, infamous Cuddy Valley...",2010-10-27 09:48:26.0,88.0,
5775,3992,0,You can find the source of the function in this thread: http://stats.stackexchange.com/questions/4017/translate-with-rcpp and try to translate from there to mathemtica.,2010-10-27 10:02:57.0,603.0,
5776,4017,3,"CW? No, it certainly has a potential single best answer.",2010-10-27 10:28:57.0,88.0,
5777,4011,0,"Which comment of mine confused you? By the way, I am not the OP. Just wanted to clarify as it feels as if your response including the last line is addressed to me.",2010-10-27 12:07:59.0,,user28
5779,2697,0,"Okay: the Eigenvalue associated with each principal component tells you how much variation in the data set it explains (in my example, how clearly it separates your bottles into groups).  They are usually expressed as a percentage of the total variation in the data set.  As for the Eigenvectors, well, that's where as claws said I follow the output of an analysis like a machine ;)  In my head, they are related to how you rotate Vince's mobile to its 'best' orientation, but this might not be the right way to think of them.",2010-10-27 13:07:34.0,266.0,
5780,4018,0,"@Dirk:> a) there are plenty of implementation of median() (say 'pull' in package pcaPP) so it's fair game. b) you mean one won't notice a sizable increase in running times ? c) okay, but i think the issue with this code is not really the translation to C++, rather the idea of calling some R functions [pnorm, dnorm,...] in C++ (of course i can be really wrong) d) can you provide the link to your mailing list ?",2010-10-27 13:21:12.0,603.0,
5781,4017,0,how do i make it cw ?,2010-10-27 13:21:55.0,603.0,
5782,4018,0,"Can we please split the sub-questions off one by one?  A) you can call R function from C++ -- for convenience but not necessarily speed.  See the examples/ in Rcpp.  B)  I said no such thing.   C)  That is a all easy since Rcpp 0.8.7, see the 'Rcpp sugar' docs, posts on Rcpp-devel and our recent presentations.  D) It hangs off the R-forge page; just google for 'rcpp-devel'.",2010-10-27 13:25:05.0,334.0,
5783,4017,3,There is no reason for CW.,2010-10-27 13:25:30.0,334.0,
5786,4018,0,"@Dirk to A)B)C)D->thanks i'll read more. Would you know of a directory/listings of examples of simple R functions together with there translation in C++ (i know there are C++ funciton in any library but i would prefer modern examples, that is, ones that use as much as possible the features of Rcpp).",2010-10-27 13:36:23.0,603.0,
5788,4018,0,"Did you see examples/ within the Rcpp package?  Also, some of the HPC tutorials are by now 'too old' for current Rcpp practice. Make sure you lick a recent one -- say from useR this summer.",2010-10-27 13:41:16.0,334.0,
5789,4018,0,"The HPC tutorial i saw was over a year ago (btw thanks for these and the rest of your contribs), so maybe there is more there. I saw the examples in '/Rcpp/inst/examples/functionCallback/' but that's really not much. I realize there may be more spread around on your mailing list (and on your HPC slides) but that's too sparse. Would you have learned regression if it were taught that way (couple of contrived examples on slides and ML)?",2010-10-27 13:56:10.0,603.0,
5790,4018,1,"1) Start at http://dirk.eddelbuettel.com/presentations.html and work your way down.  2) There are six subdirectories to examples/ so I am unsure why you focus on one.  3)  There are 770+ unit tests that double as examples if you care to look closely enough.  4) There are eight (8) vignettes in the Rcpp package.  5) We authored a few other packages that use Rcpp, you could look at those too.  6) Lastly, CRAN lists fifteen packages depending upon Rcpp -- these all are examples too.",2010-10-27 14:00:36.0,334.0,
5792,4020,0,"Thank you for your answer!

But I have some doubts. The problem is that A always outperforms B if initial params and learning/validation/testing sets are the same; but it doesn't neccessarily hold
if they differ.
See question update.",2010-10-27 14:13:14.0,1725.0,
5794,4019,0,"It would be helpful if you could give a bit more detail about the nature of the machine learning algorithms, in particular what is meant by ""randomly generated initial approximation (parameters)"" - does this mean something like a random set of initial weights for a neural network, or do you mean somplething like a random choice of kernel parameters for an SVM?  The correct evaluation protocol may depend on the nature of these ""parameters"".",2010-10-27 14:32:21.0,887.0,
5796,4018,1,"Dude: There is a mailing list for the project you are interested in. *All* our documentation suggests to ask on the mailing list. So why-oh-why do you keep piling on here?  Can we **please** stop that now.  Lastly, your 'too superficial' would require some backing up.  I will gladly review patches, just don't post them *here*. Ok?",2010-10-27 14:38:48.0,334.0,
5797,4017,2,"I *strongly disagree* with your edited title and added/edited question.  You are simply mistaken if you consider Rcpp to be a code compiler, or when asking us to rewrite code for you.",2010-10-27 14:41:37.0,334.0,
5798,4023,0,The Table did not come out right. Every odd number is a count from group 1 and every even number is the respective count from group 2,2010-10-27 14:42:40.0,1727.0,
5800,4018,2,"@kwak: Responding to ""It's something that should be outsourced to the community"": I look forward to seeing your contributions as you work through these examples yourself.",2010-10-27 14:45:56.0,1657.0,
5801,4019,0,"This initial approximation is a couple of matrixes filled in with random values drawn uniformly & independently from [0;0.02]. (In fact, I'm comparing matrix factorizations via SGD with different loss functions; this matrixes are initial approximation of factors). So, it's closer to weights for NN",2010-10-27 14:46:21.0,1725.0,
5802,4017,0,"@Dirk:> sure, what would a suggested title be (the older one?). I don't really care about this particular function I'm interested in learning ways to make my codes run faster. If you have another example, please post it. I'll happily close this one.",2010-10-27 14:49:33.0,603.0,
5803,4018,0,@Joshua Ultrich:> Sure: my question was my initial contribution to the process. Let's see if we can keep this ball rolling.,2010-10-27 14:52:40.0,603.0,
5804,4023,0,I've reformatted your question. Is the table now correct?,2010-10-27 15:04:31.0,8.0,
5805,4019,1,"In that case, I'd probably use the bootstrap, but generate a different initial matrix for each iteration, so that the resampling procedure averages over the variability due to the partitioning of the dataset and due to the initialisation, but I would use the same initialisation in each iteration for both methods.  I'd then use the Wilcoxon signed rank test for performance evaluation (fewer assumptions).",2010-10-27 15:28:30.0,887.0,
5806,4019,0,Thank you! Wilcoxon signed rank test looks promising.,2010-10-27 15:41:48.0,1725.0,
5807,4025,1,Crossposting between here and StackOverflow is discouraged.,2010-10-27 15:47:25.0,334.0,
5808,4025,2,I would suggest that this would be the more appropriate place for this question.,2010-10-27 15:48:40.0,8.0,
5809,4023,0,"yes, many thanks!",2010-10-27 15:50:08.0,1727.0,
5810,4025,0,sorry for crossposting.i didn't know here when i posted the question to stackoverflow.somebody suggested here and i posted here then. thanks.,2010-10-27 15:50:50.0,,Yuan
5811,4024,0,"Thank you for suggestions. Logically, it is not quite possible to merge phenotypes as each of them is a unique combination of three recorded parameters. Since each of these parameters can go ""up"", ""down"" or stay ""unchanged"" as a result of a mutation, so there can be 3^3=27 distinct phenotypes. In the example above I removed those phenotypes for which both groups scored ""0"", so there were only 21 of them. I do see the prevalence of certain phenotypes but i would like to have some statistical proof that distributions of such phenotypes in various groups of mutants is similar (or not). Thank you!",2010-10-27 16:03:56.0,1727.0,
5812,4019,2,http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.3325,2010-10-27 16:26:57.0,511.0,
5813,4019,0,"Yaroslav, thanks for the link!",2010-10-27 16:31:38.0,1725.0,
5814,4020,2,"Note that if you adjust parameters of an algorithm after seeing its performance on the test set, test set error is no longer guaranteed to be an unbiased estimate of true error.",2010-10-27 16:41:42.0,511.0,
5815,4020,0,"Yaroslav! Actually, I've got learn,validation and test sets. I use validation set to tune some params like regularization term weight and optimization step (via simplex [meta]optimization), while initial weights (the parameters I was talking about before) are not adjusted at all... surely, the test set is used only to calculate quality metrics",2010-10-27 16:51:31.0,1725.0,
5816,4026,0,"unfortunately, i have no background in statistics so i don't know what to expect from a ""MLE"" output. sorry for that. so, when i print model$table, i get something like below. is this MLE?                                                          
            Sepal.Length                                               
Y             [,1]      [,2]                                           
  setosa     5.006 0.3524897                                           
  versicolor 5.936 0.5161711                                           
  virginica  6.588 0.6358796",2010-10-27 17:18:24.0,,Yuan
5818,4029,1,"Interesting, Fisher's test crashed on R.",2010-10-27 18:51:38.0,930.0,
5819,4030,0,"Hope you don't mind but I added a link to ""VECM"". Feel free to correct.",2010-10-27 18:57:52.0,930.0,
5820,3982,0,"Is there a reason why the code you link to on the example page doesn't work? From my reading of the pv.Geo.scale source it appears that it supports setting the domain like pv.linear.scale does... What happens when you try that approach and attempt to zoom with the mousewheel?

Please update your question with a simple reproducible example like Shane asked for as well; if you can do that I'd be happy to take a look.",2010-10-27 19:26:32.0,1730.0,
5822,4026,1,"@Yuan I don't know, because simple ""MLE"" can mean different things... Try reading ?naivebayes, it is quite detail description what is what in table, maybe you could find something familiar there. Or try to specify what you need in more detailed way -- what is your desired use, for what you need it?",2010-10-27 20:06:18.0,88.0,
5823,4020,0,"mbq, I'm not sure if I was clear enough =| I do not want to nail this weight",2010-10-27 20:07:09.0,1725.0,
5824,4020,0,@bijey It is only that you will get a stronger result showing that A>B in unpaired test.,2010-10-27 20:12:31.0,88.0,
5825,4020,0,"...I do understand that it's better to show that A>B in unpaired test. But I'm afraid, it is very unlikely to show since (informaly) [quality(A) - quality(B)] << variance(quality(A))",2010-10-27 20:17:24.0,1725.0,
5826,3955,1,I'm still failing to understand why having one estimate with a small error makes an approach 'unusable'.,2010-10-27 20:43:00.0,449.0,
5827,3993,1,"@Brett I made some edits; I hope it helps clarifying my initial thoughts, but let me know. I could also remove any reference to CV wrt. internal/external validity if this happens to be unclear, although I think you were more concerned with CV and external validity (i.e. generalizing on new samples) while I was envisioning CV as part of model validation (hence, on the working sample).",2010-10-27 20:44:16.0,930.0,
5828,3909,0,(+1) My apologies. It seems I also forgot to upvote your very nice answer. Already voted up your helpful comments.,2010-10-27 20:56:04.0,930.0,
5829,4029,0,"Cannot upvote more, sorry. It seems I hadn't increase the wksp enough :)",2010-10-27 20:59:27.0,930.0,
5831,4024,1,"@Membran Aggregation doesn't have to be meaningful: you're free to combine bins any way you please.  A subtle problem, though, is that *post-facto* aggregation casts the p-values in doubt; the aggregation ought to be independent of the data.",2010-10-27 21:16:24.0,919.0,
5832,4020,1,"I thought so; then stick to paired test, yet don't hide this fact. This is because in a sense you can say that they are equivalent -- B can be better than A when started with luckily picked seed.",2010-10-27 22:02:35.0,88.0,
5834,4029,0,"Isn't it that Fisher's ""exact"" test actually addresses slightly different question: ""...it is used to examine the significance of the association (contingency) between the two kinds of classification"" (wiki page). In my case I sought to confirm (or refute) the hypothesis that distributions of phenotypes between 2 groups are similar (equal). When I found that online test (see the first post) named ""Chi-square test for equality of distributions"" I thought it was precisely for my problem...",2010-10-27 22:38:24.0,1727.0,
5835,2909,0,That would require about a million simulated random walks...,2010-10-27 22:40:44.0,919.0,
5836,4029,0,"Also, if you think that mentioned version of Fisher's test is fine for comparing two distributions, can it also be used for checking uniformity of distribution (i.e. to say that phenotypes within one group were distributed non-uniformly between a finite number of possible phenotypes)? One can do this even in Excel using CHITEST function, but what if I have a distribution similar to the ones above, with lots of phenotypes observed less than 5 times?",2010-10-27 22:42:57.0,1727.0,
5838,4010,0,"The measurements may be many time series, then we can derive the correlation between each two of them.",2010-10-28 02:56:43.0,,Jason Xin Liu
5840,4025,1,"At the very least, you should provide the link to the other posting.",2010-10-28 03:20:28.0,183.0,
5842,4029,0,"@Membran # 1: It is a *slightly* different question as Fisher's exact test conditions on both sets of marginal totals. This seems something of an academic statistical nicety to me though, and I'm a statistician in academia. (BTW could you clarify to *which* wiki you refer?)



@Membran #2: I would not call the conditional exact test ""Fisher's exact test"" in the case of a one-way table, but such a test should be possible.and  I would have thought more straightforward for one-way tables, but I can't currently find software to assist and I don't have time to perform the calculation without.",2010-10-28 04:07:33.0,449.0,
5843,4011,0,"There is no change in the genes, they are born with them. Therefore entering the registry does not increase risk. I don't expect a change in behavior but it is possible. There care does not change it is just tracked in the registry.",2010-10-28 04:16:02.0,1189.0,
