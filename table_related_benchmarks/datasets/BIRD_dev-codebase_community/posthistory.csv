Id,PostHistoryTypeId,PostId,RevisionGUID,CreationDate,UserId,Text,Comment,UserDisplayName
1,2,1,e58bf7fd-e60f-4c58-a6e4-dfc91cf98a69,2010-07-19 19:12:12.0,8.0,How should I elicit prior distributions from experts when fitting a Bayesian model?,,
2,1,1,e58bf7fd-e60f-4c58-a6e4-dfc91cf98a69,2010-07-19 19:12:12.0,8.0,Eliciting priors from experts,,
3,3,1,e58bf7fd-e60f-4c58-a6e4-dfc91cf98a69,2010-07-19 19:12:12.0,8.0,<bayesian><prior><elicitation>,,
4,2,2,18bf9150-f1cb-432d-b7b7-26d2f8e33581,2010-07-19 19:12:57.0,24.0,"In many different statistical methods there is an ""assumption of normality"".  What is ""normality"" and how do I know if there is normality?",,
5,1,2,18bf9150-f1cb-432d-b7b7-26d2f8e33581,2010-07-19 19:12:57.0,24.0,What is normality?,,
6,3,2,18bf9150-f1cb-432d-b7b7-26d2f8e33581,2010-07-19 19:12:57.0,24.0,<normality>,,
7,2,3,6320bb0f-c792-4a8c-8083-89507c28d375,2010-07-19 19:13:28.0,18.0,What are the most valuable Statistical Analysis open source projects available right now?,,
8,1,3,6320bb0f-c792-4a8c-8083-89507c28d375,2010-07-19 19:13:28.0,18.0,What are the most valuable Statistical Analysis open source projects available?,,
9,3,3,6320bb0f-c792-4a8c-8083-89507c28d375,2010-07-19 19:13:28.0,18.0,<open-source>,,
10,16,3,6320bb0f-c792-4a8c-8083-89507c28d375,2010-07-19 19:13:28.0,18.0,,,
11,2,4,ccbe017d-d22c-4c0a-83b0-1d82a792a098,2010-07-19 19:13:31.0,23.0,I have two groups of data.  Each with a different distribution of multiple variables.  I'm trying to determine if these two groups' distributions are different in a statistically significant way.  I have the data in both raw form and binned up in easier to deal with discrete categories with frequency counts in each.  \\n\\nWhat tests/procedures/methods should I use to determine whether or not these two groups are significantly different and how do I do that in SAS or R (or Orange)?,,
12,1,4,ccbe017d-d22c-4c0a-83b0-1d82a792a098,2010-07-19 19:13:31.0,23.0,Assessing the significance of differences in distributions,,
13,3,4,ccbe017d-d22c-4c0a-83b0-1d82a792a098,2010-07-19 19:13:31.0,23.0,<distributions><statistical-significance>,,
14,2,5,5631dbb1-2781-4924-b7bd-5a67c06c5e4e,2010-07-19 19:14:43.0,23.0,The R-project\\n\\nhttp://www.r-project.org/,,
15,16,5,5631dbb1-2781-4924-b7bd-5a67c06c5e4e,2010-07-19 19:14:43.0,-1.0,,,
16,2,6,18914b59-1d73-4c14-a8b6-25d429a1888e,2010-07-19 19:14:44.0,5.0,"Last year, I read a blog post from [Bendan O'Connor][1] entitled [""Statistics vs. Machine Learning, fight!""][2] that discussed some of the differences between the two fields.  [Andrew Gelman responded to favorably to this][3]:\\n\\nSimon Blomberg: \\n> From R's fortunes\\n> package: To paraphrase provocatively,\\n> 'machine learning is statistics minus\\n> any checking of models and\\n> assumptions'.\\n> -- Brian D. Ripley (about the difference between machine learning\\n> and statistics) useR! 2004, Vienna\\n> (May 2004) :-) Season's Greetings!\\n\\nAndrew Gelman:\\n\\n> In that case, maybe we should get rid\\n> of checking of models and assumptions\\n> more often. Then maybe we'd be able to\\n> solve some of the problems that the\\n> machine learning people can solve but\\n> we can't!\\n\\nThere was also the [**""Statistical Modeling: The Two Cultures""** paper][4] by Leo Breiman in 2001 which argued that Statisticians rely too heavily on data modeling, and that machine learning techniques are making progress by instead relying on the *predictive accuracy* of models.\\n\\nHas the Statistics field changed over the last decade in response to these critiques?  Do the *two cultures* still exist or has Statistics grown to embrace machine learning techniques such as neural networks and support vector machines?\\n\\n\\n  [1]: http://anyall.org/\\n  [2]: http://anyall.org/blog/2008/12/statistics-vs-machine-learning-fight/\\n  [3]: http://www.stat.columbia.edu/~cook/movabletype/archives/2008/12/machine-learnin.html\\n  [4]: http://www.stat.osu.edu/~bli/dmsl/papers/Breiman.pdf",,
17,1,6,18914b59-1d73-4c14-a8b6-25d429a1888e,2010-07-19 19:14:44.0,5.0,The Two Cultures: statistics vs. machine learning?,,
18,3,6,18914b59-1d73-4c14-a8b6-25d429a1888e,2010-07-19 19:14:44.0,5.0,<statistics><machine-learning>,,
19,2,7,ad3deff5-e4fd-446f-abce-77984eb5eeeb,2010-07-19 19:15:59.0,38.0,"I've been working on a new method for analyzing and parsing datasets to identify and isolate subgroups of a population without foreknowledge of any subgroup's characteristics.  While the method works well enough with artificial data samples (i.e. datasets created specifically for the purpose of identifying and segregating subsets of the population), I'd like to try testing it with live data.\\n\\nWhat I'm looking for is a freely available (i.e. non-confidential, non-proprietary) data source.  Preferably one containing bimodal or multimodal distributions or being obviously comprised of multiple subsets that cannot be easily pulled apart via traditional means.  Where would I go to find such information?",,
20,1,7,ad3deff5-e4fd-446f-abce-77984eb5eeeb,2010-07-19 19:15:59.0,38.0,Locating freely available data samples,,
21,3,7,ad3deff5-e4fd-446f-abce-77984eb5eeeb,2010-07-19 19:15:59.0,38.0,<dataset><sample><population>,,
22,2,8,8b903ea3-538d-4cbf-be6e-9aefc1ad0019,2010-07-19 19:16:21.0,37.0,"Sorry, but the emptyness was a bit overwhelming. And this has been stuck in my head since it got asked at Area51!",,
23,1,8,8b903ea3-538d-4cbf-be6e-9aefc1ad0019,2010-07-19 19:16:21.0,37.0,So how many staticians *does* it take to screw in a lightbulb?,,
24,3,8,8b903ea3-538d-4cbf-be6e-9aefc1ad0019,2010-07-19 19:16:21.0,37.0,<lightbulb><staticians>,,
25,2,9,8baa8e0d-94e3-4315-884c-de31731f8e03,2010-07-19 19:16:27.0,50.0,"[Incanter][1] is a Clojure-based, R-like platform (environment + libraries) for statistical computing and graphics. \\n\\n\\n  [1]: http://incanter.org/",,
26,16,9,8baa8e0d-94e3-4315-884c-de31731f8e03,2010-07-19 19:16:27.0,-1.0,,,
27,2,10,350cf958-2957-43bd-86f8-909a6d2b19cd,2010-07-19 19:17:47.0,24.0,Many studies in the social sciences use Likert scales?  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?,,
28,1,10,350cf958-2957-43bd-86f8-909a6d2b19cd,2010-07-19 19:17:47.0,24.0,Under what conditions should Likert scales be used as ordinal or interval data?,,
29,3,10,350cf958-2957-43bd-86f8-909a6d2b19cd,2010-07-19 19:17:47.0,24.0,<measurement><scales>,,
30,2,11,c77bd587-f3b4-406d-9a84-24a37851dc85,2010-07-19 19:18:30.0,34.0,"\\nIs there a good, modern treatment covering the various methods of multivariable interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.",,
31,1,11,c77bd587-f3b4-406d-9a84-24a37851dc85,2010-07-19 19:18:30.0,34.0,Multivariable Interpolation Approaches,,
32,3,11,c77bd587-f3b4-406d-9a84-24a37851dc85,2010-07-19 19:18:30.0,34.0,<interpolation><multivariable>,,
33,2,12,2cc68cee-98a8-47e0-b7c9-9479d86c7a0e,2010-07-19 19:18:41.0,5.0,"See my response to [""Datasets for Running Statistical Analysis on""][1] in reference to datasets in R.\\n\\n\\n  [1]: http://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/2252450#2252450",,
34,2,13,e2528eb9-342a-46fb-b85a-e7e61e92d9b5,2010-07-19 19:18:56.0,23.0,"Machine Learning seems to have its basis in the pragmatic - a Practical observation or simulation of reality.  Even within statistics, mindless ""checking of models and assumptions"" can lead to discarding methods that are useful.\\n\\nFor example, years ago, the very first commercially available (and working) Bankruptcy model implemented by the credit bureaus was created through a plain old linear regression model targeting a 0-1 outcome.  Technically, that's a bad approach, but practically, it worked.",,
35,2,14,b425ff8b-ef40-4b57-84cd-2167e96b2764,2010-07-19 19:19:03.0,36.0,I second that Jay. Why is R valuable? Here's a short list of reasons. http://www.inside-r.org/why-use-r. Also check out [ggplot2][1] - a very nice graphics package for R. Some nice tutorials [here][2].\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/ggplot2,,
36,16,14,b425ff8b-ef40-4b57-84cd-2167e96b2764,2010-07-19 19:19:03.0,-1.0,,,
37,2,15,ccdaaf70-98b2-49ae-a2b4-08960ebcee50,2010-07-19 19:19:46.0,6.0,"John Cook gives some interesting recommendations. Basically, get percentiles/quantiles (not means or obscure scale parameters!) from the experts, and fit them with the appropriate distribution.\\n\\nhttp://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/",,
38,6,8,d8ccbc08-0867-4ab7-b135-74cf7ce741a3,2010-07-19 19:20:33.0,6.0,<lightbulb><verboten>,edited tags,
39,5,5,d919c31f-f443-4bfa-9b5c-c306970ae92e,2010-07-19 19:21:15.0,23.0,"The R-project\\n\\nhttp://www.r-project.org/\\n\\nR is valuable and significant because it was the first widely-accepted Open-Source alternative to big-box packages.  It's mature, well supported, and a standard within many scientific communities.\\n\\n - [Some reasons why it is useful and valuable][1] \\n - There are some nice tutorials [here][2].\\n\\n\\n  [1]: http://www.inside-r.org/why-use-r\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/ggplot2",add content from the comments;,
40,2,16,7f07993b-9fa1-4359-8dec-f4e9aadfe8e2,2010-07-19 19:22:31.0,8.0,"Two projects spring to mind:\\n\\n 1. [Bugs][1] - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC.\\n 1. [Bioconductor][2] - perhaps the most popular tool in Bioinformatics. I know it's a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none.\\n\\n\\n  [1]: http://www.mrc-bsu.cam.ac.uk/bugs/\\n  [2]: http://www.bioconductor.org/",,
41,16,16,7f07993b-9fa1-4359-8dec-f4e9aadfe8e2,2010-07-19 19:22:31.0,-1.0,,,
42,2,17,3e975162-dbea-4bcc-af2f-86c04a2adb1f,2010-07-19 19:24:12.0,,"I have four competing models which I use to predict a binary outcome variable (say, employment status after graduating, 1 = employed, 0 = not-employed) for n subjects. A natural metric of model performance is hit rate which is the percentage of correct predictions for each one of the models. \\n\\nIt seems to me that I cannot use ANOVA in this setting as the data violates the assumptions underlying ANOVA. Is there an equivalent procedure I could use instead of ANOVA in the above setting to test for the hypothesis that all four models are equally effective?\\n\\n",,user28
43,1,17,3e975162-dbea-4bcc-af2f-86c04a2adb1f,2010-07-19 19:24:12.0,,How can I adapt ANOVA for binary data?,,user28
44,3,17,3e975162-dbea-4bcc-af2f-86c04a2adb1f,2010-07-19 19:24:12.0,,<anova>,,user28
45,2,18,917d8213-ef39-440b-b446-cec478d38b04,2010-07-19 19:24:18.0,36.0,Also see the UCI machine learning Data Repository.\\n\\nhttp://archive.ics.uci.edu/ml/,,
46,2,19,d14373a3-820e-4d7c-aec6-e87297be7405,2010-07-19 19:24:21.0,55.0,"[Gapminder][1] has a number (430 at the last look) of datasets, which may or may not be of use to you.\\n\\n\\n  [1]: http://www.gapminder.org/data/",,
47,2,20,cc8e15db-c7ce-451a-af1e-81ef63a6b882,2010-07-19 19:24:35.0,37.0,"The assumption of normality assumes your data is normally distributed (the bell curve, or gaussian distribution). You can check this by plotting the data or checking the measures for kurtosis (how sharp the peak is) and skewdness (?) (if more than half the data is on one side of the peak).",,
48,2,21,e63ccde2-b130-421e-a122-28b7ad228f51,2010-07-19 19:24:36.0,59.0,"What are some of the ways to forecast demographic census with some validation and calibration techniques?\\n\\n\\nSome of the concerns:\\n\\n - Census blocks vary in sizes as rural\\n   areas are a lot larger than condensed\\n   urban areas. Is there a need to account for the area size difference?\\n - if let's say I have census data\\n   dating back to 4 - 5 census periods,\\n   how far can i forecast it into the\\n   future?\\n - if some of the census zone change\\n   lightly in boundaries, how can i\\n   account for that change?\\n - What are the methods to validate\\n   census forecasts? for example, if i\\n   have data for existing 5 census\\n   periods, should I model the first 3\\n   and test it on the latter two? or is\\n   there another way?\\n - what's the state of practice in\\n   forecasting census data, and what are\\n   some of the state of the art methods?",,
49,1,21,e63ccde2-b130-421e-a122-28b7ad228f51,2010-07-19 19:24:36.0,59.0,forecasting demographic census,,
50,3,21,e63ccde2-b130-421e-a122-28b7ad228f51,2010-07-19 19:24:36.0,59.0,<population><census><forecast>,,
51,2,22,b235a8fa-e621-4456-a6c2-8c8e1eef48a2,2010-07-19 19:25:39.0,66.0,How would you describe in plain English the characteristics that distinguish Bayesian from Frequentist reasoning?,,
52,1,22,b235a8fa-e621-4456-a6c2-8c8e1eef48a2,2010-07-19 19:25:39.0,66.0,Bayesian and Frequentist reasoning in Plain English,,
53,3,22,b235a8fa-e621-4456-a6c2-8c8e1eef48a2,2010-07-19 19:25:39.0,66.0,<baysian><frequentist>,,
54,2,23,f737130b-6f40-4902-b38d-4a91fd7a300c,2010-07-19 19:26:04.0,69.0,How can I find the PDF (probability density function) of a distribution given the CDF (cumulative distribution function)?,,
55,1,23,f737130b-6f40-4902-b38d-4a91fd7a300c,2010-07-19 19:26:04.0,69.0,Finding the PDF given the CDF,,
56,3,23,f737130b-6f40-4902-b38d-4a91fd7a300c,2010-07-19 19:26:04.0,69.0,<distributions><pdf><cdf>,,
57,2,24,2d29e8f3-f301-478b-be9e-331a24ac5d8e,2010-07-19 19:26:13.0,61.0,"For doing a variety of MCMC tasks in Python, there's [PyMC][1], which I've gotten quite a bit of use out of.  I haven't run across anything that I can do in BUGS that I can't do in PyMC, and the way you specify models and bring in data seems to be a lot more intuitive to me.\\n\\n\\n  [1]: http://code.google.com/p/pymc/",,
58,16,24,2d29e8f3-f301-478b-be9e-331a24ac5d8e,2010-07-19 19:26:13.0,-1.0,,,
59,2,25,e9605664-43bc-4ad7-8114-2cb5fd614d40,2010-07-19 19:27:13.0,69.0,What modern tools (Windows-based) do you suggest for modeling financial time series?,,
60,1,25,e9605664-43bc-4ad7-8114-2cb5fd614d40,2010-07-19 19:27:13.0,69.0,Tools for modeling financial time series,,
61,3,25,e9605664-43bc-4ad7-8114-2cb5fd614d40,2010-07-19 19:27:13.0,69.0,<software-rec><time-series><financial><modeling>,,
62,2,26,b2197fb4-3f17-4fe0-8d27-24d8e84ac96a,2010-07-19 19:27:43.0,75.0,"What is a standard deviation, how is it calculuated and what is its use in statistics?",,
63,1,26,b2197fb4-3f17-4fe0-8d27-24d8e84ac96a,2010-07-19 19:27:43.0,75.0,What is a standard deviation?,,
64,3,26,b2197fb4-3f17-4fe0-8d27-24d8e84ac96a,2010-07-19 19:27:43.0,75.0,<statistics><standard-deviation>,,
65,2,27,9a55d623-4b24-4697-9a71-153b5bcbde95,2010-07-19 19:28:12.0,68.0,http://mathforum.org/workshops/sum96/data.collections/datalibrary/data.set6.html,,
66,2,28,7e616274-086a-4e71-9dfa-5a4e495917a6,2010-07-19 19:28:12.0,,"[GSL][1] for those of you who wish to program in C / C++ is a valuable resource as it provides several routines for random generators, linear algebra etc. While GSL is primarily available for Linux there are also ports for Windows. (See: http://gladman.plushost.co.uk/oldsite/computing/gnu_scientific_library.php and http://david.geldreich.free.fr/dev.html)\\n\\n\\n  [1]: http://www.gnu.org/software/gsl/",,user28
67,16,28,7e616274-086a-4e71-9dfa-5a4e495917a6,2010-07-19 19:28:12.0,-1.0,,,
68,2,29,bfc58ad7-1542-423f-851a-95b8f2196c9e,2010-07-19 19:28:15.0,36.0,Contingency table (chi-square). Also Logistic Regression is your friend - use dummy variables. ,,
69,2,30,83659a4b-a1da-4479-9b30-06d2ff7ca936,2010-07-19 19:28:34.0,69.0,Which methods are used for testing random variate generation algorithms?,,
70,1,30,83659a4b-a1da-4479-9b30-06d2ff7ca936,2010-07-19 19:28:34.0,69.0,Testing random variate generation algorithms,,
71,3,30,83659a4b-a1da-4479-9b30-06d2ff7ca936,2010-07-19 19:28:34.0,69.0,<random-variate><hypothesis-testing><random-generation><algorithms>,,
72,2,31,a79cca70-fdf1-4c86-b7bb-93f5562a5dc5,2010-07-19 19:28:44.0,13.0,"After taking a statistics course and then trying to help fellow students, I noticed one subject that inspires much head-desk banging is interpreting the results of statical hypothesis tests.  It seems that students easily learn how to perform the calculations required by a given test but get hung up on interpreting the results.  Many computerized tools report test results in terms of ""p values"" or ""t values"".\\n\\nHow would you explain the following points to college students taking their first course in statistics:\\n\\n  * What does a ""p-value"" mean in relation to the hypothesis being tested?  Are there cases when one should be looking for a high p-value or a low p-value?\\n\\n  * What is the relationship between a p-value and a t-value?\\n",,
73,1,31,a79cca70-fdf1-4c86-b7bb-93f5562a5dc5,2010-07-19 19:28:44.0,13.0,What is the meaning of p values and t values in statistical tests?,,
74,3,31,a79cca70-fdf1-4c86-b7bb-93f5562a5dc5,2010-07-19 19:28:44.0,13.0,<hypothesis-testing><p-value><t-value>,,
75,2,32,6fd8f953-2f27-4c66-bb40-6476dfa9dd09,2010-07-19 19:29:06.0,5.0,"I recommend R (see [the time series view on CRAN][1]).  \\n\\nSome useful references:\\n\\n - [Econometrics in R][2], by Grant Farnsworth\\n - [Multivariate time series modelling in R][3]\\n\\n\\n  [1]: http://cran.r-project.org/web/views/TimeSeries.html\\n  [2]: http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf\\n  [3]: http://stackoverflow.com/questions/1714280/multivariate-time-series-modelling-in-r/1715488#1715488",,
76,2,33,f397f5c0-7376-4ef5-96e2-cbfdf7229bc6,2010-07-19 19:30:03.0,69.0,What R packages should I install for seasonality analysis?,,
77,1,33,f397f5c0-7376-4ef5-96e2-cbfdf7229bc6,2010-07-19 19:30:03.0,69.0,R packages for seasonality analysis,,
78,3,33,f397f5c0-7376-4ef5-96e2-cbfdf7229bc6,2010-07-19 19:30:03.0,69.0,<r><seasonality-analysis><statistical-analysis>,,
79,2,34,baa7406c-26fd-4eae-9555-2bba79e64573,2010-07-19 19:30:07.0,79.0,[Schools of thought in Probability Theory][1]\\n\\n\\n  [1]: http://scienceblogs.com/goodmath/2008/04/schools_of_thought_in_probabil.php,,
80,6,22,d3653def-c161-4c06-a57e-733baf37da6c,2010-07-19 19:30:08.0,80.0,<bayesian><frequentist>,edited tags,
81,2,35,4e2f657c-3787-4d39-80ce-2f065d16c9f3,2010-07-19 19:30:30.0,54.0,"I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.\\n\\n    ## assuming a median value of 1500\\n    med = 1500\\n    rawdist = rpois(1000000,med)\\n    oDdist = rawDist + ((rawDist-med)*3)\\n\\nVisually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a [negative binomial distribution, as described here](http://en.wikipedia.org/wiki/Overdispersion#Poisson)? (If so, any pointers or links on doing so would be much appreciated).\\n\\nOh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.\\n",,
82,1,35,4e2f657c-3787-4d39-80ce-2f065d16c9f3,2010-07-19 19:30:30.0,54.0,Modelling a Poisson distribution with overdispersion,,
83,3,35,4e2f657c-3787-4d39-80ce-2f065d16c9f3,2010-07-19 19:30:30.0,54.0,<modeling><poisson><distribution><overdispersion>,,
84,2,36,89191c5b-9ead-40f4-b423-cc211fcb9fbd,2010-07-19 19:31:47.0,8.0,"We all know the old saying ""Correlation does not mean causation"". When I'm teaching I tend to use these standard examples to illustrate this point:\\n\\n1. Number of storks and birth rate in Denmark;\\n1. Number of priests in America and alcoholism\\n1. In the start of the 20th century it was noted that there was a strong correlation between `Number of radios' and 'Number of people in Insane Asylums'\\n1. and my favourite: [pirates cause global warming][1] \\n\\nHowever, I don't have any references for these examples and whilst amusing, they are obviously false.\\n\\nDoes anyone have any other good examples?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/File:PiratesVsTemp_English.jpg",,
85,1,36,89191c5b-9ead-40f4-b423-cc211fcb9fbd,2010-07-19 19:31:47.0,8.0,Correlation does not mean causation,,
86,3,36,89191c5b-9ead-40f4-b423-cc211fcb9fbd,2010-07-19 19:31:47.0,8.0,<correlation><teaching>,,
88,2,38,f6d036e2-69e9-4537-b8c4-7b3434ba6650,2010-07-19 19:32:28.0,61.0,"If your mean value for the Poisson is 1500, then you're very close to a normal distribution; you might try using that as an approximation and then modelling the mean and variance separately.",,
89,2,39,61be4085-ad02-4873-ab15-d659e428a31c,2010-07-19 19:32:29.0,59.0,"I'm looking for worked out solutions using Bayesian and/or logit analysis similar to a workbook or an annal. \\n\\nThe worked out problems could be of any field; however, I'm interested in urban planning / transportation related fields. ",,
90,1,39,61be4085-ad02-4873-ab15-d659e428a31c,2010-07-19 19:32:29.0,59.0,Sample problems on logit modeling and Bayesian methods,,
91,3,39,61be4085-ad02-4873-ab15-d659e428a31c,2010-07-19 19:32:29.0,59.0,<bayesian><logit><modeling><transportation>,,
92,2,40,9d1c7dc9-43ca-4e75-b513-ab70b3ec1760,2010-07-19 19:32:47.0,69.0,What algorithms are used in modern and good-quality random number generators? ,,
93,1,40,9d1c7dc9-43ca-4e75-b513-ab70b3ec1760,2010-07-19 19:32:47.0,69.0,Pseudo-random number generation algorithms,,
94,3,40,9d1c7dc9-43ca-4e75-b513-ab70b3ec1760,2010-07-19 19:32:47.0,69.0,<random-variate><random-generation><algorithms>,,
95,6,8,8b4cd9c4-5abe-4c33-bb9b-3da717a1b3f6,2010-07-19 19:32:54.0,60.0,<lightbulb>,edited tags,
96,2,41,ce0b902a-cd13-49de-bd5b-b1ff0cb41082,2010-07-19 19:33:13.0,83.0,"A quote from [wiki][1].\\n\\n> It shows how much variation there is from the ""average"" (mean, or expected/budgeted value). A low standard deviation indicates that the data points tend to be very close to the mean, whereas high standard deviation indicates that the data is spread out over a large range of values.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Standard_deviation",,
97,2,42,fdd08f15-ae52-4361-965e-4bb79b36dc54,2010-07-19 19:33:19.0,80.0,[Weka][1] for data mining - contains many classification and clustering algorithms in Java.\\n\\n [1]: http://www.cs.waikato.ac.nz/ml/weka  ,,
98,16,42,fdd08f15-ae52-4361-965e-4bb79b36dc54,2010-07-19 19:33:19.0,-1.0,,,
99,2,43,917d5d7b-47be-4256-980e-a34bf413348c,2010-07-19 19:33:37.0,74.0,"I wouldn't really call R ""windows based"" :) That's like saying the cmd prompt is windows based. \\n\\nRapidMiner is far better [1]. It's a free, open-source, multi-platform, GUI. Here's a video on time series forecasting:\\n\\nhttp://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1\\n\\nAlso, don't forget to read:\\n\\nhttp://www.forecastingprinciples.com/\\n\\n\\n[1] No, I don't work for them. ",,
100,4,3,780804d6-b18f-4180-875e-31e397c4acf1,2010-07-19 19:34:13.0,13.0,What are some valuable Statistical Analysis open source projects?,"Removed the word ""most"" and used ""some""- most is way too subjective.",
101,2,44,32504bd0-c677-41d6-8a96-9246b231feb9,2010-07-19 19:34:42.0,68.0,How would you explain data visualization and why it is important to a layman?,,
102,1,44,32504bd0-c677-41d6-8a96-9246b231feb9,2010-07-19 19:34:42.0,68.0,Explain data visualization,,
103,3,44,32504bd0-c677-41d6-8a96-9246b231feb9,2010-07-19 19:34:42.0,68.0,<data-visualization>,,
104,2,45,132c2391-8a68-4f88-8615-213a0316784e,2010-07-19 19:34:44.0,55.0,The [Mersenne Twister][1] is one I've come across and used before now.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Mersenne_twister,,
105,2,46,801baf8c-38c6-4ac7-9634-9c06dd0ace9f,2010-07-19 19:35:04.0,62.0,"A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. \\n\\nTo put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. ",,
106,5,3,780804d6-b18f-4180-875e-31e397c4acf1,2010-07-19 19:34:13.0,13.0,What are some valuable Statistical Analysis open source projects available right now?,"Removed the word ""most"" and used ""some""- most is way too subjective.",
107,2,47,47eb2be3-688d-4b50-b187-047bfe45decd,2010-07-19 19:36:12.0,22.0,"I have a dataset of 130k internet users characterized by 4 variables describing users' number of sessions, locations visited, avg data download and session time aggregated from four months of activity.\\n\\nDataset is very heavy-tailed. For example third of users logged only once during four months, whereas six users had more than 1000 sessions.\\n\\nI wanted to come up with a simple classification of users, preferably with indication of the most appropriate number of clusters.\\n\\nIs there anything you could recomend as a soultion?\\n\\n",,
108,1,47,47eb2be3-688d-4b50-b187-047bfe45decd,2010-07-19 19:36:12.0,22.0,"Clustering of large, heavy-tailed dataset  ",,
109,3,47,47eb2be3-688d-4b50-b187-047bfe45decd,2010-07-19 19:36:12.0,22.0,<clustering>,,
110,2,48,385bebc4-2ba0-4c16-a2f9-3b600b1b181c,2010-07-19 19:36:20.0,36.0,Flip through Freakonomics for some great examples. Their bibliography is chock full of references.,,
111,2,49,a85fcdbb-34cd-493c-95ab-aab491932394,2010-07-19 19:36:52.0,5.0,"You don't need to install any packages because this is possible with base-R functions.  Have a look at [the arima function][1].  \\n\\nThis is a basic function of [Box-Jenkins analysis][2], so you should consider reading one of the R time series text-books for an overview; my favorite is Shumway and Stoffer. ""<a href=""http://www.amazon.com/gp/product/0387293175?ie=UTF8&tag=statalgo-20&linkCode=as2&camp=1789&creative=9325&creativeASIN=0387293175"">Time Series Analysis and Its Applications: With R Examples</a><img src=""http://www.assoc-amazon.com/e/ir?t=statalgo-20&l=as2&o=1&a=0387293175"" width=""1"" height=""1"" border=""0"" alt="""" style=""border:none !important; margin:0px !important;"" />"".\\n\\n\\n  [1]: http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/ts/html/arima.html\\n  [2]: http://en.wikipedia.org/wiki/Box%E2%80%93Jenkins",,
112,2,50,98be07be-76c9-49e6-838f-014ab51d5c51,2010-07-19 19:37:31.0,62.0,"What do they mean when they say ""random variable""? ",,
113,1,50,98be07be-76c9-49e6-838f-014ab51d5c51,2010-07-19 19:37:31.0,62.0,"What is meant by a ""random variable""? ",,
114,3,50,98be07be-76c9-49e6-838f-014ab51d5c51,2010-07-19 19:37:31.0,62.0,<random><variable><random-variable>,,
115,2,51,bda0f58b-2e17-4f83-8c1b-1d6d8381bbe1,2010-07-19 19:37:55.0,68.0,Are there any objective methods of assessment or standardized tests available to measure the effectiveness of a software that does pattern recognition?,,
116,1,51,bda0f58b-2e17-4f83-8c1b-1d6d8381bbe1,2010-07-19 19:37:55.0,68.0,Measuring the effectiveness of a pattern recognition software,,
117,3,51,bda0f58b-2e17-4f83-8c1b-1d6d8381bbe1,2010-07-19 19:37:55.0,68.0,<pattern-recognition>,,
118,2,52,d3d7a55d-52d3-4f0f-b1f3-d1d3a9271464,2010-07-19 19:39:06.0,36.0,"Usually one (p=.042), but it also depends on power.",,
119,2,53,52c767e7-1b74-4a46-8021-ebc11e02ea39,2010-07-19 19:39:08.0,17.0,What are the main differences between performing Principal Components Analysis on a correlation and covariance matrix? Do they give the same results?,,
120,1,53,52c767e7-1b74-4a46-8021-ebc11e02ea39,2010-07-19 19:39:08.0,17.0,PCA on Correlation or Covariance?,,
121,3,53,52c767e7-1b74-4a46-8021-ebc11e02ea39,2010-07-19 19:39:08.0,17.0,<multivariable><statistical-analysis><modeling>,,
122,2,54,9db7fd28-939f-4429-8823-b7cf3c2237d3,2010-07-19 19:41:19.0,55.0,"As I understand UK Schools teach that the Standard Deviation is found using:\\n\\n![alt text][1]\\n\\nwhereas US Schools teach:\\n\\n![alt text][2]\\n\\n(at a basic level anyway).\\n\\nThis has caused a number of my students problems in the past as they have searched on the Internet, but found the wrong explanation.\\n\\nWhy the difference?\\n\\nWith simple datasets say 10 values, what degree of error will there be if the wrong method is applied (eg in an exam)?\\n\\n\\n  [1]: http://upload.wikimedia.org/math/e/e/4/ee485814ab9e19908f2b39d5d70406d5.png\\n  [2]: http://upload.wikimedia.org/math/8/3/a/83a7338b851dcaafaf5b64353af56596.png",,
123,1,54,9db7fd28-939f-4429-8823-b7cf3c2237d3,2010-07-19 19:41:19.0,55.0,Why do US and UK Schools Teach Different methods of Calculating the Standard Deviation?,,
124,3,54,9db7fd28-939f-4429-8823-b7cf3c2237d3,2010-07-19 19:41:19.0,55.0,<standard-deviation><error><accuracy>,,
125,2,55,3551a041-31ed-4147-9d1d-2d4ba5fc1ab3,2010-07-19 19:41:39.0,56.0,The [Diehard Test Suite][1] is something close to a Golden Standard for testing random number generators. It includes a number of tests where a good random number generator should produce result distributed according to some know distribution against which the outcome using the tested generator can then be compared.\\n\\n[1]: http://en.wikipedia.org/wiki/Diehard_tests,,
126,2,56,fc2eedef-6d1c-4870-8258-6484234cf64b,2010-07-19 19:42:28.0,,"Here is how I would explain the basic difference to my grandma:\\n\\nI have misplaced my phone somewhere in the home. I can use the phone locator on the base of the instrument to locate the phone and when I press the phone locator the phone starts beeping.\\n\\nProblem: Which area of my home should I search?\\n\\nFrequentist Reasoning: \\n\\nI can hear the phone beeping. I also have a mental model which helps me identify the area from which the sound is coming from. Therefore, upon hearing the beep, I infer the area of my home I must search to locate the phone.\\n\\nBayesian Reasoning:\\n\\nI can hear the phone beeping. Now, apart from a mental model which helps me identify the area from which the sound is coming from, I also know the locations where I have misplaced the phone in the past. So, I combine my inferences using the beeps and my prior information about the locations I have misplaced the phone in the past to identify an area I must search to locate the phone.\\n\\n ",,user28
127,2,57,a35bc82b-b294-42ad-9b5a-acb7eca62393,2010-07-19 19:42:34.0,69.0,"From [Wikipedia][1]:\\n\\n> In mathematics (especially probability\\n> theory and statistics), a random\\n> variable (or stochastic variable) is\\n> (in general) a measurable function \\n> that maps a probability space into a\\n> measurable space. Random variables\\n> mapping all possible outcomes of an\\n> event into the real numbers are\\n> frequently studied in elementary\\n> statistics and used in the sciences to\\n> make predictions based on data\\n> obtained from scientific experiments.\\n> In addition to scientific\\n> applications, random variables were\\n> developed for the analysis of games of\\n> chance and stochastic  events. The\\n> utility of random variables comes from\\n> their ability to capture only the\\n> mathematical properties necessary to\\n> answer probabilistic  questions.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable",,
128,2,58,316d8f18-8ac6-400f-bcb5-2e0778ecf554,2010-07-19 19:42:57.0,68.0,What is the backpropogation algorithm and how does it work?,,
129,1,58,316d8f18-8ac6-400f-bcb5-2e0778ecf554,2010-07-19 19:42:57.0,68.0,Can someone please explain the backpropogation algorithm?,,
130,3,58,316d8f18-8ac6-400f-bcb5-2e0778ecf554,2010-07-19 19:42:57.0,68.0,<algorithms><neural-networks>,,
131,2,59,fba078cb-5c67-4766-b3cc-16074bba0e1a,2010-07-19 19:43:20.0,39.0,"The assumption of normality is just the supposition that data is distributed [normally][1], vis:\\n![alt text][2]\\n\\n\\nThis can be checked in [multiple ways][3], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][4]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [2]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [3]: http://en.wikipedia.org/wiki/Normality_test\\n  [4]: http://en.wikipedia.org/wiki/Q-Q_plot",,
132,2,60,e3d6dd9f-676c-4462-88e2-999ff5dcd02c,2010-07-19 19:43:20.0,41.0,"[K-Means clustering][1] should work well for this type of problem.  However, it does require that you specify the number of clusters in advance.\\n\\nGiven the nature of this data, however, you may be able to work with a [hierarchical clustering algorithm][2] instead.  Since all 4 variables are most likely fairly highly correlated, you can most likely break out clusters, and stop when you reach a small enough distance between clusters.  This may be a much simpler approach in this specific case, and allows you to determine ""how many clusters"" by just stopping as soon as you've broken your set into fine enough clusters.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/K-means_algorithm\\n  [2]: http://en.wikipedia.org/wiki/Cluster_analysis#Hierarchical_clustering",,
133,2,61,ead6c1b1-f746-486b-9809-13b293bc6938,2010-07-19 19:44:35.0,74.0,"The standard deviation is a metric, meaning it is a number that represents a concept. The concept in this case is the ""spread"" or ""dispersion"" of the data.\\n\\nThere are other metrics for spread, including range and variance. \\n\\nA distribution with a mean of 1 and a standard deviation of 2 has more ""spread"" than a distribution with a mean of 1 and a standard deviation of 1. \\n\\nIf there is no spread, then the standard deviation is zero. \\n\\nStandard deviation is often used as it is in the same units as the mean, unlike variance. ",,
134,2,62,d78853d7-3f4f-4901-8112-de657608a2e2,2010-07-19 19:44:58.0,58.0,"With the recent FIFA world cup, I decided to have some fun and determine which months produced world cup football players. Turned out, most footballers in the 2010 world cup were born in the first half of the year.\\n\\nSomeone pointed out, that children born in the first half of the year had a physical advantage over others and hence ""survivorship bias"" was involved in the equation. Is this an accurate observation? Can someone please explain why he says that?\\n\\nAlso, when trying to understand the concept, I found most examples revolved around the financial sector. Are they any other everyday life examples explaining it?\\n\\nThanks!",,
135,1,62,d78853d7-3f4f-4901-8112-de657608a2e2,2010-07-19 19:44:58.0,58.0,A case of survivorship bias?,,
136,3,62,d78853d7-3f4f-4901-8112-de657608a2e2,2010-07-19 19:44:58.0,58.0,<survivorship><bias><statistical>,,
137,2,63,c711f9c2-3747-4603-b6b3-0369e2c7bfdb,2010-07-19 19:45:19.0,87.0,"It might be useful to explain that ""causes"" is an asymmetric relation (X causes Y is different from Y causes X), whereas ""is correlated with"" is a symmetric relation.\\n\\nFor instance, homeless population and crime rate might be correlated, in that both tend to be high or low in the same locations.  It is equally valid to say that homelesss population is correlated with crime rate, or crime rate is correlated with homeless population.  To say that crime causes homelessness, or homeless populations cause crime are different statements.  And correlation does not imply that either is true.  For instance, the underlying cause could be a 3rd variable such as drug abuse, or unemployment.  \\n\\nThe mathematics of statistics is not good at identifying underlying causes, which requires some other form of judgement.\\n\\n",,
138,2,64,3475f714-f1ed-4538-b78b-96c993f8dd34,2010-07-19 19:46:08.0,5.0,"Yes, there are many methods.  You would need to specify which model you're using, because it can vary.  \\n\\nFor instance, Some models will be compared based on the [AIC][1] or [BIC][2] criteria.  In other cases, one would look at the [MSE from cross validation][3] (as, for instance, with a support vector machine).\\n\\nI recommend reading [Pattern Recognition and Machine Learning][4] by Christopher Bishop.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Akaike_information_criterion\\n  [2]: http://en.wikipedia.org/wiki/Bayesian_information_criterion\\n  [3]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)\\n  [4]: http://research.microsoft.com/en-us/um/people/cmbishop/prml/",,
139,2,65,e57bcbae-5a2f-4279-bd4c-cd728e4de6b8,2010-07-19 19:46:11.0,8.0,The first formula is the *population* standard deviation and the second formula is the the *sample* standard deviation. The second formula is also the unbiased maximum likelihood estimator of the standard deviation.\\n\\nI suppose (here) in the UK they don't make the distinction between sample and population at high school. They certainly don't touch concepts such as biased estimators. ,,
140,2,66,82485ae5-2703-4c0c-9d7d-94a009444128,2010-07-19 19:46:11.0,41.0,"This is [Bessel's Correction][1].  The US version is showing the formula for the *sample standard deviation*, where the UK version above is the *standard deviation of the sample*.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bessel's_correction",,
141,2,67,f5d6ef6b-a2d6-47e6-bfe4-c3d82324427f,2010-07-19 19:47:16.0,36.0,"From [Wikipedia][1]: Data visualization is the study of the visual representation of data, meaning ""information which has been abstracted in some schematic form, including attributes or variables for the units of information""\\n\\nData viz is important for visualizing trends in data, telling a story - See [Minard's map of Napoleon's march][2] - possibly one of the best data graphics ever printed.\\n\\nAlso see any of Edward Tufte's books - especially [Visual Display of Quantitative Information.][3]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Data_visualization\\n  [2]: http://www.edwardtufte.com/tufte/posters\\n  [3]: http://www.amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20",,
143,2,69,64f3ab8a-029a-4fc9-b241-979e0ae2fcc7,2010-07-19 19:47:49.0,56.0,"Since N is the number of points in the data set, one could argue that by calculating the mean one has reduced the degree of freedom in the data set by one (since one introduced a dependency into the data set), so one should use N-1 when estimating the standard deviation from a data set for which one had to estimate the mean before.",,
144,5,57,054a085f-97d0-4de4-91e5-2770cbc9d23e,2010-07-19 19:47:54.0,69.0,"From [Wikipedia][1]:\\n\\n> In mathematics (especially probability\\n> theory and statistics), a random\\n> variable (or stochastic variable) is\\n> (in general) a measurable function \\n> that maps a probability space into a\\n> measurable space. Random variables\\n> mapping all possible outcomes of an\\n> event into the real numbers are\\n> frequently studied in elementary\\n> statistics and used in the sciences to\\n> make predictions based on data\\n> obtained from scientific experiments.\\n> In addition to scientific\\n> applications, random variables were\\n> developed for the analysis of games of\\n> chance and stochastic  events. The\\n> utility of random variables comes from\\n> their ability to capture only the\\n> mathematical properties necessary to\\n> answer probabilistic  questions.\\n\\nFrom [cnx.org][2]:\\n\\n> A random variable is a function, which assigns unique numerical values to all possible\\n> outcomes of a random experiment under fixed conditions. A random variable is not a \\n> variable but rather a function that maps events to numbers.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://cnx.org/content/m13418/latest/",added 310 characters in body,
145,2,70,ff76a5d4-d8ec-4a3b-9dbb-c2a3ba6cc68f,2010-07-19 19:48:45.0,22.0,[World Bank][1] offers quite a lot of interesting data and has been recently very active in developing nice [API][2] for it.\\n\\n\\n  [1]: http://data.worldbank.org/data-catalog\\n  [2]: http://data.worldbank.org/developers/api-overview,,
146,2,71,952cc7e1-3709-42b8-bf10-c8e60d99446d,2010-07-19 19:50:33.0,36.0,"It's an algorithm for training feedforward multilayer neural networks (multilayer perceptrons). There are several nice java applets around the web that illustrate what's happening, like this one: http://neuron.eng.wayne.edu/bpFunctionApprox/bpFunctionApprox.html. Also, [Bishop's book on NNs][1] is the standard desk reference for anything to do with NNs.\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0198538642/ref=nosim/gettgenedone-20",,
147,6,62,1f068b07-c91e-4a83-8a5d-d836258cbdb6,2010-07-19 19:50:45.0,58.0,<statistical-bias>,edited tags,
148,5,59,22f9cc27-c828-4fc6-b41c-dbe3dae4afee,2010-07-19 19:50:46.0,39.0,"The assumption of normality is just the supposition that the underlying [random variable][1] of interest is distributed [normally][2], or approximately so; vis:\\n\\n![alt text][3]\\n\\nFrom the following function:\\n![alt text][4]\\n\\n\\nThis can be checked in [multiple ways][5], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][6]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [3]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [4]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [5]: http://en.wikipedia.org/wiki/Normality_test\\n  [6]: http://en.wikipedia.org/wiki/Q-Q_plot",added 251 characters in body,
149,2,72,16e0816c-cddb-41ce-893e-70aeb2f13bc5,2010-07-19 19:51:05.0,96.0,"for overdispersed poisson, use the negative binomial, which allows you to parameterize the variance as a function of the mean precisely.  rnbinom(), etc. in R.",,
150,2,73,498e2950-a5f1-429f-ae59-b29c1d9e71f2,2010-07-19 19:51:32.0,22.0,What are the R packages you couldn't imagine your daily work with data?\\nBoth general and specific tools.,,
151,1,73,498e2950-a5f1-429f-ae59-b29c1d9e71f2,2010-07-19 19:51:32.0,22.0,What R packages do you find most useful in your daily work?,,
152,3,73,498e2950-a5f1-429f-ae59-b29c1d9e71f2,2010-07-19 19:51:32.0,22.0,<r>,,
153,2,74,7d4db1ed-7a4f-41cf-b26c-af0053630fa6,2010-07-19 19:51:34.0,88.0,"In such a discussion, I always recall the famous Ken Thompson quote \\n\\n> When in doubt, use brute force.\\n\\nIn this case, machine learning is a salvation when the assumptions are hard to catch; or at least it is much better than guessing them wrong. ",,
154,2,75,c27e7f0a-5bb3-4044-9986-ad143dcfcdb5,2010-07-19 19:52:31.0,69.0,"I'm using [**R**][1] and the manuals on the R site are really informative. However, I'd like to see some more examples and implementations with R which can help me develop my knowledge faster. Any suggestions?\\n\\n\\n  [1]: http://www.r-project.org/",,
155,1,75,c27e7f0a-5bb3-4044-9986-ad143dcfcdb5,2010-07-19 19:52:31.0,69.0,Where can I find useful R tutorials with various implementations?,,
156,3,75,c27e7f0a-5bb3-4044-9986-ad143dcfcdb5,2010-07-19 19:52:31.0,69.0,<r><books><implementation>,,
157,2,76,03001983-95cf-4200-8dd4-256dff09b148,2010-07-19 19:52:49.0,5.0,"I use [**plyr**][1] and [**ggplot2**][2] the most on a daily basis.\\n\\nI also rely heavily on time series packages; most especially, the [**zoo**][3] package.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/plyr/index.html\\n  [2]: http://cran.r-project.org/web/packages/ggplot2/index.html\\n  [3]: http://cran.r-project.org/web/packages/zoo/index.html",,
158,2,77,c3190e4b-6f57-4e8f-9892-5905fdafabf2,2010-07-19 19:54:03.0,74.0,"1. Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally.\\n\\n2. To find causation, you need generally experimental data, not observational data. ",,
159,2,78,cf3824a4-3054-41b2-ae14-faa3ad0c9faf,2010-07-19 19:54:38.0,8.0,You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales. Using the covariance matrix *standardises* the data.\\n\\nIn general they give different results. Especially when the scales are different.,,
160,2,79,1ead3165-e78d-482d-abf0-761c357687a4,2010-07-19 19:56:04.0,25.0,I am not sure this is purely a US vs. British issue. Here is a brief page I wrote [explaining the difference between using n vs. n-1 when computing a Standard Deviation](http://www.graphpad.com/faq/viewfaq.cfm?faq=1382).\\n,,
161,5,59,126a8681-d0ff-4442-a8b5-5d7e19e9a23a,2010-07-19 19:56:05.0,39.0,"The assumption of normality is just the supposition that the underlying [random variable][1] of interest is distributed [normally][2], or approximately so; vis:\\n\\n![alt text][3]\\n\\nFrom the following function:\\n![alt text][4]\\nwhere μ and σ^2 are the mean and the variance, respectively.\\n\\nIntuitively, normality may be understood as the result of the sum of a large number of independent random events.\\n\\nThis can be checked in [multiple ways][5], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][6]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [3]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [4]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [5]: http://en.wikipedia.org/wiki/Normality_test\\n  [6]: http://en.wikipedia.org/wiki/Q-Q_plot",added 115 characters in body; added 62 characters in body,
162,2,80,7ba9c721-1380-4b0e-8e7a-16eb101b4f6b,2010-07-19 19:56:43.0,93.0,"The basic idea behind this is that football clubs have an age cut-off when determining teams.  In the league my children participate in the age restrictions states that children born after July 31st are placed on the younger team.  This means that two children that are effectively the same age can be playing with two different age groups.  The child born July 31st will be playing on the older team and theoretically be the youngest and smallest on the team and in the league.  The child born on August 1st will be the oldest and largest child in the league and will be able to benefit from that.\\n\\nThe survivorship bias comes because competitive leagues will select the best players for their teams.  The best players in childhood are often the older players since they have additional time for their bodies to mature.  This means that otherwise acceptable younger players are not selected simply because of their age.  Since they are not given the same opportunities as the older kids, they don’t develop the same skills and eventually drop out of competitive soccer.\\n\\nIf the cut-off for competitive soccer in enough countries is January 1st, that would support the phenomena you see.  A similar phenomena has been observed in several other sports including baseball and ice hockey.",,
163,2,81,55bf06bc-ca75-4915-8310-9513c335053c,2010-07-19 19:58:56.0,69.0,"I use the **[xtable][1]** package. The xtable package turns tables produced by R (in particular, the tables displaying the anova results) into LaTeX tables, to be included in an article. \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/xtable/index.html",,
165,5,59,fd1cd967-cf1c-4b92-88cc-50646bf3337f,2010-07-19 20:02:04.0,39.0,"The assumption of normality is just the supposition that the underlying [random variable][1] of interest is distributed [normally][2], or approximately so.  Intuitively, normality may be understood as the result of the sum of a large number of independent random events.\\n\\nMore specifically, normal distributions are defined by the following function:\\n\\n![alt text][4]\\n\\nwhere μ and σ^2 are the mean and the variance, respectively, and which appears as follows:\\n\\n![alt text][3]\\n\\nThis can be checked in [multiple ways][5], that may be more or less suited to your problem by its features, such as the size of n.  Basically, they all test for features expected if the distribution were normal (e.g. expected [quantile distribution][6]).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_variable\\n  [2]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [3]: http://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/300px-Normal_Distribution_PDF.svg.png\\n  [4]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [5]: http://en.wikipedia.org/wiki/Normality_test\\n  [6]: http://en.wikipedia.org/wiki/Q-Q_plot",added 73 characters in body; added 4 characters in body,
166,5,77,67cc4076-c675-464c-be49-3d77d9885e7c,2010-07-19 20:02:48.0,74.0,"1. Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally.\\n\\n2. To find causation, you generally need experimental data, not observational data [1]\\n\\n\\n[1] Though, in economics, they often use observed ""shocks"" to the system to test for causation, like if a CEO dies suddenly and the stock price goes up, you can assume causation. ",added 187 characters in body,
167,2,83,966fa512-a91d-48ff-89ec-b97a7012ab24,2010-07-19 20:02:51.0,5.0,"R is designed around ideas such as ""reproducible research"" and ""trustworthy software"", as John Chambers says <a href=""http://books.google.com/books?id=UXneuOIvhEAC&printsec=frontcover"">in his excellent book ""Software for Data Analysis: Programming with R""</a>.  \\n\\nOne of the best ways to learn R is to look at the wealth of source code that available on [CRAN][1] (with 2461 packages and counting).  Simple `install.packages`, load a `library()`, and start browsing the code.\\n\\n\\n  [1]: http://cran.r-project.org/",,
168,2,84,f601b199-5fdf-4795-beca-38630dad62b6,2010-07-19 20:03:34.0,41.0,"I would explain it to a layman as:\\n\\n> Data visualization is taking data, and making a picture out of it.  This allows you to easily see and understand relationships within the data much more easily than just looking at the numbers.\\n\\n",,
169,16,73,029c0265-e658-4927-b63b-a53ddd478892,2010-07-19 20:07:49.0,22.0,,,
170,5,73,029c0265-e658-4927-b63b-a53ddd478892,2010-07-19 20:07:49.0,22.0,What are the R packages you couldn't imagine your daily work with data?\\nPlease list both general and specific tools.,added 12 characters in body,
171,2,85,4885a394-9de8-478b-803c-3eeabecfdd64,2010-07-19 20:08:00.0,87.0,"A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as ""state"", and then the random variable is a function of the state.\\n\\nExample:  Suppose we have three dice rolls (D1,D2,D3).  Then the state S=(D1,D2,D3).  A ramdom variable X is the number of 5s. X=(D1==5?)+(D2==5?)+(D3==5?).   Another random variable Y is the sum of the dice rolls Y=D1+D2+D3.  ",,
172,2,86,9bad9ae7-3b1f-4ec6-b275-69f71b0b6a98,2010-07-19 20:08:37.0,13.0,"Unlike a normal variable, a random variable may not be substituted for a single, unchanging value.  Rather **statistical properties** such as the **distribution** of the random variable may be proscribed.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.  \\n\\nRandom variables may be classified as *discreet* if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is *continuous* and is used if the distribution covers values from an uncountable set such as the real numbers.",,
175,2,89,afc1d7bc-11ff-4989-9179-cea3f4766e34,2010-07-19 20:11:47.0,55.0,"When I teach very basic statistics to Secondary School Students I talk about evolution and how we have evolved to spot patterns in pictures rather than lists of numbers and that data visualisation is one of the techniques we use to take advantage of this fact. \\n\\nPlus I try to talk about recent news stories where statistical insight contradicts what the press is implying, making use of sites like [Gapminder][1] to find the representation before choosing the story.\\n\\n\\n  [1]: http://www.gapminder.org/",,
176,5,64,c095b3e2-d112-4ee9-bab3-421f99d3bb25,2010-07-19 20:12:16.0,5.0,"Yes, there are many methods.  You would need to specify which model you're using, because it can vary.  \\n\\nFor instance, Some models will be compared based on the [AIC][1] or [BIC][2] criteria.  In other cases, one would look at the [MSE from cross validation][3] (as, for instance, with a support vector machine).\\n\\n 1. I recommend reading [Pattern Recognition and Machine Learning][4] by Christopher Bishop.\\n 2. This is also discussed in Chapter 5 on Credibility, and particularly section 5.5 ""Comparing data mining methods"" of [Data Mining: Practical Machine Learning Tools and Techniques][5] by Witten and Frank (which discusses Weka in detail).\\n 3. Lastly, you should also have a look at [The Elements of Statistical Learning][6] by Hastie, Tibshirani and Friedman which is available for free online.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Akaike_information_criterion\\n  [2]: http://en.wikipedia.org/wiki/Bayesian_information_criterion\\n  [3]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)\\n  [4]: http://research.microsoft.com/en-us/um/people/cmbishop/prml/\\n  [5]: http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0120884070/ref=ntt_at_ep_dpi_1\\n  [6]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/",added 568 characters in body,
177,2,90,680dcd60-5a29-4d16-a617-4cb7c9d2d027,2010-07-19 20:12:24.0,22.0,[R bloggers][1] has been steadily supplying me with a lot of good pragmatic content.  \\nFrom the author:\\n\\n    R-Bloggers.com is a central hub (e.g: A blog aggregator) of content \\n    collected from bloggers who write about R (in English). \\n    The site will help R bloggers and users to connect and follow \\n    the “R blogosphere”.\\n\\n\\n  [1]: http://www.r-bloggers.com/,,
178,2,91,f9d47ca6-7a93-4012-9169-2ac7851b6fc3,2010-07-19 20:15:54.0,87.0,"As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous variable, and the difference for a discrete variable. \\n\\nIn the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta ""functions"" can be used to represent these atoms.    \\n\\n ",,
180,2,93,f802855b-9345-484f-9570-d3532ee5d0d7,2010-07-19 20:17:07.0,61.0,"We're trying to use a Gaussian process to model h(t) -- the hazard function -- for a very small initial population, and then fit that using the available data.  While this gives us nice plots for credible sets for h(t) and so on, it unfortunately is also just pushing the inference problem from h(t) to the covariance function of our process.  Perhaps predictably, we have several reasonable and equally defensible guesses for this that all produce different result.  \\n\\nHas anyone run across any good approaches for addressing such a problem?  Gaussian-process related or otherwise?",,
181,1,93,f802855b-9345-484f-9570-d3532ee5d0d7,2010-07-19 20:17:07.0,61.0,Robust nonparametric estimation of hazard/survival functions based on low count data,,
182,3,93,f802855b-9345-484f-9570-d3532ee5d0d7,2010-07-19 20:17:07.0,61.0,<survivorship><hazard-function><survival-analysis><nonparametric>,,
183,2,94,ef5c28a1-db1a-4b42-9645-9bd0f259dfa7,2010-07-19 20:18:24.0,88.0,"Quick R site is basic, but quite nice for start http://www.statmethods.net/index.html . ",,
185,10,8,abf0405c-3e41-4bab-a3b0-3710520084bc,2010-07-19 20:19:46.0,-1.0,"{""Voters"":[{""Id"":60,""DisplayName"":""Jason Punyon""},{""Id"":68,""DisplayName"":""Ami""},{""Id"":55,""DisplayName"":""Amos""},{""Id"":103,""DisplayName"":""rcs""},{""Id"":69,""DisplayName"":""Mehper C. Palavuzlar""}]}",2,
186,5,78,b9e34a27-912c-4ee1-b3d5-4afabeda9000,2010-07-19 20:19:52.0,8.0,"You tend to use the covariance matrix when the variable scales are similar and the correlation matrix when variables are on different scales. Using the covariance matrix *standardises* the data.\\n\\nIn general they give different results. Especially when the scales are different.\\n\\nAs example, take a look a look at this R heptathlon data set. Some of the variables have an average value of about 1.8 (the high jump), whereas other variables (200m) are around 20.\\n\\n    library(HSAUR)\\n    # look at heptathlon data\\n    heptathlon\\n    \\n    # correlations\\n    round(cor(heptathlon[,-8]),2)   # ignoring ""score"" \\n    # covariance\\n    round(cov(heptathlon[,-8]),2)\\n    \\n    # PCA\\n    # scale=T bases the PCA on the correlation matrix\\n    hep.PC.cor = prcomp(heptathlon[,-8], scale=TRUE)\\n    hep.PC.cov = prcomp(heptathlon[,-8], scale=FALSE)\\n    \\n    # PC scores per competitor\\n    hep.scores.cor = predict(hep.PC.cor)\\n    hep.scores.cov = predict(hep.PC.cov)\\n    \\n    # Plot of PC1 vs PC2\\n    par(mfrow = c(2, 1))\\n    plot(hep.scores.cov[,1],hep.scores.cov[,2],\\n         xlab=""PC 1"",ylab=""PC 2"", pch=NA, main=""Covariance"")\\n    text(hep.scores.cov[,1],hep.scores.cov[,2],labels=1:25) \\n    \\n    plot(hep.scores.cor[,1],hep.scores.cor[,2],\\n         xlab=""PC 1"",ylab=""PC 2"", pch=NA, main=""Correlation"")\\n    text(hep.scores.cor[,1],hep.scores.cor[,2],labels=1:25) \\n\\n\\nNotice that the outlying individuals (in *this* data set) are outliers regardless of whether the covariance or correlation matrix is used.",Added R code,
187,2,95,43effdb3-3898-4c0a-88c9-9e21e9340fc6,2010-07-19 20:21:35.0,57.0,"I have been using various GARCH-based models to forecast volatility for various North American equities using historical daily data as inputs.\\n\\nAsymmetric GARCH models are often cited as a modification of the basic GARCH model to account for the 'leverage effect' i.e. volatility tends to increase more after a negative return than a similarly sized positive return.\\n\\nWhat kind of a difference would you expect to see between a standard GARCH and an asymmetric GARCH forecast for a broad-based equity index like the S&P 500 or the NASDAQ-100?\\n\\nThere is nothing particularly special about these two indices, but I think it is helpful to give something concrete to focus the discussion, as I am sure the effect would be different depending on the equities used.",,
188,1,95,43effdb3-3898-4c0a-88c9-9e21e9340fc6,2010-07-19 20:21:35.0,57.0,How Large a Difference Can Be Expected Between Standard GARCH and Asymmetric GARCH Volatility Forecasts?,,
189,3,95,43effdb3-3898-4c0a-88c9-9e21e9340fc6,2010-07-19 20:21:35.0,57.0,<garch><volatility-forecasting>,,
190,2,96,fc27569f-8ee8-41ad-ba12-2794b21c2239,2010-07-19 20:22:44.0,5.0,"Another great resource is [the LearnR blog][1], which went through an extensive study of visualizations with lattice and ggplot2.\\n\\n\\n  [1]: http://learnr.wordpress.com/",,
191,2,97,6dc08214-db6a-4a26-9e69-4e59e6ce94a3,2010-07-19 20:23:22.0,114.0,"I have some [ordinal data][1] gained from survey questions.  In my case they are [Likert style][2] responses (Strongly Disagree-Disagree-Neutral-Agree-Strongly Agree).  In my data they are coded as 1-5.\\n\\nI  don't think means would mean much here, so what basic summary statistics are considered usefull?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Ordinal_scale#Ordinal_scale\\n  [2]: http://en.wikipedia.org/wiki/Likert_scale",,
192,1,97,6dc08214-db6a-4a26-9e69-4e59e6ce94a3,2010-07-19 20:23:22.0,114.0,What are good basic statistics to use for Ordinal data,,
193,3,97,6dc08214-db6a-4a26-9e69-4e59e6ce94a3,2010-07-19 20:23:22.0,114.0,<ordinal><summary-statistics>,,
194,2,98,d807e739-dba0-4db0-9be0-a60f1949f99b,2010-07-19 20:23:57.0,39.0,"Eliciting priors is a tricky business. \\n\\n[This][1] and [this][2] are quite good practical guides for prior elicitation.  The process is outlined as follows:\\n\\n 1. background and preparation;\\n 2. identifying and recruiting the expert(s);\\n 3. motivation and training the expert(s);\\n 4. structuring and decomposition (typically deciding precisely what variables should\\nbe elicited, and how to elicit joint distributions in the multivariate case);\\n 5. the elicitation itself.\\n\\nThe elicitation results in parameters to fit to distributions modeling expert knowledge (i.e. in the Bayesian context, [Beta distributions][3] are quite popular )\\n\\n\\n  [1]: http://www.stat.cmu.edu/tr/tr808/tr808.pdf\\n  [2]: http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf\\n  [3]: http://en.wikipedia.org/wiki/Beta_distribution",,
195,5,43,fd088df0-cd62-496a-a8ed-bfa7b84575ab,2010-07-19 20:24:23.0,74.0,"R is great, but I wouldn't really call it ""windows based"" :) That's like saying the cmd prompt is windows based. I guess it is technically in a window...\\n\\nRapidMiner is far easier to use [1]. It's a free, open-source, multi-platform, GUI. Here's a video on time series forecasting:\\n\\nhttp://rapidminerresources.com/index.php?page=financial-time-series-modelling---part-1\\n\\nAlso, don't forget to read:\\n\\nhttp://www.forecastingprinciples.com/\\n\\n\\n[1] No, I don't work for them. ",added 64 characters in body,
196,2,99,4381a266-d99d-4860-b452-69b189aff068,2010-07-19 20:25:08.0,88.0,[multicore][1] is quite nice for tool for making faster scripts faster.  \\n[cacheSweave][2] saves a lot of time when using `Sweave`.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/multicore/index.html \\n  [2]: http://cran.r-project.org/web/packages/cacheSweave/index.html,,
197,16,99,4381a266-d99d-4860-b452-69b189aff068,2010-07-19 20:25:08.0,-1.0,,,
198,2,100,1f7e287e-a7cf-46e0-aa66-37fa813f32cc,2010-07-19 20:30:23.0,117.0,"I'd like to see the answer with qualitative view on the problem, not just definition. Examples and analogous from other areas of applied math also would be good.\\n\\nI understand, my question is silly, but I can't find good and intuitive introduction textbook on signal processing — if someone would suggest one, I will be happy.\\n\\nThanks.",,
199,1,100,1f7e287e-a7cf-46e0-aa66-37fa813f32cc,2010-07-19 20:30:23.0,117.0,What's the purpose of window function in spectral analysis?,,
200,3,100,1f7e287e-a7cf-46e0-aa66-37fa813f32cc,2010-07-19 20:30:23.0,117.0,<signal-processing>,,
201,2,101,961d9b70-28f4-4279-b2e9-1075ee1d8995,2010-07-19 20:32:08.0,,"Understanding p-value\\n\\nSuppose, that you want to test the hypothesis that the average height of male students at your University is 5 ft 7 inches. You collect heights of 100 students selected at random and compute the sample mean (say it turns out to be 5 ft 9 inches). Using an appropriate formula/statistical routine you compute the p-value for your hypothesis and say it turns out to be 0.06. \\n\\nIn order to interpret p=0.06 appropriately, we should keep several things in mind:\\n\\n1. The first step under classical hypothesis testing is the assumption that the hypothesis under consideration is true. (In our context, we assume that the **true** average height is 5 ft 7 inches.\\n\\n2. Imagine doing the following calculation: Compute the probability that the sample mean is greater than 5 ft 9 inches assuming that our hypothesis is in fact correct (see point 1). \\n\\nIn other words, we want to know P(Sample mean >= 5 ft 9 inches | Given true value = 5 ft 7 inches). \\n\\nThe calculation in step 2 is what is called the p-value. Therefore, a p-value of 0.06 would mean that if we were to repeat our experiment many, many times (each time we select 100 students at random and compute the sample mean) then 6 times out of 100 we can expect to see a sample mean greater than 5 ft 9 inches.\\n\\nGiven the above understanding, should we still retain our assumption that our hypothesis is true (see step 1). Well, a p=0.06 indicates that one of two things have happened: \\n\\n(A) Either our hypothesis is correct and an extremely unlikely event has occurred (e.g., all 100 students are student athletes)\\n\\nor\\n\\n(B) Our assumption is incorrect and the sample we have obtained is not that unusual. \\n\\nThe traditional way to choose between (A) and (B) is to choose an arbitrary cut-off for p. We choose (A) if p > 0.05 and (B) if p < 0.05.\\n\\nPS: Need to run now. Will add about the t-test when I find some more time.",,user28
202,2,102,abd65999-5783-4839-a1d8-c759970b1a8c,2010-07-19 20:32:14.0,22.0,For me [Illuminating the Path][1] report has been always good point of reference.   \\nFor more recent overview you can also have a look at good [article][2] by Heer and colleagues.\\n\\n\\n  [1]: http://nvac.pnl.gov/agenda.stm\\n  [2]: http://queue.acm.org/detail.cfm?id=1805128,,
203,2,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,2010-07-19 20:33:26.0,5.0,What is the best blog on data visualization?\\n\\nI'm making this question a community wiki since it is highly subjective.  Please limit each answer to one link.,,
204,1,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,2010-07-19 20:33:26.0,5.0,What is your favorite data visualization blog?,,
205,3,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,2010-07-19 20:33:26.0,5.0,<data-visualization><blog>,,
206,16,103,8e9debcf-998f-4cbf-bad2-85b7f88905a0,2010-07-19 20:33:26.0,5.0,,,
207,2,104,862150c8-ed85-492e-b806-83d706588cf6,2010-07-19 20:34:46.0,74.0,"A frequency table is a good place to start. You can do the count, and relative frequency for each level. Also, the total count, and number of missing values may be of use. \\n\\nYou can also use a contingency table to compare two variables at once. Can display using a mosaic plot too.\\n\\n\\n\\n",,
208,6,73,997aaa66-6101-41ed-a0bb-775b5a284c5b,2010-07-19 20:35:22.0,69.0,<r><subjective>,edited tags,
209,2,105,b036b8cf-e030-4db8-9b61-262e23edae87,2010-07-19 20:35:34.0,46.0,http://flowingdata.com/,,
210,16,105,b036b8cf-e030-4db8-9b61-262e23edae87,2010-07-19 20:35:34.0,-1.0,,,
214,2,107,06cf2365-faf1-4477-b975-faf0ba618162,2010-07-19 20:36:01.0,46.0,http://infosthetics.com/,,
215,16,107,06cf2365-faf1-4477-b975-faf0ba618162,2010-07-19 20:36:01.0,-1.0,,,
216,5,98,21d0b1e0-5f79-40e8-8740-283b529dee65,2010-07-19 20:36:42.0,39.0,"Eliciting priors is a tricky business. \\n\\n[This][1] and [this][2] are quite good practical guides for prior elicitation.  The process is outlined as follows:\\n\\n 1. background and preparation;\\n 2. identifying and recruiting the expert(s);\\n 3. motivation and training the expert(s);\\n 4. structuring and decomposition (typically deciding precisely what variables should\\nbe elicited, and how to elicit joint distributions in the multivariate case);\\n 5. the elicitation itself.\\nOf course, the papers also review how the elicitation results in parameters that may be fit to or define distributions for modeling expert knowledge (for instance, in the Bayesian context, [Beta distributions][3]), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow distributions) .\\n\\n\\n  [1]: http://www.stat.cmu.edu/tr/tr808/tr808.pdf\\n  [2]: http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf\\n  [3]: http://en.wikipedia.org/wiki/Beta_distribution",added 163 characters in body; added 9 characters in body,
217,2,108,0db02b44-3cab-4bd0-9c43-d7dab5a7a4e7,2010-07-19 20:36:47.0,5.0,http://www.informationisbeautiful.net/,,
218,16,108,0db02b44-3cab-4bd0-9c43-d7dab5a7a4e7,2010-07-19 20:36:47.0,-1.0,,,
219,2,109,8d56adbc-decc-49ef-a841-f11b52cc4a84,2010-07-19 20:37:21.0,25.0,"Following one-way ANOVA, there are many possible follow-up multiple comparison tests. Holm's test (or better, the Holm-Sidak) test has lots of power, but because it works in a stepwise manner, it cannot compute confidence intervals. Its advantage over the tests than can compute confidence intervals (Tukey, Dunnett) is that is has more power. But is it fair to say that the Holm method *always* has more power than the methods of Tukey and Dunnet? Or does it depend...?",,
220,1,109,8d56adbc-decc-49ef-a841-f11b52cc4a84,2010-07-19 20:37:21.0,25.0,Power of Holm's multiple comparison testing compared to others.,,
221,3,109,8d56adbc-decc-49ef-a841-f11b52cc4a84,2010-07-19 20:37:21.0,25.0,<multiple-comparisons><power>,,
222,2,110,6cece4e4-4654-4c82-99f1-38c5e36c0c77,2010-07-19 20:40:36.0,61.0,"It depends on where you apply the window function.  If you do it in the time domain, it's because you only want to analyze the periodic behavior of the function in a short duration.  You do this when you don't believe that your data is from a stationary process.  \\n\\nIf you do it in the frequency domain, then you do it to isolate a specific set of frequencies for further analysis; you do this when you believe that (for instance) high-frequency components are spurious.\\n\\nThe first three chapters of ""A Wavelet Tour of Signal Processing"" by Stephane Mallat have an excellent introduction to signal processing in general, and chapter 4 goes into a very good discussion of windowing and time-frequency representations in both continuous and discrete time, along with a few worked-out examples.",,
223,2,111,d9a70998-ef1b-4422-a92b-6c37c060bab1,2010-07-19 20:41:24.0,8.0,"In R, the default setting for random number generation are:\\n\\n1.  For U(0,1), use the Mersenne-Twister algorithm\\n2.  For Guassian numbers use  the numerical inversion of the standard normal distribution function. \\n\\nYou can easily check this, viz.\\n\\n    > RNGkind()\\n    [1] ""Mersenne-Twister"" ""Inversion""\\n\\nIt is possible to change the default generator to other PRNGs, such as Super-Duper,Wichmann-Hill, Marsaglia-Multicarry or even a user-supplied PRNG. See the ?RNGkind for further details. I have never needed to change the default PRNG.\\n\\nThe [C GSL][1] library also uses the [Mersenne-Twister][2] by default.\\n\\n\\n  [1]: http://www.gnu.org/software/gsl/manual/html_node/Random-number-environment-variables.html\\n  [2]: http://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html",,
224,5,16,6a7eb0c9-d732-4106-9d16-558372254714,2010-07-19 20:43:02.0,8.0,"Two projects spring to mind:\\n\\n 1. [Bugs][1] - taking (some of) the pain out of Bayesian statistics. It allows the user to focus more on the model and a bit less on MCMC.\\n 1. [Bioconductor][2] - perhaps the most popular statistical tool in Bioinformatics. I know it's a R repository, but there are a large number of people who want to learn R, just for Bioconductor. The number of packages available for cutting edge analysis, make it second to none.\\n\\n\\n  [1]: http://www.mrc-bsu.cam.ac.uk/bugs/\\n  [2]: http://www.bioconductor.org/",added 12 characters in body,
225,5,65,116a8336-b0dc-47c5-890a-23cae0b14435,2010-07-19 20:44:53.0,8.0,The first formula is the *population* standard deviation and the second formula is the the *sample* standard deviation. The second formula is also related to the unbiased estimator of the variance - see [wikipedia][1] for further details.\\n\\nI suppose (here) in the UK they don't make the distinction between sample and population at high school. They certainly don't touch concepts such as biased estimators. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance,Adding wikipedia link and changing sd to var,
226,6,2,501bfb02-715c-4b57-8557-87b7945312ef,2010-07-19 20:51:58.0,124.0,<normality><distribution>,edited tags,
227,2,112,61662989-9e3a-40d9-aeca-3892ef038fd7,2010-07-19 20:52:27.0,12.0,I see all my favorite blogs have been listed. So I'll give you this one:\\n\\nhttp://ilovecharts.tumblr.com/\\n\\nit's a bit light hearted.,,
228,16,112,61662989-9e3a-40d9-aeca-3892ef038fd7,2010-07-19 20:52:27.0,-1.0,,,
229,2,113,055bc3f7-18bf-43f5-b830-0a67a8f3544b,2010-07-19 20:54:23.0,39.0,"I have been looking into theoretical frameworks for method selection and have found very little systematic, mathematically-motivated work. By 'method selection', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type.\\n\\nWhat I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. [Inductive Policy: The Pragmatics of Bias Selection][1]). I may be unrealistic at this early stage of machine learning's development, but I was hoping to find something like what [measurement theory][2] does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems.\\n\\nAny suggestions?\\n\\n\\n  [1]: http://portal.acm.org/citation.cfm?id=218546\\n  [2]: ftp://ftp.sas.com/pub/neural/measurement.html",,
230,1,113,055bc3f7-18bf-43f5-b830-0a67a8f3544b,2010-07-19 20:54:23.0,39.0, What are some good frameworks for method selection?,,
231,3,113,055bc3f7-18bf-43f5-b830-0a67a8f3544b,2010-07-19 20:54:23.0,39.0,<machine-learning><methodology><theory>,,
232,5,98,41368fc2-fc87-494c-9d23-d6d297cb2026,2010-07-19 20:56:56.0,39.0,"Eliciting priors is a tricky business. \\n\\n[Statistical Methods for Eliciting Probability Distributions][1] and [Eliciting Probability Distributions][2] are quite good practical guides for prior elicitation.  The process in both papers is outlined as follows:\\n\\n 1. background and preparation;\\n 2. identifying and recruiting the expert(s);\\n 3. motivation and training the expert(s);\\n 4. structuring and decomposition (typically deciding precisely what variables should\\nbe elicited, and how to elicit joint distributions in the multivariate case);\\n 5. the elicitation itself.\\n\\nOf course, they also review how the elicitation results in information that may be fit to or otherwise define distributions (for instance, in the Bayesian context, [Beta distributions][3]), but quite importantly, they also address common pitfalls in modeling expert knowledge (anchoring, narrow and small-tailed distributions, etc.).\\n\\n\\n  [1]: http://www.stat.cmu.edu/tr/tr808/tr808.pdf\\n  [2]: http://www.jeremy-oakley.staff.shef.ac.uk/Oakley_elicitation.pdf\\n  [3]: http://en.wikipedia.org/wiki/Beta_distribution",added 2 characters in body; added 22 characters in body; added 76 characters in body,
233,5,91,9a905f43-cc63-4c62-bfff-4265a1ae556a,2010-07-19 20:58:11.0,87.0,"As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. \\n\\nIn the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta ""functions"" can be used to represent these atoms.    \\n\\n random","changed ""variable"" to ""random variable""",
235,2,114,a6a670ff-46cc-440e-98cd-b6be2d0bee68,2010-07-19 21:00:53.0,8.0,I currently subscribe to \\n\\n1. Andrew Gelman's [blog][1]\\n2. Christian Roberts' [blog][2]\\n\\nWhat other research level statistical blogs are there. \\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/blog/\\n  [2]: http://xianblog.wordpress.com/,,
236,1,114,a6a670ff-46cc-440e-98cd-b6be2d0bee68,2010-07-19 21:00:53.0,8.0,What statistical blogs would you recommend?,,
237,3,114,a6a670ff-46cc-440e-98cd-b6be2d0bee68,2010-07-19 21:00:53.0,8.0,<blog>,,
238,2,115,d15020c9-befc-4d3d-8167-b04b4f21501e,2010-07-19 21:01:35.0,127.0,http://datavis.tumblr.com/,,
239,16,115,d15020c9-befc-4d3d-8167-b04b4f21501e,2010-07-19 21:01:35.0,-1.0,,,
241,2,116,cb435917-1818-4068-8b5b-3123b0c37497,2010-07-19 21:04:16.0,72.0,"Cosma Shalizi's [blog][1], often talks about statistics, and is always interesting.\\n\\n\\n  [1]: http://www.cscs.umich.edu/~crshalizi/weblog/",,
242,2,117,6bb1ee1d-c5aa-476e-b9bd-1c275d7fec78,2010-07-19 21:04:24.0,36.0,http://www.r-bloggers.com/ is an aggregated blog from lots of blogs that talk about statistics using R. Also the [#rstats][1] hashtag on twitter is helpful. I write quite a bit about [statistics and R in genetics research][2].\\n\\n\\n  [1]: http://search.twitter.com/search?q=%23rstats\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/R,,
243,2,118,73b29f38-18d0-4de0-8e6a-d577fed6ecd0,2010-07-19 21:04:39.0,83.0,"In the definition of standard deviation, why do we have to **square** the difference from the mean to get the mean(E) and take the **square root back** at the end? Can't we just simply take **the absolute value** of the difference instead and get the expected value(mean) of those, and wouldn't that also show the variation of the data? The number is going to be different from square method(the absolute-value method will be smaller), but it should still show the spread of data. Anybody know why we take this square approach as a standard?\\n\\nThe Definition of Standard Deviation:\\n\\n![alt text][1]\\n\\n\\nCan't we just take the absolute value instead and still be a good measurement?\\n\\nσ = E[**|X-μ|**]\\n\\n\\n\\n  [1]: http://upload.wikimedia.org/math/9/2/7/9276926b3f1d5663490bc3611300d9e4.png",,
244,1,118,73b29f38-18d0-4de0-8e6a-d577fed6ecd0,2010-07-19 21:04:39.0,83.0,[Standard deviation] : Why square the difference instead of taking the absolute value?,,
245,3,118,73b29f38-18d0-4de0-8e6a-d577fed6ecd0,2010-07-19 21:04:39.0,83.0,<standard-deviation><definition>,,
246,6,2,7a47c5ea-db3f-4cb1-962d-f67ccab04d50,2010-07-19 21:04:58.0,24.0,<distributions><normality>,edited tags,
247,6,35,2840a795-c52e-461c-b6c5-e65721dbf40f,2010-07-19 21:06:54.0,24.0,<distributions><modeling><poisson><overdispersion>,edited tags,
248,2,119,a6b9b9a4-75a3-4922-adf5-21c8b58cbcef,2010-07-19 21:11:44.0,88.0,There are many reasons; probably the main is that it is a natural parameter of normal distribution.,,
249,5,114,7aa952cb-f74d-4823-89de-e92c5c58be4c,2010-07-19 21:12:43.0,8.0,I currently subscribe to \\n\\n1. Andrew Gelman's [blog][1]\\n2. Christian Roberts' [blog][2]\\n3. Darren Wilkinson's [blog][3]\\n\\nWhat other research level statistical blogs are there. \\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/blog/\\n  [2]: http://xianblog.wordpress.com/\\n  [3]: http://darrenjw.wordpress.com/,added 72 characters in body,
250,2,120,5675991b-9bcf-4271-bf38-87526d560f36,2010-07-19 21:14:07.0,41.0,"One way you can think of this is that standard deviation is similar to a ""distance from the mean"".  \\n\\nCompare this to distances in euclidean space - this gives you the true distance, where what you suggested (which, btw, is the [absolute deviation][1]) is more like a [manhattan distance][2] calculation.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Average_absolute_deviation\\n  [2]: http://en.wikipedia.org/wiki/Manhattan_distance_transform",,
251,2,121,759fa59c-050a-4292-ba8b-124da8cc9cca,2010-07-19 21:14:25.0,61.0,"The squared difference has nicer mathematical properties; it's continuously differentiable (nice when you want to minimize it), it's a sufficient statistic for the Gaussian distribution, and it's (a version of) the L2 norm which comes in handy for proving convergence and so on.\\n\\nThe mean absolute deviation (the absolute value notation you suggest) is also used as a measure of dispersion, but it's not as ""well-behaved"" as the squared error.",,
255,2,123,f2cbcdd7-a858-45fa-8251-441409307447,2010-07-19 21:15:20.0,130.0,"Squaring the difference from the mean has a couple of reasons.\\n\\n - Variance is defined as the 2nd moment of the deviation (the R.V here is (x-$\\mu$) ) and thus the square as moments are simply the expectations of higher powers of the random variable.\\n\\n - Having a square as opposed to the absolute value function gives a nice continuous and differentiable function (absolute value is not differentiable at 0) - which makes it the natural choice, especially in the context of estimation and regression analysis.\\n\\n - The squared formulation also naturally falls out of parameters of the Normal Distribution. ",,
256,2,124,e1474585-8be7-4edb-8ee1-c1aee77ddbd4,2010-07-19 21:17:30.0,131.0,"I'm a programmer without statistical background, and I'm currently looking at different classification methods for a large number of different documents that I want to classify into pre-defined categories. I've been reading about kNN, SVM and NN. However, I have some trouble getting started. What resources do you recommend? I do know single variable and multi variable calculus quite well, so my math should be strong enough. I also own Bishop's book on Neural Networks, but it has proven to be a bit dense as an introduction.",,
257,1,124,e1474585-8be7-4edb-8ee1-c1aee77ddbd4,2010-07-19 21:17:30.0,131.0,Statistical classification of text,,
258,3,124,e1474585-8be7-4edb-8ee1-c1aee77ddbd4,2010-07-19 21:17:30.0,131.0,<neural-networks><support-vector-machines><k-nearest-neighbour><classification><tutorials>,,
259,2,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,2010-07-19 21:18:12.0,5.0,Which is the best introductory textbook for Bayesian statistics?\\n\\nOnce book per answer.,,
260,1,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,2010-07-19 21:18:12.0,5.0,What is the best introductory bayesian statistics textbook?,,
261,3,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,2010-07-19 21:18:12.0,5.0,<bayesian><best-of><textbook>,,
262,16,125,4a57e4e9-fe6b-43bc-936d-2582eda8cd4f,2010-07-19 21:18:12.0,5.0,,,
263,16,114,08537f1e-91e8-43f4-ab43-5840cfedb19e,2010-07-19 21:18:51.0,8.0,,,
264,2,126,d48f2548-bad6-452a-81bc-71dcc6f1764e,2010-07-19 21:19:43.0,5.0,"My favorite is [""Bayesian Data Analysis""][1] by Gelman, et al.\\n\\n\\n  [1]: http://www.amazon.com/exec/obidos/ISBN=158488388X/",,
265,16,126,d48f2548-bad6-452a-81bc-71dcc6f1764e,2010-07-19 21:19:43.0,-1.0,,,
266,2,127,842e06ec-39f7-4110-9190-d26c94ceb3ae,2010-07-19 21:23:20.0,61.0,"Another vote for Gelman et al., but a close second for me -- being of the learn-by-doing persuasion -- is [Bayesian Computation with R][1].\\n\\n\\n  [1]: http://bayes.bgsu.edu/bcwr/",,
267,16,127,842e06ec-39f7-4110-9190-d26c94ceb3ae,2010-07-19 21:23:20.0,-1.0,,,
268,2,128,99075edd-d6f6-42ca-8341-96ae02ce25cc,2010-07-19 21:23:57.0,132.0,"In Plain English, how does one interpret a Bland-Altman plot?  \\n\\nWhat are the advantages of using a Bland-Altman plot over other methods of comparing two different measurement methods?",,
269,1,128,99075edd-d6f6-42ca-8341-96ae02ce25cc,2010-07-19 21:23:57.0,132.0,How doe one interpret a Bland-Altman plot?,,
270,3,128,99075edd-d6f6-42ca-8341-96ae02ce25cc,2010-07-19 21:23:57.0,132.0,<untagged>,,
271,2,129,7b343dfd-2975-444d-8859-4f21c99f3499,2010-07-19 21:24:58.0,8.0,I quite like [Markov Chain Monte Carlo: Stochastic Simulation for Bayesian Inference][1] by Gamerman and Lopes.\\n\\nIt more an introduction to (advanced) Bayesian computation though.\\n\\n\\n  [1]: http://www.amazon.co.uk/o/ASIN/1584885874,,
272,16,129,7b343dfd-2975-444d-8859-4f21c99f3499,2010-07-19 21:24:58.0,-1.0,,,
273,6,103,0b037236-e380-444c-960e-d1f2d130e778,2010-07-19 21:25:22.0,5.0,<data-visualization><blog><best-of>,edited tags,
274,2,130,7d38aff2-0b66-4ee8-809e-b03963b19a72,2010-07-19 21:26:27.0,90.0,"I had a plan of learning R in the near future. Reading [another question][1] I found out about Clojure. Now I don't know what to do.\\n\\nI think a big **advantage of R** for me is that some people in Economics use it, including one of my supervisors (though the other said: stay away from R!). One **advantage of Clojure** is that it is Lisp-based, and as I have started learning Emacs and I am keen on writing my own customisations, it would be helpful (yeah, I know Clojure and Elisp are different dialects of Lisp, but they are both Lisp and thus similar I would imagine).\\n\\nI can't ask which one is better, because I know this is very personal, but could someone give me the advantages (or advantages) of Clojure x R, especially in practical terms? For example, which one should be easier to learn, which one is more flexible or more powerful, which one has more libraries, more support, more users, etc?\\n\\n**My intended use**: The bulk of my estimation should be done using Matlab, so I am not looking for anything too deep in terms of statistical analysis, but rather a software to substitute Excel for the initial data manipulation and visualisation, summary statistics and charting, but also some basic statistical analysis or the initial attempts at my estimation.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/3/what-are-some-valuable-statistical-analysis-open-source-projects",,
275,1,130,7d38aff2-0b66-4ee8-809e-b03963b19a72,2010-07-19 21:26:27.0,90.0,Clojure versus R: advantages and disadvantages,,
276,3,130,7d38aff2-0b66-4ee8-809e-b03963b19a72,2010-07-19 21:26:27.0,90.0,<r>,,
277,2,131,69cb5f92-ef39-4fa8-9fac-f627215a08e5,2010-07-19 21:28:41.0,5.0,"For basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as [SICP][1]).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.\\n\\nIn general:\\n\\nThe main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.\\n\\nClojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.\\n\\n\\n  [1]: http://mitpress.mit.edu/sicp/",,
278,2,132,39074a92-6f5b-46ad-91d2-0f14fb8e1428,2010-07-19 21:29:37.0,22.0,Coming from non-statistical background I found [Introduction to Applied Bayesian Statistics and Estimation for Social Scientists][1] quite informative and easy to follow.\\n\\n\\n  [1]: http://www.princeton.edu/~slynch/bayesbook/bookinfo.html,,
279,16,132,39074a92-6f5b-46ad-91d2-0f14fb8e1428,2010-07-19 21:29:37.0,-1.0,,,
280,2,133,01aabde1-c707-4ff9-b48d-2b6d95944892,2010-07-19 21:31:53.0,139.0,"I don't know how to use SAS/R/Orange, but it sounds like the kind of test you need is a [chi-square test][1]. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Chi-square_test",,
281,5,125,a1d81e68-c963-4c2f-a46e-f864c0acc8c2,2010-07-19 21:32:09.0,8.0,Which is the best introductory textbook for Bayesian statistics?\\n\\nOne book per answer.,deleted 1 characters in body,
282,2,134,657b3316-c103-42cc-bdd9-a52a5fedc530,2010-07-19 21:32:38.0,138.0,"On smaller window sizes, `n log n` sorting might work. Are there any better algorithms to achieve this?",,
283,1,134,657b3316-c103-42cc-bdd9-a52a5fedc530,2010-07-19 21:32:38.0,138.0,Algorithms to compute the running median?,,
284,3,134,657b3316-c103-42cc-bdd9-a52a5fedc530,2010-07-19 21:32:38.0,138.0,<algorithms><running><median>,,
285,5,131,258c695e-b291-4fcc-b958-e7aeedf39191,2010-07-19 21:35:55.0,5.0,"Let me start by saying that I love both languages: you can't go wrong with either, and they are certainly better than something like C++ or Java for doing data analysis.\\n\\nFor basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as [SICP][1]).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.  Also, keep in mind that R (or S) is largely derived from Scheme, so you would benefit from Lisp knowledge when using it.\\n\\nIn general:\\n\\nThe main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.\\n\\nClojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.\\n\\n\\n  [1]: http://mitpress.mit.edu/sicp/",added 295 characters in body,
286,2,135,0c522a65-4bb8-4d51-a9f2-fb1e84f7cea4,2010-07-19 21:36:12.0,39.0,"I believe that this calls for a [two-sample Kolmogorov–Smirnov test][1], or the like.  The two-sample Kolmogorov–Smirnov test is based on comparing differences in the [empirical distribution functions][2] (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples.\\n\\nThis test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them, and run it on your sample data.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm\\n  [2]: http://en.wikipedia.org/wiki/Empirical_distribution_function",,
290,6,75,c50a6445-bbdb-481f-8ba0-2a065559b2fa,2010-07-19 21:37:34.0,13.0,<r><books><implementation><possibly-off-topic>,edited tags,
291,6,130,83e0555c-5fbe-4a16-9703-1d5c16bb9227,2010-07-19 21:37:58.0,13.0,<r><possibly-off-topic>,edited tags,
292,2,137,066fc9cf-557f-4b54-88cf-51d698c3a63c,2010-07-19 21:38:09.0,74.0,"Do you have a college, university, or city library card? If so you may have free access to Books24x7.com and SpringerLink.com which are e-book repositories. If so, I recommend these books:\\n\\n""Text Mining"" by Weiss\\nhttp://www.springerlink.com/content/k46654/?p=b318fa02471245298ce4a9fba9104dd5&pi=0\\n\\n""Text Mining Application Programming"", by Konchady\\nhttp://library.books24x7.com/toc.asp?bookid=26322\\n\\nFor software, I recommend RapidMiner or GATE, both free and open-source. \\n\\n\\nThis is my ""text mining process"":\\n\\n    * collect the documents (usually a web crawl)\\n          o [sample if too large]\\n          o timestamp\\n          o strip out markup\\n    * tokenize\\n    * break into characters, words, n-grams, or sliding windows\\n    * stemming (aka lemmatization)\\n          o [include synonyms]\\n          o see porter algorithm\\n          o pronouns and articles are usually bad predictors\\n    * remove stopwords\\n    * feature vectorization\\n          o binary (appears or doesn’t)\\n          o word count\\n          o relative frequency: tf-idf\\n          o information gain, chi square\\n          o [have a minimum value for inclusion]\\n    * weighting\\n          o weight words at top of document higher?\\n\\nThen you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. ",,
293,2,138,97c3baad-6884-40b3-a72c-429bc15cd147,2010-07-19 21:38:10.0,142.0,I'm looking to learn R on the cheap. What's the best free resource/book/tutorial for learning R?,,
294,1,138,97c3baad-6884-40b3-a72c-429bc15cd147,2010-07-19 21:38:10.0,142.0,Resources for Learning R ,,
295,3,138,97c3baad-6884-40b3-a72c-429bc15cd147,2010-07-19 21:38:10.0,142.0,<r>,,
296,2,139,780d9f7a-979b-424e-b746-5a17ad7eb099,2010-07-19 21:39:17.0,5.0,"If I had to choose one thing, make sure that you read <a href=""http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"">""The R Inferno""</a>.\\n\\nThere are many good resources on <a href=""http://www.r-project.org"">the R homepage</a>, but in particular, read <a href=""http://cran.r-project.org/doc/manuals/R-intro.pdf"">""An Introduction to R""</a> and <a href=""http://cran.r-project.org/doc/manuals/R-lang.pdf"">""The R Language Definition""</a>.",,
297,6,138,9348f066-4a87-4578-a425-a908fc13c102,2010-07-19 21:39:33.0,61.0,<r><tutorials><open-source>,edited tags,
299,2,140,4c8b0283-3855-47fb-bdfa-f12323808f95,2010-07-19 21:39:35.0,88.0,The official guides are pretty nice; check out http://cran.r-project.org/manuals.html . There is also a lot of contributed documentation there.,,
301,2,141,932070fa-305f-4eb1-afc3-e6dd94c83759,2010-07-19 21:40:02.0,142.0,Light-hearted: http://indexed.blogspot.com/,,
302,16,141,932070fa-305f-4eb1-afc3-e6dd94c83759,2010-07-19 21:40:02.0,-1.0,,,
303,2,142,4c53c0ab-279e-4372-9372-797d9f42e264,2010-07-19 21:42:57.0,8.0,"After you learn the basics, I find the following sites very useful:\\n\\n1. [R-bloggers][1]. \\n1. Subscribing to the [Stack overflow R tag][2].\\n\\n\\n  [1]: http://www.r-bloggers.com/\\n  [2]: http://stackoverflow.com/questions/tagged/R",,
304,6,138,f30d4ad1-6771-46bb-9289-b721d33794ce,2010-07-19 21:47:25.0,13.0,<r><possibly-off-topic><open-source><tutorials>,edited tags,
305,5,10,a36d340c-e18b-4f66-8268-b51a0188c153,2010-07-19 21:47:34.0,24.0,Many studies in the social sciences use Likert scales.  When is it appropriate to use Likert data as ordinal and when is it appropriate to use it as interval data?,fixed punctuation ,
306,2,143,dad82f23-4065-4a78-a65b-a196b92427a0,2010-07-19 21:48:28.0,88.0,"Neural network may be to slow for a large number of documents (also this is now pretty much obsolete).   \\nAnd you may also check Random Forest among classifiers; it is quite fast, scales nice and does not need complex tuning.",,
307,2,144,120be3f7-7b4e-4055-92de-f28358f1f35b,2010-07-19 21:48:52.0,22.0,[Quick-R][1] can be a good place to start.\\n\\n\\n  [1]: http://www.statmethods.net/index.html,,
308,2,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,2010-07-19 21:50:16.0,138.0,Where can I find freely accessible data sources?\\n\\nI'm thinking of sites like\\n\\n* [http://www2.census.gov/census_2000/datasets/][1]?\\n\\n\\n\\n  [1]: http://www2.census.gov/census_2000/datasets/,,
309,1,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,2010-07-19 21:50:16.0,138.0,Free Dataset Resources?,,
310,3,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,2010-07-19 21:50:16.0,138.0,<dataset><linked><data>,,
311,6,130,fe023c9f-0ca0-482a-b4c2-31ed29c4ebf3,2010-07-19 21:50:20.0,22.0,<r><possibly-off-topic><subjective>,edited tags,
312,16,145,18ed3fde-bd16-4f2e-9566-e7aafc62b12f,2010-07-19 21:50:16.0,138.0,,,
313,5,135,a5e35103-d092-46c6-a9ba-f70fb8125705,2010-07-19 21:52:08.0,39.0,"I believe that this calls for a [two-sample Kolmogorov–Smirnov test][1], or the like.  The two-sample Kolmogorov–Smirnov test is based on comparing differences in the [empirical distribution functions][2] (ECDF) of two samples, meaning it is sensitive to both location and shape of the the two samples.  It also generalizes out to a multivariate form.\\n\\nThis test is found in various forms in different packages in R, so if you are basically proficient, all you have to do is install one of them (e.g. [fBasics][3]), and run it on your sample data.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/ks2samp.htm\\n  [2]: http://en.wikipedia.org/wiki/Empirical_distribution_function\\n  [3]: http://cran.r-project.org/web/packages/fBasics/fBasics.pdf",added 136 characters in body,
314,2,146,a78be343-46f5-4238-ac5f-efd0c7bcfc35,2010-07-19 21:52:51.0,144.0,"A while ago a user on R-help mailing list asked about the soundness of using PCA scores in a regression. The user is trying to use one PC scores to explain variation in another PC (see full discussion <a href=""http://r.789695.n4.nabble.com/PCA-and-Regression-td2280038.html"">here</a>). The answer was that no, this is not sound because PC are orthogonal to each other. Can someone explain in a bit more detail why this is so?",,
315,1,146,a78be343-46f5-4238-ac5f-efd0c7bcfc35,2010-07-19 21:52:51.0,144.0,PCA scores in multiple regression,,
316,3,146,a78be343-46f5-4238-ac5f-efd0c7bcfc35,2010-07-19 21:52:51.0,144.0,<r><pca><scores><regression>,,
317,2,147,9095f4cf-a12f-43d8-b58a-d4b215201ebb,2010-07-19 21:53:02.0,142.0,Amazon has free Public Data sets for use with EC2. \\n\\nhttp://aws.amazon.com/publicdatasets/\\n\\nHere's a list: http://developer.amazonwebservices.com/connect/kbcategory.jspa?categoryID=243,,
318,16,147,9095f4cf-a12f-43d8-b58a-d4b215201ebb,2010-07-19 21:53:02.0,-1.0,,,
319,4,130,2a642ba3-fc3b-4750-a155-168ed66fde10,2010-07-19 21:56:05.0,90.0,Clojure versus R: advantages and disadvantages for data analysis,edited title,
320,2,148,58a77a36-2728-4a65-a3fc-b2d1cce46af6,2010-07-19 21:58:51.0,130.0,http://infochimps.org/ - is a good resource for free data sets.,,
321,16,148,58a77a36-2728-4a65-a3fc-b2d1cce46af6,2010-07-19 21:58:51.0,-1.0,,,
322,2,149,bbd31e1c-4168-4c50-ab55-07734ebba0e4,2010-07-19 22:02:10.0,74.0,"A principal component is a weighted linear combination of all your variables (X's).\\n\\nexample: PCA1 = 0.1X1 + 0.3X2\\n\\nThere will be one principal component for each X (though in general a small number are selected). \\n\\nThe principal components are created such that they have zero correlation (are orthogonal).\\n\\nTherefore, principal component 1 should not explain any variation in principal component 2.\\n\\nFor a book, I recommend Multivariate Data Analysis by Hair\\n\\nThis is also good: http://www.statsoft.com/textbook/principal-components-factor-analysis/",,
323,6,134,a6573b2a-0409-4d07-98ba-52887586892e,2010-07-19 22:05:23.0,88.0,<algorithms><possibly-off-topic><running-median>,edited tags,
324,2,150,3175fbeb-0fb0-4828-8b29-5e20e5fdcbd1,2010-07-19 22:13:29.0,25.0,"For complete beginners, try William Briggs [Breaking the Law of Averages: Real-Life Probability and Statistics in Plain English](http://www.amazon.com/Breaking-Law-Averages-Probability-Statistics/dp/0557019907/ref=sr_1_1?ie=UTF8&s=books&qid=1279577542&sr=8-1)",,
325,16,150,3175fbeb-0fb0-4828-8b29-5e20e5fdcbd1,2010-07-19 22:13:29.0,-1.0,,,
326,5,137,d2e7b795-a941-4750-a038-dee8ecba8d72,2010-07-19 22:23:21.0,74.0,"Do you have a college, university, or city library card? If so you may have free access to Books24x7.com and SpringerLink.com which are e-book repositories. If so, I recommend these books:\\n\\n""Text Mining"" by Weiss\\nhttp://www.springerlink.com/content/k46654/?p=b318fa02471245298ce4a9fba9104dd5&pi=0\\n\\n""Text Mining Application Programming"", by Konchady\\nhttp://library.books24x7.com/toc.asp?bookid=26322\\n\\nFor software, I recommend RapidMiner (with the text plugin), free and open-source. There is also GATE for advanced stuff. \\n\\n\\nThis is my ""text mining process"":\\n\\n    * collect the documents (usually a web crawl)\\n          o [sample if too large]\\n          o timestamp\\n          o strip out markup\\n    * tokenize\\n    * break into characters, words, n-grams, or sliding windows\\n    * stemming (aka lemmatization)\\n          o [include synonyms]\\n          o see porter algorithm\\n          o pronouns and articles are usually bad predictors\\n    * remove stopwords\\n    * feature vectorization\\n          o binary (appears or doesn’t)\\n          o word count\\n          o relative frequency: tf-idf\\n          o information gain, chi square\\n          o [have a minimum value for inclusion]\\n    * weighting\\n          o weight words at top of document higher?\\n\\nThen you can start the work of classifying them. kNN, SVM, or Naive Bayes as appropriate. ",added 49 characters in body,
327,2,151,75f4377a-c1f0-4599-aa96-f4e97a7721e1,2010-07-19 22:31:12.0,81.0,"If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.\\n\\nThe benefits of squaring include:\\n\\n - Squaring always gives a positive\\n   value, so the sum will not be zero.\\n - Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).\\n\\nSquaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.\\n\\nI suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes.\\n\\nIt's important to note however that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.\\n\\nMy view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.\\n\\nAn much more indepth analysis can be read [here][1].\\n\\n \\n\\n\\n  [1]: http://www.leeds.ac.uk/educol/documents/00003759.htm",,
328,2,152,199802a8-012c-455e-b18a-f5f4d075fdff,2010-07-19 22:37:38.0,,"Label switching (i.e., the posterior distribution is invariant to switching component labels) is a problematic issue when using MCMC to estimate mixture models. \\n\\n1. Is there a standard (as in widely accepted) methodology to deal with the issue?\\n\\n2. If there is no standard approach then what are the pros and cons of the leading approaches to solve the label switching problem?",,user28
329,1,152,199802a8-012c-455e-b18a-f5f4d075fdff,2010-07-19 22:37:38.0,,Is there a standard method to deal with label switching problem in MCMC estimation of mixture models?,,user28
330,3,152,199802a8-012c-455e-b18a-f5f4d075fdff,2010-07-19 22:37:38.0,,<bayesian><mcmc>,,user28
331,2,153,7d59cfbb-85fe-4883-b0e4-41c98d275c49,2010-07-19 22:39:27.0,145.0,"The simple answer is that Likert scales are always ordinal. The intervals between positions on the scale are monotonic but never so well-defined as to be numerically uniform increments.\\n\\nThat said, the distinction between ordinal and interval is based on the specific demands of the analysis being performed. Under special circumstances, you may be able to treat the responses as if they fell on an interval scale. To do this, typically the respondents need to be in close agreement regarding the meaning of the scale responses and the analysis (or the decisions made based on the analysis) should be relatively insensitive to problems that may arise.",,
332,2,154,b8495b04-69fb-47e6-a7ee-e72dc2bf1271,2010-07-19 22:40:47.0,108.0,"I am currently researching the *trial roulette method* for my masters thesis as an elicitation technique. This is a graphical method that allows an expert to represent her subjective probability distribution for an uncertain quantity.\\n\\nExperts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up to 1 - for example 20 chips of probability = 0.05 each. They are then instructed to arrange them on a pre-printed grid, with bins representing result intervals. Each column would represent their belief of the probability of getting the corresponding bin result.\\n\\nSome reasons in favour of using this technique are:\\n\\n1. Many questions about the shape of the expert's subjective probability distribution can be answered without the need to pose a long series of questions to the expert - the statistician can simply read off density above or below any given point, or that between any two points. \\n2. During the elicitation process, the experts can move around the chips if unsatisfied with the way they placed them initially - thus they can be sure of the final result to be submitted. \\n3. It forces the expert to be coherent in the set of probabilities that are provided. If all the chips are used, the probabilities must sum to one.\\n4. Graphical methods seem to provide more accurate results, especially for participants with modest levels of statistical sophistication.\\n",,
333,2,155,53e57bd0-d1a8-4c0d-92da-11a2f6a954d0,2010-07-19 22:43:50.0,154.0,I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explain a difficult statistical concept?,,
334,1,155,53e57bd0-d1a8-4c0d-92da-11a2f6a954d0,2010-07-19 22:43:50.0,154.0,What is your favorite layman's explanation for a difficult statistical concept?,,
335,3,155,53e57bd0-d1a8-4c0d-92da-11a2f6a954d0,2010-07-19 22:43:50.0,154.0,<layman>,,
336,2,156,eace5f99-4af8-494a-83b2-b1736167e248,2010-07-19 22:50:13.0,148.0,"I know this must be standard material, but I had difficulty in finding the proof in this form in the statistics books that I picked up in the library.\\n\\nLet e be a white Gaussian vector of size N.  Let all the other matrices in the following be constant.\\n\\nLet v = X*y + e, where X is an N x L matrix and y is an N x 1 vector, and let\\n\\ny_bar = (X^T*X)^(-1)X^T*v\\n\\ne_bar = v - X*y_bar. \\n\\nIf c is any constant vector, J = N - rank(X), and \\n\\nu = c^T*y_bar\\n\\ns^2 = e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c,\\n\\nthen the random variable defined as\\n\\nt = u/sqrt(s^2/J)  \\n\\nfollows a normalized student t distribution with J degrees of freedom.\\n\\nI would be grateful if you could provide an outline for its proof. \\n\\n",,
337,1,156,eace5f99-4af8-494a-83b2-b1736167e248,2010-07-19 22:50:13.0,148.0,From the general linear model to a t variable,,
338,3,156,eace5f99-4af8-494a-83b2-b1736167e248,2010-07-19 22:50:13.0,148.0,<untagged>,,
339,5,151,7fadd5e8-e116-444c-b4b8-084b020c9475,2010-07-19 22:50:32.0,81.0,"If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.\\n\\nThe benefits of squaring include:\\n\\n - Squaring always gives a positive\\n   value, so the sum will not be zero.\\n - Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).\\n\\nSquaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.\\n\\nI suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)\\n\\n***It's important to note*** however that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.\\n\\nMy view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.\\n\\nAn much more indepth analysis can be read [here][1].\\n\\n \\n\\n\\n  [1]: http://www.leeds.ac.uk/educol/documents/00003759.htm",added 304 characters in body,
340,2,157,a837e749-584f-4ed4-a120-5be4ae1d16e1,2010-07-19 22:52:22.0,36.0,Definitely the Monty Hall Problem. http://en.wikipedia.org/wiki/Monty_Hall_problem,,
341,5,155,efc25937-3798-4e5b-aca0-c49f4623ea73,2010-07-19 22:55:13.0,154.0,"I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept?\\n\\nMy favorite is <a href=""http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Co-integration/Murray93DrunkAndDog.pdf"">Murray's</a> explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their first differences are stationary.\\n\\n> The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But\\n> periodically she intones ""Oliver, where are you?"", and Oliver interrupts his aimless\\n> wandering to bark. He hears her; she hears him. He thinks, ""Oh, I can't let her get too far\\n> off; she'll lock me out."" She thinks, ""Oh, I can't let him get too far off; he'll wake\\n> me up in the middle of the night with his barking."" Each assesses how far\\n> away the other is and moves to partially close that gap.\\n\\n\\n",added 895 characters in body; added 1 characters in body,
343,2,159,b36b2449-95b2-4aeb-9439-c78823ac4c0b,2010-07-19 23:00:30.0,145.0,"[Junk Charts][1] is always interesting and thought-provoking, usually providing both criticism of visualizations in the popular media and suggestions for improvements.\\n\\n\\n  [1]: http://junkcharts.typepad.com/",,
344,16,159,b36b2449-95b2-4aeb-9439-c78823ac4c0b,2010-07-19 23:00:30.0,-1.0,,,
345,2,160,60f83d0d-4ec7-4b21-ae7f-428ec8f76919,2010-07-19 23:06:43.0,158.0,http://dataspora.com/blog/,,
346,16,160,60f83d0d-4ec7-4b21-ae7f-428ec8f76919,2010-07-19 23:06:43.0,-1.0,,,
347,2,161,ad921ca4-df6a-4994-8201-0b71b8d3ca1f,2010-07-19 23:11:36.0,154.0,"Econometricians often talk about a time series being *integrated with order k, I(k)*. *k* being the minimum number of differences required to obtain a stationary time series.\\n\\nWhat methods or statistical tests can be used to determine, given a level of confidence, the *order of integration* of a time series?",,
348,1,161,ad921ca4-df6a-4994-8201-0b71b8d3ca1f,2010-07-19 23:11:36.0,154.0,What methods can be used to determine the Order of Integration of a time series?,,
349,3,161,ad921ca4-df6a-4994-8201-0b71b8d3ca1f,2010-07-19 23:11:36.0,154.0,<timeseries>,,
350,2,162,215c226e-ad7c-4e4d-8641-b92b06d2e5aa,2010-07-19 23:13:32.0,74.0,"These aren't exactly ""difficult"" concepts. Depends how lay the man is I suppose...\\n\\nIf you carved your distribution out of wood, and tried to balance it on your finger, the balance point would be the mean.\\n\\nIf you put a stick in the middle of your scatter plot, and attached the stick to each data point with a spring, the resting point of the stick would be your regression line.\\n\\n",,
351,2,163,0070f18f-0884-4f02-b30f-e07608c0bfe7,2010-07-19 23:17:53.0,81.0,"Lets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.\\n\\nWhichever number the die lands on is the number of free text-books I will give you.\\n\\nIn this case, the amount of text books that I give you is the ***random variable*** because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die).",,
352,2,164,08cb40e3-a8e1-4e86-a923-8efd52158cd0,2010-07-19 23:19:44.0,158.0,For governmental data:\\n\\nUS: http://www.data.gov/\\n\\nWorld: http://www.guardian.co.uk/world-government-data\\n,,
353,16,164,08cb40e3-a8e1-4e86-a923-8efd52158cd0,2010-07-19 23:19:44.0,-1.0,,,
354,2,165,30dd10d3-03ec-4e8d-93ed-857371beea0c,2010-07-19 23:21:05.0,74.0,I've never found a great explanation of this technique. Thanks in advance!,,
355,1,165,30dd10d3-03ec-4e8d-93ed-857371beea0c,2010-07-19 23:21:05.0,74.0,Please explan Markov Chain Monte Carlo (MCMC) in plain English,,
356,3,165,30dd10d3-03ec-4e8d-93ed-857371beea0c,2010-07-19 23:21:05.0,74.0,<mcmc>,,
357,5,156,47ef8869-bcb4-4e23-90b8-c495ece4d609,2010-07-19 23:21:08.0,148.0,"I know this must be standard material, but I had difficulty in finding a proof in this form.\\n\\nLet e be a white Gaussian vector of size N.  Let all the other matrices in the following be constant.\\n\\nLet v = X*y + e, where X is an N x L matrix and y is an N x 1 vector, and let\\n\\ny_bar = (X^T*X)^(-1)X^T*v\\n\\ne_bar = v - X*y_bar. \\n\\nIf c is any constant vector, J = N - rank(X), and \\n\\nu = c^T*y_bar\\n\\ns^2 = e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c,\\n\\nthen the random variable defined as\\n\\nt = u/sqrt(s^2/J)  \\n\\nfollows a normalized student t distribution with J degrees of freedom.\\n\\nI would be grateful if you could provide an outline for its proof. \\n\\n",deleted 58 characters in body,
358,2,166,327722a1-82d7-47c2-a31a-4bf18f58364c,2010-07-19 23:21:35.0,154.0,"Australia is currently having an election and understandably the media reports new political poll results daily. In a country of 22 million what percentage of the population would need to be sampled to get a statistically valid result?\\n\\nIs it possible that using too large a sample could affect the results, or does statistical validity monotonically increase with sample size?",,
359,1,166,327722a1-82d7-47c2-a31a-4bf18f58364c,2010-07-19 23:21:35.0,154.0,How do you decide the sample size when polling a large population?,,
360,3,166,327722a1-82d7-47c2-a31a-4bf18f58364c,2010-07-19 23:21:35.0,154.0,<polling>,,
361,5,155,22c29e8a-c1d5-40d1-b281-2fb3ec053f50,2010-07-19 23:22:33.0,154.0,"I really enjoy hearing simple explanations to complex problems. What is your favorite analogy or anecdote that explains a difficult statistical concept?\\n\\nMy favorite is <a href=""http://www-stat.wharton.upenn.edu/~steele/Courses/434/434Context/Co-integration/Murray93DrunkAndDog.pdf"">Murray's</a> explanation of cointegration using a drunkard and her dog. Murray explains how two random processes (a wandering drunk and her dog, Oliver) can have unit roots but still be related (cointegrated) since their joint first differences are stationary.\\n\\n> The drunk sets out from the bar, about to wander aimlessly in random-walk fashion. But\\n> periodically she intones ""Oliver, where are you?"", and Oliver interrupts his aimless\\n> wandering to bark. He hears her; she hears him. He thinks, ""Oh, I can't let her get too far\\n> off; she'll lock me out."" She thinks, ""Oh, I can't let him get too far off; he'll wake\\n> me up in the middle of the night with his barking."" Each assesses how far\\n> away the other is and moves to partially close that gap.\\n\\n\\n",added 6 characters in body,
362,2,167,6c2eb023-f320-4f0e-b5fd-d11e89589648,2010-07-19 23:26:31.0,159.0,"Principal components are orthogonal by definition, so any pair of PCs will have zero correlation.\\n\\nHowever, PCA can be used in regression if there are a large number of explanatory variables. These can be reduced to a small number of principal components and used as predictors in a regression.",,
363,2,168,4b64323b-7e66-4e8c-afd6-2313c9e8a4ae,2010-07-19 23:26:44.0,8.0,"For univariate kernel density estimators (KDE), I use Silverman's rule for calculating h:\\n\\n0.9 min(sd, IQR/1.34)*n^(-0.2)\\n\\nWhat are the standard rules for multivariate KDE (assuming a Normal kernel).\\n\\n",,
364,1,168,4b64323b-7e66-4e8c-afd6-2313c9e8a4ae,2010-07-19 23:26:44.0,8.0,Choosing a Bandwidth for kernel density estimators,,
365,3,168,4b64323b-7e66-4e8c-afd6-2313c9e8a4ae,2010-07-19 23:26:44.0,8.0,<kde><kernel>,,
366,6,95,a95e4349-8eda-4acc-9397-8b34780cfaec,2010-07-19 23:27:03.0,57.0,<time-series><garch><volatility-forecasting>,edited tags,
367,2,169,4a818ce5-fa98-40da-9e53-2988eff93f75,2010-07-19 23:27:36.0,159.0,"For time series data, try the [Time Series Data Library][1].\\n\\n\\n  [1]: http://robjhyndman.com/TSDL",,
368,16,169,4a818ce5-fa98-40da-9e53-2988eff93f75,2010-07-19 23:27:36.0,-1.0,,,
369,2,170,7791570c-de35-41e0-98d2-55119430df0e,2010-07-19 23:29:54.0,8.0,Are there any free statistical textbooks available? \\n\\n,,
370,1,170,7791570c-de35-41e0-98d2-55119430df0e,2010-07-19 23:29:54.0,8.0,Free statistical textbooks,,
371,3,170,7791570c-de35-41e0-98d2-55119430df0e,2010-07-19 23:29:54.0,8.0,<teaching><textbook>,,
372,2,171,03f6e514-4551-4772-bbe3-ea8c5a244b49,2010-07-19 23:32:30.0,159.0,"There are a number of statistical tests (known as ""unit root tests"") for dealing with this problem. The most popular is probably the ""Augmented Dickey-Fuller"" (ADF) test, although the Phillips-Perron (PP) test and the KPSS test are also widely used. \\n\\nBoth the ADF and PP tests are based on a null hypothesis of a unit root (i.e., an I(1) series). The KPSS test is based on a null hypothesis of stationarity (i.e., an I(0) series). Consequently, the KPSS test can give quite different results from the ADF or PP tests.",,
373,2,172,7a5a2d5b-b5d1-460d-8dc2-abbee8df947e,2010-07-19 23:34:18.0,74.0,"It doesn't much depend on the population size, which is counter-intuitive to many.\\n\\nMost polling companies use 400 or 1000 people in their samples.\\n\\nThere is a reason for this:\\n\\nA sample size of 400 will give you a confidence interval of +/-5% 19 times out of 20 (95%)\\n\\nA sample size of 1000 will give you a confidence interval of +/-3% 19 times out of 20 (95%)\\n\\nWhen the proportion that you are measuring is near 50% anyways. \\n\\nThis calculator isn't bad:\\n\\nhttp://www.raosoft.com/samplesize.html\\n",,
374,2,173,f37eaaad-cc84-4e94-b158-83ba39fe93ae,2010-07-19 23:37:22.0,71.0,"I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc.  I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not.  Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:\\n\\n![alt text][1]\\n\\n![alt text][2]\\n\\n\\nI've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.\\n\\n\\n  [1]: http://img827.imageshack.us/img827/1927/activetbcases.png ""Cases by month""\\n  [2]: http://img827.imageshack.us/img827/4348/tbcasedistribution.png ""Distribution of counts""",,
375,1,173,f37eaaad-cc84-4e94-b158-83ba39fe93ae,2010-07-19 23:37:22.0,71.0,"Time series for count data, with counts < 20",,
376,3,173,f37eaaad-cc84-4e94-b158-83ba39fe93ae,2010-07-19 23:37:22.0,71.0,<r><time-series><count-data><epidemiology>,,
377,2,174,f0c672c7-b4ee-43ee-9e32-cd0366e43192,2010-07-19 23:37:43.0,159.0,The most widely used and probably the best of what is available is\\nhttp://www.statsoft.com/textbook/\\n\\nOther online stats books include\\n\\n - http://davidmlane.com/hyperstat/\\n - http://faculty.vassar.edu/lowry/webtext.html\\n - http://www.psychstat.missouristate.edu/multibook2/mlt.htm\\n - http://bookboon.com/uk/student/statistics\\n - http://www.freebookcentre.net/SpecialCat/Free-Statistics-Books-Download.html,,
378,6,161,1a3b3122-9113-4ac4-bad7-f624f3299ed4,2010-07-19 23:39:49.0,159.0,<time-series>,edited tags,
379,2,175,dd786a6b-9a21-4304-839f-a31b55ba8146,2010-07-19 23:39:49.0,13.0,"Often times a statistical analyst is handed a set dataset and asked to fit a modle using a technique such as linear regression.  Very frequently the dataset is accompanied with a disclaimer similar to ""Oh yeah, we messed up collecting some of these data points- do what you can"".\\n\\nThis situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following:\\n\\n  - It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it ""makes the fit look bad"".\\n\\n  - In real life, the people who collected the data are frequently not available to answer questions such as ""when generating this data set, which of the points did you mess up, exactly?""\\n\\nWhat statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis?\\n\\nAre there any special considerations for multilinear regression?",,
380,1,175,dd786a6b-9a21-4304-839f-a31b55ba8146,2010-07-19 23:39:49.0,13.0,How should outliers be dealt with in linear regression analysis?,,
381,3,175,dd786a6b-9a21-4304-839f-a31b55ba8146,2010-07-19 23:39:49.0,13.0,<outliers><regression>,,
382,2,176,c50d8fa9-d796-4343-8ca0-69f273c28dfd,2010-07-19 23:40:01.0,81.0,"Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book\\n\\nThe Frequentist would say that each outcome has an equal 1 in 6 chance of occurring.\\n\\nThe Bayesian however would say hang on a second, I know that man, he's David Blane, a famous trickster, I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on Heads BUT I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll increase the chance from 1% to something slightly higher, otherwise I'll reduce it. \\n",,
383,2,177,0a265080-0dc2-44ce-b0bd-05e812201416,2010-07-19 23:45:44.0,159.0,"Rather than exclude outliers, you can use a robust method of regression. In R, for example, the [`rlm()` function from the MASS package][1] can be used instead of the `lm()` function. The method of estimation can be tuned to be more or less robust to outliers.\\n\\n\\n  [1]: http://sekhon.berkeley.edu/library/MASS/html/rlm.html",,
384,2,178,2c98faff-dc16-4f10-98c6-904ad806334b,2010-07-19 23:48:50.0,74.0,[RapidMiner][1] for data mining\\n\\n[GATE][2] for text mining\\n\\n\\n  [1]: http://rapid-i.com/\\n  [2]: http://gate.ac.uk/,,
385,16,178,2c98faff-dc16-4f10-98c6-904ad806334b,2010-07-19 23:48:50.0,-1.0,,,
386,5,156,e4a1683e-8793-46ad-bfe2-11bff2d228d0,2010-07-19 23:52:14.0,148.0,"I know this must be standard material, but I had difficulty in finding a proof in this form.\\n\\nLet e be a standard white Gaussian vector of size N.  Let all the other matrices in the following be constant.\\n\\nLet v = X*y + e, where X is an N x L matrix and y is an N x 1 vector, and let\\n\\ny_bar = (X^T*X)^(-1)X^T*v\\n\\ne_bar = v - X*y_bar. \\n\\nIf c is any constant vector, J = N - rank(X), and \\n\\nu = c^T*y_bar\\n\\ns^2 = e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c,\\n\\nthen the random variable defined as\\n\\nt = u/sqrt(s^2/J)  \\n\\nfollows a normalized student t distribution with J degrees of freedom.\\n\\nI would be grateful if you could provide an outline for its proof. \\n\\n",added 9 characters in body,
387,5,86,6d5e4008-cee7-4285-8337-8c35ca0b8bc4,2010-07-19 23:58:01.0,13.0,"Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather **statistical properties** such as the **distribution** of the random variable may be proscribed.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.  \\n\\nRandom variables may be classified as *discreet* if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is *continuous* and is used if the distribution covers values from an uncountable set such as the real numbers.",added 1 characters in body,
388,2,179,1545a29e-3032-4028-92ed-0c92a1927aa8,2010-07-19 23:59:29.0,159.0,"For a univariate KDE, you are better off using something other than Silverman's rule which is based on a normal approximation. One excellent approach is the Sheather-Jones method, easily implemented in R; for example,\\n\\n    plot(density(precip, bw=""SJ""))\\n\\nThe situation for multivariate KDE is not so well studied, and the tools are not so mature. Rather than a bandwidth, you need a bandwidth matrix. To simplify the problem, most people assume a diagonal matrix, although this may not lead to the best results. The [ks package in R][1] provides some very useful tools including allowing a full (not necessarily diagonal) bandwidth matrix.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/ks/",,
389,2,180,d8b94140-6161-4562-9581-83febc83ccfd,2010-07-20 00:06:20.0,90.0,"I really like the [FRED][1], from the St. Louis Fed (economics data). You can chart the series or more than one series, you can do some transformations to your data and chart it, and the NBER recessions are shaded.\\n\\n\\n  [1]: http://research.stlouisfed.org/fred2/",,
390,16,180,d8b94140-6161-4562-9581-83febc83ccfd,2010-07-20 00:06:20.0,-1.0,,,
391,2,181,b1d1b7fc-6b47-42b2-8ce3-4e3af217fbb5,2010-07-20 00:15:02.0,159.0,"Is there a standard and accepted method for selecting the number of layers, and the number of nodes in each layer, in a FF NN? I'm interested in automated ways of building neural networks.",,
392,1,181,b1d1b7fc-6b47-42b2-8ce3-4e3af217fbb5,2010-07-20 00:15:02.0,159.0,How to choose the number of hidden layers and nodes in a feedforward neural network?,,
393,3,181,b1d1b7fc-6b47-42b2-8ce3-4e3af217fbb5,2010-07-20 00:15:02.0,159.0,<neural-networks>,,
394,2,182,3f2efcec-0d7a-416d-a895-cd77c774e37b,2010-07-20 00:15:47.0,74.0,"Sometimes outliers are bad data, and should be excluded, such as typos. Sometimes they are Wayne Gretzky or Michael Jordan, and should be kept. \\n\\nOutlier detection methods include:\\n\\nUnivariate -> boxplot\\n\\nBivariate -> scatterplot with confidence ellipse\\n\\nMultivariate -> Mahalanobis D2 distance\\n\\nMark those observations as outliers.\\n\\nRun a logistic regression (on Y=IsOutlier) to see if there are any systematic patterns. \\n\\nRemove ones that you can demonstrate they are not representative of any sub-population. \\n",,
395,6,21,de547e71-54d9-4f9f-b9e0-800b23e1ad9f,2010-07-20 00:19:20.0,159.0,<population><census><forecasting>,edited tags,
396,6,33,4e4ca71f-03be-4224-aa7b-06cc89027900,2010-07-20 00:20:37.0,159.0,<r><statistical-analysis><seasonality>,edited tags,
397,2,183,7c3b015d-c52b-4c9f-a107-a67c71be82c8,2010-07-20 00:20:51.0,166.0,"I need to analyze the 100k MovieLens dataset for clustering with two algorithms of my choice, between the likes of k-means, agnes, diana, dbscan, and several others. What tools (like Rattle, or Weka) would be best suited to help me make some simple clustering analysis over this dataset?",,
398,1,183,7c3b015d-c52b-4c9f-a107-a67c71be82c8,2010-07-20 00:20:51.0,166.0,What tools could be used for applying clustering algorithms on MovieLens?,,
399,3,183,7c3b015d-c52b-4c9f-a107-a67c71be82c8,2010-07-20 00:20:51.0,166.0,<clustering>,,
400,10,145,fa10f885-f225-4626-a896-0f0b6123c342,2010-07-20 00:21:48.0,-1.0,"{""OriginalQuestionIds"":[7],""Voters"":[{""Id"":74,""DisplayName"":""el chief""},{""Id"":13,""DisplayName"":""Sharpie""},{""Id"":8,""DisplayName"":""Colin Gillespie""},{""Id"":88,""DisplayName"":""mbq""},{""Id"":80,""DisplayName"":""Fabian Steeg""}]}",1,
401,5,145,e222a997-773f-4b6a-b4da-57895b07a4e0,2010-07-20 00:21:48.0,-1.0,"> **Possible Duplicate:**  
> [Locating freely available data samples](http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples)  

<!-- End of automatically inserted text -->

Where can I find freely accessible data sources?\\n\\nI'm thinking of sites like\\n\\n* [http://www2.census.gov/census_2000/datasets/][1]?\\n\\n\\n\\n  [1]: http://www2.census.gov/census_2000/datasets/",insert duplicate link,
402,2,184,8553d06a-a42c-47c7-ba1e-c233c802d9cb,2010-07-20 00:21:58.0,159.0,Try using the `stl()` function for time series decomposition. It provides a very flexible method for extracting a seasonal component from a time series.,,
403,5,165,fc59b87b-60cf-482e-bf69-b444ef495128,2010-07-20 00:22:24.0,74.0,"Maybe the concept, why it's used, and an example. Thanks!",deleted 17 characters in body; edited title,
404,4,165,fc59b87b-60cf-482e-bf69-b444ef495128,2010-07-20 00:22:24.0,74.0,How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?,deleted 17 characters in body; edited title,
405,2,185,8676cc2b-d6d0-4998-8575-5aeecf93ae8b,2010-07-20 00:30:00.0,80.0,"A great introductory text covering the topics you mentioned is [Introduction to Information Retrieval][1], which is available online in full text for free.\\n\\n![Introduction to Information Retrieval][2]\\n\\n  [1]: http://www.informationretrieval.org\\n  [2]: http://nlp.stanford.edu/IR-book/iir.jpg",,
407,2,187,3d84abd0-1e90-4c09-b816-0def604184dd,2010-07-20 00:47:45.0,119.0,"As far as I know there is no way to select automatically the number of layers and neurons in each layer. But there are networks that can build automatically their topology, like EANN (Evolutionary Artificial Neural Networks, which use Genetic Algorithms to evolved the topology).\\n\\nThere are several approaches, a more or less modern one that seemed to give good results was NEAT (Neuro Evolution of Augmented Topologies). You can get more info:\\n\\nhttp://nn.cs.utexas.edu/?neat",,
408,2,188,284bc08b-1cfa-4136-9f6d-4992a16ebe4e,2010-07-20 00:52:13.0,61.0,"I'd probably say something like this:\\n\\n""Anytime we want to talk about probabilities, we're really integrating a density.  In Bayesian analysis, a lot of the densities we come up with aren't analytically tractable: you can only integrate them -- if you can integrate them at all -- with a great deal of suffering.  So what we do instead is simulate the random variable a lot, and then figure out probabilities from our simulated random numbers.  If we want to know the probability that X is less than 10, we count the proportion of simulated random variable results less than 10 and use that as our estimate.  That's the ""Monte Carlo"" part, it's an estimate of probability based off of random numbers.  With enough simulated random numbers, the estimate is very good, but it's still inherently random.\\n\\n""So why ""Markov Chain""?  Because under certain technical conditions, you can generate a memoryless process (aka a Markovian one) that has the same limiting distribution as the random variable that you're trying to simulate.  You can iterate any of a number of different kinds of simulation processes that generate correlated random numbers (based only on the current value of those numbers), and you're guaranteed that once you pool enough of the results, you will end up with a pile of numbers that looks ""as if"" you had somehow managed to take independent samples from the complicated distribution you wanted to know about.\\n\\n""So for example, if I want to estimate the probability that a standard normal random variable was less than 0.5, I could generate ten thousand independent realizations from a standard normal distribution and count up the number less than 0.5; say I got 6905 that were less than 10000; my estimate for P(Z<0.5) would be 0.6905, which isn't that far off from the actual value.  That'd be a Monte Carlo estimate.\\n\\n""Now imagine I couldn't draw independent normal random variables, instead I'd start at 0, and then with every step add some uniform random number between -0.5 and 0.5 to my current value, and then decide based on a particular test whether I liked that new value or not; if I liked it, I'd use the new value as my current one, and if not, I'd reject it and stick with my old value.  Because I only look at the new and current values, this is a Markov chain.  If I set up the test to decide whether or not I keep the new value correctly (it'd be a random walk Metropolis-Hastings, and the details get a bit complex), then even though I never generate a single normal random variable, if I do this procedure for long enough, the list of numbers I get from the procedure will be distributed like a large number of draws from something that generates normal random variables.  This would give me a Markov Chain Monte Carlo simulation for a standard normal random variable.  If I used this to estimate probabilities, that would be a MCMC estimate.""",,
410,2,189,473253fd-8480-4717-ac83-e91d48ca25f4,2010-07-20 00:59:34.0,173.0,For a continuous random variable you can always approximate the pdf by calculating (CDF(x2) - CDF(x1))/(x2 - x1) where x1 and x2 are on either side of the point where you want to know the pdf and the distance |x2 - x1| is small.,,
411,2,190,a2ab6a04-abdb-418c-a30c-b9e5abf70195,2010-07-20 01:07:38.0,25.0,[A New View of Statistics](http://www.sportsci.org/resource/stats/) by Will G. Hopkins is great!,,
412,4,156,e052614d-50a9-4dfe-88fa-5c99fe930f23,2010-07-20 01:13:21.0,148.0,How to get to a t variable from linear regression,edited title,
413,2,191,1b9cce05-ba3a-41e5-a3c8-1c4c881ead89,2010-07-20 01:17:17.0,173.0,"The Bland-Altman plot is more widely known as the **Tukey Mean-Difference Plot** (one of many charts devised by John Tukey http://en.wikipedia.org/wiki/John_Tukey).\\n\\nThe idea is that x-axis is the mean of your two measurements, which is your best guess as to the ""correct"" result and the y-axis is the difference between the two measurement differences. The chart can then highlight certain types of anomalies in the measurements. For example, if one method always gives too high a result, then you'll get all of your points above or all below the zero line. It can also reveal, for example, that one method over-estimates high values and under-estimates low values.\\n\\nIf you see the points on the Bland-Altman plot scattered all over the place, above and below zero, then the suggests that there is no consistent bias of one approach versus the other (of course, there could be hidden biases that this plot does not show up).\\n\\nEssentially, it is a good first step for exploring the data. Other techniques can be used to dig into more particular sorts of behaviour of the measurements.",,
414,2,192,2d896c09-3484-41ad-ae2f-3f6c0e6f0bb8,2010-07-20 01:18:11.0,1356.0,"I'm aware that this one is far from *yes or no* question, but I'd like to know which techniques do you prefer in categorical data analysis - i.e. cross tabulation with two categorical variables.\\n\\nI've come up with: \\n\\n - &chi;<sup>2</sup> test - well, this is quite self-explanatory\\n  - Fisher's exact test - when n < 40,\\n  - Yates' continuity correction - when n > 40,\\n - Cramer's V - measure of association for tables which have more than *2 x 2* cells,\\n - &Phi; coefficient - measure of association for *2 x 2* tables,\\n - contingency coefficient (C) - measure of association for *n x n* tables,\\n - odds ratio - independence of two categorical variables,\\n - McNemar marginal homogeniety test,\\n\\nAnd my question here is: ***which statistical techniques for cross-tabulated data (two categorical variables) do you consider relevant (and why)?***",,
415,1,192,2d896c09-3484-41ad-ae2f-3f6c0e6f0bb8,2010-07-20 01:18:11.0,1356.0,Cross tabulation of two categorical variables: recommended techniques,,
416,3,192,2d896c09-3484-41ad-ae2f-3f6c0e6f0bb8,2010-07-20 01:18:11.0,1356.0,<statistical-analysis><nonparametric><contingency-tables><categorical-data>,,
417,2,193,ef966819-8b18-491b-8c0c-d983c5d98dc7,2010-07-20 01:45:12.0,,"Suppose that you want to know what percentage of people would vote for a particular candidate (say, pi. Note: by definition pi is between 0 and 100). You sample N voters at random to find out how they would vote and your survey of these N voters tells you that the percentage is p. So, you would like to establish a confidence interval for the true percentage. \\n\\nIf you assume that p is normally distributed (an assumption that may or may not be justified depending on how 'big' N is) then your confidence interval for pi would be of the following form:\\n\\nCI = [ p - k * sd(p) , p + k * sd(p)]\\n\\nwhere k is a constant that depends on the extent of confidence you want (i.e., 95% or 99% etc).\\n\\nFrom a polling perspective, you want the width of your confidence interval to be 'low'. Usually, pollsters work with the margin of error which is basically one-half of the CI. In other words:\\n\\nMargin of Error = k * sd(p). \\n\\nHere is how we would go about calculating sd(p).\\n\\nBy definition, p = Sum X_i / N \\n\\nwhere,\\n\\nX_i = 1 if voter i votes for candidate and 0 otherwise.\\n\\nSince, we sampled the voters at random, we could assume that X_i is a i.i.d bernoulli random variable. Therefore, \\n\\nVariance(P) =  V ( Sum X_i / N) = [ Sum V(X_i) ] / N^2 = N pi (1-pi) / N^2 = pi  (1-pi) / N.\\n\\nThus, \\n\\nsd(p) = sqrt ( pi * (1-pi) / N)\\n\\nNow to estimate margin of error we need to know pi which we do not know obviously. But, an inspection of the numerator suggests that the 'worst' estimate for sd(p) in the sense that we get the 'largest' standard deviation is when pi = 50. Therefore,\\n\\nWorst possible sd(p) = sqrt (50 * 50 / N ) = 50 / sqrt(N)\\n\\nSo, you see that the margin of error falls off exponentially with N and thus you really do not need very big samples to reduce your margin of error or in other words N need not be very large for you to obtain a narrow confidence interval.\\n\\nFor example, for a 95 % confidence interval (i.e., k= 1.96) and N = 1000, the confidence interval is: \\n\\n[p - 1.96 * 50/sqrt(1000), p + 1.96 * 50/sqrt(1000)] = [p - 3, p + 3] \\n\\nAs we increase N the costs of polling go up linearly but the gains go down exponentially. That is the reason why pollsters usually cap N at 1000 as that gives them a reasonable error of margin under the worst possible assumption of pi = 50%.\\n\\n ",,user28
418,2,194,1be7c2b5-98fd-4a0c-be46-adabb288d853,2010-07-20 01:47:36.0,175.0,"I am sure that everyone who's trying to find patterns in historical stock market data or betting history would like to know about this. Given a huge sets of data, and thousands of random variables that may or may not affect it, it makes sense to ask any patterns that you extract out from the data are indeed true patterns, not statistical fluke.\\n\\nA lot of patterns are only valid when they are tested in the samples. And even those that are patterns that are valid out of samples may cease to become valid when you apply it in the real world. \\n\\nI understand that it is not possible to completely 100% make sure a pattern is valid all the time, but besides in and out of samples tests, are their any tests that could establish the validness of a pattern?",,
419,1,194,1be7c2b5-98fd-4a0c-be46-adabb288d853,2010-07-20 01:47:36.0,175.0,Data Mining-- How to Tell Whether the Pattern Extracted is Meaningful?,,
420,3,194,1be7c2b5-98fd-4a0c-be46-adabb288d853,2010-07-20 01:47:36.0,175.0,<data-mining>,,
421,2,195,23df9034-fb0b-453f-9242-3bf5e3c5c649,2010-07-20 02:01:05.0,173.0,I am looking at fitting distributions to data (with a particular focus on the tail) and am leaning towards Anderson-Darling tests rather than Kolmogorov-Smirnov. What do you think are the relative merits of these or other tests for fit (e.g. Cramer-von Mises)?,,
422,1,195,23df9034-fb0b-453f-9242-3bf5e3c5c649,2010-07-20 02:01:05.0,173.0,What do you think is the best goodness of fit test?,,
423,3,195,23df9034-fb0b-453f-9242-3bf5e3c5c649,2010-07-20 02:01:05.0,173.0,<hypothesis-testing><fitting>,,
424,5,46,c80e33a5-c893-44bf-9989-11fe09d120a9,2010-07-20 02:13:12.0,62.0,"A standard deviation is the square root of the second central moment of a distribution. A central moment is the expected difference from the expected value of the distribution. A first central moment would usually be 0, so we define a second central moment as the expected value of the squared distance of a random variable from its expected value. \\n\\nTo put it on a scale that is more in line with the original observations, we take the square root of that second central moment and call it the standard deviation. \\n\\nStandard deviation is a property of a population. It measures how much average ""dispersion"" there is to that population. Are all the obsrvations clustered around the mean, or are they widely spread out? \\n\\nTo estimate the standard deviation of a population, we often calculate the standard deviation of a ""sample"" from that population. To do this, you take observations from that population, calculate a mean of those observations, and then calculate the square root of the average squared deviation from that ""sample mean"". \\n\\nTo get an unbiased estimator of the variance, you don't actually calculate the average squared deviation from the sample mean, but instead, you divide by (N-1) where N is the number of observations in your sample. Note that this ""sample standard deviation"" is not an unbiased estimator of the standard deviation, but the square of the ""sample standard deviation"" is an unbiased estimator of the variance of the population. \\n\\n\\n",added 963 characters in body,
425,2,196,246ebe84-5e41-4b73-bfe6-7a05843da6bd,2010-07-20 02:17:24.0,87.0,"Besides gnuplot and [ggobi][1], what open source tools are people using to for visualizing multi-dimensional data?\\n\\nGnuplot is more or less a basic plotting package. \\n\\nGgobi can do a number of nifty things, such as:\\n\\n -  animate data along a dimension or among discrete collections\\n -  animate linear combinations varying the coefficients\\n -  compute principal components and other transformations\\n -  visualize and rotate 3 dimensional data clusters\\n -  use colors to represent a different dimension\\n\\nI would like to hear about other useful approaches that are based in open source and thus freely reusable or customizable.  Thanks.\\n\\n\\n  [1]: http://www.ggobi.org/",,
426,1,196,246ebe84-5e41-4b73-bfe6-7a05843da6bd,2010-07-20 02:17:24.0,87.0,Open source tools for visualizing multi-dimensional data ?,,
427,3,196,246ebe84-5e41-4b73-bfe6-7a05843da6bd,2010-07-20 02:17:24.0,87.0,<data-visualization>,,
428,2,197,125d6609-4d33-41c8-8359-35e9bbf3a4fc,2010-07-20 02:24:38.0,5.0,How about R with ggplot2?,,
429,2,198,b90e2003-1aca-4102-8df8-ddd49289bfef,2010-07-20 02:32:43.0,61.0,"Start with the distribution of y_bar, show that since v is normal, y_bar is multivariate normal and that consequently u must also be a multivariate normal; also show that the covariance matrix of y_bar is of the form sigma^2*(X^T * X)^-1 and thus -- if sigma^2 were known -- the variance of u would be sigma^2*c^T*(X^T*X)^(-1)*c.  Show that the distribution of e_bar^T * e_bar must be chi-squared and (*carefully*) find the degrees of freedom.  Think about how what the operation e_bar^T*e_bar*c^T*(X^T*X)^(-1)*c must therefore produce, and what it's distribution and degrees of freedom are.\\n\\nThe result follows (almost) immediately from the definition of the t-distribution.",,
430,2,199,8f52d8e2-b0ba-43fc-bd17-c517a659e6e8,2010-07-20 02:32:53.0,5.0,You could try:\\n\\n - Bagging http://en.m.wikipedia.org/wiki/Bootstrap_aggregating\\n - Boosting http://en.m.wikipedia.org/wiki/Boosting\\n - Cross validation http://en.m.wikipedia.org/wiki/Cross-validation_(statistics),,
431,16,196,4ce06585-d0c4-43e2-9fa6-76b59109fe63,2010-07-20 02:35:32.0,87.0,,,
432,5,197,53148ad4-41cd-4c44-b1f4-0af5b45c825d,2010-07-20 02:42:01.0,5.0,How about R with [ggplot2][3]?\\n\\nOther tools that I really like:\\n\\n - [Processing][10]\\n - [Prefuse][11]  \\n - [Protovis][12]\\n\\n  [3]: http://had.co.nz/ggplot2/\\n  [10]: http://www.processing.org/\\n  [11]: http://prefuse.org/\\n  [12]: http://vis.stanford.edu/protovis/\\n ,added 249 characters in body,
433,2,200,c7ff66cb-91a1-42be-9a91-436681bbf21d,2010-07-20 02:48:45.0,174.0,"If you want to know that a pattern is meaningful, you need to show what it actually *means*. Statistical tests do not do this. Unless your data can be said to be in some sense ""complete"", inferences draw from the data will always be provisional.\\n\\nYou can increase your *confidence* in the validity of a pattern by testing against more and more out of sample data, but that doesn't protect you from it turning out to be an artefact. The broader your range of out of sample data -- eg, in terms of how it is acquired and what sort of systematic confounding factors might exist within it -- the better the validation.\\n\\nIdeally, though, you need to go beyond identifying patterns and come up with a persuasive theoretical framework that *explains* the patterns you've found, and then test *that* by other, independent means. (This is called ""science"".)",,
434,6,165,d5320234-6c94-45a8-85b7-2f91aeda7178,2010-07-20 03:01:29.0,61.0,<bayesian><mcmc><teaching>,edited tags,
435,4,118,1d7716e8-ba6c-4741-9368-fb18c91e932d,2010-07-20 03:05:58.0,83.0,Standard deviation : Why square the difference instead of taking the absolute value?,edited title,
436,2,201,cf198915-d4d3-4b73-a71f-10c1219b61fb,2010-07-20 03:11:36.0,183.0,"Start R and type `data()`. This will show all datasets in the search path.\\nMany additional datasets are available in add-on packages.\\nFor example, there are some interesting real-world social science datasets in the `AER` package.",,
437,2,202,84825c72-d4d9-4ea2-8c88-e953acc18cdf,2010-07-20 03:13:22.0,183.0,"If you like learning through videos, I collated a list of R training videos:\\nhttp://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html",,
438,2,203,cc5187e0-636e-4611-8a01-124a7fb06973,2010-07-20 03:31:45.0,183.0,"Following on from [this question][1]:\\nImagine that you want to test for differences in central tendency between two groups (e.g., males and females)\\non a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied).\\nI think a t-test would be sufficiently accurate for most purposes,\\n but that a bootstrap test of differences between group means would often provide more accurate p-values.\\nWhat statistical test would you use?\\n\\n  [1]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data",,
439,1,203,cc5187e0-636e-4611-8a01-124a7fb06973,2010-07-20 03:31:45.0,183.0,Group differences on a five point Likert item,,
440,3,203,cc5187e0-636e-4611-8a01-124a7fb06973,2010-07-20 03:31:45.0,183.0,<scales><ordinal><t-test><interval>,,
441,2,204,94778fd8-d5b8-4126-94bd-3abecaed9143,2010-07-20 03:35:58.0,183.0,The lattice package in R\\n\\n,,
442,16,204,94778fd8-d5b8-4126-94bd-3abecaed9143,2010-07-20 03:35:58.0,-1.0,,,
443,5,203,33e11b2d-6cd4-49da-84a3-3e52378fac8a,2010-07-20 03:43:57.0,183.0,"Following on from [this question][1]:\\nImagine that you want to test for differences in central tendency between two groups (e.g., males and females)\\non a 5-point Likert item (e.g., satisfaction with life: Dissatisfied to Satisfied).\\nI think a t-test would be sufficiently accurate for most purposes,\\n but that a bootstrap test of differences between group means would often provide more accurate estimate of confidence intervals.\\nWhat statistical test would you use?\\n\\n  [1]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data",added 24 characters in body,
444,2,205,c9587fc0-c357-412d-8efa-c1bc25773530,2010-07-20 03:51:24.0,187.0,"I'm curious about why we treat fitting GLMS as though they were some special optimization problem.  Are they?  It seems to me that they're just maximum likelihood, and that we write down the likelihood and then ... we maximize it!  So why do we use Fisher scoring instead of any of the myriad of optimization schemes that has been developed in the applied math literature? ",,
445,1,205,c9587fc0-c357-412d-8efa-c1bc25773530,2010-07-20 03:51:24.0,187.0,Why do we make a big fuss about using Fisher scoring when we fit a GLM?,,
446,3,205,c9587fc0-c357-412d-8efa-c1bc25773530,2010-07-20 03:51:24.0,187.0,<generalized-linear-model>,,
447,2,206,1b62c7aa-1144-4376-8309-3e10a492c891,2010-07-20 03:53:54.0,188.0,What is the difference between discrete data and continuous data?,,
448,1,206,1b62c7aa-1144-4376-8309-3e10a492c891,2010-07-20 03:53:54.0,188.0,Discrete and Continuous,,
449,3,206,1b62c7aa-1144-4376-8309-3e10a492c891,2010-07-20 03:53:54.0,188.0,<discrete-data><continuous-data>,,
450,6,196,2aeb3f60-ec23-4142-b345-17dbab1257c0,2010-07-20 03:56:44.0,18.0,<data-visualization><open-source>,edited tags,
451,2,207,2ad73f24-6ddb-4e60-ae03-ff8efdd07ad8,2010-07-20 04:00:14.0,,"First, we need to understand what is a markov chain. Consider the following [weather][1] example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:\\n\\nProbability(Next day is sunny | Given today is rainy ) = 0.50\\n\\nSince, the next day's weather is either sunny or rainy it follows that:\\n\\nProbability(Next day is Rainy | Given today is rainy ) = 0.50 \\n\\nSimilarly, let:\\n\\nProbability(Next day is rainy | Given today is sunny ) = 0.10\\n\\nTherefore, it follows that:\\n\\nProbability(Next day is sunny | Given today is sunny ) = 0.90\\n\\nThe above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:\\n\\n             S   R\\n    P = S [ 0.9 0.1\\n        R   0.5 0.5]\\n\\nWe might ask several questions whose answers follow:\\n\\nQ1: If the weather is sunny today then what is the weather likely to be tomorrow?\\n\\nA1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. \\n\\nQ2: What about two days from today?\\n\\nA2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now:\\n\\nFirst day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9  0.9. \\n\\nOr\\n\\nFirst day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 * 0.5\\n\\nTherefore, the probability that the weather will be sunny in two days is:\\n\\nProb(Sunny two days from now) = 0.9  0.9 + 0.1  0.5 = 0.81 + 0.05 = 0.86 \\n\\nSimilarly, the probability that it will be rainy is:\\n\\nProb(Rainy two days from now) = 0.1 * 0.5 + 0.9 0.1 = 0.05 + 0.09 = 0.14\\n\\nIf you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities:\\n\\nProb(Sunny) = 0.833\\nProb(Rainy) = 0.167\\n\\nIn other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.\\n\\nThe above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions):\\n\\nIrrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.\\n\\nMarkov Chain Monte Carlo exploits the above feature as follows: \\n\\nWe want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. \\n\\nIf we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. \\n\\nWe then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monet carlo component.\\n\\nThere are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm).\\n\\n  [1]: http://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_very_simple_weather_model",,user28
452,2,208,632070fc-2901-4863-8e2b-9f9c9cf7cc4e,2010-07-20 04:07:11.0,74.0,"Temperatures are continuous. It can be 23 degrees, 23.1 degrees, 23.100004 degrees. \\n\\nGender is discrete. You can only be male or female (insert san francisco joke here). Something you would represent with a whole number like 0, 1, 2, etc\\n\\nThe difference is important as many statistical and data mining algorithms can handle one type but not the other. For example in regular regression, the Y must be continuous. In logistic regression the Y is discrete. ",,
453,2,209,b25dbbbd-7e13-40a0-8721-287576bc5356,2010-07-20 04:16:52.0,174.0,"Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.\\n\\nContinuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric.\\n",,
454,2,210,946596bb-bbf3-4d39-9b17-099fdce34968,2010-07-20 04:19:12.0,183.0,"Data is always discrete. Given a sample of `n` values on a variable, the maximum number of distinct values the variable can take is equal to  `n`.\\n\\nData on a variable are typically assumed to be drawn from a random variable.\\nThe random variable is continuous over a range if there is an infinite number of possible values that the variable can take between any two different points in the range.\\nFor example, height, weight, and time are typically assumed to be continuous.\\nOf course, any measurement of these variables will be finitely accurate and in some\\n sense discrete.\\n\\nIt is useful to distinguish between ordered (i.e., ordinal), unordered (i.e., nominal),  \\nand binary discrete variables.\\n\\nSome introductory textbooks confuse a continuous variable with a numeric variable.\\nFor example, a score on a computer game is discrete even though it is numeric.\\n\\nSome introductory textbooks confuse a ratio variable with continuous variables. A count variable is a ratio variable, but it is not continuous.\\n\\nIn actual practice, a variable is often treated as continuous when it can take on a sufficiently large number of different values.\\n\\n",,
455,2,211,fcb56ad8-88a9-47f1-bb69-68754a96161e,2010-07-20 04:49:07.0,187.0,I have written a document that is freely available at my website and on CRAN. See the linked page:\\n\\n[icebreakeR][1]\\n\\nThe datasets that are used in the document are also linked from that page.  Feedback is welcome and appreciated!\\n\\nAndrew\\n\\n\\n  [1]: http://www.ms.unimelb.edu.au/~andrewpr/r-users/,,
456,2,212,b0426c88-7ec6-4b42-8cf3-d298cdd56ec7,2010-07-20 04:54:20.0,190.0,"I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate.\\n\\nWhat methods do I have to test for statistical significance of my new results?",,
457,1,212,b0426c88-7ec6-4b42-8cf3-d298cdd56ec7,2010-07-20 04:54:20.0,190.0,What method to use to test Statistical Significance of ASR results,,
458,3,212,b0426c88-7ec6-4b42-8cf3-d298cdd56ec7,2010-07-20 04:54:20.0,190.0,<statistical-significance>,,
459,2,213,c2c56819-8032-44fc-93a9-3dd2ec1a30d4,2010-07-20 05:02:33.0,159.0,"Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.\\n\\nI am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.\\n\\nOne possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches.",,
460,1,213,c2c56819-8032-44fc-93a9-3dd2ec1a30d4,2010-07-20 05:02:33.0,159.0,What is the best way to identify outliers in multivariate data?,,
461,3,213,c2c56819-8032-44fc-93a9-3dd2ec1a30d4,2010-07-20 05:02:33.0,159.0,<multivariable><outliers>,,
462,2,214,18295242-39ea-4237-be6a-1d6d26d94f00,2010-07-20 05:02:42.0,40.0,Some free Stats textbooks are also available [here][1].\\n\\n\\n  [1]: http://www.e-booksdirectory.com/mathematics.php,,
463,2,215,9e0b9514-5295-446c-af10-b858ddfd2b43,2010-07-20 05:03:12.0,187.0,"I'm not sure about these tests, so this answer may be off-topic.  Apologies if so.  But, are you sure that you want a test?  It really depends on what the purpose of the exercise is.  Why are you fitting the distributions to the data, and what will you do with the fitted distributions  afterward?  \\n\\nIf you want to know what distribution fits best just because you're interested, then a test may help.  \\n\\nOn the other hand, if you want to actually do something with the distribution, then you'd be better off developing a loss function based on your intentions, and using the distribution that gives you the most satisfactory value for the loss function.  \\n\\nIt sounds to me from your description (particular focus on the tail) that you want to actually do something with the distribution.  If so, it's hard for me to imagine a situation where an existing test will provide better guidance than comparing the effects of the fitted distributions in situ, somehow.",,
464,2,216,192c804a-f93b-43c2-8c3d-5e4493cb3c3d,2010-07-20 05:04:40.0,191.0,What are some good visualization libraries for online use? Are they easy to use and is there good documentation?\\n\\nThanks.,,
465,1,216,192c804a-f93b-43c2-8c3d-5e4493cb3c3d,2010-07-20 05:04:40.0,191.0,Web visualization libraries,,
466,3,216,192c804a-f93b-43c2-8c3d-5e4493cb3c3d,2010-07-20 05:04:40.0,191.0,<data-visualization><library><protovis>,,
467,2,217,9391e1ba-d506-4ba4-b3cf-b8e761f698d3,2010-07-20 05:10:08.0,5.0,"IMO, **[Protovis][12]** is the best and is very well documented and supported.  It is the basis for my [webvis][http://cran.r-project.org/web/packages/webvis/index.html] R package.  \\n\\nThese are also very good, although they have more of a learning curve:\\n\\n - [Processing][10]\\n - [Prefuse][11]  \\n\\n  [10]: http://www.processing.org/\\n  [11]: http://prefuse.org/\\n  [12]: http://vis.stanford.edu/protovis/",,
468,2,218,e22633d6-e8c8-4440-be33-61ece2f8c455,2010-07-20 05:13:21.0,187.0,"This interesting question is the subject of some research in [ACERA][1].  The lead researcher is Andrew  Speirs-Bridge, and his work is eminently google-able :)\\n\\n\\n  [1]: http://www.acera.unimelb.edu.au/",,
469,5,217,a223f60f-fc0d-4352-8974-9870efb9c35b,2010-07-20 05:15:51.0,5.0,"IMO, **[Protovis][12]** is the best and is very well documented and supported.  It is the basis for my [webvis][1] R package.  \\n\\nThese are also very good, although they have more of a learning curve:\\n\\n - [Processing][10]\\n - [Prefuse][11]  \\n\\n  [1]: http://cran.r-project.org/web/packages/webvis/index.html\\n  [10]: http://www.processing.org/\\n  [11]: http://prefuse.org/\\n  [12]: http://vis.stanford.edu/protovis/",added 10 characters in body,
470,2,219,25f497bf-a2fe-4e6a-ac44-8172ed315824,2010-07-20 05:21:14.0,154.0,The Xorshift PNG designed by George Marsaglia. Not as good as the Mersenne-Twister but very simple to implement and highly parallelizable. Performs well on many-core architectures such as DSP chips and Nvidia's Tesla.,,
471,2,220,c6cc7bd6-93fa-4eb1-bee2-f5905b2eb908,2010-07-20 05:23:04.0,85.0,"If X1, ..., Xn are independent identically-distributed random variables, what can be said about the distribution of min(X1, ..., Xn) in general?\\n\\n",,
472,1,220,c6cc7bd6-93fa-4eb1-bee2-f5905b2eb908,2010-07-20 05:23:04.0,85.0,How is the minimum of a set of rvs distributed?,,
473,3,220,c6cc7bd6-93fa-4eb1-bee2-f5905b2eb908,2010-07-20 05:23:04.0,85.0,<distributions><random-variables><minimum>,,
474,5,209,0cd94239-601a-40e0-bbf7-f2b5dbea9589,2010-07-20 05:25:22.0,174.0,"Discrete data can only take particular values. There may potentially be an infinite number of those values, but each is distinct and there's no grey area in between. Discrete data can be numeric -- like numbers of apples -- but it can also be categorical -- like red or blue, or male or female, or good or bad.\\n\\nContinuous data are not restricted to defined separate values, but can occupy any value over a continuous range. Between any two continuous data values there may be an infinite number of others. Continuous data are always essentially numeric.\\n\\nIt sometimes makes sense to treat numeric data that is properly of one type as being of the other. For example, something like *height* is continuous, but often we don't really care too much about tiny differences and instead group heights into a number of discrete **bins**. Conversely, if we're counting large amounts of some discrete entity -- grains of rice, or termites, or pennies in the economy -- we may choose not to think of 2,000,006 and 2,000,008 as crucially different values but instead as nearby points on an approximate continuum.\\n\\nIt can also sometimes be useful to treat numeric data as categorical, eg: underweight, normal, obese. This is usually just another kind of binning.\\n\\nIt seldom makes sense to consider categorical data as continuous.",added stuff about treating one kind of data as another,
475,5,213,fb6d122c-d4d0-4c59-bf15-00ea2882499e,2010-07-20 05:28:56.0,159.0,"Suppose I have a large set of multivariate data with at least three variables. How can I find the outliers? Pairwise scatterplots won't work as it is possible for an outlier to exist in 3 dimensions that is not an outlier in any of the 2 dimensional subspaces.\\n\\nI am not thinking of a regression problem, but of true multivariate data. So answers involving robust regression or computing leverage are not helpful.\\n\\nOne possibility would be to compute the principal component scores and look for an outlier in the bivariate scatterplot of the first two scores. Would that be guaranteed to work? Are there better approaches?",edited body,
476,5,219,8b758f55-4a59-45be-b02b-e0edf268f580,2010-07-20 05:33:00.0,154.0,The Xorshift PNG designed by George Marsaglia. Its period (2^128-1) is much shorter than the Mersenne-Twister but the algorithm is very simple to implement and lends itself to parallelization. Performs well on many-core architectures such as DSP chips and Nvidia's Tesla.,added 20 characters in body; added 24 characters in body; added 6 characters in body; added 4 characters in body,
477,2,221,9c2c5269-92ca-481d-8438-395b1237da36,2010-07-20 05:35:48.0,159.0,"If the cdf of Xi is denoted by F(x), then the cdf of the minimum is given by [1-F(x)]^n.\\n\\n",,
478,2,222,07f7ceae-2a2b-4498-b2ca-c217ce7aa13b,2010-07-20 05:37:46.0,191.0,I know this is probably simplistic but what are Principal component scores?\\n\\nThis question originates from my attempt to understand this question [here][1].\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/213/what-is-the-best-way-to-identify-outliers-in-multivariate-data,,
479,1,222,07f7ceae-2a2b-4498-b2ca-c217ce7aa13b,2010-07-20 05:37:46.0,191.0,What are principal component scores?,,
480,3,222,07f7ceae-2a2b-4498-b2ca-c217ce7aa13b,2010-07-20 05:37:46.0,191.0,<component-scores><fundamentals>,,
481,4,220,96945a15-87e9-4f80-94f7-c7ce8640532d,2010-07-20 05:43:42.0,85.0,How is the minimum of a set of random variables distributed?,edited title,
482,2,223,46b8f548-570b-4bec-a930-510fb003068b,2010-07-20 05:54:15.0,79.0,I have a friend who is an MD and wants to refresh his Statistics. So is there any recommended resource online (or offline) ? He did stats ~20 years ago.,,
483,1,223,46b8f548-570b-4bec-a930-510fb003068b,2010-07-20 05:54:15.0,79.0,Intro to statistics for an MD?,,
484,3,223,46b8f548-570b-4bec-a930-510fb003068b,2010-07-20 05:54:15.0,79.0,<textbook><online><introductory>,,
485,2,224,c37eed7c-d60c-4dfd-a834-aee0dc15eb5c,2010-07-20 06:03:59.0,128.0,"Which visualization libraries would you suggest to use in a standalone application (Linux, .Net, Windows, whatever). Reasonable performance would be nice as well.",,
486,1,224,c37eed7c-d60c-4dfd-a834-aee0dc15eb5c,2010-07-20 06:03:59.0,128.0,Visualization Libraries,,
487,3,224,c37eed7c-d60c-4dfd-a834-aee0dc15eb5c,2010-07-20 06:03:59.0,128.0,<data-visualization>,,
488,2,225,42db41e9-e1f9-4e06-985e-27ba324f8b9f,2010-07-20 06:07:37.0,196.0,"Why is the average of the highest value from 100 draws from a normal distribution different from the 98% percentile of the normal distribution?  It seems that by definition that they should be the same.  But...\\n\\nCode in R:\\n\\n    NSIM <- 10000\\n    x <- rep(NA,NSIM)\\n    for (i in 1:NSIM)\\n    {\\n    	x[i] <- max(rnorm(100))\\n    }\\n    qnorm(.98)\\n    qnorm(.99)\\n    mean(x)\\n    median(x)\\n    hist(x)\\n\\nI imagine that I'm misunderstanding something about what the maximum of a 100 draws from the normal distribution should be.  As is demonstrated by an unexpectedly asymetrical distribution of maximum values.\\n",,
489,1,225,42db41e9-e1f9-4e06-985e-27ba324f8b9f,2010-07-20 06:07:37.0,196.0,Why is the average of the highest value from 100 draws from a normal distribution different from the 99% percentile of the normal distribution?,,
490,3,225,42db41e9-e1f9-4e06-985e-27ba324f8b9f,2010-07-20 06:07:37.0,196.0,<r><distributions><maximum>,,
491,6,95,2ee192b9-7866-4b5d-a8b5-4b6518349139,2010-07-20 06:14:40.0,154.0,<time-series><garch><volatility-forecasting><finance>,edited tags,
492,2,226,7a6ececc-db65-4a12-ac6a-c5b403c951b2,2010-07-20 06:23:21.0,173.0,"Principal component analysis (PCA) is one popular approach analyzing variance when you are dealing with multivariate data. You have random variables X1, X2,...Xn which are all correlated (positively or negatively) to varying degrees, and you want to get a better understanding of what's going on. PCA can help.\\n\\nWhat PCA gives you is a change of variable into Y1, Y2,..., Yn (i.e. the same number of variables) which are linear combinations of the Xs. For example, you might have Y1 = 2.1 X1 - 1.76 X2 + 0.2 X3...\\n\\nThe Ys the nice property that each of these have zero correlation with each other. Better still, you get them in decreasing order of variance. So, Y1 ""explains"" a big chunk of the variance of the original variables, Y2 a bit less and so on. Usually after the first few Ys, the variables become somewhat meaningless. The PCA score for any of the Xi is just it's coefficient in each of the Ys. In my earlier example, the score for X2 in the first principal component (Y1) is 1.76.\\n\\nThe way PCA does this magic is by computing eigenvectors of the covariance matrix.\\n\\nTo give a concrete example, imagine X1,...X10 are changes in 1 year, 2 year, ..., 10 year Treasury bond yields over some time period. When you compute PCA you generally find that the first component has scores for each bond of the same sign and about the same sign. This tells you that most of the variance in bond yields comes from everything moving the same way: ""parallel shifts"" up or down. The second component typically shows ""steepening"" and ""flattening"" of the curve and has opposite signs for X1 and X10.",,
493,2,227,96318b27-a589-4d1a-a261-513ee81214e6,2010-07-20 06:24:32.0,144.0,"Let i be N rows and j be M columns. Suppose you linearize the combination of variables (columns):\\n\\n    Z_i1 = c_11×Y_i1 + c_12×Yi2 + ... + c_1M×YiM\\n\\nThe above formula basically says to multiply row elements with a certain value c (loadings) and sum them by columns.\\n\\nA principal component (PC) is a linear combination Z_1 = (Z_11, ..., Z_N1) (values by columns). In essence, the PC should present the most important features of variables (columns). Ergo, you can extract as many PC as there are variables (or less).\\n\\nAn output from <a href=""http://cran.r-project.org/"">R</a> on PCA (a fake example) looks like this. PC1, PC2... are principal components 1, 2... The example below is showing only the first 8 principal components (out of 17). You can also extract other elements from PCA, like loadings and scores.\\n\\n    Importance of components:\\n                              PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8\\n    Standard deviation     1.0889 1.0642 1.0550 1.0475 1.0387 1.0277 1.0169 1.0105\\n    Proportion of Variance 0.0697 0.0666 0.0655 0.0645 0.0635 0.0621 0.0608 0.0601\\n    Cumulative Proportion  0.0697 0.1364 0.2018 0.2664 0.3298 0.3920 0.4528 0.5129\\n\\n",,
494,2,228,c5dd83a0-7925-42f5-8894-05b50be424fa,2010-07-20 06:27:16.0,10.0, - http://insideria.com/2009/12/28-rich-data-visualization-too.html 28 Rich Data Visualization Tools\\n - http://www.rgraph.net/ R graph\\n - http://vis.stanford.edu/protovis/\\n \\n ,,
495,2,229,abf06717-575e-468a-82b3-01ab4216582e,2010-07-20 06:28:47.0,159.0,"The maximum does not have a normal distribution. Its cdf is Phi^100(x) where Phi(x) is the standard normal cdf. In general the moments of this distribution are tricky to obtain analytically. There is an ancient paper on this by [Tippett (*Biometrika*, 1925)][1].\\n\\n\\n  [1]: http://www.jstor.org/stable/2332087",,
496,2,230,fae807e1-530c-42f5-83e8-f763dcfdeec0,2010-07-20 06:35:21.0,173.0,You could have a look at Processing: http://processing.org/,,
497,5,176,28ca4081-210f-4fb7-a0ac-c884314aaf42,2010-07-20 06:40:59.0,81.0,"Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book\\n\\nThe Frequentist would say that each outcome has an equal 1 in 6 chance of occurring.\\n\\nThe Bayesian however would say hang on a second, I know that man, he's David Blane, a famous trickster, I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on 3 BUT I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll increase the chance from 1% to something slightly higher, otherwise I'll reduce it even further. \\n",deleted 4 characters in body; added 13 characters in body,
498,2,231,6234f949-9a78-4de6-8a87-a8895a18a071,2010-07-20 06:41:13.0,199.0,"This is one I've used successfully:\\n\\nhttp://www.amazon.co.uk/Statistics-without-Psychology-Christine-Dancey/dp/013124941X\\n\\nI just stumlbed on this too, this might be useful:\\n\\nhttp://www-users.york.ac.uk/~mb55/pubs/pbstnote.htm\\n\\nI'm sure I knew of a free pdf that some doctors I know use, but I can't seem to find it at the moment. I will try to dig it out.",,
499,2,232,37d9d13c-cda0-4ed0-a585-2a9fc033f782,2010-07-20 06:41:25.0,103.0,"\\n\\n - [Mondrian][1]: Exploratory data analysis with focus on large data and databases.\\n - [iPlots][2]: a package for the R statistical environment which provides high interaction statistical graphics, written in Java.\\n\\n\\n  [1]: http://rosuda.org/mondrian/\\n  [2]: http://rosuda.org/iplots/",,
500,16,232,37d9d13c-cda0-4ed0-a585-2a9fc033f782,2010-07-20 06:41:25.0,-1.0,,,
502,2,234,bbd70139-028c-407a-993b-0c57dd9f8ba7,2010-07-20 06:47:14.0,198.0,"Say you have a cloud of N points in, say, 3D (which can be listed in a 100x3 array). Then, the principal components analysis (PCA) fits an arbitrarily oriented ellipsoid into the data. The principal component score is the length of the diameters of the ellipsoid. \\n\\nIn the direction in which the diameter is large, the data varies a lot, while in the direction in which the diameter is small, the data varies litte. If you wanted to project N-d data into a 2-d scatter plot, you plot them along the two largest principal components, because with that approach you display most of the variance in the data.",,
503,5,224,f1b8b1b9-8e3a-4df5-92d1-0dc793e5f525,2010-07-20 06:49:14.0,128.0,"Which visualization libraries (plots, graphs, ...) would you suggest to use in a standalone application (Linux, .Net, Windows, whatever). Reasonable performance would be nice as well.",added 21 characters in body,
504,2,235,949b6bbf-368b-4f34-ba45-8c42bcd5fb9a,2010-07-20 07:17:58.0,13.0,"[RaphaelJS](http://raphaeljs.com/) can do some pretty amazing stuff and it just got some major backing from [Sencha](http://www.sencha.com/) (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, [gRaphael](http://g.raphaeljs.com/), that focuses on drawing charts and graphs.\\n\\nThe [MIT SIMILE Project](http://simile.mit.edu/) also has some interesting JavaScript libraries:\\n\\n  - [Timeplot](http://www.simile-widgets.org/timeplot/)\\n  - [Timeline](http://www.simile-widgets.org/timeline/)\\n\\nThere is also a project to port Processing to JavaScript: [ProcessingJS](http://processingjs.org/)",,
505,6,222,3bebe9fd-0a7d-4adc-a92a-b63895ebc2ca,2010-07-20 07:20:49.0,144.0,<scores><fundamentals><principal><components>,edited tags,
506,5,232,e11f550f-4c97-4fb3-bd65-3702bfd8bc67,2010-07-20 07:23:11.0,103.0,"\\n - [Mondrian][1]: Exploratory data analysis with focus on large data and databases.\\n - [iPlots][2]: a package for the R statistical environment which provides high interaction statistical graphics, written in Java. The package provides interactive mosaic plots, bar plots, box plots, parallel plots, scatter plots and histograms that can be linked and color brushed. \\n\\n\\n  [1]: http://rosuda.org/mondrian/\\n  [2]: http://rosuda.org/iplots/",added 151 characters in body,
507,8,232,f3f66fe3-be8c-41fa-a793-436d2c449924,2010-07-20 07:26:11.0,103.0,"\\n\\n - [Mondrian][1]: Exploratory data analysis with focus on large data and databases.\\n - [iPlots][2]: a package for the R statistical environment which provides high interaction statistical graphics, written in Java.\\n\\n\\n  [1]: http://rosuda.org/mondrian/\\n  [2]: http://rosuda.org/iplots/",Rollback to [37d9d13c-cda0-4ed0-a585-2a9fc033f782],
508,5,235,60c7e1b8-0c37-4f5e-a2a9-d74365bed696,2010-07-20 07:29:42.0,13.0,"[RaphaelJS](http://raphaeljs.com/) can do some pretty amazing stuff and it just got some major backing from [Sencha](http://www.sencha.com/) (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, [gRaphael](http://g.raphaeljs.com/), that focuses on drawing charts and graphs.\\n\\nThe [MIT SIMILE Project](http://simile.mit.edu/) also has some interesting JavaScript libraries:\\n\\n  - [Timeplot](http://www.simile-widgets.org/timeplot/)\\n  - [Timeline](http://www.simile-widgets.org/timeline/)\\n\\nThere is also a project to port Processing to JavaScript: [ProcessingJS](http://processingjs.org/)\\n\\nAnd for an open source alternative to Google Maps, there is the excellent [OpenLayers](http://www.openlayers.org) JavaScript library which powers the frontend of the equally excellent [OpenStreetMap](http://www.openstreetmap.org/).",added 137 characters in body; added 98 characters in body,
509,5,227,312b33fa-0a1c-44ac-96c1-ef311151f5da,2010-07-20 07:30:08.0,144.0,"Let i be N rows and j be M columns. Suppose you linearize the combination of variables (columns):\\n\\n    Z_i1 = c_11×Y_i1 + c_12×Yi2 + ... + c_1M×YiM\\n\\nThe above formula basically says to multiply row elements with a certain value c (loadings) and sum them by columns. Resulting values (Y values times the loading) are scores.\\n\\nA principal component (PC) is a linear combination Z_1 = (Z_11, ..., Z_N1) (values by columns which are called scores). In essence, the PC should present the most important features of variables (columns). Ergo, you can extract as many PC as there are variables (or less).\\n\\nAn output from <a href=""http://cran.r-project.org/"">R</a> on PCA (a fake example) looks like this. PC1, PC2... are principal components 1, 2... The example below is showing only the first 8 principal components (out of 17). You can also extract other elements from PCA, like loadings and scores.\\n\\n    Importance of components:\\n                              PC1    PC2    PC3    PC4    PC5    PC6    PC7    PC8\\n    Standard deviation     1.0889 1.0642 1.0550 1.0475 1.0387 1.0277 1.0169 1.0105\\n    Proportion of Variance 0.0697 0.0666 0.0655 0.0645 0.0635 0.0621 0.0608 0.0601\\n    Cumulative Proportion  0.0697 0.1364 0.2018 0.2664 0.3298 0.3920 0.4528 0.5129\\n\\n",added 82 characters in body,
510,2,236,d86b02cf-7f1d-4f12-ad37-577452cee632,2010-07-20 07:30:34.0,138.0,"Unfortunately, it only runs on macs, but otherwise a great application:\\n\\n* [http://nodebox.net/code/index.php/Home][1]\\n\\n> NodeBox is a Mac OS X application that lets you create 2D visuals (static, animated or interactive) using Python programming code and export them as a PDF or a QuickTime movie. NodeBox is free and well-documented.\\n\\n\\n  [1]: http://nodebox.net/code/index.php/Home",,
511,2,237,b6928025-802c-45a6-ae4f-94057b537a50,2010-07-20 07:31:08.0,199.0,"I'm going to leave the main question alone, because I think I will get it wrong (although I too analyse data for a healthcare provider, and to be honest, if I had these data, I would just analyse them using standard techniques and hope for the best, they look pretty okay to me).\\n\\nAs for R packages, I have found the TSA library and its accompanying book:\\n\\nhttp://www.amazon.co.uk/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=sr_1_2?ie=UTF8&s=books&qid=1279610835&sr=8-2\\n\\nvery useful indeed. The armasubsets command, particularly, I think is a great timesaver.",,
512,2,238,254e93c3-776e-4df4-bcab-8852c29678dc,2010-07-20 07:33:09.0,138.0,"There is always lovely gnuplot:\\n\\n* [http://www.gnuplot.info/][1]\\n\\n> Gnuplot is a portable command-line driven graphing utility for linux, OS/2, MS Windows, OSX, VMS, and many other platforms. The source code is copyrighted but freely distributed (i.e., you don't have to pay for it). It was originally created to allow scientists and students to visualize mathematical functions and data interactively, but has grown to support many non-interactive uses such as web scripting. It is also used as a plotting engine by third-party applications like Octave. Gnuplot has been supported and under active development since 1986.\\n\\n> Gnuplot supports many types of plots in either 2D and 3D. It can draw using lines, points, boxes, contours, vector fields, surfaces, and various associated text. It also supports various specialized plot types. \\n\\n\\n  [1]: http://www.gnuplot.info/",,
514,2,240,4ac6cc20-2703-494a-81a0-3bc82faf211f,2010-07-20 07:55:30.0,114.0,"My first response would be that if you can do multivariate regression on the data, then to use the residuals from that regression to spot outliers. (I know you said it's not a regression problem, so this might not help you, sorry !)\\n\\nI'm copying some of this from a [Stackoverflow question I've previously answered][1] which has some example [R][2] code\\n\\nFirst, we'll create some data, and then taint it with an outlier;\\n\\n    > testout<-data.frame(X1=rnorm(50,mean=50,sd=10),X2=rnorm(50,mean=5,sd=1.5),Y=rnorm(50,mean=200,sd=25)) \\n    > #Taint the Data \\n    > testout$X1[10]<-5 \\n    > testout$X2[10]<-5 \\n    > testout$Y[10]<-530 \\n     \\n    > testout \\n             X1         X2        Y \\n    1  44.20043  1.5259458 169.3296 \\n    2  40.46721  5.8437076 200.9038 \\n    3  48.20571  3.8243373 189.4652 \\n    4  60.09808  4.6609190 177.5159 \\n    5  50.23627  2.6193455 210.4360 \\n    6  43.50972  5.8212863 203.8361 \\n    7  44.95626  7.8368405 236.5821 \\n    8  66.14391  3.6828843 171.9624 \\n    9  45.53040  4.8311616 187.0553 \\n    10  5.00000  5.0000000 530.0000 \\n    11 64.71719  6.4007245 164.8052 \\n    12 54.43665  7.8695891 192.8824 \\n    13 45.78278  4.9921489 182.2957 \\n    14 49.59998  4.7716099 146.3090 \\n    <snip> \\n    48 26.55487  5.8082497 189.7901 \\n    49 45.28317  5.0219647 208.1318 \\n    50 44.84145  3.6252663 251.5620 \\n\\nIt's often most usefull to examine the data graphically (you're brain is much better at spotting outliers than maths is)\\n\\n    > #Use Boxplot to Review the Data \\n    > boxplot(testout$X1, ylab=""X1"") \\n    > boxplot(testout$X2, ylab=""X2"") \\n    > boxplot(testout$Y, ylab=""Y"") \\n\\nYou can then use stats to calculate critical cut off values, here using the Lund Test (See Lund, R. E. 1975, ""Tables for An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 4, pp. 473-476. and Prescott, P. 1975, ""An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 1, pp. 129-132.)\\n\\n    > #Alternative approach using Lund Test \\n    > lundcrit<-function(a, n, q) { \\n    + # Calculates a Critical value for Outlier Test according to Lund \\n    + # See Lund, R. E. 1975, ""Tables for An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 4, pp. 473-476. \\n    + # and Prescott, P. 1975, ""An Approximate Test for Outliers in Linear Models"", Technometrics, vol. 17, no. 1, pp. 129-132. \\n    + # a = alpha \\n    + # n = Number of data elements \\n    + # q = Number of independent Variables (including intercept) \\n    + F<-qf(c(1-(a/n)),df1=1,df2=n-q-1,lower.tail=TRUE) \\n    + crit<-((n-q)*F/(n-q-1+F))^0.5 \\n    + crit \\n    + } \\n     \\n    > testoutlm<-lm(Y~X1+X2,data=testout) \\n     \\n    > testout$fitted<-fitted(testoutlm) \\n     \\n    > testout$residual<-residuals(testoutlm) \\n     \\n    > testout$standardresid<-rstandard(testoutlm) \\n     \\n    > n<-nrow(testout) \\n     \\n    > q<-length(testoutlm$coefficients) \\n     \\n    > crit<-lundcrit(0.1,n,q) \\n     \\n    > testout$Ynew<-ifelse(testout$standardresid>crit,NA,testout$Y) \\n     \\n    > testout \\n             X1         X2        Y    newX1   fitted    residual standardresid \\n    1  44.20043  1.5259458 169.3296 44.20043 209.8467 -40.5171222  -1.009507695 \\n    2  40.46721  5.8437076 200.9038 40.46721 231.9221 -31.0183107  -0.747624895 \\n    3  48.20571  3.8243373 189.4652 48.20571 203.4786 -14.0134646  -0.335955648 \\n    4  60.09808  4.6609190 177.5159 60.09808 169.6108   7.9050960   0.190908291 \\n    5  50.23627  2.6193455 210.4360 50.23627 194.3285  16.1075799   0.391537883 \\n    6  43.50972  5.8212863 203.8361 43.50972 222.6667 -18.8306252  -0.452070155 \\n    7  44.95626  7.8368405 236.5821 44.95626 223.3287  13.2534226   0.326339981 \\n    8  66.14391  3.6828843 171.9624 66.14391 148.8870  23.0754677   0.568829360 \\n    9  45.53040  4.8311616 187.0553 45.53040 214.0832 -27.0279262  -0.646090667 \\n    10  5.00000  5.0000000 530.0000       NA 337.0535 192.9465135   5.714275585 \\n    11 64.71719  6.4007245 164.8052 64.71719 159.9911   4.8141018   0.118618011 \\n    12 54.43665  7.8695891 192.8824 54.43665 194.7454  -1.8630426  -0.046004311 \\n    13 45.78278  4.9921489 182.2957 45.78278 213.7223 -31.4266180  -0.751115595 \\n    14 49.59998  4.7716099 146.3090 49.59998 201.6296 -55.3205552  -1.321042392 \\n    15 45.07720  4.2355525 192.9041 45.07720 213.9655 -21.0613819  -0.504406009 \\n    16 62.27717  7.1518606 186.6482 62.27717 169.2455  17.4027250   0.430262983 \\n    17 48.50446  3.0712422 228.3253 48.50446 200.6938  27.6314695   0.667366651 \\n    18 65.49983  5.4609713 184.8983 65.49983 155.2768  29.6214506   0.726319931 \\n    19 44.38387  4.9305222 213.9378 44.38387 217.7981  -3.8603382  -0.092354925 \\n    20 43.52883  8.3777627 203.5657 43.52883 228.9961 -25.4303732  -0.634725264 \\n    <snip> \\n    49 45.28317  5.0219647 208.1318 45.28317 215.3075  -7.1756966  -0.171560291 \\n    50 44.84145  3.6252663 251.5620 44.84145 213.1535  38.4084869   0.923804784 \\n           Ynew \\n    1  169.3296 \\n    2  200.9038 \\n    3  189.4652 \\n    4  177.5159 \\n    5  210.4360 \\n    6  203.8361 \\n    7  236.5821 \\n    8  171.9624 \\n    9  187.0553 \\n    10       NA \\n    11 164.8052 \\n    12 192.8824 \\n    13 182.2957 \\n    14 146.3090 \\n    15 192.9041 \\n    16 186.6482 \\n    17 228.3253 \\n    18 184.8983 \\n    19 213.9378 \\n    20 203.5657 \\n    <snip> \\n    49 208.1318 \\n    50 251.5620 \\n\\nObviosuly there are other outlier tests than the Lund test (Grubbs springs to mind), but I'm not sure which are better suited to multivariate data.\\n  [1]: http://stackoverflow.com/questions/1444306/how-to-use-outlier-tests-in-r-code/1444548#1444548\\n  [2]: http://en.wikipedia.org/wiki/R_(programming_language)",,
515,2,241,8ebce7bf-456e-4aee-9e9b-cafd0c3e1f43,2010-07-20 07:56:06.0,196.0,"I'm not sure what you mean when you say you aren't thinking of a regression problem but of ""true multivariate data"".  My initial response would be to calculate the Mahalanobis distance since it doesn't require that you specify a particular IV or DV, but at its core (as far as I understand it) it is related to a leverage statistic.",,
516,2,242,2ffb1296-9d20-45ca-84d8-baee3922addd,2010-07-20 07:56:16.0,199.0,"This is a bit of a flippant question, but I have a serious interest in the answer. I work in a psychiatric hospital and I have three years' of data, collected every day across each ward regarding the level of violence on that ward.\\n\\nClearly the model which fits these data is a time series model. I had to difference the scores in order to make them more normal. I fit an ARMA model with the differenced data, and the best fit I think was a model with one degree of differencing and first order auto-correlation at lag 2.\\n\\nMy question is, what on earth can I use this model for? Time series always seems so useful in the textbooks when it's about hare populations and oil prices, but now I've done my own the result seems so abstract as to be completely opaque. The differenced scores correlate with each other at lag two, but I can't really advise everyone to be on high alert two days after a serious incident in all seriousness.\\n\\nOr can I?",,
517,1,242,2ffb1296-9d20-45ca-84d8-baee3922addd,2010-07-20 07:56:16.0,199.0,Uses for time series analysis?,,
518,3,242,2ffb1296-9d20-45ca-84d8-baee3922addd,2010-07-20 07:56:16.0,199.0,<time-series>,,
519,2,243,42d6af73-b512-4c32-b721-11f25ef7bfb6,2010-07-20 08:05:04.0,196.0,"Depending on the size of the dataset in question, a permutation test might be preferable to a bootstrap in that it may be able to provide an exact test of the hypothesis.",,
520,2,244,7b89b9e3-12e8-48a8-b922-4ae0f62fc0f2,2010-07-20 08:13:31.0,13.0,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.  \\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface to `Fortran`!  Not open source or free for commercial use though.",,
521,2,245,08a85590-40b9-460d-b680-3761325fe8cd,2010-07-20 08:20:36.0,196.0,"Principal component scores are a group of scores that are obtained following a Principle Components Analysis (PCA).  In PCA the relationships between a group of scores is analyzed such that an equal number of new ""imaginary"" variables (aka principle components) are created.  The first of these new imaginary variables is maximally correlated with all of the original group of variables.  The next is somewhat less correlated, and so forth until the point that if you used all of the principal components scores to predict any given variable from the initial group you would be able to explain all of its variance.  The way in which PCA proceeds is complex and has certain restrictions.  Among these is the restriction that the correlation between any two principal components (i.e. imaginary variables) is zero; thus it doesn't make sense to try to predict one principal component with another.",,
522,2,246,26b71b7e-a6c4-4197-83e4-6941a06d3b18,2010-07-20 08:28:52.0,198.0,"You fitted the model to the differences, which means that you're describing the change in levels of violence. You get a lag of 2 days. A lag is indicative of the memory of the process. In other words, the change in levels of violence today has some dependency on the change in levels of violence in the last two days. For longer time-scales, the contribution of random influences becomes strong enough so that there is no clear link anymore.\\n\\nIs the auto-correlation positive? Then a change of levels of violence today suggests a similar change in levels of violence in two days. Is it negative? Then violence might stay higher for two days.\\n\\nOf course, you may want to have to control for confounding effects. For example, after a serious incident, people may be more likely to report minor incidents, but this ""sensitization"" would be going away after two days.",,
523,5,244,d126bab9-5e21-474e-bb75-67f9dd369d3b,2010-07-20 08:31:04.0,13.0,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.  \\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nIf we were to bend the concept of ""standalone application"" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:\\n\\n  - [Asymptote](http://asymptote.sourceforge.net/) can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader ([example](http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf)).\\n\\n  - [PGF/TikZ](http://sourceforge.net/projects/pgf/) provides a wonderful vector drawing language to TeX documents.  The [manual](http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf) is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  [PGFPlots](http://sourceforge.net/projects/pgfplots/) provides an abstraction layer for drawing plots.  A wondeful showcase can be found at [TeXample](http://www.texample.net/tikz/examples/all/).\\n\\n  - [PSTricks](http://www.tug.org/PSTricks/main.cgi/) served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface to `Fortran`!  Not open source or free for commercial use though.",added 1242 characters in body,
524,5,244,3cc6a11a-2529-4f38-9542-05c13d8a91d1,2010-07-20 08:36:07.0,13.0,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.\\n\\n[igraph](http://igraph.sourceforge.net/) can also be used for visualization of tree-like data structures.  Contains nice interfaces to scripting languages such as R and Python along with a stand-alone C library.\\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nIf we were to bend the concept of ""standalone application"" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:\\n\\n  - [Asymptote](http://asymptote.sourceforge.net/) can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader ([example](http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf)).\\n\\n  - [PGF/TikZ](http://sourceforge.net/projects/pgf/) provides a wonderful vector drawing language to TeX documents.  The [manual](http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf) is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  [PGFPlots](http://sourceforge.net/projects/pgfplots/) provides an abstraction layer for drawing plots.  A wondeful showcase can be found at [TeXample](http://www.texample.net/tikz/examples/all/).\\n\\n  - [PSTricks](http://www.tug.org/PSTricks/main.cgi/) served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface to `Fortran`!  Not open source or free for commercial use though.",added 213 characters in body,
525,2,247,a9936605-e7e1-4d44-95d4-d4544b0e3fbc,2010-07-20 08:36:36.0,171.0,"Might be a bit narrow in scope, but if you're doing any work in Clojure on the JVM there's the excellent [Incanter][1]:\\n\\n> Incanter is a Clojure-based, R-like platform for statistical computing and graphics.\\n\\n [1]: http://incanter.org",,
526,2,248,b7cdb094-f1d7-4ada-8872-d586ad0488eb,2010-07-20 08:38:38.0,211.0,"ggobi and the R links to Ggobi are really rather good for this.   There are simpler visualisations (iPlots is very nice, also interactive, as mentioned).\\n\\nBut it depends whether you are doing something more specialised.   For example TreeView lets you visualise the kind of cluster dendrograms you get out of microarrays.",,
527,16,248,b7cdb094-f1d7-4ada-8872-d586ad0488eb,2010-07-20 08:38:38.0,-1.0,,,
528,5,235,8b5fa715-2835-44c2-a621-6519ee972f02,2010-07-20 08:45:07.0,13.0,"[RaphaelJS](http://raphaeljs.com/) can do some pretty amazing stuff and it just got some major backing from [Sencha](http://www.sencha.com/) (formerly ExtJS).  Raphael is pretty smart about browsers by using a VML backend for Internet Explorer and SVG for everything else.  However, the library is pretty low-level.  Fortunately, the author has started another project, [gRaphael](http://g.raphaeljs.com/), that focuses on drawing charts and graphs.\\n\\nThe [MIT SIMILE Project](http://simile.mit.edu/) also has some interesting JavaScript libraries:\\n\\n  - [Timeplot](http://www.simile-widgets.org/timeplot/)\\n  - [Timeline](http://www.simile-widgets.org/timeline/)\\n\\nThere is also a project to port Processing to JavaScript: [ProcessingJS](http://processingjs.org/)\\n\\n[Jmol](http://jmol.sourceforge.net/) is a Java applet for viewing chemical structures, but it is used as the display engine for 3D graphics in the [SAGE](http://www.sagemath.org/) system, which has a completely browser-based GUI.\\n\\nAnd for an open source alternative to Google Maps, there is the excellent [OpenLayers](http://www.openlayers.org) JavaScript library which powers the frontend of the equally excellent [OpenStreetMap](http://www.openstreetmap.org/).",added 229 characters in body; added 4 characters in body,
529,5,176,865f3b7e-640f-4e5e-bc6d-1c28dcaaec37,2010-07-20 08:46:03.0,81.0,"Let us say a man rolls a six sided die and it has outcomes 1, 2, 3, 4, 5, or 6. Furthermore, he says that if it lands on a 3, he'll give you a free text book.\\n\\nThen informally:\\n\\nThe ***Frequentist*** would say that each outcome has an equal 1 in 6 chance of occurring.\\n\\nThe ***Bayesian*** however would say hang on a second, I know that man, he's David Blaine, a famous trickster! I have a feeling he's up to something. I'm going to say that there's only a 1% chance of it landing on a 3  *BUT*  I'll re-evaluate that beliefe and change it the more times he rolls the die. If I see the other numbers come up equally often, then I'll iteratively increase the chance from 1% to something slightly higher, otherwise I'll reduce it even further. \\n",improved meaning; added 1 characters in body; added 1 characters in body; added 1 characters in body,
530,4,225,924c68e5-40e9-4384-8190-4f4339f1298a,2010-07-20 08:47:33.0,196.0,Why is the average of the highest value from 100 draws from a normal distribution different from the 98th percentile of the normal distribution?,Fixed a value in the title to match the question,
531,5,244,fd42158b-2fdc-48bb-9069-c7f3b4351bc3,2010-07-20 08:47:44.0,13.0,"The Visualization Tool Kit [VTK](http://www.vtk.org) is pretty impressive for 3D visualizations of numerical data.  Unfortunately, it is also pretty low level.\\n\\n[Graphviz](http://graphviz.org/) is used pretty extensively for visualizing graphs and other tree-like data structures.\\n\\n[igraph](http://igraph.sourceforge.net/) can also be used for visualization of tree-like data structures.  Contains nice interfaces to scripting languages such as R and Python along with a stand-alone C library.\\n\\nThe [NCL](http://www.ncl.ucar.edu/) (NCAR Command Language) library contains some pretty neat graphing routines- especially if you are looking at spatially distributed, multidimensional data such as wind fields.  Which makes sense as NCAR is the National Center for Atmospheric Research.\\n\\nIf you are willing to relax the executable requirement, or try a tool like [py2exe](http://www.py2exe.org/), there is the possibility of leveraging some neat Python libraries and applications such as:\\n\\n  - [MayaVi](http://code.enthought.com/projects/mayavi/): A higher level front-end to VTK developed by Enthought.\\n\\n  - [Chaco](http://code.enthought.com/chaco/): Another Enthought library focused on 2D graphs.\\n\\n  - [Matplotlib](http://matplotlib.sourceforge.net/): Another 2D plotting library.  Has nice support for TeX-based mathematical annotation.\\n\\n  - [Basemap](http://sourceforge.net/projects/matplotlib/files/matplotlib-toolkits/): An add-on to Matplotlib for drawing maps and displaying geographic data ([sexy examples here](http://www.scipy.org/Cookbook/Matplotlib/Maps)).\\n\\n\\nIf we were to bend the concept of ""standalone application"" even further to include PDF files, there are some neat graphics libraries available to LaTeX users:\\n\\n  - [Asymptote](http://asymptote.sourceforge.net/) can generate a variety of graphs, but its  crown jewel is definitely the ability to embed 3D graphs into PDF documents that can be manipulated (zoomed, rotated, animated, etc) by anyone using the Adobe Acrobat reader ([example](http://asymptote.sourceforge.net/gallery/3D%20graphs/helix.pdf)).\\n\\n  - [PGF/TikZ](http://sourceforge.net/projects/pgf/) provides a wonderful vector drawing language to TeX documents.  The [manual](http://tug.ctan.org/tex-archive/graphics/pgf/base/doc/generic/pgf/pgfmanual.pdf) is hands-down the most well-written, comprehensive and beautiful piece of documentation I have ever seen in an open source project.  [PGFPlots](http://sourceforge.net/projects/pgfplots/) provides an abstraction layer for drawing plots.  A wondeful showcase can be found at [TeXample](http://www.texample.net/tikz/examples/all/).\\n\\n  - [PSTricks](http://www.tug.org/PSTricks/main.cgi/) served as an inspiration for TikZ and allows users to leverage the power of the PostScript language to create some neat graphics.\\n\\n\\nAnd for kicks, there's [DISLIN](http://www.mps.mpg.de/dislin/), which has a native interface for `Fortran`!  Not open source or free for commercial use though.",added 1 characters in body,
532,2,249,302c254f-60c1-45ca-9604-73042d55303f,2010-07-20 08:49:13.0,213.0,"I have a set of *N* bodies, which is a random sample from a population whose mean and variance I want to estimate. A property of each body is being measured m_i times (m_i>1) and different for each body index i  identifies which body it is; the property is expected to be distributed around zero). I would like to describe the resulting measurement. Particularly I'm interested in average property value and in the variance.\\n\\nThe average value is simple. First calculate the mean values for each body and then calculate the mean of means.\\n\\nThe variance is more tricky. There are two variances: the variance of measurement and the variance of property values. In order to have an idea on the confidence we have in any single measurement, we need to account for both the sources. Unfortunately, I can't think of a good method. It is obvious that putting all the numbers in a single pool and calculating the stdev of this pool isn't a good idea.\\n\\nAny suggestion?\\n\\n",,
533,1,249,302c254f-60c1-45ca-9604-73042d55303f,2010-07-20 08:49:13.0,213.0,Variance components,,
534,3,249,302c254f-60c1-45ca-9604-73042d55303f,2010-07-20 08:49:13.0,213.0,<standard-deviation><anova><variance>,,
535,5,163,06b96a64-039a-4b09-ab09-a4c14017ced2,2010-07-20 08:50:33.0,81.0,"Lets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.\\n\\nWhichever number the die lands on is the number of free text-books I will give you.\\n\\nIn this case, the *final amount* of free text books that I give you is the **random variable** because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die) and is not known before I roll the die.",deleted 2 characters in body; added 2 characters in body; added 51 characters in body; deleted 1 characters in body,
536,5,91,73c7e7a1-18c8-46ae-9c56-06e51a161fd4,2010-07-20 08:52:27.0,87.0,"As user28 said in comments above, the pdf is the first derivative of the cdf for a continuous random variable, and the difference for a discrete random variable. \\n\\nIn the continuous case, wherever the cdf has a discontinuity the pdf has an atom.  Dirac delta ""functions"" can be used to represent these atoms.    \\n\\n ",typo -- word random at end -- no reason,
537,2,250,4adb1fae-ed9e-4481-8c19-c98b9ec62cbe,2010-07-20 08:53:15.0,171.0,"Here's a data point: R has a [""big data ceiling""][1], useful to know if you plan on working with huge data sets.\\n\\nI'm unsure whether the same limitations apply to Clojure/Incanter, whether it outperforms R or is actually worse. I imagine the JVM can probably handle large datasets, especially if you manage to harness the power of Clojure's lazy features.\\n\\n [1]: http://www.bytemining.com/2010/05/hitting-the-big-data-ceiling-in-r/",,
541,5,243,c2d09690-a22d-4b8c-8747-9351e041d329,2010-07-20 09:09:01.0,196.0,"Depending on the size of the dataset in question, a permutation test might be preferable to a bootstrap in that it may be able to provide an exact test of the hypothesis (and an exact CI).",added 18 characters in body,
542,2,252,26214128-c858-4b89-b234-b5dab46962b1,2010-07-20 09:13:29.0,81.0,"For me personally, I use the following three packages the most, all available from the awesome [Omega Project for Statistical Computing][1] (I do not claim to be an expert, but for my purposes they are very easy to use):\\n\\n - **RCurl**: It has lots of options which allows access to websites that the default functions in base R would have difficulty with I think it's fair to say. It is an R-interface to the libcurl library, which has the *added* benefit of a whole community outside of R developing it. \\n\\n - **XML**: It is very forgiving of parsing malformed XML/HTML. It is an R-interface to the libxml2 library and again has the *added* benefit of a whole community outside of R developing it\\n - **RJSONIO**: It allows one to parse the text returned from a json call and organise it into a list structure for further analysis.The competitor to this package is rjson but this one has the advantage of being vectorised, readily extensible through S3/S4, fast and scalable to large data.  \\n\\n\\n\\n  [1]: http://www.omegahat.org",,
543,16,252,26214128-c858-4b89-b234-b5dab46962b1,2010-07-20 09:13:29.0,-1.0,,,
544,2,253,0f83430f-d739-4f96-a47f-46b6e67d9bfa,2010-07-20 09:26:05.0,215.0,If you're an economist/econometrician then Grant Farnworth's paper is indispensable and is available on CRAN at:\\nhttp://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf,,
545,2,254,3c9aff0c-d41b-45d9-bb06-0495e5d88123,2010-07-20 09:30:18.0,8.0,"I think if I understand your description correctly, you need to use a [linear mixed model][1]. However, this maybe overkill, since these models are used to find differences between groups. For example, if you have two types of bodies and you wish to determine if they are different.\\n\\nBasically, you have *between* subject variation and *within* subject variation. To fit these models in R, you can use the `lmer` function from the `lme4` library. So if I understand you correctly, your function will look something like this:\\n\\n    fm1 = lmer(measurement ~ (1|Subject), data)\\nIf you are looking for differences between bodies, then it will look something like:\\n    \\n    fm2 = lmer(measurement ~ body + (body|Subject), data)\\n\\nThe command `summary(fm1)` should give the values you are after.\\n\\nBTW, the subject part is usually called the random effect. However, there a many different  views on what a random effect is. See Ch11.4 of [Data analysis using regression][2] by Gelman and Hill for more details.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_effects_model\\n  [2]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/0521867061",,
546,2,255,79f9b990-3e28-47dc-8c3f-9a39d92c011f,2010-07-20 09:30:51.0,88.0,"It may be an overshoot, but you may train an unsupervised Random Forest on the data and use the object proximity measure to detect outliers. More details [here][1].\\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#outliers",,
547,2,256,621e2a12-827f-4729-bdf0-57f1033f923c,2010-07-20 09:34:22.0,217.0,"What is the easiest way to understand boosting?\\n\\nWhy doesn't it boost very weak classifiers ""to infinity"" (perfectness?) ?",,
548,1,256,621e2a12-827f-4729-bdf0-57f1033f923c,2010-07-20 09:34:22.0,217.0,How boosting works?,,
549,3,256,621e2a12-827f-4729-bdf0-57f1033f923c,2010-07-20 09:34:22.0,217.0,<boosting>,,
550,2,257,2b0f781d-dd6f-4f63-8799-bb33e6fdab91,2010-07-20 09:38:44.0,217.0,We may assume that we have CSV file and we want a very basic line plot with several lines on one plot and a simple legend.,,
551,1,257,2b0f781d-dd6f-4f63-8799-bb33e6fdab91,2010-07-20 09:38:44.0,217.0,What is the easiest way to create publication-quality plots under Linux?,,
552,3,257,2b0f781d-dd6f-4f63-8799-bb33e6fdab91,2010-07-20 09:38:44.0,217.0,<data-visualization><plotting><csv-file>,,
553,2,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,2010-07-20 09:43:23.0,217.0,Rules:\\n\\n * one classifier per answer\\n * vote up if you agree \\n * downvote/remove duplicates.\\n * put your application in the comment,,
554,1,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,2010-07-20 09:43:23.0,217.0,Poll: What is the best out-of-the-box 2-class classifier for your application?,,
555,3,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,2010-07-20 09:43:23.0,217.0,<classification><application>,,
556,16,258,3cb17abe-f14c-425a-81b2-aa8ca6b38a03,2010-07-20 09:43:23.0,217.0,,,
557,2,259,1c25a269-6ecc-48ab-a779-d04957c522ae,2010-07-20 09:44:11.0,217.0,[Support vector machine][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Support_vector_machine,,
558,16,259,1c25a269-6ecc-48ab-a779-d04957c522ae,2010-07-20 09:44:11.0,-1.0,,,
559,2,260,de6a66dd-8396-40d1-bcfb-a62ef4ee5720,2010-07-20 09:45:06.0,217.0,[Random forest][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_forest,,
560,16,260,de6a66dd-8396-40d1-bcfb-a62ef4ee5720,2010-07-20 09:45:06.0,-1.0,,,
561,2,261,cddda61d-81b8-44e2-92c2-aa808dff32b9,2010-07-20 09:45:18.0,159.0,"It's hard to go past R for graphics. You could do what you want in 3 lines. For example, assuming the csv file has four columns:\\n\\n    x <- read.csv(""file.csv"")\\n    matplot(x[,1],x[,2:4],type=""l"",col=1:3)\\n    legend(""topleft"",legend=c(""A"",""B"",""C""),lty=1,col=1:3)\\n\\n",,
562,2,262,42070c99-3512-4032-9f45-5b4ede8e90d2,2010-07-20 09:50:45.0,8.0,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then you can a combination of `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or lattice. In `ggplot2` the following commands should get you started.\\n\\n    require(ggplot2)\\n    \\n    #You would use read.csv here\\n    N = 20\\n    d = data.frame(x=1:N,y=runif(N),z=rnorm(N))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y, colour=""Type 1""))\\n    p = p+geom_point(aes(x, z, colour=""Type 2""))\\n    \\n    #Optional, if you want your own colours\\n    p = p+scale_colour_manual(""Type"",c('blue','red'))\\n    p\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n",,
563,2,263,ff419f0c-d347-4cbf-af52-5e9ed20f29fd,2010-07-20 09:53:11.0,190.0,My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/,,
564,5,262,90822f7e-201a-4594-82d2-a888a977c0bf,2010-07-20 09:57:26.0,8.0,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then you can a combination of `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or lattice. In `ggplot2` the following commands should get you started.\\n    \\n    require(ggplot2)\\n    #You would use read.csv here\\n    N = 10\\n    d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y1, colour=""Type 1""))\\n    p = p+geom_line(aes(x, y2, colour=""Type 2""))\\n    p = p+geom_line(aes(x, y3, colour=""Type 3""))\\n    #Add points\\n    p = p+geom_point(aes(x, y3, colour=""Type 3""))\\n    p   \\n\\nThis would give you the following plot:\\n\\n![Line plot][2]\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://img84.imageshack.us/img84/6393/tmpq.jpg",added 161 characters in body; deleted 10 characters in body,
565,2,264,60321bd8-f86c-4f8f-82ff-fbdd073007c0,2010-07-20 09:59:00.0,217.0,"[Here][1] is an article describing one possible algorith. Source code included and a quite serious application (gravitational wave detection based on laser interferometry), so you can expect it to be well tested.\\n\\n  [1]: http://www.ligo.caltech.edu/docs/T/T030168-00.pdf",,
566,2,265,d2e69d67-ce13-4ce5-97d8-66407a6b4aed,2010-07-20 10:05:48.0,88.0,"In plain English: If your classifier misclassifies some data, train another copy of it only on this misclassified part with hope that it will discover something subtle. And then, as usual, iterate. On the way there are some voting schemes that allow to combine all those classifiers' predictions in sensible way.\\n\\nBecause sometimes it is impossible (the noise is just hiding some of the information, or it is not even present in the data); on the other hand, boosting too much may lead to overfitting.",,
567,5,263,f02959d2-6138-44f9-a3a1-fbee808cac75,2010-07-20 10:11:30.0,190.0,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n\\n\\nSpecifically, for different file formats like svg and eps, use the format parameter of [savefig][2]\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/\\n  [2]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig",added 193 characters in body,
568,5,265,425b3728-b705-4abb-8708-67c9c77b7e8f,2010-07-20 10:17:16.0,88.0,"In plain English: If your classifier misclassifies some data, train another copy of it mainly on this misclassified part with hope that it will discover something subtle. And then, as usual, iterate. On the way there are some voting schemes that allow to combine all those classifiers' predictions in sensible way.\\n\\nBecause sometimes it is impossible (the noise is just hiding some of the information, or it is not even present in the data); on the other hand, boosting too much may lead to overfitting.",added 2 characters in body,
569,6,256,43f53a05-26a2-4a93-8332-51b4bb458079,2010-07-20 10:18:12.0,88.0,<machine-learning><boosting>,edited tags,
570,6,222,f6190f90-acb9-4116-9b87-f3ed31d4d6d1,2010-07-20 10:18:50.0,144.0,<scores><fundamentals><principal-components>,edited tags,
571,2,266,9097a62f-a7df-446c-96ef-c329308171fe,2010-07-20 10:23:56.0,128.0,As you mentioned sorting would be `O(n·log n)` for a window of length `n`. Doing this moving adds another `l=vectorlength` making the total cost `O(l·n·log n)`.\\n\\nThe simplest way to push this is by keeping an ordered list of the last n elements in memory when moving from one window to the next one. As removing/inserting one element from/into an ordered list are both `O(n)` this would result in costs of `O(l·n)`.\\n\\nPseudocode:\\n\\n    l = length(input)\\n    aidvector = sort(input(1:n))\\n    output(i) = aid(n/2)\\n    for i = n+1:l\\n        remove input(i-n) from aidvector\\n        sort aid(n) into aidvector\\n        output(i) = aid(n/2)\\n    \\n        \\n    ,,
572,2,267,cb3c3703-bf18-4a1c-a139-67cc6cf96e60,2010-07-20 10:35:55.0,194.0,"If I have two lists A and B, both of which are subsets of a much larger list C, how can I determine if the degree of overlap of A and B is greater than I would expect by chance?\\n\\nShould I just randomly select elements from C of the same lengths as lists A and B and determine that random overlap, and do this many times to determine some kind or empirical p-value? Is there a better way to test this?",,
573,1,267,cb3c3703-bf18-4a1c-a139-67cc6cf96e60,2010-07-20 10:35:55.0,194.0,How do I calculate if the degree of overlap between two lists is significant?,,
574,3,267,cb3c3703-bf18-4a1c-a139-67cc6cf96e60,2010-07-20 10:35:55.0,194.0,<statistical-significance>,,
575,5,262,3354eb64-08ac-487c-8d8c-9a237dedde9a,2010-07-20 10:43:46.0,8.0,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then you can a combination of `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or lattice. In `ggplot2` the following commands should get you started.\\n    \\n    require(ggplot2)\\n    #You would use read.csv here\\n    N = 10\\n    d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y1, colour=""Type 1""))\\n    p = p+geom_line(aes(x, y2, colour=""Type 2""))\\n    p = p+geom_line(aes(x, y3, colour=""Type 3""))\\n    #Add points\\n    p = p+geom_point(aes(x, y3, colour=""Type 3""))\\n    print(p)   \\n\\nThis would give you the following plot:\\n\\n![Line plot][2]\\n\\n**Saving plots in R**\\n\\nSaving plots in R is straightforward:\\n\\n    #Look at ?jpeg to other different saving options\\n    jpeg(""figure.jpg"")\\n    print(p)#for ggplot2 graphics\\n    dev.off()\\n\\nInstead of `jpeg`'s you can also save as a `pdf` or postscript file:\\n\\n    #This example uses R base graphics\\n    #Just change to print(p) for ggplot2\\n    pdf(""figure.pdf"")\\n    plot(d$x,y1, type=""l"")\\n    lines(d$x, y2)\\n    dev.off()\\n\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://img84.imageshack.us/img84/6393/tmpq.jpg\\n",added 449 characters in body,
576,5,254,27669397-2ee1-4a29-aca5-13baa3719588,2010-07-20 10:50:24.0,8.0,"I think if I understand your description correctly, you need to use a [linear mixed model][1]. However, this maybe overkill, since these models are used to find differences between groups. For example, if you have two types of bodies and you wish to determine if they are different.\\n\\nBasically, you have *between* subject variation and *within* subject variation.\\n\\nTo fit these models in R, you can use the `lmer` function from the `lme4` library. So if I understand you correctly, your function will look something like this:\\n\\n    #Load the R library\\n    library(lme4)\\n\\n    #data is a R data frame that contains your data\\n    #measurement and Subject are variables\\n    fm1 = lmer(measurement ~ (1|Subject), data)\\nIf you are looking for differences between bodies, then it will look something like:\\n    \\n    fm2 = lmer(measurement ~ body + (body|Subject), data)\\n\\nThe command `summary(fm1)` should give the values you are after.\\n\\nHere are some resources that will help you get started:\\n\\n1. [Documentation][2] for the lme4 package\\n1. [Statistics with R][3]\\n\\nMost statistical software will be able to fit models of this type.\\n\\n\\nBTW, the subject part is usually called the random effect. However, there a many different  views on what a random effect is. See Ch11.4 of [Data analysis using regression][4] by Gelman and Hill for more details.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_effects_model\\n  [2]: http://lme4.r-forge.r-project.org/\\n  [3]: http://zoonek2.free.fr/UNIX/48_R/all.html\\n  [4]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/0521867061",added 443 characters in body,
577,5,263,c3a4169f-30c9-46e6-bc7e-c4bae33b0384,2010-07-20 10:51:40.0,190.0,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n\\n\\nSpecifically, for different file formats like svg and eps, use the format parameter of [savefig][2]\\n\\nAn example:\\ninput.csv\\n<pre>""Line 1"",0.5,0.8,1.0,0.9,0.9\\n""Line 2"",0.2,0.7,1.2,1.1,1.1</pre>\\n\\nCode:\\n\\n    import csv\\n    import matplotlib.pyplot as plt\\n\\n    legends = []\\n    for row in csv.reader(open('input.csv')):\\n        legends.append(row[0])\\n        plt.plot(row[1:])\\n    \\n    plt.legend(legends)\\n    plt.savefig(""out.svg"", format='svg')\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/\\n  [2]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig",Added some code,
578,2,268,c4123c1f-2c56-4f2f-8d49-10bcdeb7709b,2010-07-20 11:01:06.0,107.0,"You could of for a supervised self-organizing map (e.g. with [kohonen][1] package for R), and use the login frequency as dependent variable. That way, the clustering will focus on separating the frequent visitors from the rare visitors. By plotting the number of users on each map unit, you may get an idea in clusters present in your data.\\n\\nBecause SOMs are non-linear mapping methods, this approach is particularly interesting for tailed data.\\n\\n  [1]: http://cran.r-project.org/web/packages/kohonen/index.html",,
579,2,269,c6791c41-3dcc-45a7-ab56-7f8a02e82392,2010-07-20 11:07:42.0,62.0,"What is the difference between a population and a sample? What common variables and statistics are used for each one, and how do those relate to each other? ",,
580,1,269,c6791c41-3dcc-45a7-ab56-7f8a02e82392,2010-07-20 11:07:42.0,62.0,What is the difference between a population and a sample? ,,
581,3,269,c6791c41-3dcc-45a7-ab56-7f8a02e82392,2010-07-20 11:07:42.0,62.0,<standard-deviation><population><sample><variance><sample-variance>,,
582,2,270,f4c0576e-e6e2-4578-a05b-8bcdd100e048,2010-07-20 11:08:47.0,90.0,"Due to the factorial in a poisson distribution, it becomes unpractical to estimate poisson models (for example, using maximum likelihood) when the observations are large. So, for example, if I am trying to estimate a model to explain the number of suicides in a given year (only annual data are available), and say, there are thousands of suicides every year, is it wrong to express suicides in hundreds, so that 2999 would be 29.99 ~= 30? In other words, is it wrong to change the unit of measurement to make the data manageable? ",,
583,1,270,f4c0576e-e6e2-4578-a05b-8bcdd100e048,2010-07-20 11:08:47.0,90.0,Poisson regression with large data: is it wrong to change the unit of measurement?,,
584,3,270,f4c0576e-e6e2-4578-a05b-8bcdd100e048,2010-07-20 11:08:47.0,90.0,<modeling><data><poisson>,,
585,2,271,d5a0fe51-fff2-4def-a129-f3826d28ebe6,2010-07-20 11:10:42.0,8.0,"If I understand your question correctly, you need to use the [Hypergeometric distribution][1]. This distribution is usually associated with urn models, i.e there are n balls in an urn, *y* are painted red, and you draw *m* balls from the urn. Then if *X* is the number of balls in your sample of *m* that are red, *X* has a hyper-geometric distribution.\\n\\nFor your specific example, let n_A, n_B and n_C denote the lengths of your three lists and let n_A_B denote the overlap between A and B. Then \\n\\nn_A_B ~ HG(n_A, n_C, n_B)\\n\\nTo calculate a p-value, you could use this R command: \\n\\n    #Some example values\\n    n_A = 100;n_B = 200; n_C = 500; n_A_B = 50\\n    1-phyper(n_A_B, n_B, n_C-n_B, n_A)\\n    [1] 0.008626697\\n\\nWord of caution. Remember multiple testing, i.e. if you have lots of *A* and *B* lists, then you will need to adjust your p-values with a correction. For the example the FDR or Bonferroni corrections.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Hypergeometric_distribution",,
586,2,272,8409944f-ff4e-49ef-97fa-d9e7d8d03670,2010-07-20 11:14:41.0,216.0,"Also, for some elaborate discussion (including bashing of ADF / PP / KPSS :) you might want to have a look at the book by Maddala and Kim:\\n\\nhttp://www.amazon.com/Cointegration-Structural-Change-Themes-Econometrics/dp/0521587824\\n\\nQuite extensive and not very easy to read sometimes, but a useful reference.",,
587,2,273,87602318-e5c3-40f5-ae80-e0060cec839d,2010-07-20 11:18:01.0,216.0,"* Clearly R\\n\\n* RadidMiner is nice, but switching to thinking in terms of operators takes a moment\\n\\n* Matlab / Octave\\n\\nIf you describe a specific problem, I may be able to get more specific.",,
588,5,249,819480f7-fed2-4d92-8e64-ca88471b8f2b,2010-07-20 11:19:46.0,213.0,"I have a set of *N* bodies, which is a random sample from a population whose mean and variance I want to estimate. A property of each body is being measured m_i times (m_i>1) and different for each body index i  identifies which body it is; the property is expected to be distributed around zero). I would like to describe the resulting measurement. Particularly I'm interested in average property value and in the variance.\\n\\nThe average value is simple. First calculate the mean values for each body and then calculate the mean of means.\\n\\nThe variance is more tricky. There are two variances: the variance of measurement and the variance of property values. In order to have an idea on the confidence we have in any single measurement, we need to account for both the sources. Unfortunately, I can't think of a good method. It is obvious that putting all the numbers in a single pool and calculating the stdev of this pool isn't a good idea.\\n\\nAny suggestion?\\n\\n**EDIT**\\nColin Gillespie suggests applying Random Effects Model. This model seems to be the right solution for my case, except for the fact that it is described (in Wikipedia) for the cases where each group (body in my case) is sampled equally (m_i is constant for all the bodies), which is not correct in my case\\n\\n",added 318 characters in body; edited tags; edited tags,
589,6,249,819480f7-fed2-4d92-8e64-ca88471b8f2b,2010-07-20 11:19:46.0,213.0,<standard-deviation><variance><anova><random-effects-model>,added 318 characters in body; edited tags; edited tags,
590,2,274,c3465352-1611-492d-ae45-ce668c8d9b84,2010-07-20 11:21:59.0,90.0,"The population is the whole set of values, or individuals, you are interested in. The sample is a subset of the population, and is the set of values you actually use in your estimation.\\n\\nSo, for example, if you want to know the average height of the residents of China, that is your population, ie, the population of China. The thing is, this is quite large a number, and you wouldn't be able to get data for everyone there. So you draw a sample, that is, you get some observations, or the height of some of the people in China (a subset of the population, the sample) and do your inference based on that. ",,
591,2,275,d96deb70-98d1-408f-9db2-2222b7e621be,2010-07-20 11:29:53.0,88.0,"In case of Poisson it is bad, since counts are counts -- their unit is an unity. On the other hand, if you'd use some advanced software like R, its Poisson handling functions will be aware of such large numbers and would use some numerical tricks to handle them.",,
592,5,253,3ef84eac-5e28-41ca-8704-cf97c7e17fdc,2010-07-20 11:37:25.0,215.0,If you're an economist/econometrician then Grant Farnworth's paper on using R is indispensable and is available on CRAN at:\\nhttp://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf,added 11 characters in body,
593,2,276,7b985e90-5544-4f51-9c19-3de8d9f83968,2010-07-20 11:43:25.0,90.0,"Is there a rule-of thumb or even any way at all to tell how large a sample should be in order to estimate a model with a given number of parameters? \\n\\nSo, for example, if I want to estimate a least-squares regression with 5 parameters, how large should the sample be? \\n\\nDoes it matter what estimation technique you are using (e.g. maximum likelihood, least squares, GMM), or how many or what tests you are going to perform? Should the sample variability be taken into account when making the decision?",,
594,1,276,7b985e90-5544-4f51-9c19-3de8d9f83968,2010-07-20 11:43:25.0,90.0,How large should a sample be for a given estimation technique and parameters?,,
595,3,276,7b985e90-5544-4f51-9c19-3de8d9f83968,2010-07-20 11:43:25.0,90.0,<sample-size><estimation><least-squares><maximum-likelihood>,,
596,2,277,d8cbe22f-ad70-487a-a0c2-69090041f56b,2010-07-20 11:49:02.0,215.0,When would one prefer to use a Conditional Autoregressive model over a Simultaneous Autoregressive model when modelling autocorrelated geo-referenced areal data?,,
597,1,277,d8cbe22f-ad70-487a-a0c2-69090041f56b,2010-07-20 11:49:02.0,215.0,Spatial Statistics Models - CAR v SAR,,
598,3,277,d8cbe22f-ad70-487a-a0c2-69090041f56b,2010-07-20 11:49:02.0,215.0,<modeling><spatial>,,
599,2,278,880786c2-bddc-45fe-a04f-a3d739ee9f8f,2010-07-20 11:49:27.0,221.0,"When a non-hierarchical cluster analysis is carried out, the order of observations in the data file determine the clustering results, especially if the data set is small (i.e, 5000 observations). To deal with this problem I usually performed a random reorder of data observations. My problem is that if I replicate the analysis n times, the results obtained are different and sometimes these differences are great. \\n\\nHow can I deal with this problem? Maybe I could run the analysis several times and after consider that one observation belong to the group in which more times was assigned. Has someone a better approach to this problem?\\n\\nManuel Ramon",,
600,1,278,880786c2-bddc-45fe-a04f-a3d739ee9f8f,2010-07-20 11:49:27.0,221.0,How to deal with the effect of the order of observations in a non hierarchical cluster analysis? ,,
601,3,278,880786c2-bddc-45fe-a04f-a3d739ee9f8f,2010-07-20 11:49:27.0,221.0,<clustering><nonhierarchical><order><repeatability>,,
602,2,279,bad63b7f-9c81-40ef-80bb-e076053602e7,2010-07-20 11:54:15.0,62.0,"When you're dealing with a Poisson distribution with large values of \\lambda (its parameter), it is common to use a normal approximation to the Poisson distribution. \\n\\nAs [this site][1] mentions, it's all right to use the normal approximation when \\lambda gets over 20, and the approximation improves as \\lambda gets even higher. \\n\\nThe Poisson distribution is defined only over the state space consisting of the non-negative integers, so rescaling and rounding is going to introduce odd things into your data. \\n\\nUsing the normal approx. for large Poisson statistics is VERY common.\\n\\n\\n  [1]: http://www.stat.ucla.edu/~dinov/courses_students.dir/Applets.dir/NormalApprox2PoissonApplet.html ",,
603,2,280,1c8a9441-a3fd-4012-b37a-0c20ac629776,2010-07-20 11:54:50.0,221.0,"The [R project][1] website has lots of manuals to start, and I suggest you the [Nabble R forum][2] and the [R-bloggers][3] site as well. \\n\\n\\n  [1]: http://www.r-project.org/\\n  [2]: http://r.789695.n4.nabble.com/\\n  [3]: http://www.r-bloggers.com/",,
604,2,281,4029043d-567f-40f0-8239-329dbfd1ffe8,2010-07-20 11:59:56.0,215.0,"\\nDay-to-day the most useful package must be ""foreign"" which has functions for reading and writing data for other statistical packages e.g. Stata, SPSS, Minitab, SAS, etc. Working in a field where R is not that commonplace means that this is a very important package.",,
605,16,281,4029043d-567f-40f0-8239-329dbfd1ffe8,2010-07-20 11:59:56.0,-1.0,,,
606,2,282,6d08f421-4ebd-4186-9ee6-66a62986de24,2010-07-20 12:02:26.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define Principal Component Analysis:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is form of multidimensional scaling. It ia linear transformation of the variables into a lower dimensional space which maintains all of the variances of the variables. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = TRUE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                   PC1        PC2   \\n    Maths   -0.5477226 -0.4082483 \\n    Science  0.6210590 -0.1543033  \\n    English  0.5477226 -0.4082483 \\n    Music    0.1195229  0.8017837 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                      y\\n    John -0.55*80 + 0.62*90 + 0.55*60 + 0.12*55  -0.41*80 + -0.15*85 + -0.41*60 + 0.80*55 \\n    Mike -0.55*90 + 0.62*85 + 0.55*70 + 0.12*45  -0.41*90 + -0.15*85 + -0.41*70 + 0.80*45\\n    Kate -0.55*95 + 0.62*80 + 0.55*40 + 0.12*50  -0.41*95 + -0.15*80 + -0.41*40 + 0.80*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  51.4    -26.15 \\n    Mike  47.1    -42.35\\n    Kate  25.35   -27.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n",,
607,5,282,74b8eeee-9d86-41fc-8f12-c9cf54cd3719,2010-07-20 12:07:52.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define Principal Component Analysis:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which maintains all of the variances of the variables. For example, this would mean we could compare which person did best across all subjuects. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = TRUE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                   PC1        PC2   \\n    Maths   -0.5477226 -0.4082483 \\n    Science  0.6210590 -0.1543033  \\n    English  0.5477226 -0.4082483 \\n    Music    0.1195229  0.8017837 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                      y\\n    John -0.55*80 + 0.62*90 + 0.55*60 + 0.12*55  -0.41*80 + -0.15*85 + -0.41*60 + 0.80*55 \\n    Mike -0.55*90 + 0.62*85 + 0.55*70 + 0.12*45  -0.41*90 + -0.15*85 + -0.41*70 + 0.80*45\\n    Kate -0.55*95 + 0.62*80 + 0.55*40 + 0.12*50  -0.41*95 + -0.15*80 + -0.41*40 + 0.80*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  51.4    -26.15 \\n    Mike  47.1    -42.35\\n    Kate  25.35   -27.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, but I hope you get the idea.\\n",added 94 characters in body; added 94 characters in body,
608,2,283,81644edd-0912-4bc9-b8e0-fa8bde868bfe,2010-07-20 12:09:08.0,215.0,What is meant when we say we have a saturated model?,,
609,1,283,81644edd-0912-4bc9-b8e0-fa8bde868bfe,2010-07-20 12:09:08.0,215.0,"What is a ""saturated"" model?",,
610,3,283,81644edd-0912-4bc9-b8e0-fa8bde868bfe,2010-07-20 12:09:08.0,215.0,<modeling><regression><random-variables>,,
611,5,282,52ace550-9f77-4bb5-99a6-322aa036043b,2010-07-20 12:14:11.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define Principal Component Analysis:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjuects. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = TRUE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                   PC1        PC2   \\n    Maths   -0.5477226 -0.4082483 \\n    Science  0.6210590 -0.1543033  \\n    English  0.5477226 -0.4082483 \\n    Music    0.1195229  0.8017837 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                      y\\n    John -0.55*80 + 0.62*90 + 0.55*60 + 0.12*55  -0.41*80 + -0.15*85 + -0.41*60 + 0.80*55 \\n    Mike -0.55*90 + 0.62*85 + 0.55*70 + 0.12*45  -0.41*90 + -0.15*85 + -0.41*70 + 0.80*45\\n    Kate -0.55*95 + 0.62*80 + 0.55*40 + 0.12*50  -0.41*95 + -0.15*80 + -0.41*40 + 0.80*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  51.4    -26.15 \\n    Mike  47.1    -42.35\\n    Kate  25.35   -27.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, but I hope you get the idea.\\n",added 9 characters in body,
612,2,284,a336071c-12a1-49db-8519-e4f3980ea013,2010-07-20 12:25:30.0,215.0,"\\nA random variable, usually denoted X, is a variable where the outcome is uncertain. The observation of a particular outcome of this variable is called a realisation. More concretely, it is a function which maps a probability space into a measurable space, usually called a state space. Random variables are discrete (can take a number of distinct values) or continuous (can take an infinite number of values). \\n\\nConsider the random variable X which is the total obtained when rolling two dice. It can take any of the values 2-12 (with equal probability given fair dice) and the outcome is uncertain until the dice are rolled. ",,
613,2,285,0cd0436c-873e-400d-bd01-8e4b5732d46d,2010-07-20 12:26:39.0,183.0,"You might consider transforming (perhaps a log) the positively skewed variables.\\n\\nIf after exploring various clustering algorithms you find that the four variables simply reflect varying intensity levels of usage, you might think about a theoretically based classification. Presumably this classification is going to be used for a purpose and that purpose could drive meaningful cut points on one or more of the variables.",,
614,5,275,8e397344-768c-453d-8c17-1dd0c5f87761,2010-07-20 12:26:55.0,88.0,"In case of Poisson it is bad, since counts are counts -- their unit is an unity. On the other hand, if you'd use some advanced software like R, its Poisson handling functions will be aware of such large numbers and would use some numerical tricks to handle them.\\n\\nObviously I agree that normal approximation is another good approach.",added 73 characters in body,
615,2,286,010717b7-b5c3-46ba-8cc1-74b6628851d1,2010-07-20 12:28:32.0,211.0,There's a superb Probability book here:\\nhttp://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html\\nwhich you can also buy in hardcopy.;,,
616,2,287,ce653f04-8786-4f7b-8a2a-a62572d71626,2010-07-20 12:29:17.0,90.0,"Can someone explain to me the difference between method of moments and GMM (general method of moments), their relationship, and when should one or the other be used?",,
617,1,287,ce653f04-8786-4f7b-8a2a-a62572d71626,2010-07-20 12:29:17.0,90.0,What is the difference/relationship between method of moments and GMM?,,
618,3,287,ce653f04-8786-4f7b-8a2a-a62572d71626,2010-07-20 12:29:17.0,90.0,<estimation-technique><gmm><method-of-moments>,,
619,2,288,b859d50f-cb99-436b-9f68-2fe4377e04e2,2010-07-20 12:29:34.0,220.0,"Suppose that I culture cancer cells in _n_ different dishes _g₁_, _g₂_, … , _g<sub>n</sub>_ and observe the number of cells _n<sub>i</sub>_ in each dish that look different than normal.  The total number of cells in dish _g<sub>i</sub>_ is _t<sub>i</sub>_.  There is individual differences between individual cells, but also differences between the populations in different dishes because each dish has a slightly different temperature, amount of liquid, and so on.\\n\\nI model this as a beta-binomial distribution: _n<sub>i</sub>_ ~ Binomial(_p<sub>i</sub>_, _t<sub>i</sub>_) where _p<sub>i</sub>_ ~ Beta(_α_, _β_).  Given a number of observations of _n<sub>i</sub>_ and _t<sub>i</sub>_, how can I estimate _α_ and _β_?",,
620,1,288,b859d50f-cb99-436b-9f68-2fe4377e04e2,2010-07-20 12:29:34.0,220.0,Estimating beta-binomial distribution,,
621,3,288,b859d50f-cb99-436b-9f68-2fe4377e04e2,2010-07-20 12:29:34.0,220.0,<beta-binomial>,,
622,2,289,8a0441c7-e5aa-4c74-b2fd-68b429e2bdb3,2010-07-20 12:32:41.0,80.0,"For visualizing graphs in a Java/SWT environment, check out Zest: http://eclipse.org/gef/zest",,
623,2,290,3d26e3c2-4e43-4d2f-bb63-3d5504168f53,2010-07-20 12:33:30.0,189.0,I know of Cameron and Trivedi's Microeconometrics Using Stata. \\n\\nWhat are other good text for learning the Stata? ,,
624,1,290,3d26e3c2-4e43-4d2f-bb63-3d5504168f53,2010-07-20 12:33:30.0,189.0,Resources for learning Stata,,
625,3,290,3d26e3c2-4e43-4d2f-bb63-3d5504168f53,2010-07-20 12:33:30.0,189.0,<textbook>,,
626,2,291,8fba3f4f-f734-4b0b-be44-dc37ddbf67b9,2010-07-20 12:35:43.0,211.0,http://lib.stat.cmu.edu/DASL/,,
627,2,292,1d599a2d-cdc1-4d54-97e4-02c8e7435783,2010-07-20 12:38:38.0,56.0,"It should always be large enough! ;)\\n\\nAll parameter estimates come with an estimate uncertainty, which is determined by the sample size. If you carry out a regression analysis, it helps to remind yourself that the &Chi;<sup>2</sup> distribution is constructed from the input data set. If your model had 5 parameters and you had 5 data points, you would only be able to calculate a single point of the &Chi;<sup>2</sup> distribution. Since you will need to minimize it, you could only pick that one point as a guess for the minimum, but would have to assign infinite errors to your estimated parameters. Having more data points would allow you to map the parameter space better leading to a better estimate of the minimum of the &Chi;<sup>2</sup> distribution and thus smaller estimator errors.\\n\\nWould you be using a Maximum Likelihood estimator instead the situation would be similar: More data points leads to better estimate of the minimum.\\n\\nAs for point variance, you would need to model this as well. Having more data points would make clustering of points around the ""true"" value more obvious (due to the Central Limit Theorem) and the danger of interpreting a large, chance flucuation as the true value for that point would go down. And as for any other parameter your estimate for the point variance would become more stable the more data points you have.",,
628,2,293,9147a5db-6c69-4ffc-97db-0cd66fe3f855,2010-07-20 12:43:02.0,211.0,I think you need to rework this question.   It all depends on the problem/data which has generated the cross-tab.   ,,
629,2,294,1dc91fa3-07cd-4a01-b078-7d2383fb769d,2010-07-20 12:47:39.0,211.0,"Bernard Flury, in his excellent book introducing multivariate analysis, described this as an anti-property of principal components.   It's actually worse than choosing between correlation or covariance.   If you changed the units (e.g. US style gallons, inches etc. and EU style litres, centimetres) you will get substantively different projections of the data.\\n\\nThe argument against automatically using correlation matrices is that it is quite a brutal way of standardising your data.   The problem with automatically using the covariance matrix, which is very apparent with that heptathalon data, is that the variables with the highest variance will dominate the first principal component (the variance maximising property).\\n\\nSo the ""best"" method to use is based on a subjective choice, careful thought and some experience.",,
630,5,282,0522d301-748f-4fb5-b863-f0fe78ff066c,2010-07-20 12:52:16.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjects. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                PC1         PC2\\nMaths    0.27795606  0.76772853 \\nScience -0.17428077 -0.08162874 \\nEnglish -0.94200929  0.19632732 \\nMusic    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n",added 133 characters in body,
631,2,295,7b6012b4-1c0c-40fd-83b5-a868d0cf88d7,2010-07-20 12:52:55.0,62.0,"A nice definition of p-value is ""the probability of observing a test statistic at least as large as the one calculated assuming the null hypothesis is true"". \\n\\nThe problem with that is that it requires an understanding of ""test statistic"" and ""null hypothesis"". But, that's easy to get across. If the null hypothesis is true, usually something like ""parameter from population A is equal to parameter from population B"", and you calculate statistics to estimate those parameters, what is the probability of seeing a test statistic that says, ""they're this different""?\\n\\nE.g., If the coin is fair, what is the probability I'd see 60 heads out of 100 tosses? That's testing the null hypothesis, ""the coin is fair"", or ""p = .5"" where p is the probability of heads.\\n\\nThe test statistic in that case would be the number of heads. \\n\\nNow, I *assume* that what you're calling ""t-value"" is a generic ""test statistic"", not a value from a ""t distribution"". They're not the same thing, and the term ""t-value"" isn't (necessarily) widely used and could be confusing.\\n\\nWhat you're calling ""t-value"" is probably what I'm calling ""test statistic"". In order to calculate a p-value (remember, it's just a probability) you need a distribution, and a value to plug into that distribution which will return a probability. Once you do that, the probability you return is your p-value. You can see that they are related because under the same distribution, different test-statistics are going to return different p-values. More extreme test-statistics will return lower p-values giving greater indication that the null hypothesis is false. \\n\\nI've ignored the issue of one-sided and two-sided p-values here.  \\n",,
632,2,296,f3660aa9-db1c-4832-96c2-47b6956a13cb,2010-07-20 12:54:28.0,127.0,"[MLComp][1] has quite a few interesting datasets, and as a bonus your algorithm will get ranked if you upload it.\\n\\n\\n  [1]: http://mlcomp.org/",,
633,2,297,a891f0fc-5407-4155-8c8c-add58bc55160,2010-07-20 13:08:42.0,56.0,"I really enjoy working with [RooFit][1] for easy proper fitting of signal and background distributions and [TMVA][2] for quick principal component analyses and modelling of multivariate problems with some standard tools (like genetic algorithms and neural networks, also does BDTs). They are both part of the [ROOT][3] C++ libraries which have a pretty heavy bias towards particle physics problems though. \\n\\n[1]: http://roofit.sourceforge.net/\\n[2]: http://tmva.sourceforge.net/\\n[3]: http://root.cern.ch/drupal/",,
634,16,297,a891f0fc-5407-4155-8c8c-add58bc55160,2010-07-20 13:08:42.0,-1.0,,,
635,2,298,c2be7b80-7399-44fc-8fe8-fc96ec7716d1,2010-07-20 13:11:50.0,125.0,"Am I looking for a better behaved distribution for the independent variable in question, or to reduce the effect of outliers, or something else?",,
636,1,298,c2be7b80-7399-44fc-8fe8-fc96ec7716d1,2010-07-20 13:11:50.0,125.0,"In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?",,
637,3,298,c2be7b80-7399-44fc-8fe8-fc96ec7716d1,2010-07-20 13:11:50.0,125.0,<distributions><regression><logarithm>,,
638,2,299,eb59ca47-6607-42b2-bc9b-6c8e7b4ca104,2010-07-20 13:16:29.0,5.0,"One typically takes the log of an input variable to scale it and change the distribution (e.g. to make it normally distributed).  This is discussed in most introductory statistics texts.  \\n\\nYou can also read Andrew Gelman's paper on [""Scaling regression inputs by dividing by two standard deviations""][1] for a discussion on this.\\n\\nTaking the log is not an appropriate method for dealing with bad data/outliers.\\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf",,
639,2,300,4dd5a372-81ba-4191-9ac6-8bf91ae74749,2010-07-20 13:19:31.0,56.0,"A ""right"" answer cannot depend on an arbitrary ordering of some method you are using.\\n\\nYou need to consider all possible orderings (or some representative sample) and estimate your parameters for every case. This will give you distributions for the parameters you are trying to estimate. Estimate the ""true"" parameter values from these distributions (this will also give you an estimate for your estimator error).\\n\\nAlternatively use a method that doesn't introduce an ordering.",,
640,5,299,a289dbc5-0ba1-410b-97f1-b011c707cfd4,2010-07-20 13:22:18.0,5.0,"One typically takes the log of an input variable to scale it and change the distribution (e.g. to make it normally distributed).  It cannot be done blindly however; you need to be careful when making any scaling to ensure that the results are still interpretable.  \\n\\nThis is discussed in most introductory statistics texts.  You can also read Andrew Gelman's paper on [""Scaling regression inputs by dividing by two standard deviations""][1] for a discussion on this.  He also has a very nice discussion on this at the beginning of [""Data Analysis Using Regression and Multilevel/Hierarchical Models""][2].\\n\\nTaking the log is not an appropriate method for dealing with bad data/outliers.\\n\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/research/published/standardizing7.pdf\\n  [2]: http://www.stat.columbia.edu/~gelman/arm/",added 323 characters in body,
641,2,301,f3f10e2c-a188-4ce3-83f9-0a5fc4cd5eee,2010-07-20 13:22:40.0,8.0,"You tend to take logs of the data when there is a problem with the residuals. For example, if you plot the residuals against a particular covariate and observe an increasing/decreasing pattern (a funnel shape), then a transformation may be appropriate. Non-random residuals usually indicate that your model assumptions are wrong, i.e. non-normal data.\\n\\nSome data types automatically lend themselves to logarithmic transformations. For example, I usually take logs when dealing with concentrations or age. \\n\\nAlthough transformations aren't primarily used to deal outliers, they do help since taking logs squashes your data.",,
642,5,282,52b9a42b-5a2d-4b05-a068-126ddb3eb456,2010-07-20 13:34:57.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 16 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjects. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n",added 20 characters in body,
643,4,242,678e8a67-4d1e-4a24-9288-3509ca6af74f,2010-07-20 13:58:58.0,199.0,Using time series analysis to analyze/predict violent behavior,edited title,
644,2,302,3fa50263-2bfd-46db-90c2-973e16b8bfa3,2010-07-20 14:04:12.0,196.0,"I've heard two rules of thumb in this regard.  One holds that so long as there are enough observations in the error term to evoke the central limit theorem, e.g. 20 or 30, you are fine.  The other holds that for each estimated slope one should have at least 20 or 30 observations.  The difference between using 20 or 30 as the target number is based on different thoughts regarding when there are enough observations to reasonably evoke the Central Limit Theorem. ",,
645,2,303,b70a987b-72c8-422e-8116-ccc796cd7a73,2010-07-20 14:08:42.0,,"You have a hierarchical bayesian model. Brief details below:\\n\\nLikelihood Function: \\n\\nf(n<sub>i</sub> | p<sub>i</sub>, t<sub>i</sub>) = (t<sub>i</sub> n<sub>i</sub>) p<sub>i</sub><sup>n<sub>i</sub></sup> (1-p<sub>i</sub>)<sup>(t<sub>i</sub> - n<sub>i</sub>)</sup>\\n\\nPriors on p<sub>i</sub>, &alpha;, &beta;:\\n\\npi ~ Beta(&alpha;, &beta;)\\n\\n&alpha; ~ N(&alpha;_mean, &alpha;_var) I(&alpha; > 0)\\n\\n&beta; ~ N(&beta;_mean, &beta;_var) I(&beta; > 0)\\n\\nPosteriors are:\\n\\np<sub>i</sub> ~ Beta(&alpha; + ni, &beta; + ti-ni)\\n\\n&alpha; &prop; I(&alpha; > 0) &prod; p<sub>i</sub><sup>(&alpha;-1)</sup> exp(-(&alpha;-&alpha;_mean)<sup>2</sup>) / (2 &alpha;_var)\\n\\n&beta; &prop; I(&beta; > 0) &prod; (1-p<sub>i</sub>)^(&beta;-1) exp(-(&beta;-&beta;_mean)<sup>2</sup>) / (2 &beta;_var)\\n\\nYou can then use a combination of Gibbs and Metropolis-Hastings to draw from the posterior distributions.",,user28
646,2,304,7df5c293-9de3-408f-9668-6a63f1269b46,2010-07-20 14:13:50.0,196.0,"Shane's point that taking the log to deal with bad data is well taken.  As is Colin's regarding the importance of normal residuals.  In practice I find that usually you can get normal residuals if the input and output variables are also relatively normal.  In practice this means eyeballing the distribution of the transformed and untransformed datasets and assuring oneself that they have become more normal and/or conducting tests of normality (e.g. Shapiro-Wilk or Kolmogorov-Smirnov tests) and determining whether the outcome is more normal.  Interpretablity and tradition are also important.  For example, in cognitive psychology log transforms of reaction time are often used, however, to me at least, the interpretation of a log RT is unclear.  Furthermore, one should be cautious using log transformed values as the shift in scale can change a main effect into an interaction and vice versa.",,
647,5,263,b1c38694-6f90-4d72-955a-0d670db1ea3d,2010-07-20 14:14:17.0,190.0,"My favorite tool is Python with [mathplotlib][1]\\n\\nThe advantages:\\n\\n- Immediate export from the environment where I do my experiments in\\n- Support for the scipy/numpy data structures\\n- Familiar syntax/options (matlab background)\\n- Full latex support for labels/legends etc. So same typesetting as in the rest of your document!\\n\\n\\nSpecifically, for different file formats like svg and eps, use the format parameter of [savefig][2]\\n\\nAn example:\\ninput.csv\\n<pre>""Line 1"",0.5,0.8,1.0,0.9,0.9\\n""Line 2"",0.2,0.7,1.2,1.1,1.1</pre>\\n\\nCode:\\n\\n    import csv\\n    import matplotlib.pyplot as plt\\n\\n    legends = []\\n    for row in csv.reader(open('input.csv')):\\n        legends.append(row[0])\\n        plt.plot(row[1:])\\n    \\n    plt.legend(legends)\\n    plt.savefig(""out.svg"", format='svg')\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/\\n  [2]: http://matplotlib.sourceforge.net/api/pyplot_api.html#matplotlib.pyplot.savefig",added 99 characters in body,
648,6,212,a4c1b0c1-43f0-4bbf-b757-8e698a343249,2010-07-20 14:19:26.0,190.0,<statistical-significance>,edited tags,
649,2,305,f78bfc90-da5a-45ad-91fa-6da337e423b1,2010-07-20 14:19:41.0,196.0,It seems like when the assumption of homogeneity of variance is met that the results from a Welch adjusted t-test and a standard t-test are approximately the same.  Why not simply always use the Welch adjusted t?,,
650,1,305,f78bfc90-da5a-45ad-91fa-6da337e423b1,2010-07-20 14:19:41.0,196.0,When conducting a t-test why would one prefer to assume (or test for) equal variances rather than always use a Welch approximation of the df?,,
651,3,305,f78bfc90-da5a-45ad-91fa-6da337e423b1,2010-07-20 14:19:41.0,196.0,<variance><t-test><homogeneity><of>,,
652,2,306,5fe00df2-0a10-4696-82ba-1f17fe2250ab,2010-07-20 14:21:37.0,5.0,"The model that fits the data doesn't have to be a time series model; I would advise thinking outside the box a little.  \\n\\nIf you have multiple variables (e.g. age, gender, diet, ethnicity, illness, medication) you can use these for a different model.  Maybe having certain patients in the same room is an important predictor?  Or perhaps it has to do with the attending staff?  Or consider using a multi-variate time series model (e.g. VECM) if you have other variables that you can use.  Look at the relationships between violence across patients: do certain patients act out together?  \\n\\nThe time series model is useful if time has some important role in the behavior.  For instance, there might be a clustering of violence.  Look at the volatility clustering literature.  As @Jonas suggests, with a lag order of 2, you may need to be on higher alert on the day following a burst in violence.  But that doesn't help you prevent the first day: there may be other information that you can link into the analysis to actually *understand* the cause of the violence, rather than simply forcasting it in a time series fashion.\\n\\nLastly, as a technical suggestion: if you're using R for the analysis, you might have a look at [the forecast package][1] from Rob Hyndman (the creator of this site).  This has many very nice features; see the paper [""Automatic Time Series Forecasting: The forecast Package for R""][2] in the Journal of Statistical Software.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/forecast/index.html\\n  [2]: http://www.jstatsoft.org/v27/i03",,
653,2,307,5f6e50cb-0598-4541-aa1c-792e4921ba06,2010-07-20 14:23:30.0,229.0,"A saturated model is one in which there are as many estimated parameters as data points. By definition, this will lead to a perfect fit, but will be of little use statistically, as you have no data left to estimate variance.\\n\\nFor example, if you have 6 data points and fit a 5th-order polynomial to the data, you would have a saturated model (one parameter for each of the 5 powers of your independant variable plus one for the constant term).",,
654,6,283,64482c84-ec66-420b-bd66-36b9e01f5319,2010-07-20 14:26:17.0,,<modeling><regression>,edited tags,user28
655,5,163,1be5733c-1e25-4149-9572-7b21be461afe,2010-07-20 14:37:39.0,81.0,"**Definition:**\\n\\nA random variable is a measurable function from a probability space into a measurable space  known as the state space.\\n\\n**Example:**\\n\\nLets say I roll a fair six-sided die, with outcomes being one of the following: 1, 2, 3, 4, 5, or 6.\\n\\nWhichever number the die lands on is the number of free text-books I will give you.\\n\\nIn this case, the *final amount* of free text books that I give you is the **random variable** because its value is based on the outcome (1, 2, 3, 4, 5, or 6 free text books) of a random event (rolling the die) and is not known before I roll the die.",added 157 characters in body,
656,2,308,4f8bb450-e10e-401b-a4c2-b7fc0c7fcccd,2010-07-20 14:38:07.0,22.0,There are couple of good links with introductory material at Princton Uni Library [website][1].\\n\\n\\n  [1]: http://libguides.princeton.edu/content.php?pid=27916&sid=459449,,
657,2,309,f6f61d6b-57d9-4d0b-9a6d-eb1c8d51ec36,2010-07-20 14:40:32.0,88.0,The fact that something more complex reduces to something less complex when some assumption is checked is not enough to throw the simpler method away. ,,
658,2,310,6c4f6d01-b97d-405c-990e-429fb29df097,2010-07-20 14:41:33.0,,You can simply ignore the 'factorial' when using maximum likelihood. Here is the reasoning for your suicides example. Let:\\n\\n&lambda; : Be the expected number of suicides per year\\n\\nk<sub>i</sub>: Be the number of suicides in year i.\\n\\nThen you would maximize the log-likelihood as:\\n\\nLL = &sum; ( k<sub>i</sub> log(&lambda;) - &lambda; - k<sub>i</sub>! )\\n\\nMaximizing the above is equivalent to maximizing the following as k<sub>i</sub>! is a constant :\\n\\nLL<sup>'</sup> = &sum; ( k<sub>i</sub> log(&lambda;) - &lambda; )\\n\\nCould explain why the factorial is an issue? Am I missing something?,,user28
659,2,311,78ac29d0-3df6-4e8c-bf7a-49be87d99441,2010-07-20 14:47:31.0,89.0,"Rob Hyndman gave the easy exact answer for a fixed n.  If you're interested in asymptotic behavior for large n, this is handled in the field of [extreme value theory][1].  There is a small family of possible limiting distributions; see for example the first chapters of [this book][2].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Extreme_value_theory\\n  [2]: http://books.google.com/books?id=3ZKmAAAAIAAJ&q=leadbetter+and+lindgren&dq=leadbetter+and+lindgren&hl=en&ei=z7ZFTPPFH9T-nAfa58HaAw&sa=X&oi=book_result&ct=result&resnum=1&ved=0CC0Q6AEwAA",,
660,5,262,7f8929c1-dbb7-4835-886f-61c7815a51b0,2010-07-20 14:48:45.0,8.0,"The easiest way is to use R \\n\\nUse `read.csv` to enter the data into R, then use a combination of the `plot` and `line` commands\\n\\nIf you want something really special, then look at the libraries [ggplot2][1] or [lattice][2]. \\n\\nIn `ggplot2` the following commands should get you started.\\n    \\n    require(ggplot2)\\n    #You would use read.csv here\\n    N = 10\\n    d = data.frame(x=1:N,y1=runif(N),y2=rnorm(N), y3 = rnorm(N, 0.5))\\n    p = ggplot(d)\\n    \\n    p = p+geom_line(aes(x, y1, colour=""Type 1""))\\n    p = p+geom_line(aes(x, y2, colour=""Type 2""))\\n    p = p+geom_line(aes(x, y3, colour=""Type 3""))\\n    #Add points\\n    p = p+geom_point(aes(x, y3, colour=""Type 3""))\\n    print(p)   \\n\\nThis would give you the following plot:\\n\\n![Line plot][3]\\n\\n**Saving plots in R**\\n\\nSaving plots in R is straightforward:\\n\\n    #Look at ?jpeg to other different saving options\\n    jpeg(""figure.jpg"")\\n    print(p)#for ggplot2 graphics\\n    dev.off()\\n\\nInstead of `jpeg`'s you can also save as a `pdf` or postscript file:\\n\\n    #This example uses R base graphics\\n    #Just change to print(p) for ggplot2\\n    pdf(""figure.pdf"")\\n    plot(d$x,y1, type=""l"")\\n    lines(d$x, y2)\\n    dev.off()\\n\\n\\n  [1]: http://had.co.nz/ggplot2/\\n  [2]: http://lmdvr.r-forge.r-project.org/figures/figures.html\\n  [3]: http://img84.imageshack.us/img84/6393/tmpq.jpg",Added a link to lattice,
661,6,31,4c9e71d4-b0cb-4fa2-8dd6-25bdda1ab816,2010-07-20 14:55:42.0,62.0,<hypothesis-testing><p-value><t-value>,edited tags,
662,5,151,d99850c8-1e4e-4ae4-b59c-397f7354cae0,2010-07-20 14:56:51.0,81.0,"If the goal of the standard deviation is to summarise the spread of a symmetrical data set (i.e. in general how far each datum is from the mean), then we need a good method of defining how to measure that spread.\\n\\nThe benefits of squaring include:\\n\\n - Squaring always gives a positive\\n   value, so the sum will not be zero.\\n - Squaring emphasizes larger differences - a feature that turns out to be both good and bad (think of the effect outliers have).\\n\\nSquaring however does have a problem as a measure of spread and that is that the units are all squared, where as we'd might prefer the spread to be in the same units as the original data (think of squared pounds or squared dollars or squared apples). Hence the square root allows us to return to the original units.\\n\\nI suppose you could say that absolute difference assigns equal weight to the spread of data where as squaring emphasises the extremes. Technically though, as others have pointed out, squaring makes the algebra much easier to work with and offers properties that the absolute method does not (for example, the variance is equal to the expected value of the square of the distribution minus the square of the mean of the distribution)\\n\\n***It's important to note*** ***however*** that there's no reason you couldn't take the absolute difference if that is your preference on how you wish to view 'spread' (sort of how some people see 5% as some magical thresh hold for p-values, when in fact it's situation dependent). Indeed, there are in fact several competing methods for measuring spread.\\n\\nMy view is to use the squared values because I like to think of how it relates to the Pythagorean Theorem of Statistics: c = sqrt(a^2 + b^2)  ...this also helps me remember that when working with independent random variables, variances add, standard deviations don't. But that's just my personal subjective preference.\\n\\nAn much more indepth analysis can be read [here][1].\\n\\n \\n\\n\\n  [1]: http://www.leeds.ac.uk/educol/documents/00003759.htm",added 6 characters in body,
663,5,282,a71e6a62-f9d1-4fc1-93e9-d38ebf91f284,2010-07-20 14:58:15.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 12 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could compare which person did best across all subjects. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n",edited body,
664,2,312,e72a8baa-2b07-4bb8-a08d-55707ff8e2fd,2010-07-20 15:03:40.0,,"I'm a physics graduate who ended up doing infosec so most of the statistics I ever learned is useful for thermodynamics. I'm currently trying to think of a model for working out how many of a population of computers are infected with viruses, though I assume the maths works out the same way for real-world diseases so references in or answers relevant to that field would be welcome too.\\n\\nHere's what I've come up with so far:\\n\\n - assume I know the total population of computers, N.\\n - I know the fraction D of computers that have virus-detection software (i.e. the amount of the population that is being screened)\\n - I know the fraction I of computers that have detection software _that has reported an infection_\\n - I don't know, but can find out or estimate, the probability of Type I and II errors in the detection software.\\n - I don't (yet) care about the time evolution of the population.\\n\\nSo where do I go from here? Would you model infection as a binomial distribution with probability like (I given D), or as a Poisson? Or is the distribution different?",,user209
665,1,312,e72a8baa-2b07-4bb8-a08d-55707ff8e2fd,2010-07-20 15:03:40.0,,What approach could be used for modelling virus infections?,,user209
666,3,312,e72a8baa-2b07-4bb8-a08d-55707ff8e2fd,2010-07-20 15:03:40.0,,<distributions><modeling><poisson><binomial><disease>,,user209
667,6,10,2c0d7014-af9f-4f4d-9b2e-03cc17e82868,2010-07-20 15:09:06.0,24.0,<scales><ordinal><interval><measurement>,added relevant tags,
668,2,313,6bc4b6a8-72ba-4e62-aba2-6a56bd98eec8,2010-07-20 15:10:15.0,226.0,"Imagine you have a bag containing 900 black marbles and 100 white, i.e. 10% of the marbles are white. Now imagine you take 1 marble out, look at it and record its colour, take out another, record its colour etc.. and do this 100 times. At the end of this process you will have a number for white marbles which, ideally, we would expect to be 10, i.e. 10% of 100, but in actual fact may be 8, or 13 or whatever simply due to randomness. If you repeat this 100 marble withdrawal experiment many, many times and then plot a histogram of the number of white marbles drawn per experiment, you'll find you will have a Bell Curve centred about 10. \\n\\nThis represents your 10% hypothesis: with any bag containing 1000 marbles of which 10% are white, if you randomly take out 100 marbles you will find 10 white marbles in the selection, give or take 4 or so. The p-value is all about this ""give or take 4 or so."" Let's say by referring to the Bell Curve created earlier you can determine that less than 5% of the time would you get 5 or fewer white marbles and another < 5% of the time accounts for 15 or more white marbles i.e. > 90% of the time your 100 marble selection will contain between 6 to 14 white marbles inclusive.\\n\\nNow assuming someone plonks down a bag of 1000 marbles with an unknown number of white marbles in it, we have the tools to answer these questions\\n\\ni) Are there fewer than 100 white marbles?\\n\\nii) Are there more than 100 white marbles?\\n\\niii) Does the bag contain 100 white marbles?\\n\\nSimply take out 100 marbles from the bag and count how many of this sample are white. \\n\\na) If there are 6 to 14 whites in the sample you cannot reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 6 through 14 will be > 0.05. \\n\\nb) If there are 5 or fewer whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 5 or fewer will be < 0.05. You would expect the bag to contain < 10% white marbles.\\n\\nc) If there are 15 or more whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 15 or more will be < 0.05. You would expect the bag to contain > 10% white marbles.",,
669,2,314,8ae52f0a-02db-4f6e-83ae-f72271ddc46b,2010-07-20 15:10:34.0,5.0,"R is definitely the answer.  I would just add to what Rob and Colin already said:\\n\\nTo improve the quality of your plots, you should consider using [the **Cairo** package][1] for the output device.  That will greatly improve the *quality* of the final graphics.  You simply call the function before plotting and it redirects to Cairo as the output device.\\n\\n    Cairo(600, 600, file=""plot.png"", type=""png"", bg=""white"")\\n    plot(rnorm(4000),rnorm(4000),col=""#ff000018"",pch=19,cex=2) # semi-transparent red\\n    dev.off() # creates a file ""plot.png"" with the above plot\\n\\nLastly, in terms of putting it in a publication, that's the role that `Sweave` plays.  It makes combining plots with your paper a trivial operation (and has the added benefit of leaving you with something that is *reproducible* and understandable).  Use `cacheSweave` if you have long-running computations.\\n\\n  [1]: http://cran.r-project.org/web/packages/Cairo/index.html",,
670,6,305,4bf20bde-c38e-4c24-adc2-2e1dd43a3093,2010-07-20 15:12:15.0,24.0,<variance><t-test><homogeneity><homogeneity-of-variance>,edited tags,
671,5,117,44374837-e615-4d17-99b5-eb891333adb1,2010-07-20 15:13:37.0,36.0,"http://www.r-bloggers.com/ is an aggregated blog from lots of blogs that talk about statistics using R, and the [#rstats][1] hashtag on twitter is also helpful. I write quite a bit about [statistics and R in genetics research][2].\\n\\n\\n  [1]: http://search.twitter.com/search?q=%23rstats\\n  [2]: http://gettinggeneticsdone.blogspot.com/search/label/R",added 4 characters in body,
672,2,315,6f408f72-a5ba-428b-8821-8baff647f91d,2010-07-20 15:15:57.0,36.0,If you can use R try [ggplot2][1].\\n\\n\\n  [1]: http://had.co.nz/ggplot2/,,
673,2,316,2a4bd3d1-26d4-4a0c-81c6-e533acaa1b29,2010-07-20 15:20:38.0,36.0,**ggplot2** - hands down best visualization for R.\\n\\n**RMySQL/RSQLite/RODBC** - for connecting to a databases\\n\\n**sqldf** - manipulate data.frames with SQL queries\\n\\n**Hmisc/rms** - packages from Frank Harrell containing convenient miscellaneous functions and nice functions for regression analyses.\\n\\n**GenABEL** - nice package for genome-wide association studies\\n\\n**Rcmdr** - a decent GUI for R if you need one.,,
674,16,316,2a4bd3d1-26d4-4a0c-81c6-e533acaa1b29,2010-07-20 15:20:38.0,-1.0,,,
675,2,317,91dafa9b-adf1-47f4-a02f-4d6fae744279,2010-07-20 15:21:01.0,56.0,[The Endeavour][1] sometimes features statistics posts. Otherwise it is mostly around the interplay of computer science and math.\\n\\n[1]: http://www.johndcook.com/blog/,,
676,16,317,91dafa9b-adf1-47f4-a02f-4d6fae744279,2010-07-20 15:21:01.0,-1.0,,,
677,2,318,ad0ffa4f-7401-4aed-9b5b-03f1eab57017,2010-07-20 15:23:53.0,,"Computer virus propagation is structurally similar to infectious diseases propagation (vaccinations = anti-virus software, virus via email = getting a virus from someone etc).\\n\\nUse the following links http://en.wikipedia.org/wiki/Mathematical_modelling_in_epidemiology and http://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology as jumping points for your model.",,user28
678,5,316,0685016d-0b14-4439-8de6-8195d82e6255,2010-07-20 15:28:42.0,36.0,**ggplot2** - hands down best visualization for R.\\n\\n**RMySQL/RSQLite/RODBC** - for connecting to a databases\\n\\n**sqldf** - manipulate data.frames with SQL queries\\n\\n**Hmisc/rms** - packages from Frank Harrell containing convenient miscellaneous functions and nice functions for regression analyses.\\n\\n**GenABEL** - nice package for genome-wide association studies\\n\\n**Rcmdr** - a decent GUI for R if you need one.\\n\\n\\nAlso check out [CRANtastic - this link][1] has a list of the most popular R packages. Many on the top of the list have already been ment\\n\\n\\n  [1]: http://crantastic.org/popcon,added 183 characters in body,
679,2,319,3ed48ff3-13cc-4c78-bdb0-85735f01b008,2010-07-20 15:33:42.0,71.0,"No amount of verbal explanation or calculations really helped me to understand *at a gut level* what p-values were, but it really snapped into focus for me once I took a course that involved simulation.  That gave me the ability to actually *see* data generated by the null hypothesis and to plot the means/etc. of simulated samples, then look at where my sample's statistic fell on that distribution.\\n\\nI think the key advantage to this is that it lets students forget about the math and the test statistic distributions for a minute and focus on the concepts at hand.  Granted, it required that I learn *how* to simulate that stuff, which will cause problems for an entirely different set of students.  But it worked for me, and I've used simulation countless times to help explain statistics to others with great success (e.g., ""This is what your data looks like; this is what a Poisson distribution looks like overlaid.  Are you SURE you want to do a Poisson regression?"").\\n\\nThis doesn't exactly answer the questions you posed, but for me, at least, it made them trivial.",,
680,2,320,1f357836-7bbc-4d6a-9eb7-a65535574ea4,2010-07-20 15:47:04.0,61.0,"A saturated model is a model that is overparameterized to the point that it is basically just interpolating the data.  In some settings, such as image compression and reconstruction, this isn't necessarily a bad thing, but if you're trying to build a predictive model it's very problematic.\\n\\nIn short, saturated models lead to extremely high-variance predictors that are being pushed around by the noise more than the actual data.\\n\\nAs a thought experiment, imagine you've got a saturated model, and there is noise in the data, then imagine fitting the model a few hundred times, each time with a different realization of the noise, and then predicting a new point.  You're likely to get radically different results each time, both for your fit and your prediction (and polynomial models are especially egregious in this regard); in other words the variance of the fit and the predictor are extremely high.\\n\\nBy contrast a model that is not saturated will (if constructed reasonably) give fits that are more consistent with each other even under different noise realization, and the variance of the predictor will also be reduced.",,
681,6,134,eabdccd9-fcd9-47a7-a1aa-76e66eef254d,2010-07-20 15:47:15.0,190.0,<algorithms><running-median>,edited tags,
683,6,242,7c31e774-884a-45fd-b120-f7a2dbec5093,2010-07-20 15:53:21.0,5.0,<time-series><forecasting>,edited tags,
684,6,134,c541c63b-6843-45e8-aeb3-ff70f45b4d85,2010-07-20 15:56:17.0,13.0,<algorithms><possibly-off-topic><running-median>,edited tags,
685,2,321,4e355f03-d3bf-4665-a1e7-c971a64ccb82,2010-07-20 16:01:25.0,220.0,There is a variant of boosting called [gentleboost][1].  How does gentle boosting differ from other boosting variants?\\n\\n\\n  [1]: http://dx.doi.org/10.1214/aos/1016218223,,
686,1,321,4e355f03-d3bf-4665-a1e7-c971a64ccb82,2010-07-20 16:01:25.0,220.0,How does gentle boosting differ from other boosting variants?,,
687,3,321,4e355f03-d3bf-4665-a1e7-c971a64ccb82,2010-07-20 16:01:25.0,220.0,<machine-learning><boosting>,,
688,2,322,445f6517-f06c-4eb1-a398-448ec97c2533,2010-07-20 16:03:40.0,3807.0,I'm looking for a book or online resource that explains different kinds of entropy such as Sample Entropy and Shannon Entropy and their advantages and disadvantages.\\nCan someone point me in the right direction?,,
689,1,322,445f6517-f06c-4eb1-a398-448ec97c2533,2010-07-20 16:03:40.0,3807.0,Good introduction into different kinds of entropy,,
690,3,322,445f6517-f06c-4eb1-a398-448ec97c2533,2010-07-20 16:03:40.0,3807.0,<entropy>,,
691,6,2,64527c0d-9b5e-4694-9cd4-c127f0e18bf4,2010-07-20 16:04:03.0,24.0,<distributions><fundamentals><normality>,edited tags,
692,6,31,1bfa4ca1-480f-4c81-a17b-642b56a57965,2010-07-20 16:04:36.0,24.0,<hypothesis-testing><fundamentals><p-value><t-value>,edited tags,
693,6,26,2226f2d8-c323-4754-9710-e0b6a8f019d7,2010-07-20 16:04:59.0,24.0,<standard-deviation><fundamentals><statistics>,edited tags,
694,2,323,7f2579de-3756-4b87-8226-e2ecd0c469a6,2010-07-20 16:10:16.0,3807.0,"[Statistical Modeling, Causal Inference, and Social Science][1] from Andrew Gelman is a good blog.\\n\\n\\n  [1]: http://www.stat.columbia.edu/~cook/movabletype/mlm/",,
695,16,323,7f2579de-3756-4b87-8226-e2ecd0c469a6,2010-07-20 16:10:16.0,-1.0,,,
696,2,324,15489984-5ec1-4f0c-906a-8aa11cf7f203,2010-07-20 16:12:08.0,3807.0,[Darren Wilkinson's research blog][1]\\n\\n\\n  [1]: http://darrenjw.wordpress.com/,,
697,16,324,15489984-5ec1-4f0c-906a-8aa11cf7f203,2010-07-20 16:12:08.0,-1.0,,,
701,2,326,b7997e40-1728-4a79-921b-50b6568fc9a1,2010-07-20 16:13:03.0,3807.0,[XI'AN'S OG][1]\\n\\n\\n  [1]: http://xianblog.wordpress.com/,,
702,16,326,b7997e40-1728-4a79-921b-50b6568fc9a1,2010-07-20 16:13:03.0,-1.0,,,
704,2,327,35020ef9-a770-4808-9610-60f4f2c91b47,2010-07-20 16:19:51.0,3807.0,Malcom Gladewell analyses the problem in the book Outliers by analyzing Hockey Players.,,
705,2,328,4a99d03b-3a62-4d33-ad9e-868dd5497d53,2010-07-20 16:27:08.0,75.0,"I realize that the statistical analysis of financial data is a huge topic, but that is exactly why it is necessary for me to ask my question as I try to break into the world of financial analysis.\\n\\nAs at this point I know next to nothing about the subject, the results of my google searches  are overwhelming.  Many of the matches advocate learning specialized tools or the R programming language. While I will learn these when they are necessary, I'm first interested in books, articles or any other resources that explain modern methods of statistical analysis specifically for financial data.  I assume there are a number of different wildly varied methods for analyzing data, so ideally I'm seeking an overview of the various methods that are practically applicable.  I'd like something that utilizes real world examples that a beginner is capable of grasping but that aren't overly simplistic.\\n\\nWhat are some good resources for learning bout the statistical analysis of financial data?",,
706,1,328,4a99d03b-3a62-4d33-ad9e-868dd5497d53,2010-07-20 16:27:08.0,75.0,Resources for learning about the Statistical Analysis of Financial Data,,
707,3,328,4a99d03b-3a62-4d33-ad9e-868dd5497d53,2010-07-20 16:27:08.0,75.0,<data><finance><analysis><resources>,,
708,2,329,88a67037-f226-4f71-9457-2c0d33771472,2010-07-20 16:31:26.0,5.0,"My favorite books on the subject:\\n\\n - I strongly recommend starting with [**Statistics and Finance**][7], by David Ruppert ([the R code for the book is available][8]).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.\\n - [Modeling Financial Time Series with S-Plus][6], by Eric Zivot\\n - [Analysis of Financial Time Series][9], by Ruey Tsay\\n - [Time Series Analysis][10], by Jonathan D. Cryer\\n\\nBeyond that, you may want some general resources, and the ""bible"" of finance is [Options, Futures, and Other Derivatives][11] by John Hull.  \\n\\n\\n  [6]: http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387279652/ref=sr_1_1?ie=UTF8&s=books&qid=1267799203&sr=1-1\\n  [7]: http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706\\n  [8]: http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm\\n  [9]: http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0471690740/ref=sr_1_1?ie=UTF8&s=books&qid=1267799281&sr=1-1\\n  [10]: http://www.amazon.com/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=pd_sim_b_2\\n  [11]: http://www.rotman.utoronto.ca/~hull/ofod/\\n",,
709,5,329,ebefd968-8212-4ac4-857b-ffce70ac5bfd,2010-07-20 16:38:57.0,5.0,"My favorite books on the subject:\\n\\n - I strongly recommend starting with [**Statistics and Finance**][1], by David Ruppert ([the R code for the book is available][2]).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.\\n - [Modeling Financial Time Series with S-Plus][3], by Eric Zivot\\n - [Analysis of Financial Time Series][4], by Ruey Tsay\\n - [Time Series Analysis][5], by Jonathan D. Cryer\\n\\nBeyond that, you may want some general resources, and the ""bible"" of finance is [Options, Futures, and Other Derivatives][6] by John Hull.  \\n\\nLastly, in terms of some good general books, you might start with these two:\\n\\n - [A Random Walk Down Wall Street][7]\\n - [Against the Gods: The Remarkable Story of Risk][8] \\n\\n\\n  [1]: http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706\\n  [2]: http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm\\n  [3]: http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387279652/ref=sr_1_1?ie=UTF8&s=books&qid=1267799203&sr=1-1\\n  [4]: http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0471690740/ref=sr_1_1?ie=UTF8&s=books&qid=1267799281&sr=1-1\\n  [5]: http://www.amazon.com/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=pd_sim_b_2\\n  [6]: http://www.rotman.utoronto.ca/~hull/ofod/\\n  [7]: http://www.amazon.com/Random-Walk-Down-Wall-Street/dp/0393325350/ref=dp_ob_title_bk\\n  [8]: http://www.amazon.com/Against-Gods-Remarkable-Story-Risk/dp/0471295639/ref=sr_1_1?ie=UTF8&s=books&qid=1279643845&sr=1-1",added 393 characters in body,
714,2,331,f08499af-545b-4916-a7cb-e4f9433a5145,2010-07-20 16:43:42.0,61.0,"Because exact results are preferable to approximations, and avoid odd edge cases where the approximation may lead to a different result than the exact method.  \\n\\nThe Welch method isn't a quicker way to do any old t-test, it's a tractable approximation to an otherwise very hard problem: how to construct a t-test under unequal variances.  The equal-variance case is well-understood, simple, and exact, and therefore should always be used when possible.",,
717,5,329,adc65f3e-b647-43cf-8887-81e0236843be,2010-07-20 16:49:17.0,5.0,"You might start with this [series of lectures by Robert Shiller at Yale][1].  He gives a good overview of the field.\\n\\nMy favorite books on the subject:\\n\\n - I strongly recommend starting with [**Statistics and Finance**][2], by David Ruppert ([the R code for the book is available][3]).  This is a great introduction and covers the basics of finance and statistics so it's appropriate as a first book.\\n - [Modeling Financial Time Series with S-Plus][4], by Eric Zivot\\n - [Analysis of Financial Time Series][5], by Ruey Tsay\\n - [Time Series Analysis][6], by Jonathan D. Cryer\\n\\nBeyond that, you may want some general resources, and the ""bible"" of finance is [Options, Futures, and Other Derivatives][7] by John Hull.  \\n\\nLastly, in terms of some good general books, you might start with these two:\\n\\n - [A Random Walk Down Wall Street][8]\\n - [Against the Gods: The Remarkable Story of Risk][9] \\n\\n\\n  [1]: http://oyc.yale.edu/economics/financial-markets/\\n  [2]: http://www.amazon.com/Statistics-Finance-Introduction-David-Ruppert/dp/0387202706\\n  [3]: http://www.stat.tamu.edu/~ljin/Finance/stat689-R.htm\\n  [4]: http://www.amazon.com/Modeling-Financial-Time-S-PLUS%C2%AE-Zivot/dp/0387279652/ref=sr_1_1?ie=UTF8&s=books&qid=1267799203&sr=1-1\\n  [5]: http://www.amazon.com/Analysis-Financial-Wiley-Probability-Statistics/dp/0471690740/ref=sr_1_1?ie=UTF8&s=books&qid=1267799281&sr=1-1\\n  [6]: http://www.amazon.com/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=pd_sim_b_2\\n  [7]: http://www.rotman.utoronto.ca/~hull/ofod/\\n  [8]: http://www.amazon.com/Random-Walk-Down-Wall-Street/dp/0393325350/ref=dp_ob_title_bk\\n  [9]: http://www.amazon.com/Against-Gods-Remarkable-Story-Risk/dp/0471295639/ref=sr_1_1?ie=UTF8&s=books&qid=1279643845&sr=1-1",added 177 characters in body,
719,2,333,6a10da1b-c826-4a06-b287-afdad005bed3,2010-07-20 17:17:21.0,74.0,"Ed Thorpe started the whole thing (statistical arbitrage). He has a website, and some good articles.\\n\\nhttp://edwardothorp.com/\\n\\nYou should also read Nassim Taleb's ""Fooled By Randomness"".\\n\\nAlso, go on Google Scholar and read the top articles by Markowitz, Sharpe, Fama, Modigliani. ",,
726,2,337,dc31f588-ab91-4fa2-9eda-ac11a26f3469,2010-07-20 17:20:41.0,88.0,The entropy is only one (as a concept) -- there are only many its generalizations. Sample entropy is only some entropy-like descriptor used in heart rate analysis.,,
727,2,338,723debd0-610a-420d-a6d5-f2557e66bb78,2010-07-20 17:22:01.0,89.0,"Cover and Thomas's book *Elements of Information Theory* is a good source on entropy and its applications, although I don't know that it addresses exactly the issues you have in mind.",,
728,2,339,0274c81f-7f90-4226-8ba6-12d519a51afd,2010-07-20 17:31:49.0,218.0,"If the information required is the distribution of data about the mean, standard deviation comes in handy.\\n\\nThe sum of the difference of each value from the mean is zero (obviously, since the value are evenly spread around the mean), hence we square each difference so as to convert negative values to positive, sum them across the population, and take their square root. This value is then divided by the number of samples (or, the size of the population). This gives the standard deviation.",,
729,5,337,85be02a5-b620-4c62-9c85-c61f827c81fd,2010-07-20 17:36:23.0,88.0,The entropy is only one (as a concept) -- the amount of information needed to describe some system; there are only many its generalizations. Sample entropy is only some entropy-like descriptor used in heart rate analysis.,added 58 characters in body,
730,2,340,c154a027-379c-4e3b-aa03-9007accaf164,2010-07-20 17:41:00.0,74.0,"The population is everything in the group of study. For example, if you are studying the price of Apple's shares, it is the historical, current, and even all future stock prices. Or, if you run an egg factory, it is all the eggs made by the factory.\\n\\nYou don't always have to sample, and do statistical tests. If your population is your immediate living family, you don't need to sample, as the population is small. \\n\\nSampling is popular for a variety of reasons:\\n\\n- it is cheaper than a census (sampling the whole population)\\n- you don't have access to future data, so must sample the past\\n- you have to destroy some items by testing them, and don't want to destroy them all (say, eggs)\\n\\n",,
731,5,86,739a4dac-d913-40df-9987-61ae77c5cd8c,2010-07-20 17:46:55.0,13.0,"Unlike a regular variable, a random variable may not be substituted for a single, unchanging value.  Rather **statistical properties** such as the **distribution** of the random variable may be stated.  The distribution is a function that provides the probability the variable will take on a given value, or fall within a range given certain parameters such as the mean or standard deviation.\\n\\nRandom variables may be classified as *discrete* if the distribution describes values from a countable set, such as the integers.  The other classification for a random variable is *continuous* and is used if the distribution covers values from an uncountable set such as the real numbers.",deleted 6 characters in body,
732,6,130,18998643-d455-49de-9ec6-23fb32d636d0,2010-07-20 18:11:20.0,13.0,<r><subjective>,edited tags,
733,6,138,55636248-1835-4bde-a691-79363f0540f8,2010-07-20 18:11:56.0,13.0,<r><open-source><tutorials>,edited tags,
734,2,341,75f3cc85-ecb3-4479-b73b-7c17cae3d0a3,2010-07-20 18:12:29.0,88.0,"Do you think that unbalanced classes is a big problem for k-nearest neighbor? If so, do you know any smart way to handle this?",,
735,1,341,75f3cc85-ecb3-4479-b73b-7c17cae3d0a3,2010-07-20 18:12:29.0,88.0,kNN and unbalanced classes,,
736,3,341,75f3cc85-ecb3-4479-b73b-7c17cae3d0a3,2010-07-20 18:12:29.0,88.0,<machine-learning><k-nearest-neighbour><unbalanced-classes>,,
737,6,134,91e1ac65-2a05-465e-8a14-176318e40a18,2010-07-20 18:12:30.0,13.0,<algorithms><running-median>,edited tags,
738,6,75,c1c5258c-0758-4b9d-99e0-64dff16b729c,2010-07-20 18:13:33.0,13.0,<r><books><implementation>,edited tags,
739,2,342,a7accaf9-ff88-4f2b-9684-75bebe8b7581,2010-07-20 18:13:44.0,139.0,"If you're willing to tolerate an approximation, there are other methods. For example, one approximation is a value whose rank is within some (user specified) distance from the true median. For example, the median has (normalized) rank 0.5, and if you specify an error term of 10%, you'd want an answer that has rank between 0.45 and 0.55. \\n\\nIf such an answer is appropriate, then there are many solutions that can work on sliding windows of data. The basic idea is to maintain a sample of the data of a certain size (roughly 1/error term) and compute the median on this sample. It can be shown that with high probability, regardless of the nature of the input, the resulting median satisfies the properties I mentioned above.\\n\\nThus, the main question is how to maintain a running sample of the data of a certain size, and there are many approaches for that, including the technique known as reservoir sampling. For example, this paper: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7136",,
740,2,343,862fc056-2b44-48ca-9ae2-2675f56b6185,2010-07-20 18:16:17.0,88.0,[kNN][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm,,
741,16,343,862fc056-2b44-48ca-9ae2-2675f56b6185,2010-07-20 18:16:17.0,-1.0,,,
742,2,344,a2e44028-7a25-49fd-8c00-84c75e2389cc,2010-07-20 18:17:25.0,88.0,[Naive Bayes][1] and [Random Naive Bays][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Naive_bayes\\n  [2]: http://en.wikipedia.org/wiki/Random_naive_Bayes,,
743,16,344,a2e44028-7a25-49fd-8c00-84c75e2389cc,2010-07-20 18:17:25.0,-1.0,,,
744,6,258,63d31190-33d6-4863-8d1a-355e43a602bb,2010-07-20 18:18:08.0,88.0,<machine-learning><classification><application>,edited tags,
745,6,269,8e72333e-d70a-4b7f-b8b4-239d7e05dc4b,2010-07-20 18:51:26.0,24.0,<standard-deviation><fundamentals><variance><population><sample>,edited tags,
746,10,134,27606156-98ee-43d6-992a-478139afd75a,2010-07-20 18:54:42.0,-1.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":13,""DisplayName"":""Sharpie""},{""Id"":103,""DisplayName"":""rcs""},{""Id"":28,""DisplayName"":""Srikant Vadali""}]}",2,
747,2,345,c5865774-11c0-4540-b523-c61dd34b6c18,2010-07-20 19:08:50.0,247.0,"Also good is ""Statistical Analysis of Financial Data in S-PLUS"" by Rene A. Carmona",,
748,2,346,9d205183-645f-44ea-94c9-43732e6192c6,2010-07-20 19:21:16.0,247.0,"I'm looking for a good algorithm (meaning minimal computation, minimal storage requirements) to estimate the median of a data set that is too large to store, such that each value can only be read once (unless you explicitly store that value). There are no bounds on the data that can be assumed.\\n\\nApproximations are fine, as long as the accuracy is known.\\n\\nAny pointers?",,
749,1,346,9d205183-645f-44ea-94c9-43732e6192c6,2010-07-20 19:21:16.0,247.0,What is a good algorithm for estimating the median of a huge read-once data set?,,
750,3,346,9d205183-645f-44ea-94c9-43732e6192c6,2010-07-20 19:21:16.0,247.0,<algorithms><median>,,
751,6,50,3ed86ca4-48e1-4781-9731-d72349fcdec6,2010-07-20 19:23:12.0,24.0,<fundamentals><random><variable><random-variable>,edited tags,
752,5,321,061f7cb8-a528-4f36-83c4-0c8dffa02ada,2010-07-20 19:25:13.0,220.0,There is a variant of boosting called [gentleboost][1].  How does gentle boosting differ from the better-known [AdaBoost][2]?\\n\\n\\n  [1]: http://dx.doi.org/10.1214/aos/1016218223\\n  [2]: http://en.wikipedia.org/wiki/AdaBoost,capitalization,
753,4,321,061f7cb8-a528-4f36-83c4-0c8dffa02ada,2010-07-20 19:25:13.0,220.0,How does gentle boosting differ from AdaBoost?,capitalization,
754,2,347,a5d5e811-4c32-4a86-9720-83c64178bd1f,2010-07-20 19:50:57.0,15.0,I found this rather helpful: http://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf,,
755,2,348,599d4cc5-a9d6-4011-aaae-8c02d2fa19ab,2010-07-20 19:54:13.0,24.0,"A male cat and a female cat are penned up in a steel chamber, along with enough food and for 70 days.  \\n\\nA Frequentist would say the average gestation period for [felines][1] is 66 days, the female was in heat when the cats were penned up, and once in heat she will mate repeatedly for 4 to 7 days.  Since there were likely many acts of propagation and enough subsequent time for gestation, the odds are, when the box is opened on day 70, there's a litter of newborn kittens.\\n\\nA Bayesian would say, I heard some serious Marvin Gaye coming from the box on day 1 and then this morning I heard many kitten-like sounds coming from the box.  So without knowing much about cat reproduction, the odds are, when the box is opened on day 70, there's a litter of newborn kittens. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Cat#Reproduction",,
756,2,349,a2668c97-1881-47c0-9368-2849b00ee56e,2010-07-20 19:59:33.0,54.0,"How about something like a binning procedure?  Assume you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] \\n\\nThen, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the number of bins. You're limited only by how much memory you have.",,
757,5,349,8fc79b22-4441-404b-8dfe-c105e285418d,2010-07-20 20:04:37.0,54.0,"How about something like a binning procedure?  Assume (for illustration purposes) that you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] \\n\\nThen, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the size of the bins. You're limited only by how much memory you have.\\n\\nSince you don't know how big your values may get, just pick a bin size large enough that you aren't likely to run out of memory, using some quick back-of-the-envelope calculations. You might also store the bins sparsely, such that you only add a bin if it contains a value.",added 244 characters in body; added 68 characters in body,
758,2,350,7dd81642-0b68-425d-acff-28e9f7da21c4,2010-07-20 20:12:26.0,178.0,"You can try to find a median based on grouped frequency distribution, [here is some details][1]\\n\\n\\n  [1]: http://www.statcan.gc.ca/edu/power-pouvoir/ch11/median-mediane/5214872-eng.htm",,
759,6,173,21682257-5d1b-4faf-9a80-1bd92a378cd3,2010-07-20 20:16:38.0,71.0,<r><time-series><poisson><count-data><epidemiology>,"Tags have a little more definition now; adding ""poisson"" might help people find this.",
760,16,7,8a2b77e9-634d-408a-bfc7-a2cc5f0d8770,2010-07-20 20:50:48.0,38.0,,,
761,6,7,8a2b77e9-634d-408a-bfc7-a2cc5f0d8770,2010-07-20 20:50:48.0,38.0,<population><dataset><sample>,Flagged as Community Wiki,
762,2,351,23c6767b-3ede-4793-b2f4-9d4ab33d855e,2010-07-20 21:05:35.0,16.0,[Timetric][1] provides a web interface to data and provide a list of the [publicly available data sets][2] they use\\n\\n\\n  [1]: http://timetric.com\\n  [2]: http://timetric.com/dataset/,,
763,16,351,23c6767b-3ede-4793-b2f4-9d4ab33d855e,2010-07-20 21:05:35.0,-1.0,,,
764,2,352,c172ee5a-105f-408a-bfea-c7cb51889811,2010-07-20 21:18:45.0,8.0,"I've never had to do this, so this is just a suggestion.\\n\\nI see two (other) possibilities. \\n\\n**Half data**\\n\\n1. Load in half the data and sort\\n1. Next read in the remaining values and compare against the your sorted list. \\n  1. If the new value is larger, discard it.\\n  1. else put the value in the sorted list and removing the largest value from that list.\\n\\n\\n**Sampling distribution**\\n\\nThe other option, is to use an approximation involving the sampling distribution. If your data is Normal, then the standard error for moderate n is:\\n\\n1.253 * sd / sqrt(n)\\n\\nJust determine the size of *n* that you would be happy with. To test this, I ran a quick monte-carlo simulation in R\\n\\n    n = 10000\\n    outsi\\n    de.ci.uni = 0\\n    outside.ci.nor = 0\\n    N=1000\\n    for(i in 1:N){\\n      #Theoretical median is 0\\n      uni = runif(n, -10, 10)\\n      nor  = rnorm(n, 0, 10)\\n      \\n      if(abs(median(uni)) > 1.96*1.253*sd(uni)/sqrt(n))\\n        outside.ci.uni = outside.ci.uni + 1\\n    \\n      if(abs(median(nor)) > 1.96*1.253*sd(nor)/sqrt(n))\\n        outside.ci.nor = outside.ci.nor + 1\\n    }\\n    \\n    outside.ci.uni/N\\n    outside.ci.nor/N\\n\\nFor n=10000, 15% of the uniform median estimates were outside the CI.\\n\\n\\n\\n",,
766,2,353,47c73b3d-7bf3-4e8f-881d-03e00b121a2b,2010-07-20 22:11:54.0,61.0,"Fisher's scoring is just a version of Newton's method that happens to be identified with GLMs, there's nothing particularly special about it, other than the fact that the Fisher's information matrix happens to be rather easy to find for random variables in the exponential family.  It also ties in to a lot of other math-stat material that tends to come up about the same time, and gives a nice geometric intuition about what exactly Fisher information means.\\n\\nThere's absolutely no reason I can think of not to use some other optimizer if you prefer, other than that you might have to code it by hand rather than use a pre-existing package.  I suspect that any strong emphasis on Fisher scoring is a combination of (in order of decreasing weight) pedagogy, ease-of-derivation, historical bias, and ""not-invented-here"" syndrome.  ",,
767,2,354,9643afb3-0750-4c64-a2fa-509ef1830433,2010-07-20 22:26:26.0,3807.0,Why do we seek to minimize x^2 instead of minimizing x^1.95 or x^2.05.\\nAre there reasons why the number should be exactly two or is it simply a convention that has the advantage of simplifying the math?,,
768,1,354,9643afb3-0750-4c64-a2fa-509ef1830433,2010-07-20 22:26:26.0,3807.0,Bias towards natural numbers in the case of least squares.,,
769,3,354,9643afb3-0750-4c64-a2fa-509ef1830433,2010-07-20 22:26:26.0,3807.0,<least-squares>,,
770,2,355,1b929a74-a3ce-4cbd-a131-840af5663896,2010-07-20 22:27:42.0,158.0,"Check out [Wilmott.com][1] as well.  It's oriented toward more advanced practitioners, but if I had to choose one person from whom to learn financial math, it would be Paul Wilmott.  Brilliant but grounded.\\n\\n\\n  [1]: http://wilmott.com/",,
771,2,356,238266ae-6246-452d-ba44-821f4d033551,2010-07-20 22:31:45.0,8.0,"To fit the model you can use [JAGS][1] or [Winbugs][2]. In fact if you look at the week 3 of the lecture notes at Paul Hewson's [webpage][3], the rats JAGS example is a beta binomial model. He puts gamma priors on alpha and beta.\\n\\n\\n  [1]: http://www-fis.iarc.fr/~martyn/software/jags/\\n  [2]: http://www.mrc-bsu.cam.ac.uk/bugs/\\n  [3]: http://users.aims.ac.za/~paulhewson/",,
772,5,349,4f0dd4df-76d6-40af-a1f5-6426f48cd93d,2010-07-20 22:40:50.0,54.0,"How about something like a binning procedure?  Assume (for illustration purposes) that you know that the values are between 1 and 1 million.  Set up N bins, of size S. So if S=10000, you'd have 100 bins, corresponding to values [1:10000, 10001:20000, ... , 990001:1000000] \\n\\nThen, step through the values. Instead of storing each value, just increment the counter in the appropriate bin. Using the midpoint of each bin as an estimate, you can make a reasonable approximation of the median. You can scale this to as fine or coarse of a resolution as you want by changing the size of the bins. You're limited only by how much memory you have.\\n\\nSince you don't know how big your values may get, just pick a bin size large enough that you aren't likely to run out of memory, using some quick back-of-the-envelope calculations. You might also store the bins sparsely, such that you only add a bin if it contains a value.\\n\\nEdit:\\n\\nThe link ryfm provides gives an example of doing this, with the additional step of using the cumulative percentages to more accurately estimate the point within the median bin, instead of just using midpoints. This is a nice improvement.",added 250 characters in body,
773,5,348,a99dd212-e7c6-47d4-a57d-9e4ae38d53f1,2010-07-20 22:52:03.0,24.0,"A male cat and a female cat are penned up in a steel chamber, along with enough food and water for 70 days.  \\n\\nA Frequentist would say the average gestation period for [felines][1] is 66 days, the female was in heat when the cats were penned up, and once in heat she will mate repeatedly for 4 to 7 days.  Since there were likely many acts of propagation and enough subsequent time for gestation, the odds are, when the box is opened on day 70, there's a litter of newborn kittens.\\n\\nA Bayesian would say, I heard some serious Marvin Gaye coming from the box on day 1 and then this morning I heard many kitten-like sounds coming from the box.  So without knowing much about cat reproduction, the odds are, when the box is opened on day 70, there's a litter of newborn kittens. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Cat#Reproduction",fixed grammar,
774,2,357,258da2e0-4d84-4699-adab-aa49f9bb41a2,2010-07-20 23:07:48.0,3807.0,"You can't know whether there normality and that's why you have to make an assumption that's there.\\nYou can only prove the absence of normality with statistic tests.\\n\\nEven worse, when you work with real world data it's almost certain that there isn't true normality in your data.\\n\\nThat means that your statistical test is always a bit biased. The question is whether you can live with it's bias.\\nTo do that you have to understand your data and the kind of normality that your statistical tool assumes.\\n\\nIt's the reason why Frequentist tools are such as subjective as Bayesian tools. You can't determine based on the data that it's normally distributed. You have to assume normality.",,
775,2,358,ba7723d0-2792-4b26-8503-22106489cdc7,2010-07-20 23:21:05.0,88.0,We try to minimize the variance that is left within descriptors. Why variance? Read [this question][1]; this also comes together with the (mostly silent) assumption that errors are normally distributed.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/118/standard-deviation-why-square-the-difference-instead-of-taking-the-absolute-val,,
776,2,359,997b5499-97fb-44fd-b6fe-a74332eeeab7,2010-07-20 23:28:49.0,90.0,"The Wald, Likelihood Ratio and Lagrange Multiplier tests in the context of maximum likelihood estimation are asymptotically equivalent. However, for small samples, they tend to diverge quite a bit, and in some cases they result in different conclusions.\\n\\nHow can they be ranked according to how likely they are to reject the null? What to do when the tests have conflicting answers? Can you just pick the one which gives the answer you want or is there a ""rule"" or ""guideline"" as to how to proceed?",,
777,1,359,997b5499-97fb-44fd-b6fe-a74332eeeab7,2010-07-20 23:28:49.0,90.0,The trinity of tests in maximum likelihood: what to do when faced with contradicting conclusions?,,
778,3,359,997b5499-97fb-44fd-b6fe-a74332eeeab7,2010-07-20 23:28:49.0,90.0,<hypothesis-testing><maximum-likelihood>,,
779,2,360,66babed3-0f3a-40f6-b112-25e4593b4dba,2010-07-20 23:56:33.0,88.0,"Does it really need some advanced model? Based on what I know about TB, in case there is no epidemy the infections are stochastic acts and so the count form month N shouldn't be correlated with count from month N-1. (You can check this assumption with autocorrelation). If so, analyzing just the distribution of monthly counts may be sufficient to decide if some count is significantly higher than normal.  \\nOn the other hand you can look for correlations with some other variables, like season, travel traffic, or anything that you can imagine that may be correlated. If you would found something like this, it could be then used for some data normalization.",,
780,2,361,9e2a94de-8a6b-45cd-b20a-b359a52d492a,2010-07-21 00:09:09.0,170.0,[Logistic Regression][1]:\\n\\n - fast and perform well on most datasets\\n - almost no parameters to tune\\n - handles both discrete/continuous features\\n - model is easily interpretable\\n - (not really restricted to binary classifications)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Logistic_regression,,
781,16,361,9e2a94de-8a6b-45cd-b20a-b359a52d492a,2010-07-21 00:09:09.0,-1.0,,,
782,5,354,831ed5c9-7654-48a8-b4e9-968a297b8b2c,2010-07-21 00:19:03.0,3807.0,Why do we seek to minimize `x^2` instead of minimizing `|x|^1.95` or `|x|^2.05`.\\nAre there reasons why the number should be exactly two or is it simply a convention that has the advantage of simplifying the math?,added 10 characters in body,
783,2,362,47492cb7-f60a-4302-819f-86df9d13f334,2010-07-21 00:24:35.0,196.0,What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?  When will results from these two methods differ?,,
784,1,362,47492cb7-f60a-4302-819f-86df9d13f334,2010-07-21 00:24:35.0,196.0,What is the difference between the Shapiro-Wilk test of normality and the Kolmogorov-Smirnov test of normality?,,
785,3,362,47492cb7-f60a-4302-819f-86df9d13f334,2010-07-21 00:24:35.0,196.0,<distributions><statistical-significance><normality>,,
786,5,173,9555b175-d6a7-4740-b450-6e53c0077f7b,2010-07-21 00:25:04.0,71.0,"I recently started working for a tuberculosis clinic.  We meet periodically to discuss the number of TB cases we're currently treating, the number of tests administered, etc.  I'd like to start modeling these counts so that we're not just guessing whether something is unusual or not.  Unfortunately, I've had very little training in time series, and most of my exposure has been to models for very continuous data (stock prices) or very large numbers of counts (influenza).  But we deal with 0-18 cases per month (mean 6.68, median 7, var 12.3), which are distributed like this:\\n\\n![alt text][1]\\n\\n![alt text][2]\\n\\n\\nI've found a few articles that address models like this, but I'd greatly appreciate hearing suggestions from you - both for approaches and for R packages that I could use to implement those approaches.\\n\\n**EDIT:**  mbq's answer has forced me to think more carefully about what I'm asking here; I got too hung-up on the monthly counts and lost the actual focus of the question.  What I'd like to know is: does the (fairly visible) decline from, say, 2008 onward reflect a downward trend in the overall number of cases?  It looks to me like the number of cases monthly from 2001-2007 reflects a stable process; maybe some seasonality, but overall stable.  From 2008 through the present, it looks like that process is changing: the overall number of cases is declining, even though the monthly counts might wobble up and down due to randomness and seasonality.  How can I test if there's a real change in the process?  And if I can identify a decline, how could I use that trend and whatever seasonality there might be to estimate the number of cases we might see in the upcoming months?\\n\\nWhew.  Thanks for bearing with me.\\n\\n\\n  [1]: http://img827.imageshack.us/img827/1927/activetbcases.png ""Cases by month""\\n  [2]: http://img827.imageshack.us/img827/4348/tbcasedistribution.png ""Distribution of counts""",added 975 characters in body; deleted 53 characters in body,
787,5,313,b25caff4-9276-4048-91e0-60b8bd958002,2010-07-21 00:33:56.0,226.0,"Imagine you have a bag containing 900 black marbles and 100 white, i.e. 10% of the marbles are white. Now imagine you take 1 marble out, look at it and record its colour, take out another, record its colour etc.. and do this 100 times. At the end of this process you will have a number for white marbles which, ideally, we would expect to be 10, i.e. 10% of 100, but in actual fact may be 8, or 13 or whatever simply due to randomness. If you repeat this 100 marble withdrawal experiment many, many times and then plot a histogram of the number of white marbles drawn per experiment, you'll find you will have a Bell Curve centred about 10. \\n\\nThis represents your 10% hypothesis: with any bag containing 1000 marbles of which 10% are white, if you randomly take out 100 marbles you will find 10 white marbles in the selection, give or take 4 or so. The p-value is all about this ""give or take 4 or so."" Let's say by referring to the Bell Curve created earlier you can determine that less than 5% of the time would you get 5 or fewer white marbles and another < 5% of the time accounts for 15 or more white marbles i.e. > 90% of the time your 100 marble selection will contain between 6 to 14 white marbles inclusive.\\n\\nNow assuming someone plonks down a bag of 1000 marbles with an unknown number of white marbles in it, we have the tools to answer these questions\\n\\ni) Are there fewer than 100 white marbles?\\n\\nii) Are there more than 100 white marbles?\\n\\niii) Does the bag contain 100 white marbles?\\n\\nSimply take out 100 marbles from the bag and count how many of this sample are white. \\n\\na) If there are 6 to 14 whites in the sample you cannot reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 6 through 14 will be > 0.05. \\n\\nb) If there are 5 or fewer whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 5 or fewer will be < 0.05. You would expect the bag to contain < 10% white marbles.\\n\\nc) If there are 15 or more whites in the sample you can reject the hypothesis that there are 100 white marbles in the bag and the corresponding p-values for 15 or more will be < 0.05. You would expect the bag to contain > 10% white marbles.\\n\\n*In response to Baltimark's comment*\\n\\nGiven the example above, there is an approximately:-\\n\\n4.8% chance of getter 5 white balls or fewer\\n\\n1.85% chance of 4 or fewer\\n\\n0.55% chance of 3 or fewer\\n\\n0.1% chance of 2 or fewer \\n\\n6.25% chance of 15 or more\\n\\n3.25% chance of 16 or more\\n\\n1.5% chance of 17 or more\\n\\n0.65% chance of 18 or more\\n\\n0.25% chance of 19 or more\\n\\n0.1% chance of 20 or more\\n\\n0.05% chance of 21 or more\\n\\nThese numbers were estimated from an empirical distribution generated by a simple Monte Carlo routine run in R and the resultant quantiles of the sampling distribution. \\n\\nFor the purposes of answering the original question, suppose you draw 5 white balls, there is only an approximate 4.8% chance that if the 1000 marble bag really does contain 10% white balls you would pull out only 5 whites in a sample of 100. This equates to a p value < 0.05. You now have to choose between\\n\\ni) There really are 10% white balls in the bag and I have just been ""unlucky"" to draw so few\\n\\nor\\n\\nii) I have drawn so few white balls that there can't really be 10% white balls (reject the hypothesis of 10% white balls)\\n\\n\\n\\n",Response to comment; added 8 characters in body,
788,2,363,c9566a48-96b4-4723-82bc-a17aef5e67b8,2010-07-21 00:44:08.0,74.0,"If you could go back in time and tell yourself to read a specific book at the beginning of your career as a statistician, which book would it be?",,
789,1,363,c9566a48-96b4-4723-82bc-a17aef5e67b8,2010-07-21 00:44:08.0,74.0,What is the single most influential book every statistician should read?,,
790,3,363,c9566a48-96b4-4723-82bc-a17aef5e67b8,2010-07-21 00:44:08.0,74.0,<books>,,
794,16,363,f4d3ac15-1925-41a4-acf1-61fd4d5aa18b,2010-07-21 00:54:29.0,74.0,,,
796,2,365,962d01c8-a675-491c-bb58-41df5432205d,2010-07-21 01:04:08.0,226.0,Another option is [Gnuplot][1]\\n\\n\\n  [1]: http://www.gnuplot.info/,,
797,2,366,eabd2fdf-e54e-4076-a2db-ab3a5b9decf0,2010-07-21 01:23:00.0,226.0,For a linear regression you could use a repeated median straight line fit.,,
798,2,367,ede43ea2-125c-4b68-93b6-eb0ee488a001,2010-07-21 01:35:13.0,90.0,"I am no statistician, and I haven't read that much on the topic, but perhaps \\n\\n[Lady Tasting Tea: How Statistics Revolutionized Science in the Twentieth Century][1]\\n\\nshould be mentioned? It is no textbook, but still worth reading.\\n\\n\\n  [1]: http://www.amazon.com/Lady-Tasting-Tea-Statistics-Revolutionized/dp/0805071342",,
799,16,367,ede43ea2-125c-4b68-93b6-eb0ee488a001,2010-07-21 01:35:13.0,-1.0,,,
800,2,368,7dc89390-3dcb-4d99-8edc-12411936cae1,2010-07-21 03:01:00.0,,"Suppose that the text has N words and that you require that an ASR should correctly predict at least 95% of words in the text. You currently have the observed error rate for the two methods. You can perform two type of tests.\\n\\nTest 1: Do the ASR models meet your criteria of 95% prediction?\\n\\nTest 2: Are the two ASR models equally good in speech recognition?\\n\\nYou could make different type of assumptions regarding the data generating mechanism for your ASR models. The simplest, although a bit naive, would assume that word detection of each word in the text is an iid bernoulli variable.\\n\\nUnder the above assumption you could do a test of proportions where you check if the error rate for each model is consistent with a true error rate of 5% (test 1) or a test of difference in proportions where you check if the error rates between the two models is the same (test 2). ",,user28
801,6,288,c44aa6fd-7660-4ccd-af2d-34c65915879c,2010-07-21 03:10:58.0,,<estimation><beta-binomial>,edited tags,user28
802,6,50,69ea4103-b11a-43ff-a627-1245cd183866,2010-07-21 03:11:45.0,,<fundamentals><random-variable>,edited tags,user28
803,6,75,a4fc2a77-67df-47c7-9ee3-773c7a647022,2010-07-21 03:12:51.0,,<r><books><code>,edited tags,user28
804,2,369,90cc17b6-1510-44b7-bc48-30ddf7bbe962,2010-07-21 03:47:17.0,191.0,"Say I've got a program that monitors a news feed and as I'm monitoring it I'd like to discover when a bunch of stories come out with a particular keyword in the title. Ideally I want to know when there are an unusual number of stories clustered around one another.\\n\\nI'm entirely new to statistical analysis and I'm wondering how you would approach this problem. How do you select what variables to consider? What characteristics of the problem affect your choice of an algorithm? Then, what algorithm do you choose and why?\\n\\nThanks, and if the problem needs clarification please let me know.",,
805,1,369,90cc17b6-1510-44b7-bc48-30ddf7bbe962,2010-07-21 03:47:17.0,191.0,Working through a clustering problem,,
806,3,369,90cc17b6-1510-44b7-bc48-30ddf7bbe962,2010-07-21 03:47:17.0,191.0,<clustering><analysis>,,
807,2,370,eb88f174-996f-43b8-af6b-55d50c6e9bcc,2010-07-21 03:48:55.0,159.0,Here are two to put on the list:\\n\\n[Tufte. The visual display of quantitative information][1]<br>\\n[Tukey. Exploratory data analysis][2]\\n\\n\\n  [1]: http://www.amazon.com/dp/096139210X?tag=prorobjhyn-20&linkCode=ur2&camp=1789&creative=9325\\n  [2]: http://www.amazon.com/dp/0201076160?tag=prorobjhyn-20&linkCode=ur2&camp=1789&creative=9325,,
808,16,370,eb88f174-996f-43b8-af6b-55d50c6e9bcc,2010-07-21 03:48:55.0,-1.0,,,
810,2,371,780ca46f-632c-499c-89f4-a6c304ce4f9b,2010-07-21 04:19:00.0,34.0,[Probability Theory: The Logic of Science][1]\\n\\n\\n  [1]: http://www.amazon.com/Probability-Theory-Logic-Science-Vol/dp/0521592712,,
811,16,371,780ca46f-632c-499c-89f4-a6c304ce4f9b,2010-07-21 04:19:00.0,-1.0,,,
812,2,372,cf02cf90-0c32-483e-8916-f974ae8c29c2,2010-07-21 04:26:07.0,252.0,What topics in Statistics are most useful/relevant to Data Mining? ,,
813,1,372,cf02cf90-0c32-483e-8916-f974ae8c29c2,2010-07-21 04:26:07.0,252.0,What are the key statistical concepts that relate to data-mining?,,
814,3,372,cf02cf90-0c32-483e-8916-f974ae8c29c2,2010-07-21 04:26:07.0,252.0,<data-mining><cart><probability>,,
815,2,373,b36bb39e-e988-4bb2-803a-ad9d38ad8143,2010-07-21 04:30:50.0,252.0,"From Wikipedia :\\n\\n> Suppose you're on a game show, and\\n> you're given the choice of three\\n> doors: Behind one door is a car;\\n> behind the others, goats. You pick a\\n> door, say No. 1, and the host, who\\n> knows what's behind the doors, opens\\n> another door, say No. 3, which has a\\n> goat. He then says to you, ""Do you\\n> want to pick door No. 2?"" Is it to\\n> your advantage to switch your choice?\\n\\nThe answer is, of course, yes - but it's incredibly un-inituitive. What misunderstanding do most people have about probability that leads to us scratching our heads -- or better put; what general rule can we take away from this puzzle to better train our intuition in the future?\\n",,
816,1,373,b36bb39e-e988-4bb2-803a-ad9d38ad8143,2010-07-21 04:30:50.0,252.0,The Monty Hall Problem - where does our intuition fail us?,,
817,3,373,b36bb39e-e988-4bb2-803a-ad9d38ad8143,2010-07-21 04:30:50.0,252.0,<probability><games>,,
818,2,374,a335d476-cfa7-4155-842f-048af06993a2,2010-07-21 04:32:42.0,252.0,"What are pivot tables, and how can they be helpful in analyzing data?",,
819,1,374,a335d476-cfa7-4155-842f-048af06993a2,2010-07-21 04:32:42.0,252.0,"What are pivot tables, and how can they be helpful in analyzing data?",,
820,3,374,a335d476-cfa7-4155-842f-048af06993a2,2010-07-21 04:32:42.0,252.0,<analysis><pivot-table><pivots>,,
821,2,375,b3dfe5fb-66bc-4b9d-b483-9345aa7c9d5c,2010-07-21 04:34:45.0,252.0,"What's a good way to test a series of numbers to see if they're random (or at least psuedo-random)? Is there a good statistical measure of randomness that can be used to determine how random a set is?\\n\\nMore importantly, how can one *prove* a method of generating numbers is psuedo-random?",,
822,1,375,b3dfe5fb-66bc-4b9d-b483-9345aa7c9d5c,2010-07-21 04:34:45.0,252.0,Testing (and proving) the randomness of numbers,,
823,3,375,b3dfe5fb-66bc-4b9d-b483-9345aa7c9d5c,2010-07-21 04:34:45.0,252.0,<random><psuedo-random><proof>,,
824,2,376,3a8cfb81-9e65-4d6d-99cd-4837ef7d5361,2010-07-21 04:48:50.0,61.0,"There's no reason you couldn't try to minimize norms other than x^2, there have been entire books written on quantile regression, for instance, which is more or less minimizing |x| if you're working with the median.  It's just generally harder to do and, depending on the error model, may not give good estimators (depending on whether that means low-variance or unbiased or low MSE estimators in the context).  \\n\\nAs for why we prefer integer moments over real-number-valued moments, the main reason is likely that while integer powers of real numbers always result in real numbers, non-integer powers of negative real numbers create complex numbers, thus requiring the use of an absolute value.  In other words, while the 3rd moment of a real-valued random variable is real, the 3.2nd moment is not necessarily real, and so causes interpretation problems.\\n\\nOther than that...\\n\\n 1. Analytical expressions for the integer moments of random variables are typically much easier to find than real-valued moments, be it by generating functions or some other method.  Methods to minimize them are thus easier to write.\\n 2. The use of integer moments leads to expressions that are more tractable than real-valued moments.\\n 3. I can't think of a compelling reason that (for instance) the 1.95th moment of the absolute value of X would provide better fitting properties than (for instance) the 2nd moment of X, although that could be interesting to investigate\\n 4. Specific to the L2 norm (or squared error), it can be written via dot products, which can lead to vast improvements in speed of computation.  It's also the only Lp space that's a Hilbert space, which is a nice feature to have.",,
825,2,377,eed2bbf0-5eb0-465e-a6f3-5854425ab88c,2010-07-21 05:45:43.0,173.0,"This doesn't give a general rule, but I think that one reason why it's a challenging puzzle is that our intuition doesn't handle conditional probability very well. There are plenty of [other probability puzzles that play on the same phenomenon][1]. Since I'm linking to my blog, here's [a post specifically on Monty Hall][2].\\n\\n\\n  [1]: http://www.stubbornmule.net/2010/06/probability-paradoxes/\\n  [2]: http://www.stubbornmule.net/2010/06/monty-hall/",,
826,2,378,85da83da-5d09-4924-916f-30e31443ee02,2010-07-21 05:54:25.0,68.0,"Consider two simple variations of the problem: \\n\\n 1. No doors are opened for the contestant. The host offers no help in picking a door. In this case it is obvious that the odds of picking the correct door are 1/3.\\n 2. Before the contestant is asked to venture a guess, the host opens a door and reveals a goat. After the host reveals a goat, the contestant has to pick the car from the two remaining doors. In this case it is obvious that the odds of picking the correct door is 1/2.\\n\\nFor a contestant to know the probability of his door choice being correct, he has to know how many positive outcomes are available to him and divide that number by the amount of possible outcomes. Because of the two simple cases outlined above, it is very natural to think of all the possible outcomes available as the number of doors to choose from, and the amount of positive outcomes as the number of doors that conceal a car. Given this intuitive assumption, even if the host opens a door to reveal a goat *after* the contestant makes a guess, the probability of either door containing a car remains 1/2.\\n\\nIn reality, probability recognizes a set of possible outcomes larger than the three doors and it recognizes a set of positive outcomes that is larger than the singular door with the car. In the correct analysis of the problem, the host provides the contestant with new information making a new question to be addressed: what is the probability that my original guess is such that the new information provided by the host is sufficient to inform me of the correct door? In answering this question, the set of positive outcomes and the set of possible outcomes are not tangible doors and cars but rather abstract arrangements of the goats and car. The three possible outcomes are the three possible arrangements of two goats and one car behind three doors. The two positive outcomes are the two possible arrangements where the first guess of the contestant is false. In each of these two arrangements, the information given by the host (one of the two remaining doors is empty) is sufficient for the contestant to determine the door that conceals the car.\\n\\n**In summation:**\\n\\nWe have a tendency to look for a simple mapping between physical manifestations of our choices (the doors and the cars) and the number of possible outcomes and desired outcomes in a question of probability. This works fine in cases where no new information is provided to the contestant. However, if the contestant is provided with more information (ie one of the doors you didn't choose is certainly not a car), this mapping breaks down and the correct question to be asked is found to be more abstract.",,
827,2,379,185268b0-8686-4623-a8b4-50f0a49661c4,2010-07-21 06:09:43.0,190.0,"A pivot-table is a tool to dynamically show a slice and group multivariate data in tabular form.\\n\\nFor example, when we have the following data structure\\n\\n    Region  Year  Product  Sales \\n    US      2008  Phones   125 \\n    EU      2008  Phones   352 \\n    US      2008  Mouses   52 \\n    EU      2008  Mouses   65 \\n    US      2009  Phones   140 \\n    EU      2009  Phones   320 \\n    US      2009  Mouses   60 \\n    EU      2009  Mouses   100\\n\\nA pivot table can for example display a table with the sum of all products with in the rows the years and in the columns the regions. \\n\\nAll dimensions of the table can be switched easily. Also the data fields shown can be changed. This is called pivoting.\\n\\nThe tool is useful in exploratory data analyses. Because it is a dynamic tool, it can be used to visually detect patterns and outliers etc.\\n\\n\\nMost spreadsheet applications have support for this kind of tables.",,
828,6,374,0121e3d0-37de-49f4-a931-7c965c8f0fc6,2010-07-21 06:10:08.0,190.0,<analysis><pivot-table>,edited tags,
829,2,380,b03e85fd-8b4a-4de8-9253-e39771e1022a,2010-07-21 06:15:36.0,88.0,"You cannot prove, because it is impossible; you can only check if there are no any embarrassing autocorrelations or distribution disturbances, and indeed [Diehard][1] is a standard for it. This is for statistics/physics, cryptographers will also mainly check (among other things) how hard is it to fit the generator to the data to obtain the future values.  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Diehard_tests",,
830,5,212,c81677f0-f635-4e03-a206-f6e1b8a48b54,2010-07-21 06:19:29.0,190.0,"I have 2 ASR (Automatic Speech Recognition) models, providing me with text transcriptions for my testdata. The error measure I use is Word Error Rate.\\n\\nWhat methods do I have to test for statistical significance of my new results?\\n\\n**An example:**\\n\\nI have an experiment with 10 speaker, all having 100 (the same) sentences, total 900 words per speaker. Method A has an WER (word error rate) of 19.0%, Method B 18.5%.\\n\\nHow do I test whether Method B is significantly better?",Added an example,
831,6,212,c81677f0-f635-4e03-a206-f6e1b8a48b54,2010-07-21 06:19:29.0,190.0,<statistical-significance>,Added an example,
832,2,381,a0f69f88-9fb5-4f71-9fef-fcedf0b9554f,2010-07-21 06:22:51.0,223.0,"Understanding **multivariate normal distribution** http://en.wikipedia.org/wiki/Multivariate_normal_distribution is important. \\n\\nThe concept of **correlation** and more generally (non linear) dependence is important. \\n\\n**Concentration of measure, asymptotic normality, convergence of random variables**.... how to make something from random to deterministic! http://en.wikipedia.org/wiki/Convergence_of_random_variables\\n\\n**maximum likelihood estimation** http://en.wikipedia.org/wiki/Maximum_likelihood and before that, statistical modeling :) and more generally minimum contrast estimation. \\n\\n**stationary process** http://en.wikipedia.org/wiki/Stationary_process and more generally stationnarity assumption and ergodic property. \\n\\nas Peter said, the question is so broad ... that the answer couldn't be given in a post ... ",,
833,6,375,c977ab3e-7b81-4b1c-a60e-a890e31a2f49,2010-07-21 06:26:45.0,103.0,<random-generation><random><proof>,edited tags,
834,2,382,a6c9ef59-b29c-47cc-8e68-93936f22c7e8,2010-07-21 06:28:23.0,190.0,Firstly I can recommend you the book [Foundations of statistical natural language processing][1] by Manning and Schütze.\\n\\nThe methods I would use are word-frequency distributions and ngram language models. The first works very well when you want to classify on topic and your topics are specific and expert (having keywords). Ngram modelling is the best way when you want to classify writing styles etc. \\n\\n\\n\\n\\n  [1]: http://nlp.stanford.edu/fsnlp/,,
835,2,383,5b483fbf-b2cb-48f8-97e6-e1f3f10b4b01,2010-07-21 06:28:43.0,251.0,"You might be interested in applying relative distribution methods.  Call one group the reference group, and the other the comparison group.  In a way similar to constructing a probability-probability plot, you can construct a relative CDF/PDF, which is a ratio of the densities.  This relative density can be used for inference.  If the distributions are identical, you expect a uniform relative distribution.  There are tools, graphical and statistical, to explore and examine departures from uniformity.  \\n\\nA good starting point to get a better sense is [Applying Relative Distrbution Methods in R][1] and the [reldist][2] package in R.  For details, you'll need to refer to the book, [Relative Distribution Methods in the Social Sciences][3] by Handcock and Morris.  There's also a [paper][4] by the authors covering the relevant techniques.\\n\\n\\n  [1]: http://www.csss.washington.edu/Papers/wp27.pdf\\n  [2]: http://cran.r-project.org/web/packages/reldist/index.html\\n  [3]: http://csde.washington.edu/~handcock/RelDist/\\n  [4]: http://www.jstor.org/pss/270964\\n",,
836,6,124,3d32f10e-8abb-4c48-baa9-6c10cb98a958,2010-07-21 06:29:15.0,190.0,<classification><text><information-retrieval>,edited tags,
837,2,384,cfb0cfc5-a070-4fea-9dbb-44642d63a97f,2010-07-21 06:33:36.0,190.0,"I would start with a frequency distribution. Collect for a big corpus the word-frequencies and select smartly the words that are keywords (not misspellings, with a very low frequency, and not stop words like ""and"", ""or"")\\n\\nThen when you have a number of new feeds, compare their distribution with the distribution that you build from your training data. Look to the big differences in frequencies and select so the important keywords of that moment.",,
838,2,385,07b1a40e-7643-4eca-90df-b03f71f0b055,2010-07-21 06:39:38.0,190.0,"In principal unbalanced classes are not a problem at all for the k-nearest neighbor algorithm. \\n\\nBecause the algorithm is totally not influenced by the size of the class, it will not favor any on size. Try to run k-means with an obvious outlier and k+1 and you will see that most of the time the outlier will get it's own class.\\n\\nOff course, with hard datasets it is always advisable to run the algorithm multiple times. This to avoid trouble because of a bad initialization.",,
839,2,386,ca2fbaa3-853c-4658-b034-18a91f334437,2010-07-21 06:46:11.0,223.0,"I would do some sort of ""leave one out testing algorithm"" (n is the number of data):\\n\\nfor i=1 to n\\n\\n 1. **compute a density estimation of the data set obtained by throwing Xi away**. (This density estimate should be done with some assumption if the dimension is high, for example, a gaussian assumption for which the density estimate is easy: mean and covariance)\\n 2. **Calculate the likelihood of Xi for the density estimated in step 1**. call it Li. \\n\\nend for \\n\\nsort the Li (for i=1,..,n) and use a multiple hypothesis testing procedure to say which are not good ... \\n \\nThis will work if n is sufficiently large... you can also use ""leave k out strategy"" which can be more relevent when you have ""groups"" of outliers ...",,
840,6,341,0ebae9d1-70b6-458c-81de-2d26da43ed16,2010-07-21 06:50:32.0,190.0,<k-nearest-neighbour><unbalanced-classes>,edited tags,
841,2,387,94a04ab7-911a-4c82-a9ec-38fdfe461271,2010-07-21 07:04:26.0,69.0,"The following list contains many data sets you may be interested:\\n\\n - [America's Best Colleges - U.S. News & World Reports][1]\\n - [American FactFinder][2]\\n - [The Baseball Archive][3]\\n - [The Bureau of Justice Statistics][4]\\n - [The Bureau of Labor Statistics][5]\\n - [The Bureau of Transportation Statistics][6]\\n - [The Census Bureau][7]\\n - [Data and Story Library (DASL)][8]\\n - [Data Sets, UCLA Statistics Department][9]\\n - [DIG Stats][10]\\n - [Economic Research Service, US Department of Agriculture][11]\\n - [Energy Information Administration][12]\\n - [Eurostat][13]\\n - [Exploring Data][14]\\n - [FedStats][15]\\n - [The Gallop Organization][16]\\n - [International Fuel Prices][17]\\n - [Journal of Statistics Education Data Archive][18]\\n - [Kentucky Derby Race Statistics][19]\\n - [National Center for Education Statistics][20]\\n - [National Center for Health Statistics][21]\\n - [National Climatic Data Center][22]\\n - [National Geophysical Data Center][23]\\n - [National Oceanic and Atmospheric Administration][24]\\n - [Sports Data Resources][25]\\n - [Statistics Canada][26]\\n - [StatLib---Datasets Archive][27]\\n - [UK Government Statistical Service][28]\\n - [United Nations: Cyber SchoolBus Resources][29]\\n\\n\\n  [1]: http://www.usnews.com/usnews/edu/college/rankings/rankindex_brief.php\\n  [2]: http://factfinder.census.gov/servlet/BasicFactsServlet\\n  [3]: http://www.baseball1.com/\\n  [4]: http://www.albany.edu/sourcebook/\\n  [5]: http://www.bls.gov/\\n  [6]: http://www.bts.gov/\\n  [7]: http://www.census.gov/\\n  [8]: http://lib.stat.cmu.edu/DASL/\\n  [9]: http://www.stat.ucla.edu/data/\\n  [10]: http://www.cvgs.k12.va.us/DIGSTATS/\\n  [11]: http://www.ers.usda.gov/Briefing/\\n  [12]: http://www.eia.doe.gov/index.html\\n  [13]: http://epp.eurostat.ec.europa.eu/\\n  [14]: http://exploringdata.cqu.edu.au/\\n  [15]: http://www.fedstats.gov/\\n  [16]: http://www.gallup.com/\\n  [17]: http://www.gtz.de/en/themen/umwelt-infrastruktur/transport/10285.htm\\n  [18]: http://www.amstat.org/publications/jse/\\n  [19]: http://www.kentuckyderby.com/2003/derby_history/derby_statistics/\\n  [20]: http://www.ed.gov/NCES/\\n  [21]: http://www.cdc.gov/nchs/\\n  [22]: http://www.ncdc.noaa.gov/oa/ncdc.html\\n  [23]: http://www.ngdc.noaa.gov/\\n  [24]: http://www.noaa.gov/\\n  [25]: http://www.amstat.org/sections/SIS/Sports%20Data%20Resources/\\n  [26]: http://www.statcan.ca/start.html\\n  [27]: http://lib.stat.cmu.edu/datasets/\\n  [28]: http://www.statistics.gov.uk/\\n  [29]: http://www.un.org/cyberschoolbus/index.asp",,
842,16,387,94a04ab7-911a-4c82-a9ec-38fdfe461271,2010-07-21 07:04:26.0,-1.0,,,
843,2,388,863bafc1-8aca-46e1-932c-a78d17815dc0,2010-07-21 07:15:54.0,251.0,On the math/foundations side: Harald Cramér's [Mathematical Methods of Statistics][1]. \\n\\n  [1]: http://www.amazon.com/Mathematical-Methods-Statistics-Harald-Cramer/dp/0691005478/\\n,,
844,16,388,863bafc1-8aca-46e1-932c-a78d17815dc0,2010-07-21 07:15:54.0,-1.0,,,
845,2,389,9194ec8d-f652-4682-a8b3-4157cb6ed11b,2010-07-21 07:51:34.0,199.0,"I do think there is something to be said for just excluding the outliers. A regression line is supposed to summarise the data. Because of leverage you can have a situation where 1% of your data points affects the slope by 50%.\\n\\nIt's only dangerous from a moral and scientific point of view if you don't tell anybody that you excluded the outliers. As long as you point them out you can say:\\n\\n""This regression line fits pretty well for most of the data. 1% of the time a value will come along that doesn't fit this trend, but hey, it's a crazy world, no system is perfect""",,
847,5,379,6ce6c95e-f1e7-44df-ab58-30ee7bac1ac9,2010-07-21 08:00:10.0,190.0,"A pivot-table is a tool to dynamically show a slice and group multivariate data in tabular form.\\n\\nFor example, when we have the following data structure\\n\\n    Region  Year  Product  Sales \\n    US      2008  Phones   125 \\n    EU      2008  Phones   352 \\n    US      2008  Mouses   52 \\n    EU      2008  Mouses   65 \\n    US      2009  Phones   140 \\n    EU      2009  Phones   320 \\n    US      2009  Mouses   60 \\n    EU      2009  Mouses   100\\n\\nA pivot table can for example display a table with the sum of all products with in the rows the years and in the columns the regions. \\n\\nAll dimensions of the table can be switched easily. Also the data fields shown can be changed. This is called pivoting.\\n\\nThe tool is useful in exploratory data analyses. Because it is a dynamic tool, it can be used to visually detect patterns and outliers etc.\\n\\n\\nMost spreadsheet applications have support for this kind of tables.\\n\\nAn image from wikipedia: ![http://upload.wikimedia.org/wikipedia/commons/6/63/Pivottable-Pivoted.PNG][1]\\n\\n\\n  [1]: http://upload.wikimedia.org/wikipedia/commons/6/63/Pivottable-Pivoted.PNG",added 194 characters in body,
848,2,390,1c70b9fa-cacc-4754-adb3-371531da11ea,2010-07-21 08:01:54.0,74.0,"Sweet, a good list so far.\\n\\nI would say Fooled By Randomness by Taleb, and Statistical Rules of Thumb by van Belle myself.",,
849,16,390,1c70b9fa-cacc-4754-adb3-371531da11ea,2010-07-21 08:01:54.0,-1.0,,,
852,16,372,882037f9-679f-4db1-9ff0-1970ddac02f7,2010-07-21 08:05:16.0,252.0,,,
853,6,372,882037f9-679f-4db1-9ff0-1970ddac02f7,2010-07-21 08:05:16.0,252.0,<data-mining><probability><cart>,edited tags,
854,2,392,9981dae4-bedd-459c-aea5-00ca5e2ccb15,2010-07-21 08:19:36.0,251.0,"Take a look at the sample galleries for three popular visualization libraries:\\n\\n- [matplotlib gallery][1] (Python)\\n- [R graph gallery][2] (R) -- (also see [ggplot2][3], scroll down to reference)\\n- [prefuse visualization gallery][4] (Java)\\n\\nFor the first two, you can even view the associated source code -- the simple stuff is simple, not many lines of code.  The prefuse case will have the requisite Java boilerplate code.  All three support a number of backends/devices/renderers (pdf, ps, png, etc).  All three are clearly capable of high quality graphics.\\n\\nI think it pretty much boils down to which language are you most comfortable working in.  Go with that.\\n\\n  [1]: http://matplotlib.sourceforge.net/gallery.html\\n  [2]: http://addictedtor.free.fr/graphiques/\\n  [3]: http://had.co.nz/ggplot2/\\n  [4]: http://prefuse.org/gallery/\\n",,
855,5,154,fc15042b-738d-460b-a4dd-34b2e25c4312,2010-07-21 08:37:04.0,108.0,"I am currently researching the *trial roulette method* for my masters thesis as an elicitation technique. This is a graphical method that allows an expert to represent her subjective probability distribution for an uncertain quantity.\\n\\nExperts are given counters (or what one can think of as casino chips) representing equal densities whose total would sum up to 1 - for example 20 chips of probability = 0.05 each. They are then instructed to arrange them on a pre-printed grid, with bins representing result intervals. Each column would represent their belief of the probability of getting the corresponding bin result.\\n\\nExample: A student is asked to predict the mark in a future exam. The\\nfigure below shows a completed grid for the elicitation of\\na subjective probability distribution. The horizontal axes of the\\ngrid shows the possible bins (or mark intervals) that the student was\\nasked to consider. The numbers in top row record the number of chips\\nper bin. The completed grid (using a total of 20 chips) shows that the\\nstudent believes there is a 30% chance that the mark will be between\\n60 and 64.9.\\n\\n![Eliciting priors from experts - Trial Roulette Method][1]\\n\\n\\n  [1]: http://img641.imageshack.us/img641/4716/chipsbinscrisp.png\\n\\n\\nSome reasons in favour of using this technique are:\\n\\n1. Many questions about the shape of the expert's subjective probability distribution can be answered without the need to pose a long series of questions to the expert - the statistician can simply read off density above or below any given point, or that between any two points. \\n\\n2. During the elicitation process, the experts can move around the chips if unsatisfied with the way they placed them initially - thus they can be sure of the final result to be submitted. \\n\\n3. It forces the expert to be coherent in the set of probabilities that are provided. If all the chips are used, the probabilities must sum to one.\\n\\n4. Graphical methods seem to provide more accurate results, especially for participants with modest levels of statistical sophistication.\\n",added 639 characters in body,
857,2,393,e87f1844-158c-4d49-a16c-ad10f34aded2,2010-07-21 09:02:11.0,223.0,"The book from hastie, Tibshirani and Friedman http://www-stat.stanford.edu/~tibs/ElemStatLearn/ should be in any statistician's library ! ",,
858,16,393,e87f1844-158c-4d49-a16c-ad10f34aded2,2010-07-21 09:02:11.0,-1.0,,,
860,5,358,afe6d715-a587-48af-8c8e-c1eb98141c83,2010-07-21 09:31:29.0,88.0,"We try to minimize the variance that is left within descriptors. Why variance? Read [this question][1]; this also comes together with the (mostly silent) assumption that errors are normally distributed.\\n\\n**Extension:**  \\nTwo additional arguments:  \\n\\n 1. For variances, we have this nice ""law"" that the sum of variances is equal to the variance of sum, for uncorrelated samples. If we assume that the error is not correlated with the case, minimizing residual of squares will work straightforward to maximizing explained variance, what is maybe a not-so-good but still popular quality measure.  \\n\\n 2. If we assume normality of an error, least squares error estimator is a maximal likelihood one.\\n\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/118/standard-deviation-why-square-the-difference-instead-of-taking-the-absolute-val",added 501 characters in body,
861,2,394,dc24015f-3a84-4bc6-99ca-8ce58860ed61,2010-07-21 09:53:40.0,210.0,"My understanding is that wbecause we are trying to mimimise errors, we need to find a way of not getting ourselves in a situation where the sum of the negative difference in errors is equal to the sum of the positive difference in errors but we haven't found a good fit. We do this by squaring the sum of the difference in errors which means the negative and positive difference in errors both become positive (-1*-1 = 1). If we raised x to the power of anything other than a positve integer we wouldn't address this problem because the erros would not have the same sign, or if we raised to the power of something that isn't an integer we'd enter the realms of complex numbers. ",,
862,2,395,6cd17769-43db-46e3-a0dc-d032daaf000b,2010-07-21 10:13:25.0,210.0,"I have a data set where a series of measurements are being taken each week. In general the data set shows a +/- 1mm change each week with a mean measurement staying at about 0mm. In plotting the data this week it appears that some noticeable movement has cocured at two points and looking back at the data set, it is also possible that movement occured last week as well. \\nWhat is the best way of looking at this data set to see how likely it is that the movements that have been seen are real movements rather than just some effect caused by tthe natural tolerance in the readings.  ",,
863,1,395,6cd17769-43db-46e3-a0dc-d032daaf000b,2010-07-21 10:13:25.0,210.0,How to tell if something happened in a data set which monitors a value over time,,
864,3,395,6cd17769-43db-46e3-a0dc-d032daaf000b,2010-07-21 10:13:25.0,210.0,<variance><monitoring>,,
865,2,396,b310dcf9-3ca0-4e10-b0e4-7de98e4cdafa,2010-07-21 11:00:44.0,,"I usually make my own idiosyncratic choices when preparing plots. However, I wonder if there are any best practices for generating plots. \\n\\nNote: [Rob's][1] comment to an answer to this [question][2] is very relevant here. \\n \\n\\n\\n  [1]: http://stats.stackexchange.com/users/159/rob-hyndman\\n  [2]: http://stats.stackexchange.com/questions/257/what-is-the-easiest-way-to-create-publication-quality-plots-under-linux",,user28
866,1,396,b310dcf9-3ca0-4e10-b0e4-7de98e4cdafa,2010-07-21 11:00:44.0,,What best practices should I follow when preparing plots?,,user28
867,3,396,b310dcf9-3ca0-4e10-b0e4-7de98e4cdafa,2010-07-21 11:00:44.0,,<plotting><best-practices>,,user28
868,2,397,10cda2ca-6899-4347-a3c9-2f983426c816,2010-07-21 11:11:47.0,8.0,"Just to add a bit to honk's answer, the [Diehard Test Suite][1] (developed by George Marsaglia) are the standard tests for PRNG.\\n\\nThere's a nice [Diehard C library][2] that gives you access to these tests. As well as the standard Diehard tests it also provides functions for a few other PRNG tests involving (amongst other things) checking bit order. There is also a facilty for testing the speed of the RNG and writing your own tests.\\n\\nThere is a R interface to the Diehard library, called [RDieHarder][3]. However, I couldn't get this to work.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Diehard_tests\\n  [2]: http://www.phy.duke.edu/~rgb/General/dieharder.php\\n  [3]: http://dirk.eddelbuettel.com/code/rdieharder.html",,
869,5,390,84042f91-3a99-4d03-9040-298a3f3494a7,2010-07-21 11:15:14.0,190.0,"\\nI would say Fooled By Randomness by Taleb, and Statistical Rules of Thumb by van Belle myself.",deleted 28 characters in body,
870,2,398,cc17e688-715a-42fd-b25a-8460dff2c0c2,2010-07-21 11:16:55.0,5.0,"We could stay here all day denoting best practices, but you should start by reading Tufte.  My primary recommendation:\\n\\n**Keep it simple.** \\n\\nOften people try to load up their charts with information. But you should really just have one main idea that you're trying to convey and if someone doesn't get your message almost immediately, then you should rethink how you have presented it.  So don't start working on your chart until the message itself is clear. Occam's razor applies here too. ",,
871,2,399,23b03306-3409-4181-a2e0-efd25569b909,2010-07-21 11:18:56.0,190.0,The Tufte principles are very good practices when preparing plots. See also his book [Beautiful Evidence][1]\\n\\nThe principles include:\\n\\n- Keep a high data-ink ratio\\n- Remove chart junk\\n- Give graphical element multiple functions\\n- Keep in mind the data density\\n\\nThe term to search for is Information Visualization\\n\\n\\n  [1]: http://www.amazon.com/Beautiful-Evidence-Edward-R-Tufte/dp/0961392177/ref=ntt_at_ep_dpt_2,,
872,2,400,6b7ab56d-24d2-4146-a731-96a8df2c6557,2010-07-21 11:20:45.0,210.0,"One rule of thumb that I don't always follow but which is on occiasion useful is to take into accoutn that it is likely that your plot will be sent by fax at some point in it's future. You need to try and make your plots clear enough that even if they are sent by fax at some point in the future, the information the plot is trying to convey is still legible. ",,
873,2,401,104923de-7f0f-4660-acec-073787ce5590,2010-07-21 12:22:21.0,256.0,"this problem you are asking about is known as test mining!\\n\\nthere are a few things you need to consider.. for example in your question you mentioned using keywords in titles.. one may ask why not the text in the article rather than just the title? which brings me to the first consideration.... what data do you limit yourself to?\\n\\nSecondly as the previous answer suggests using frequencies is a great start.... to take the analysis further you may start looking at what words occur frequently together! for example the word happy may occur very frequently... however if always accompanied with a ""not"" your conclusions would be very different!\\n\\nThere is a very nice Australian piece of software i have used in the past called Leximancer i would advise anybody nterested in text mining to have a look at their site and the examples they have.... from memory one of which analysed speaches by 2 US president candidant it makes for some very interesting reading!",,
874,2,402,5cd70f27-73fb-4958-8fe6-fe604cbbfff8,2010-07-21 12:30:43.0,226.0,You might consider applying a [Tukey Control chart][1] to the data.\\n\\n\\n  [1]: http://gunston.gmu.edu/708/frTukey.asp,,
875,2,403,51835900-5569-4c1e-9fa9-5058b94e3807,2010-07-21 12:33:43.0,256.0,I must agree.. there is no single best analysis!\\nnot just in cross tabulations or analysis of categorical data but in any data analysis... and thank god for that!\\nif there was just a single best way to address these analyses well many of us would not have a job to start with... not to mention the loss of the thrill of the hunt!\\n\\nthe joy of analysis is the unknown and the search for answers and evidence and how one question leads to another... that is what i love about statistics!\\n\\nSo back to the categorical data analysis... it really depends on what your doing. Are you looking to find how different variables affect each other as in drug tests for example we may look at treatment vs placebo crossed with disease and no disease... the question here is does treatment reduce disease.... chi square usually does well here (given a good sample size).\\nAnother context ihad today was looking at missing value trends... i was looking to find if missing values in one categorical variable relate to another... in some cases i knew the result should be missing and yet there were observations that had values... a completely different context to the drug test!,,
876,2,404,9e48bae4-a6bc-4cf2-a5d0-6785f71387ac,2010-07-21 12:37:31.0,46.0,"You might want to have a look at [strucchange][1]: \\n\\n> Testing, monitoring and dating structural changes in (linear) regression models. strucchange features tests/methods from the generalized fluctuation test framework as well as from the F test (Chow test) framework. This includes methods to fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM, recursive/moving estimates) and F statistics, respectively. It is possible to monitor incoming data online using fluctuation processes. Finally, the breakpoints in regression models with structural changes can be estimated together with confidence intervals. Emphasis is always given to methods for visualizing the data.""\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/strucchange/index.html\\n\\nPS. Nice graphics ;)",,
877,2,405,b9ed5129-899e-4601-b43d-ef2ce2907f75,2010-07-21 12:38:51.0,190.0,"What kind of movement are we talking about?\\n\\nYou could of course fit a distribution over your data and see whether the new weeks fit in this distribution or are in the tail of it (which means it is likely something significant, real that you are observing)\\n\\nHowever, more information from your side would be helpful. Maybe you could provide a part of the dataset?",,
878,6,396,7f0055e9-f04b-4fca-9a2e-29bb5e9438a5,2010-07-21 12:42:45.0,190.0,<plotting><best-practices><information-visualizatio>,edited tags,
879,2,406,11d2e58a-e320-4863-94fa-e6ae221213d8,2010-07-21 12:43:00.0,256.0,I disagree with this question as it suggests that machine learning and statistics are different or conflicting sciences.... when the opposite is true!\\n\\nmachine learning makes extensive use of statistics... a quick survey of any Machine learning or data mining software package will reveal Clustering techniques such as k-means also found in statistics.... will also show dimension reduction techniques such as Principal components analysis also a statistical technique... even logistic regression yet another.\\n\\nIn my view the main difference is that traditionally statistics was used to proove a pre conceived theory and usually the analysis was design around that principal theory. Where with data mining or machine learning the opposite approach is usually the norm in that we have the outcome we just want to find a way to predict it rather than ask the question or form the theory is this the outcome!,,
880,2,407,b1eb98ed-96cd-4c13-9e01-87b58290f0f0,2010-07-21 12:59:00.0,256.0,One of the above answers touched in mahalanobis distances.... perhaps anpther step further and calculating simultaneous confidence intervals would help detect outliers!,,
881,2,408,2d2efe75-15b0-4e9f-804e-63348e4673cd,2010-07-21 13:01:39.0,56.0,"In addition to conveying a clear message I always try to remember the plotsmanship:\\n\\n* font sizes for labels and legends should be big enough, preferably the same font size and font used in the final publication.\\n* linewidths should be big enough (1 pt lines tend to disappear if plots are shrunk only slightly). I try to go to linewidths of 3 to 5 pt.\\n* if plotting multiple datasets/curves with color make sure that they can be understood if printed in black-and-white, e.g. by using different symbols or linestyles in addition to color.\\n* always use a lossless (or close to lossless) format, e.g. a vector format like pdf, ps or svg or high resolution png or gif (jpeg doesn't work at all and was never designed for line art).\\n* prepare graphics in the final aspect ratio to be used in the publication. Changing the aspect ration later can give irritating font or symbol shapes.\\n* always remove useless clutter from the plotting program like unused histogram information, trend lines (hardly useful) or default titles.\\n\\nI have configured my plotting software (matplotlib, ROOT or root2matplotlib) to do most of this right by default. Before I was using `gnuplot` which needed extra care here.",,
882,2,409,f8a693cc-da1b-44c4-880b-671cfe44b986,2010-07-21 13:11:33.0,219.0,At the moment I use standard deviation of the mean to estimate uncertainty:\\n\\n![definition = stdev over sq.root of N](http://upload.wikimedia.org/math/8/a/e/8ae657bf495238caf69d82d754e70ed8.png)\\n\\nand then present it like this: ![mean plus-minus stdev about the mean](http://rogercortesi.com/eqn/tempimagedir/eqn1898.png) for each element in the time series.\\n\\nIs this valid? Is this appropriate for time series?,,
883,1,409,f8a693cc-da1b-44c4-880b-671cfe44b986,2010-07-21 13:11:33.0,219.0,How to approximate measurement uncertainty?,,
884,3,409,f8a693cc-da1b-44c4-880b-671cfe44b986,2010-07-21 13:11:33.0,219.0,<time-series><standard-deviation><mean><uncertainty>,,
885,2,410,a4762071-bc71-4aa9-ab17-82fa382f7d49,2010-07-21 13:20:53.0,56.0,"The answer to this question depends a lot on how your measurement uncertainty arises. If it is due to to uncorrelated normally distributed fluctuations in your measurement your measurement outcomes will also be normally distributed.\\n\\nIf this assumption is valid can be hard to prove, but plotting histograms of outcomes of independent measurement should give you a feeling for the shape (and size) of the uncertainty.\\n\\nSince the normal distribution works on a unlimited range of values, typical cases where measurements are not normally distributed are counting experiments where the number to be measured is small (e.g. less than 20) and/or fluctuations are large, or when the measured quantity is defined to be in a range, e.g. a fraction. One might still use normal approximation in a certain range though.",,
886,2,411,081c0a34-fcd2-4710-a4f3-7be4fe0015da,2010-07-21 13:39:06.0,89.0,"There are many ways to measure how similar two probability distributions are.  Among methods which are popular (in different circles) are:\\n\\n1. the Kolmogorov distance: the sup-distance between the distribution functions;\\n\\n2. the Kantorovich-Rubinstein distance: the maximum difference between the expectations w.r.t. the two distributions of functions with Lipschitz constant 1, which also turns out to be the L^1 distance between the distribution functions;\\n\\n3. the bounded-Lipschitz distance: like the K-R distance but the functions are also required to have absolute value at most 1.\\n\\nThese have different advantages and disadvantages.  Only convergence in the sense of 3. actually corresponds precisely to convergence in distribution; convergence in the sense of 1. or 2. is slightly stronger in general.  (In particular, if X_n=1/n with probability 1, then X_n converges to 0 in distribution, but not in the Kolmogorov distance. However, if the limit distribution is continuous then this pathology doesn't occur.) \\n\\nFrom the perspective of elementary probability or measure theory, 1. is very natural because it compares the probabilities of being in some set. A more sophisticated probabilistic perspective, on the other hand, tends to focus more on expectations than probabilities. Also, from the perspective of functional analysis, distances like 2. or 3. based on duality with some function space are very appealing, because there is a large set of mathematical tools for working with such things.\\n\\nHowever, my impression (correct me if I'm wrong!) is that in statistics, the Kolmogorov distance is the usually preferred way of measuring similarity of distributions.  I can guess one reason: if one of the distributions is discrete with finite support -- in particular, if it is the distribution of some real-world data -- then the Kolmogorov distance to a model distribution is easy to compute.  (The K-R distance would be slightly harder to compute, and the B-L distance would probably be impossible in practical terms.)\\n\\nSo my question (finally) is, are there other reasons, either practical or theoretical, to favor the Kolmogorov distance (or some other distance) for statistical purposes?",,
887,1,411,081c0a34-fcd2-4710-a4f3-7be4fe0015da,2010-07-21 13:39:06.0,89.0,Motivation for Kolmogorov distance between distributions,,
888,3,411,081c0a34-fcd2-4710-a4f3-7be4fe0015da,2010-07-21 13:39:06.0,89.0,<distributions><probability>,,
889,2,412,4f94620b-c1b9-4dc9-994d-9a71e44b6f34,2010-07-21 13:40:02.0,8.0,"I think you need look at statistical [control charts][1]. The most common of which are cusum and Shewhart charts.\\n\\nBasically, data arrives sequentially and is tested against a number of rules. For example,\\n\\n1. Is the data far away from the cumulative mean - say 3 standard deviations \\n1. Has the data been increasing for the last few points.\\n1. Does the data alternate between positive and negative values.\\n\\nIn R you can use the [qcc][2] package.\\n\\nFor example,\\n \\n    #Taken from the documentation\\n    library(qcc)\\n    data(orangejuice)\\n    attach(orangejuice)\\n    plot(qcc(D[trial], sizes=size[trial], type=""p""))\\n\\nGives the following plot, with possible problem points highlighted in red.\\n\\n![control chart][3]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Control_chart\\n  [2]: http://cran.r-project.org/web/packages/qcc/index.html\\n  [3]: http://img805.imageshack.us/img805/5858/tmp.jpg",,
890,5,409,96a49b7b-4935-43b4-849f-c2e05f060a02,2010-07-21 13:40:27.0,219.0,At the moment I use standard deviation of the mean to estimate uncertainty:\\n\\n![definition = stdev over sq.root of N](http://upload.wikimedia.org/math/8/a/e/8ae657bf495238caf69d82d754e70ed8.png)\\n\\nwhere *N* is in hundreds and mean is a time series (monthly) mean. I\\npresent it then like this: ![mean plus-minus stdev about the mean](http://rogercortesi.com/eqn/tempimagedir/eqn1898.png) for each element (month) in the (annual) time series.\\n\\nIs this valid? Is this appropriate for time series?,added 83 characters in body,
891,2,413,8fa466ad-5d0c-47c1-a341-bffa9222ee6f,2010-07-21 13:48:15.0,223.0,"I don't really know what the conceptual/historical difference between machine learning and statistic is but I am sure it is not that obvious... and I am not really interest in knowing if I am a machine learner or a statistician, I think 10 years after Breiman's paper, lots of people are both...\\n\\nAnyway, I found  **interesting the question about predictive accuracy of models**. We have to remember that it is not always possible to measure the accuracy of a model and more precisely we are most often implicitly making some modeling when measuring errors.\\n\\nFor Example, mean absolute error in time series forecast is a mean over time and it measures the performance of a procedure to forecast the median with the assumption that performance is, in some sense, **stationary** and shows some **ergodic** property. If (for some reason) you need to forecast the mean temperature on earth for the next 50 years and if your modeling performs well for the last 50 years... it does not means that... \\n\\nMore generally, (if I remember, it is called no free lunch) you can't do anything without modeling... In addition, I think statistic is trying to find an answer to the question : ""is something significant or not "", this is a very important question in science and can't be answered through a learning process. To state John Tukey (was he a statistician ?) :\\n\\n> *The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data* \\n\\n \\nHope this helps ! ",,
892,2,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,2010-07-21 13:50:08.0,89.0,"Can anyone recommend a good introduction to statistics for a mathematician who is already well-versed in probability?  I have two distinct motivations for asking, which may well lead to different suggestions:\\n\\n1. I'd like to better understand the statistics motivation behind many problems considered by probabilists.\\n\\n2. I'd like to know how to better interpret the results of Monte Carlo simulations which I sometimes do to form mathematical conjectures.\\n\\nI'm open to the possibility that the best way to go is not to look for something like ""Statistics for Probabilists"" and just go to a more introductory source.",,
893,1,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,2010-07-21 13:50:08.0,89.0,Intro to statistics for mathematicians,,
894,3,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,2010-07-21 13:50:08.0,89.0,<textbook>,,
895,16,414,23a48000-b33d-4d5f-9323-faf18fa1bb63,2010-07-21 13:50:08.0,89.0,,,
896,2,415,4e08983d-d95e-44dc-820a-837d9c07962d,2010-07-21 13:53:13.0,223.0,I think you should take a look to the similar post from mathoverflow at http://mathoverflow.net/questions/31655/statistics-for-mathematicians/31665#31665\\n\\nMy answer to this post was Asymptotic statistics from Van der Vaart http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521784504. ,,
897,16,415,4e08983d-d95e-44dc-820a-837d9c07962d,2010-07-21 13:53:13.0,-1.0,,,
898,2,416,ca041f71-ff72-4b07-8705-cd28d919c434,2010-07-21 14:00:03.0,215.0,"\\nThe population is the set of entities under study. For example, the mean height of men. This is a hypothetical population because it includes all men that have lived, are alive and will live in the future. I like this example because it drives home the point that we, as analysts, choose the population that we wish to study. Typically it is impossible to survey/measure the entire population because not all members are observable (e.g. men who will exist in the future). If it is possible to enumerate the entire population it is often costly to do so and would take a great deal of time. In the example above we have a population ""men"" and a parameter of interest, their height.\\n\\nInstead, we could take a subset of this population called a sample and use this sample to draw inferences about the population under study, given some conditions. Thus we could measure the mean height of men in a sample of the population which we call a statistic and use this to draw inferences about the parameter of interest in the population. It is an inference because there will be some uncertainty and inaccuracy involved in drawing conclusions about the population based upon a sample. This should be obvious - we have fewer members in our sample than our population therefore we have lost some information.\\n\\nThere are many ways to select a sample and the study of this is called sampling theory. A commonly used method is called Simple Random Sampling (SRS). In SRS each member of the population has an equal probability of being included in the sample, hence the term ""random"". There are many other sampling methods e.g. stratified sampling, cluster sampling, etc which all have their advantages and disadvantages.\\n\\nIt is important to remember that the sample we draw from the population is only one from a large number of potential samples. If ten researchers were all studying the same population, drawing their own samples then they may obtain different answers. Returning to our earlier example, each of the ten researchers may come up with a different mean height of men i.e. the statistic in question (mean height) varies of sample to sample -- it has a distribution called a sampling distribution. We can use this distribution to understand the uncertainty in our estimate of the population parameter.\\n\\nThe sampling distribution of the sample mean is known to be a normal distribution with a standard deviation equal to the sample standard deviation divided by the sample size. Because this could easily be confused with the standard deviation of the sample it more common to call the standard deviation of the sampling distribution the **standard error**.\\n\\n",,
899,2,417,3f950b8d-69ac-4f00-a8f1-3e4b3a78d0aa,2010-07-21 14:01:46.0,251.0,"[Mathematical Methods of Statistics][1], Harald Cramér is really great if you're coming to Statistics from the mathematical side.  It's a bit dated, but still relevant for all the basic mathematical statistics.\\n\\nTwo other noteworthy books come to mind for inference and estimation theory:\\n\\n- [Theory of Point Estimation][2], E. L. Lehmann\\n- [Theory of Statistics][3], Schervish\\n\\nNot entirely sure if this is what you wanted, but you can check out the reviews and see if they meet your expectations.\\n\\n\\n  [1]: http://www.powells.com/biblio/61-9780691005478-1\\n  [2]: http://www.powells.com/biblio/9780387985022\\n  [3]: http://www.powells.com/biblio/0387945466\\n",,
900,16,417,3f950b8d-69ac-4f00-a8f1-3e4b3a78d0aa,2010-07-21 14:01:46.0,-1.0,,,
901,2,418,74b8bd44-5d11-4b0b-a3d3-62a5c4cdccbd,2010-07-21 14:30:42.0,77.0,"Coming from the field of computer vision, I've often used the [RANSAC][1] (Random Sample Consensus) method for fitting models to data with lots of outliers. \\n\\nHowever, I've never seen it used by statisticians, and I've always been under the impression that it wasn't considered a ""statistically-sound"" method. Why is that so? It is random in nature, which makes it harder to analyze, but so are bootstrapping methods. \\n\\nOr is simply a case of academic silos not talking to one another?\\n\\n  [1]: http://en.wikipedia.org/wiki/RANSAC",,
902,1,418,74b8bd44-5d11-4b0b-a3d3-62a5c4cdccbd,2010-07-21 14:30:42.0,77.0,Why isn't RANSAC most widely used in statistics?,,
903,3,418,74b8bd44-5d11-4b0b-a3d3-62a5c4cdccbd,2010-07-21 14:30:42.0,77.0,<outliers><bootstrapping>,,
904,2,419,71d9bb14-10ea-4b0f-868a-79ce9f132ac9,2010-07-21 14:43:31.0,215.0,"\\nI agree that students find this problem very difficult. The typical response I get is that after you've been shown a goat there's a 50:50 chance of getting the car so why does it matter? Students seem to divorce their first choice from the decision they're now being asked to make i.e. they view these two actions as independent. I then remind them that they were twice as likely to have chosen the wrong door initially hence why they're better off switching. \\n\\nIn recent years I've started actually playing the game in glass and it helps students to understand the problem much better. I use three cardboard toilet roll ""middles"" and in two of them are paper clips and in the third is a £5 note. ",,
905,5,395,c5f0c50b-d83a-4b5f-b400-cf2d6be2c005,2010-07-21 14:46:28.0,210.0,"I have a data set where a series of measurements are being taken each week. In general the data set shows a +/- 1mm change each week with a mean measurement staying at about 0mm. In plotting the data this week it appears that some noticeable movement has occured at two points and looking back at the data set, it is also possible that movement occured last week as well. \\nWhat is the best way of looking at this data set to see how likely it is that the movements that have been seen are real movements rather than just some effect caused by \\nthe natural tolerance in the readings.\\n\\n**Edit**\\n\\nSome more information on the data set. Measurements have been taken at 39 locations which should behave in a similar way although only some of the points may show signs of movement. At each point the readings have now been taken 10 times on a bi-weekly basis and up until the most recent set of readings the measurements were between -1mm and 1mm. The measurements can only be taken with mm accuracy so we only recieve results to the nearest mm. The results for one of the points showing a movement is 0mm, 1mm, 0mm, -1mm, -1mm, 0mm, -1mm, -1mm, 1mm, 3mm. We are not looking for statisitically signficiant information, just an indicator of what might have occured. The reason is that if a measurement reaches 5mm in a subsequent week we have a problem and we'd like to be forwarned that this might occur. ",added additional measurement data,
906,2,420,e687de7c-faf1-44ab-acb7-2597d5c674d9,2010-07-21 14:55:06.0,89.0,"I'd modify what Graham Cookson said slightly.  I think the really crucial thing that people overlook is not their first choice, but the *host's* choice, and the assumption that the host made sure *not* to reveal the car.  \\n\\nIn fact, when I discuss this problem in a class, I present it in part as a case study in being clear on your assumptions. It is to your advantage to switch *if the host is making sure only to reveal a goat*.  On the other hand, if the host picked randomly between doors 2 and 3, and happened to reveal a goat, then there is no advantage to switching.\\n\\n(Of course, the practical upshot is that if you don't know the host's strategy, you should switch anyway.)",,
907,2,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,2010-07-21 15:01:21.0,219.0,"What book would you recommend for scientists who are not statisticians?\\n\\nClear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.",,
908,1,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,2010-07-21 15:01:21.0,219.0,What book would you recommend for non-statistician?,,
909,3,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,2010-07-21 15:01:21.0,219.0,<books><science>,,
910,16,421,cafcf180-f874-4b98-a9a4-8167bc6c8e35,2010-07-21 15:01:21.0,219.0,,,
911,2,422,f3f91749-49b8-4098-817a-1ff7ccc23ae3,2010-07-21 15:09:11.0,247.0,"That'll depend very much on their background, but I found ""Statistics in a Nutshell"" to be pretty good.",,
912,16,422,f3f91749-49b8-4098-817a-1ff7ccc23ae3,2010-07-21 15:09:11.0,-1.0,,,
913,2,423,f43ac356-f089-4201-b246-91cced19a08b,2010-07-21 15:13:21.0,5.0,This is one of my favorites:\\n\\n![alt text][1]\\n\\nOne entry per answer.  This is in the vein of [this StackOverflow question][2].\\n\\nP.S. Do not hotlink the cartoon without the site's permission please.\\n\\n\\n  [1]: http://imgs.xkcd.com/comics/correlation.png\\n  [2]: http://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon,,
914,1,423,f43ac356-f089-4201-b246-91cced19a08b,2010-07-21 15:13:21.0,5.0,"What is your favorite ""data analysis"" cartoon?",,
915,3,423,f43ac356-f089-4201-b246-91cced19a08b,2010-07-21 15:13:21.0,5.0,<humor>,,
916,16,423,f43ac356-f089-4201-b246-91cced19a08b,2010-07-21 15:13:21.0,5.0,,,
917,2,424,02e9b914-9e84-4c77-b0df-a9f61419f7b3,2010-07-21 15:21:33.0,88.0,"Was XKCD, so time for Dilbert:\\n\\n![alt text][1]\\n\\n\\n  [1]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/2000/300/2318/2318.strip.print.gif\\n\\n",,
918,16,424,02e9b914-9e84-4c77-b0df-a9f61419f7b3,2010-07-21 15:21:33.0,-1.0,,,
919,2,425,511c25d7-dfba-43f7-8a3f-0224e22caced,2010-07-21 15:23:53.0,13.0,One of my favorites from [xckd](http://www.xkcd.com):\\n\\n![alt text][1]\\n\\n> RFC 1149.5 specifies 4 as the standard IEEE-vetted random number.\\n\\n  [1]: http://imgs.xkcd.com/comics/random_number.png,,
920,16,425,511c25d7-dfba-43f7-8a3f-0224e22caced,2010-07-21 15:23:53.0,-1.0,,,
924,5,352,cc75cec1-2ad2-4729-8c20-9505d85df317,2010-07-21 15:29:25.0,8.0,"I've never had to do this, so this is just a suggestion.\\n\\nI see two (other) possibilities. \\n\\n**Half data**\\n\\n1. Load in half the data and sort\\n1. Next read in the remaining values and compare against the your sorted list. \\n  1. If the new value is larger, discard it.\\n  1. else put the value in the sorted list and removing the largest value from that list.\\n\\n\\n**Sampling distribution**\\n\\nThe other option, is to use an approximation involving the sampling distribution. If your data is Normal, then the standard error for moderate *n* is:\\n\\n1.253 * sd / sqrt(n)\\n\\nTo determine the size of *n* that you would be happy with, I ran a quick Monte-Carlo simulation in R\\n\\n    n = 10000\\n    outside.ci.uni = 0\\n    outside.ci.nor = 0\\n    N=1000\\n    for(i in 1:N){\\n      #Theoretical median is 0\\n      uni = runif(n, -10, 10)\\n      nor  = rnorm(n, 0, 10)\\n      \\n      if(abs(median(uni)) > 1.96*1.253*sd(uni)/sqrt(n))\\n        outside.ci.uni = outside.ci.uni + 1\\n    \\n      if(abs(median(nor)) > 1.96*1.253*sd(nor)/sqrt(n))\\n        outside.ci.nor = outside.ci.nor + 1\\n    }\\n    \\n    outside.ci.uni/N\\n    outside.ci.nor/N\\n\\nFor n=10000, 15% of the uniform median estimates were outside the CI.\\n\\n\\n\\n",Fixed a typo; deleted 6 characters in body,
925,2,427,c488472b-5407-49df-a7da-22a4ba581195,2010-07-21 15:33:11.0,215.0,"\\n*Before touching this topic, I always make sure that students are happy moving between percentages, decimals, odds and fractions. If they are not completely happy with this then they can get confused very quickly.*\\n\\nI like to explain hypothesis testing for the first time (and therefore p-values and test statistics) through Fisher's classic tea experiment. I have several reasons for this:\\n\\n(i) I think working through an experiment and defining the terms as we go along makes more sense that just defining all of these terms to begin with.\\n(ii) You don't need to rely explicitly on probability distributions, areas under the curve, etc to get over the key points of hypothesis testing.\\n(iii) It explains this ridiculous notion of ""as or more extreme than those observed"" in a fairly sensible manner\\n(iv) I find students like to understand the history, origins and back story of what they are studying as it makes it more real than some abstract theories.\\n(v) It doesn't matter what discipline or subject the students come from, they can relate to the example of tea (N.B. Some international students have difficulty with this peculiarly British institution of tea with milk.)\\n\\n[Note: I originally got this idea from Dennis Lindley's wonderful article ""The Analysis of Experimental Data: The Appreciation of Tea & Wine"" in which he demonstrates why Bayesian methods are superior to classical methods.]\\n\\nThe back story is that Muriel Bristol visits Fisher one afternoon in the 1920's at Rothamsted Experimental Station for a cup of tea. When Fisher put the milk in last she complained saying that she could also tell whether the milk was poured first (or last) and that she preferred the former. To put this to the test he designed his classic tea experiment where Muriel is presented with a pair of tea cups and she must identify which one had the milk added first. This is repeated with six pairs of tea cups. Her choices are either Right (R) or Wrong (W) and her results are: RRRRRW.\\n\\nSuppose that Muriel is actually just guessing and has no ability to discriminate whatsoever. This is called the **Null Hypothesis**. According to Fisher the purpose of the experiment is to discredit this null hypothesis. If Muriel is guessing she will identify the tea cup correctly with probability 0.5 on each turn and as they are independent the observed result has 0.5^6 = 0.016 (or 1/64). Fisher then argues that either:\\n\\n(a) the null hypothesis (Muriel is guessing) is true and an event of small probability has occurred or,\\n(b) the null hypothesis is false and Muriel has discriminatory powers.\\n\\nThe p-value (or probability value) is the probability of observing this outcome (RRRRRW) given the null hypothesis is true - it's the small probability referred to in (a), above. In this instance it's 0.016. Since events with small probabilities only occur rarely (by definition) situation (b) might be a more preferable explanation of what occurred than situation (a). When we reject the null hypothesis we're in fact accepting the opposite hypothesis which is we call the alternative hypothesis. In this example, Muriel has discriminatory powers is the alternative hypothesis.\\n\\nAn important consideration is what do we class as a ""small"" probability? What's the cutoff point at which we're willing to say that an event is unlikely? The standard benchmark is 5% (0.05) and this is called the significance level. When the p-value is smaller than the significance level we reject the null hypothesis as being false and accept our alternative hypothesis. It is common parlance to claim a result is ""significant"" when the p-value is smaller than the significance level i.e. when the probability of what we observed occurring given the null hypothesis is true is smaller than our cutoff point. It is important to be clear that using 5% is completely subjective (as is using the other common significance levels of 1% and 10%). \\n\\nFisher realised that this doesn't work; every possible outcome with one wrong pair was equally suggestive of discriminatory powers. The relevant probability for situation (a), above, is therefore 6(0.5)^6 = 0.094 (or 6/64) which now is **not significant** at a significance level of 5%. To overcome this Fisher argued that if 1 error in 6 is considered evidence of discriminatory powers then so is no errors i.e. outcomes that more strongly indicate discriminatory powers than the one observed should be included when calculating the p-value. This resulted in the following amendment to the reasoning, either:\\n\\n(a) the null hypothesis (Muriel is guessing) is true and the probability of events as, or more, extreme than that observed is small, or\\n(b) the null hypothesis is false and Muriel has discriminatory powers.\\n\\nBack to our tea experiment and we find that the p-value under this set-up is 7(0.5)^6 = 0.109 which still is not significant at the 5% threshold. \\n\\nI then get students to work with some other examples such as coin tossing to work out whether or not a coin is fair. This drills home the concepts of the null/alternative hypothesis, p-values and significance levels. We then move onto the case of a continuous variable and introduce the notion of a test-statistic. As we have already covered the normal distribution, standard normal distribution and the z-transformation in depth it's merely a matter of bolting together several concepts. \\n\\nAs well as calculating test-statistics, p-values and making a decision (significant/not significant) I get students to work through published papers in a fill in the missing blanks game. \\n\\n",,
926,2,428,28d9cebb-8e3b-4c41-9152-8a3fe0826813,2010-07-21 15:35:45.0,210.0,"Could you group the data set into much smaller data sets (say 100 or 1000 or 10,000 data points) If you then calculated the median of each of the groups. If you did this with enough data sets you could plot something like the average of the results of each of the smaller sets and this woul, by running enough smaller data sets converge to an 'average' solution. ",,
927,2,429,1cfb2515-9716-453f-9e4c-99fceb61da31,2010-07-21 15:36:34.0,39.0,"Briefly stated, the Shapiro-Wilk test is a specific test for normality, whereas the method used by [Kolmogorov-Smirnov test][1] is more general, but less powerful (meaning it correctly rejects the null hypothesis of normality less often). Both statistics take normality as the null and establishes a test statistic based on the sample, but how they do so is different from one another in ways that make them more or less sensitive to features of normal distributions.\\n\\nHow exactly W (the test statistic for Shapiro-Wilk) is calculated is [a bit involved][2], but conceptually, it involves arraying the sample values by size and measuring fit against expected means, variances and covariances.  These multiple comparisons against normality, as I understand it, give the test more power than the the Kolmogorov-Smirnov test, which is one way in which they may differ.\\n\\nBy contrast, the Kolmogorov-Smirnov test for normality is derived from a general approach for assessing goodness of fit by comparing the expected cumulative distribution against the empirical cumulative distribution, vis:\\n\\n![alt text][3] \\n\\nAs such, it is sensitive at the center of the distribution, and not the tails.  However, the K-S is test is convergent, in the sense that as n tends to infinity, the test converges to the true answer in probability (I believe that [Glivenko-Cantelli Theorem][4] applies here, but someone may correct me). These are two more ways in which these two tests might differ in their evaluation of normality.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\\n  [2]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/wilkshap.htm\\n  [3]: http://www.itl.nist.gov/div898/handbook/eda/section3/gif/ecdf.gif\\n  [4]: http://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem",,
928,2,430,cdaff380-5ac0-4bed-a45d-fee0352c0219,2010-07-21 15:37:17.0,251.0,"I find that people find the solution a more intuitive if you change it to 100 doors, close first, second, to 98 doors.  Similarly for 50 doors, etc.  ",,
929,2,431,110b7955-7ec9-40c6-b4d4-4112e6798481,2010-07-21 15:38:11.0,215.0,"\\nI think every statistician should read Stigler's *The History of Statistics: The Measurement of Uncertainty before 1900*\\n\\nIt is beautifully written, thorough and it isn't a historian's perspective but a mathematician's, hence it doesn't avoid the technical details. ",,
930,16,431,110b7955-7ec9-40c6-b4d4-4112e6798481,2010-07-21 15:38:11.0,-1.0,,,
931,2,432,14ef398f-b785-48b8-b35a-84de0b4dccfe,2010-07-21 15:38:46.0,8.0,My favourite Dilbert cartoon:\\n\\n![alt text][1]\\n\\n\\n  [1]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/5000/600/5651/5651.strip.gif,,
932,16,432,14ef398f-b785-48b8-b35a-84de0b4dccfe,2010-07-21 15:38:46.0,-1.0,,,
933,2,433,3b8730d2-6b08-440d-bfdf-998e713f91dd,2010-07-21 15:43:44.0,5.0,Here's [another one from Dilbert][1]:\\n\\n![alt text][2]\\n\\n\\n  [1]: http://dilbert.com/strips/comic/2010-07-02/\\n  [2]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/90000/3000/400/93464/93464.strip.gif,,
934,16,433,3b8730d2-6b08-440d-bfdf-998e713f91dd,2010-07-21 15:43:44.0,-1.0,,,
935,5,113,ff22a260-1351-49d5-94bb-43af20ff29eb,2010-07-21 15:44:07.0,39.0,"I have been looking into theoretical frameworks for method selection (note: not model selection) and have found very little systematic, mathematically-motivated work. By 'method selection', I mean a framework for distinguishing the appropriate (or better, optimal) method with respect to a problem, or problem type.\\n\\nWhat I have found is substantial, if piecemeal, work on particular methods and their tuning (i.e. prior selection in Bayesian methods), and method selection via bias selection (e.g. [Inductive Policy: The Pragmatics of Bias Selection][1]). I may be unrealistic at this early stage of machine learning's development, but I was hoping to find something like what [measurement theory][2] does in prescribing admissible transformations and tests by scale type, only writ large in the arena of learning problems.\\n\\nAny suggestions?\\n\\n\\n  [1]: http://portal.acm.org/citation.cfm?id=218546\\n  [2]: ftp://ftp.sas.com/pub/neural/measurement.html",added 28 characters in body,
936,2,434,3d7037f2-062a-42bd-9ceb-3ec242a1f432,2010-07-21 15:50:43.0,215.0,"\\n*Very* crudely I would say that:\\n\\n**Frequentist:** Sampling is infinite and decision rules can be sharp. Data are a repeatable random sample - there is a frequency. Underlying parameters are fixed i.e. they remain constant during this repeatable sampling process.\\n\\n**Bayesian:**  Unknown quantities are treated probabilistically and the state of the world can always be updated. Data are observed from the realised sample. Parameters are unknown and described probabilistically. It is the data which are fixed.\\n\\nThere is a brilliant [blog post][1] which gives an indepth example of how a Bayesian and Frequentist would tackle the same problem. Why not answer the problem for yourself and then check?\\n\\nThe problem (taken from Panos Ipeirotis' blog):\\n\\nYou have a coin that when flipped ends up head with probability p and ends up tail with probability 1-p. (The value of p is unknown.)\\n\\nTrying to estimate p, you flip the coin 100 times. It ends up head 71 times.\\n\\nThen you have to decide on the following event: ""In the next two tosses we will get two heads in a row.""\\n\\nWould you bet that the event will happen or that it will not happen?\\n\\n\\n  [1]: http://behind-the-enemy-lines.blogspot.com/2008/01/are-you-bayesian-or-frequentist-or.html\\n  ",,
937,2,435,2e9e6a78-c9e8-4ad9-a588-ad29b2d888d4,2010-07-21 15:53:59.0,88.0,One more [Dilbert][1]:\\n![alt text][2]\\n\\n\\n  [1]: http://dilbert.com/fast/2008-05-08/\\n  [2]: http://dilbert.com/dyn/str_strip/000000000/00000000/0000000/000000/00000/5000/600/5652/5652.strip.print.gif,,
938,16,435,2e9e6a78-c9e8-4ad9-a588-ad29b2d888d4,2010-07-21 15:53:59.0,-1.0,,,
939,5,207,65d9f33f-6c41-43e8-a605-2798d01277e0,2010-07-21 16:00:43.0,,"First, we need to understand what is a markov chain. Consider the following [weather][1] example from Wikipedia. Suppose that weather on any given day can be classified into two states only: sunny and rainy. Based on past experience, we know the following:\\n\\nProbability(Next day is sunny | Given today is rainy ) = 0.50\\n\\nSince, the next day's weather is either sunny or rainy it follows that:\\n\\nProbability(Next day is Rainy | Given today is rainy ) = 0.50 \\n\\nSimilarly, let:\\n\\nProbability(Next day is rainy | Given today is sunny ) = 0.10\\n\\nTherefore, it follows that:\\n\\nProbability(Next day is sunny | Given today is sunny ) = 0.90\\n\\nThe above four numbers can be compactly represented as a transition matrix which represents the probabilities of the weather moving from one state to another state as follows:\\n\\n             S   R\\n    P = S [ 0.9 0.1\\n        R   0.5 0.5]\\n\\nWe might ask several questions whose answers follow:\\n\\nQ1: If the weather is sunny today then what is the weather likely to be tomorrow?\\n\\nA1: Since, we do not know what is going to happen for sure, the best we can say is that there is a 90% chance that it is likely to be sunny and 10% that it will be rainy. \\n\\nQ2: What about two days from today?\\n\\nA2: One day prediction: 90% sunny, 10% rainy. Therefore, two days from now:\\n\\nFirst day it can be sunny and the next day also it can be sunny. Chances of this happening are: 0.9  0.9. \\n\\nOr\\n\\nFirst day it can be rainy and second day it can be sunny. Chances of this happening are: 0.1 * 0.5\\n\\nTherefore, the probability that the weather will be sunny in two days is:\\n\\nProb(Sunny two days from now) = 0.9  0.9 + 0.1  0.5 = 0.81 + 0.05 = 0.86 \\n\\nSimilarly, the probability that it will be rainy is:\\n\\nProb(Rainy two days from now) = 0.1 * 0.5 + 0.9 0.1 = 0.05 + 0.09 = 0.14\\n\\nIf you keep forecasting weather like this you will notice that eventually the nth day forecast where n is very large (say 30) settles to the following 'equilibrium' probabilities:\\n\\nProb(Sunny) = 0.833\\nProb(Rainy) = 0.167\\n\\nIn other words, your forecast for the nth day and the n+1th day remain the same. In addition, you can also check that the 'equilibrium' probabilities do not depend on the weather today. You would get the same forecast for the weather if you start of by assuming that the weather today is sunny or rainy.\\n\\nThe above example will only work if the state transition probabilities satisfy several conditions which I will not discuss here. But, notice the following features of this 'nice' markov chain (nice = transition probabilities satisfy conditions):\\n\\nIrrespective of the initial starting state we will eventually reach an equilibrium probability distribution of states.\\n\\nMarkov Chain Monte Carlo exploits the above feature as follows: \\n\\nWe want to generate random draws from a target distribution. We then identify a way to construct a 'nice' markov chain such that its equilibrium probability distribution is our target distribution. \\n\\nIf we can construct such a chain then we arbitrarily start from some point and iterate the markov chain many times (like how we forecasted the weather n times). Eventually, the draws we generate would appear as if they are coming from our target distribution. \\n\\nWe then approximate the quantities of interest (e.g. mean) by taking the sample average of the draws after discarding a few initial draws which is the monte carlo component.\\n\\nThere are several ways to construct 'nice' markov chains (e.g., gibbs sampler, Metropolis-Hastings algorithm).\\n\\n  [1]: http://en.wikipedia.org/wiki/Examples_of_Markov_chains#A_very_simple_weather_model",fixed typo,user28
940,2,436,c839ffa2-6f6b-450a-8c47-62b42b870fda,2010-07-21 16:04:18.0,215.0,"\\nThe answer would most definitely depend on their discipline, the methods/techniques that they would like to learn and their existing mathematical/statistical abilities.\\n\\nFor example, economists/social scientists who want to learn about cutting edge empirical econometrics could read Angrist and Pischke's Mostly Harmless Econometrics. This is a non-technical book covering the ""natural experimental revolution"" in economics. The book only presupposes that they know what regression is.\\n\\nBut I think the best book on applied regression is Gelman and Hill's Data Analysis Using Regression and Hierarchical/Multilevel Models. This covers basic regression, multilevel regression, and Bayesian methods in a clear and intuitive way. It would be good for any scientist with a basic background in statistics.\\n\\n",,
941,16,436,c839ffa2-6f6b-450a-8c47-62b42b870fda,2010-07-21 16:04:18.0,-1.0,,,
942,2,437,26f52b9e-25b6-4bf8-9c66-10d07666844d,2010-07-21 16:07:20.0,215.0,"\\nFor you I would suggest:\\n\\nIntroduction to the Mathematical and Statistical Foundations of Econometrics by Herman J. Bierens, CUP. The word ""Introduction"" in the title is a sick joke for most PhD econometrics students.\\n\\nMarkov Chain Monte Carlo by Dani Gamerman, Chapman & Hall is also concise.\\n\\n",,
943,16,437,26f52b9e-25b6-4bf8-9c66-10d07666844d,2010-07-21 16:07:20.0,-1.0,,,
944,2,438,5db3168a-7a52-47ee-a568-f93b1a7c7054,2010-07-21 16:42:43.0,215.0,"\\nOK here's my best attempt at an informal and crude explanation.\\n\\nA Markov Chain is a random process that has the property that the future depends only on the current state of the process and not the past i.e. it is memoryless. An example of a random process could be the stock exchange. An example of a Markov Chain would be a board game like Monopoly or Snakes and Ladders where your future position (after rolling the die) would depend only on where you started from before the roll, not any of your previous positions. A textbook example of a Markov Chain is the ""drunkard's walk"". Imagine somebody who is drunk and can move only left or right by one pace. The drunk moves left or right with equal probability. This is a Markov Chain where the drunk's future/next position depends only upon where he is at present.\\n\\nMonte Carlo methods are computational algorithms (simply sets of instructions) which randomly sample from some process under study. They are a way of estimating something which is too difficult or time consuming to find deterministically. They're basically a form of computer simulation of some mathematical or physical process. The Monte Carlo moniker comes from the analogy between a casino and random number generation. Returning to our board game example earlier, perhaps we want to know if some properties on the Monopoly board are visited more often than others. A Monte Carlo experiment would involve rolling the dice repeatedly and counting the number of times you land on each property. It can also be used for calculating numerical integrals. (Very informally, we can think of an integral as the area under the graph of some function.)  Monte Carlo integration works great on a high-dimensional functions by taking a random sample of points of the function and calculating some type of average at these various points. By increasing the sample size, the law of large numbers tells us we can increase the accuracy of our approximation by covering more and more of the function.\\n\\nThese two concepts can be put together to solve some difficult problems in areas such as Bayesian inference, computational biology, etc where multi-dimensional integrals need to be calculated to solve common problems. The idea is to construct a Markov Chain which converges to the desired probability distribution after a number of steps. The state of the chain after a large number of steps is then used as a sample from the desired distribution and the process is repeated. There many different MCMC algorithms which use different techniques for generating the Markov Chain. Common ones include the Metropolis-Hastings and the Gibbs Sampler.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n",,
945,2,439,099f72e6-7e89-4cbc-bf19-69484a45d0b1,2010-07-21 16:50:35.0,30.0,"As you said, it's not necessarily the case that a mathematician may want a rigorous book. Maybe the goal is to get some intuition of the concepts quickly, and then fill in the details. I recommend two books from CMU professors, both published by Springer: ""All of Statistics"" by Larry Wasserman is quick and informal. ""Theory of Statistics"" by Mark Schervish is rigorous and relatively complete. It has decision theory, finite sample, some asymptotics and sequential analysis.",,
946,16,439,099f72e6-7e89-4cbc-bf19-69484a45d0b1,2010-07-21 16:50:35.0,-1.0,,,
947,2,440,d6c19c35-bacd-4d4a-8881-41005bd8cf13,2010-07-21 16:53:35.0,36.0,[Peter Dalgaard's Introductory Statistics with R][1] is a great book for some introductory statistics with a focus on the R software for data analysis.\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0387790535/ref=nosim/gettgenedone-20,,
948,16,440,d6c19c35-bacd-4d4a-8881-41005bd8cf13,2010-07-21 16:53:35.0,-1.0,,,
949,2,441,e2207497-bbe5-4cd1-99fc-0e13d244bdcf,2010-07-21 16:54:38.0,36.0,UCLA has the best free resources you'll find anywhere.\\n\\nhttp://www.ats.ucla.edu/stat/stata/,,
950,2,442,20724ac3-60e5-4a31-85be-3b7e1692a3fc,2010-07-21 16:56:45.0,36.0,"I say the [visual display of quantitative information][1] by Tufte, and [Freakonomics][2] for something fun.\\n\\n\\n  [1]: http://amazon.com/o/ASIN/0961392142/ref=nosim/gettgenedone-20\\n  [2]: http://amazon.com/o/ASIN/0060731338/ref=nosim/gettgenedone-20",,
951,16,442,20724ac3-60e5-4a31-85be-3b7e1692a3fc,2010-07-21 16:56:45.0,-1.0,,,
952,2,443,7d8cdf20-871d-4492-b1c6-3ea2d876b604,2010-07-21 17:06:32.0,215.0,"\\nThere's two aspects to this *post hoc ergo propter hoc* problem that I like to cover: (i) reverse causality and (ii) endogeneity \\n\\nAn example of ""possible"" reverse causality:\\nSocial drinking and earnings - drinkers earn more money according to Bethany L. Peters & Edward Stringham, 2006. ""No Booze? You May Lose: Why Drinkers Earn More Money Than Nondrinkers,"" Journal of Labor Research, Transaction Publishers, vol. 27(3), pages 411-421, June. Or do people who earn more money drink more either because they have a greater disposable income or due to stress? This is a great paper to discuss for all sorts of reasons including measurement error, response bias, causality, etc.\\n\\nAn example of ""possible"" endogeneity:\\nThe Mincer Equation explains log earnings by education, experience and experience squared. There is a long literature on this topic. Labour economists want to estimate the causal relationship of education on earnings but perhaps education is endogenous because ""ability"" could increase the amount of education an individual has (by lowering the cost of obtaining it) and could lead to an increase in earnings, irrespective of the level of education. A potential solution to this could be an instrumental variable. Angrist and Pischke's book, Mostly Harmless Econometrics covers this and relates topics in great detail and clarity.\\n\\n\\nOther silly examples that I have no support for include:\\n- Number of televisions per capita and the numbers of mortality rate. So let's send TVs to developing countries. Obviously both are endogenous to something like GDP.\\n- Number of shark attacks and ice cream sales. Both are endogenous to the temperature perhaps?\\n\\nI also like to tell the terrible joke about the lunatic and the spider. A lunatic is wandering the corridors of an asylum with a spider he's carrying in the palm of his hand. He sees the doctor and says, ""Look Doc, I can talk to spiders. Watch this. Spider, go left!"" The spider duly moves to the left. He continues, ""Spider, go right."" The spider shuffles to the right of his palm. The doctor replies, ""Interesting, maybe we should talk about this in the next group session."" The lunatic retorts, ""That's nothing Doc. Watch this."" He pulls of each of the spider's legs one by one and then shouts, ""Spider, go left!"" The spider lies motionless on his palm and the lunatic turns to the doctor and concludes, ""If you pull of a spider's legs he'll go deaf."" \\n\\n\\n\\n",,
956,2,445,1d756af2-9087-48fb-b0cb-f7ac3f6d9742,2010-07-21 17:13:33.0,215.0,\\nThe UCLA resource listed by Stephen Turner (above) are excellent if you just want to apply methods you're already familiar with using Stata.\\n\\nIf you're looking for textbooks which teach you statistics/econometrics while using Stata then these are solid recommendations (but it depends at what level you're looking at):\\n\\n**Introductory Methods**\\nAn Introduction to Modern Econometrics Using Stata by Chris Baum\\nIntroduction to Econometrics by Chris Dougherty\\n\\n**Advanced/Specialised Methods**\\nMultilevel and Longitudinal Modeling Using Stata by Rabe-Hesketh and Skrondal\\nRegression Models for Categorical Dependent Variables Using Stata by Long and Freese\\n,,
957,2,446,0a66e040-8908-41af-9142-ab9e249c1d85,2010-07-21 17:17:45.0,215.0,"\\nThe Gelman books are all excellent but not necessarily introductory in that they assume that you know some statistics already. Therefore they are an introduction to the Bayesian way of doing statistics rather than to statistics in general. I would still give them the thumbs up, however.\\n\\nAs an introductory statistics/econometrics book which takes a Bayesian perspective, I would recommend Gary Koop's Bayesian Econometrics.",,
958,16,446,0a66e040-8908-41af-9142-ab9e249c1d85,2010-07-21 17:17:45.0,-1.0,,,
959,2,447,43841708-f403-4cc5-9b94-b336e54e95fc,2010-07-21 17:26:14.0,215.0,\\nI really like these two books by Daniel McFadden of Berkeley:\\n\\n\\nhttp://elsa.berkeley.edu/users/mcfadden/e240a_sp98/e240a.html\\n\\nhttp://elsa.berkeley.edu/users/mcfadden/e240b_f01/e240b.html\\n\\n,,
960,2,448,643d2a93-597f-44a3-b521-355cd2b88b2e,2010-07-21 17:26:15.0,77.0,"Here are my guidelines, based on the most common errors I see (in addition to all the other good points mentioned)\\n\\n- Use scatter graphs, not line plots, if element order is not relevant.\\n- When preparing plots that are meant to be compared, use the same scale factor for all of them. \\n- Even better - find a way to combine the data in a single graph (eg: boxplots are a better than several histograms to compare a large number of distributions).\\n- Do not forget to specify units\\n- Use a legend only if you must - it's generally clearer to label curves directly. \\n- If you must use a legend, move it inside the plot, in a blank area.\\n- For line graphs, aim for an aspect ratio which yields [lines that are roughly at 45o with the page][1].\\n\\n\\n  [1]: http://processtrends.com/pg_data_vis_bank_to_45.htm",,
961,2,449,cbe9113b-01b7-429c-9642-0faec97c8239,2010-07-21 17:28:57.0,215.0,\\nMore from an economics perspectives I think these two sets of lecture notes are very good:\\n\\nhttp://home.datacomm.ch/paulsoderlind/Courses/OldCourses/FinEcmtAll.pdf\\n\\nhttp://personal.lse.ac.uk/mele/files/fin_eco.pdf\\n\\nThe first provides econometric methods for analysing financial data whereas the second provides the financial economics theory behind the models being applied. They're both MSc level texts.,,
962,2,450,b1a1f180-ea20-4efa-9e39-ccd6f0497128,2010-07-21 17:30:02.0,88.0,"For us, it is just one example of a robust regression -- I believe it is used by statisticians also, but maybe not so wide because it has some better known alternatives.",,
964,6,418,81ab183c-0cd2-489b-a32b-2896457dc560,2010-07-21 17:30:38.0,88.0,<outliers><bootstrap><robust>,edited tags,
965,2,451,0a1f4010-1e5c-4a4d-bbe8-c351d0030e36,2010-07-21 17:38:08.0,215.0,"\\nWhen describing a variable we typically summarise it using two measures: a measure of centre and a measure of spread. Common measures of spread include the mean, median and mode. Common measure of spread include the variance and interquartile range.\\n\\nThe variance (represented by the Greek lowercase sigma) is commonly used when the mean is reported. The variance is the average squared deviation of variable. The deviation is calculated by subtracting the mean from each observation. This is squared because the sum would otherwise be zero and squaring removes this problem while maintaining the relative size of the deviations. The problem with using the variation as a measure of spread is that it is in squared units. For example if our variable of interest was height measured in inches then the variance would be reported in squared-inches which makes little sense. The standard deviation (represented by the Greek lowercase sigma raised to the power 2) is the square-root of the variance and returns the measure of spread to the original units. This is much more intuitive and is therefore more popular than the variance.\\n\\nWhen using the standard deviation, one has to be careful of outliers as they will skew the standard deviation (and the mean) as they are not resistant measures of spread. A simple example will illustrate this property. The mean of my terrible cricket batting scores of 13, 14, 16, 23, 26, 28, 33, 39, and 61 is 28.11. If we consider 61 to be an outlier and deleted it, the mean would be 24. \\n\\n\\n\\n\\n",,
966,2,452,362324e1-68ee-4fc4-b39f-67385c3f0a4f,2010-07-21 17:45:01.0,215.0,It has been suggested by Angrist and Pischke that Robust (i.e. robust to heteroskedasticity or unequal variances) Standard Errors are reported as a matter of course rather than testing for it. Two questions:\\n\\n(1) What is impact on the standard errors of doing so when there is homoskedasticity?\\n(2) Does anybody actually do this in their work?,,
967,1,452,362324e1-68ee-4fc4-b39f-67385c3f0a4f,2010-07-21 17:45:01.0,215.0,Always Report Robust (White) Standard Errors?,,
968,3,452,362324e1-68ee-4fc4-b39f-67385c3f0a4f,2010-07-21 17:45:01.0,215.0,<regression><error><standard>,,
969,2,453,426606e1-99d5-45df-b069-e5f62de226b2,2010-07-21 17:54:05.0,74.0,"[Statistics in Plain English](http://www.amazon.com/gp/product/041587291X?ie=UTF8&tag=httpvancouveb-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=041587291X"") is pretty good.\\n\\n4.5 on Amazon, 11 reviews.\\n\\nExplains ANOVA pretty well too. ",,
970,16,453,426606e1-99d5-45df-b069-e5f62de226b2,2010-07-21 17:54:05.0,-1.0,,,
971,2,454,18562440-a8ab-4735-afc9-be6e5616cadf,2010-07-21 18:11:59.0,62.0,"It depends on the way in which the plots will be discussed. \\n\\nFor instance, if I'm sending out plots for a group meeting that will be done with callers from different locations, I prefer putting them together in Powerpoint as opposed to Excel, so it's easier to flip around. \\n\\nFor one-on-one technical calls, I'll put something in excel so that the client be able to move a plot aside, and view the raw data. Or, I can enter p-values into cells along side regression coefficients, e.g. \\n\\nKeep in mind: plots are cheap, especially for a slide show, or for emailing to a group. I'd rather make 10 clear plots that we can flip through than 5 plots where I try to put distinct cohorts (e.g. ""males and females"") on the same graph. \\n",,
972,2,455,a3cd71d3-e15e-4855-a429-ce270f242e7d,2010-07-21 18:18:13.0,13.0,"Allright, I think this one is hilarious- but let's see if it passes the Statistical Analysis Miller test.\\n\\n## Fermirotica\\n\\n[![alt text][1]](http://xkcd.com/563/)\\n\\n> I love how Google handles dimensional analysis.  Stats are ballpark and vary wildly by time of day and whether your mom is in town.\\n\\n  [1]: http://imgs.xkcd.com/comics/fermirotica.png",,
973,16,455,a3cd71d3-e15e-4855-a429-ce270f242e7d,2010-07-21 18:18:13.0,-1.0,,,
974,5,425,aa823d93-3479-431f-8f34-d42d546424df,2010-07-21 18:19:37.0,13.0,One of my favorites from [xckd](http://www.xkcd.com):\\n\\n##Random Number\\n[![alt text][1]](http://xkcd.com/221/)\\n\\n> RFC 1149.5 specifies 4 as the standard IEEE-vetted random number.\\n\\n  [1]: http://imgs.xkcd.com/comics/random_number.png,added 41 characters in body,
975,2,456,ea1d6b54-60db-45f7-af6c-b5f84dbfe3bc,2010-07-21 18:26:02.0,39.0,"As the [Encyclopedia of GIS][1] states, the conditional autoregressive model (CAR) is appropriate for situations with first order dependency or relatively local spatial autocorrelation, and simultaneous autoregressive model (SAR ) is  more suitable where there are second order dependency or a more global spatial autocorrelation.\\n\\nThis is made clear by the fact that CAR obeys the spatial version of the [Markov property][2], namely that the state of neighbors is due only to their neighbors (i.e. spatially “memoryless”, instead of temporally), whereas SAR does not.  This is due to the different ways in which they specify their variance-covariance matrixes.  So, when the spatial Markov property obtains, CAR provides a simpler way to model autocorrelated geo-referenced areal data.\\n\\nSee [Gis And Spatial Data Analysis: Converging Perspectives][3] for more details.\\n\\n\\n  [1]: http://books.google.com/books?id=6q2lOfLnwkAC&dq=Encyclopedia+of+GIS\\n  [2]:  http://en.wikipedia.org/wiki/Markov_property\\n  [3]: http://www.geog.ucsb.edu/~good/papers/387.pdf",,
976,2,457,5acf72e6-36ca-4025-b375-581fc13e1e96,2010-07-21 18:28:27.0,,"I do not know the literature in the area well enough to offer a direct response. However, it seems to me that if the three tests differ then that is an indication that you need further research/data collection in order to definitively answer your question. \\n\\nYou may also want to look at [this][1] Google Scholar search\\n\\n\\n  [1]: http://scholar.google.com/scholar?q=small%20sample%20properties%20of%20wald%20likelihood%20ratio&um=1&ie=UTF-8&sa=N&hl=en&tab=ws",,user28
977,5,236,e07f6f8a-4c58-4086-a44b-6960022918f7,2010-07-21 18:33:33.0,138.0,"Unfortunately, it only runs on macs, but otherwise a great application (basically *Processing* in python):\\n\\n* [http://nodebox.net/code/index.php/Home][1]\\n\\n> NodeBox is a Mac OS X application that lets you create 2D visuals (static, animated or interactive) using Python programming code and export them as a PDF or a QuickTime movie. NodeBox is free and well-documented.\\n\\n\\n  [1]: http://nodebox.net/code/index.php/Home",added 35 characters in body,
978,2,458,67ed99b0-ab92-49da-b60f-0292361da368,2010-07-21 18:41:12.0,226.0,"One way to test patterns in stock market data is discussed [here][1]. A similar approach would be to randomise the stock market data and identify your patterns of interest, which would obviously be devoid of any meaning due to the deliberate randomising process. These randomly generated patterns and their returns would form your null hypothesis. By statistically comparing the pattern returns in the actual data with the returns from the null hypothesis randomised data patterns you may be able to distinguish patterns which actually have some meaning or predictive value. \\n\\n\\n  [1]: http://www.evidencebasedta.com/",,
979,2,459,1bd18628-2703-48b8-8784-feb2ae3893d6,2010-07-21 19:59:48.0,259.0,"We're plotting time-series metrics in the context of network/server operations. The data has a 5-minute sample rate, and consists of things like CPU utilization, error rate, etc.\\n\\nWe're adding a horizontal ""threshold"" line to the graphs, to visually indicate a value threshold above which people should worry/take notice. For example, in the CPU utilization example, perhaps the ""worry"" threshold is 75%.\\n\\nMy team has some internal debate over what color this line should be:\\n\\n1. Something like a bright red that clearly stands out from the background grid and data lines, and indicates this is a warning condition\\n2. Something more subtle and definitely NOT red, since the ""ink"" for the line doesn't represent any actual data, and thus attention shouldn't be drawn to it unnecessarily.\\n\\nWould appreciate guidance / best practices...\\n\\nThanks,\\nMike",,
980,1,459,1bd18628-2703-48b8-8784-feb2ae3893d6,2010-07-21 19:59:48.0,259.0,"Good line color for ""threshold"" line in a time-series graph?",,
981,3,459,1bd18628-2703-48b8-8784-feb2ae3893d6,2010-07-21 19:59:48.0,259.0,<time-series><data-visualization>,,
982,2,460,fa0074cc-0f0a-41c5-8380-42984877e90e,2010-07-21 20:03:40.0,61.0,"For moderate dimensions, like 3, then some sort of kernel cross-validation technique as suggested elsewhere seems reasonable and is the best I can come up with.\\n\\nFor higher dimensions, I'm not sure that the problem is solvable; it lands pretty squarely into 'curse-of-dimensionality' territory.  The issue is that distance functions tend to converge to very large values very quickly as you increase dimensionality, including distances derived from distributions.  If you're defining an outlier as ""a point with a comparatively large distance function relative to the others"", and all your distance functions are beginning to converge because you're in a high-dimensional space, well, you're in trouble.\\n\\nWithout some sort of distributional assumption that will let you turn it into a probabilistic classification problem, or at least some rotation that lets you separate your space into ""noise dimensions"" and ""informative dimensions"", I think that the geometry of high-dimensional spaces is going to prohibit any easy -- or at least robust -- identification of outliers.",,
983,2,461,25ea324c-4818-4cf2-a8b9-e9c96e2aabd4,2010-07-21 20:07:32.0,71.0,"To me, whether or not the line represents actual data seems irrelevant.  What's the point of the plot?  If it's so that somebody will do something when utilization crosses a threshold, the line marking the threshold had better be very visible.  If the point of the plot is to give an overview of utilization over time, then why include the line at all?  Just put the major gridlines of your plot at intervals that will coincide with your threshold (25% in your example), and let the reader figure it out.\\n\\n... y'all been reading too much Tufte.",,
984,2,462,4866f182-901f-466d-bbea-d5553ba9e74f,2010-07-21 20:11:02.0,88.0,"In the physics field there is a rule that the whole paper/report should be understandable only from quick look at the plots. So I would mainly advice that they should be self-explanatory.  \\nThis also implies that you must always check whether your audience is familiar with some kind of plot -- I had once made a big mistake assuming that every scientist knows what boxplots are, and then waisted an hour to explain it.",,
985,2,463,b183492f-95a6-4b81-a930-a10e06f9c3a2,2010-07-21 20:29:12.0,90.0,"In addition to ""The History of Statistics"" suggested by Graham, another Stigler book worth reading is\\n\\n[Statistics on the Table: The History of Statistical Concepts and Methods][1]\\n\\n\\n  [1]: http://www.amazon.com/Statistics-Table-History-Statistical-Concepts/dp/0674009797/ref=sr_1_1?ie=UTF8&s=books&qid=1279743969&sr=8-1",,
986,16,463,b183492f-95a6-4b81-a930-a10e06f9c3a2,2010-07-21 20:29:12.0,-1.0,,,
987,2,464,64952dbd-eef4-4537-a65a-3a3e4030ec15,2010-07-21 20:34:43.0,88.0,"If this is about your ""Qnotifier"" I think that you should plot the threshold line in some darker gray so it is distinguishable but not disturbing. Then I would color the part of the  plot that reaches over the threshold in some alarmistic hue, like red. ",,
988,2,465,a56b0125-0379-468f-8014-407e90572478,2010-07-21 20:45:08.0,90.0,"I thought that the White Standard Error and the Standard Error computed in the ""normal"" way (eg, Hessian and/or OPG in the case of maximum likelihood) were asymptotically equivalent in the case of homoskedasticity? \\n\\nOnly if there is heteroskedasticity will the ""normal"" standard error be inappropriate, which means that the White Standard Error is appropriate with or without heteroskedasticity, that is, even when your model is homoskedastic.\\n\\nI can't really talk about 2, but I don't see the why one wouldn't want to calculate the White SE and include in the results.",,
989,2,466,da844589-3519-4b9c-ac95-6c6919b44c43,2010-07-21 20:53:54.0,115.0,"Not Statistics specific, but a good resource is:  http://www.reddit.com/r/mathbooks\\nAlso, George Cain at Georgia Tech maintains a list of freely available maths texts that includes some statistical texts.  http://people.math.gatech.edu/~cain/textbooks/onlinebooks.html\\n\\n",,
990,16,466,da844589-3519-4b9c-ac95-6c6919b44c43,2010-07-21 20:53:54.0,115.0,,,
991,2,467,37ba9731-3538-4def-a649-6758f2228689,2010-07-21 21:04:03.0,115.0,"http://www.reddit.com/r/datasets  and also, http://www.reddit.com/r/opendata both contain a constantly growing list of pointers to various datasets.\\n\\n",,
992,16,467,37ba9731-3538-4def-a649-6758f2228689,2010-07-21 21:04:03.0,-1.0,,,
993,4,128,e46e23b6-13b3-4c2a-9ff0-d0450df6aaf5,2010-07-21 21:29:10.0,132.0,How does one interpret a Bland-Altman plot?,edited title,
994,16,170,ea59aefb-4e66-488b-b9ee-471a3512ad13,2010-07-21 22:02:27.0,8.0,,,
995,6,170,ea59aefb-4e66-488b-b9ee-471a3512ad13,2010-07-21 22:02:27.0,8.0,<textbook><teaching>,Changing to community wiki,
996,2,468,6c23eca3-891a-405d-9119-c5e4201a0f5e,2010-07-21 22:05:04.0,251.0,"If you're coming from the programming side, one option is to use the [Natural Language Toolkit][1] (NLTK) for Python.  There's an O'Reilly book, [available freely][2], which might be a less dense and more practical introduction to building classifiers for documents among other things.  \\n\\nIf you're interested in beefing up on the statistical side, Roger Levy's book in progress, [Probabilistic Models in Study of Language][3], might not be bad to peruse.  It's written for cogsci/compsci grad students starting out with statistical NLP techniques.\\n\\n\\n  [1]: http://www.nltrk.org\\n  [2]: http://www.nltk.org/book\\n  [3]: http://idiom.ucsd.edu/~rlevy/textbook/text.html",,
997,2,469,ff62c928-21c7-471d-b782-e9fefdd74aed,2010-07-21 22:13:19.0,251.0,"The [Handbook of Computation Statistics][1] (Gentle, Härdle, Mori) is available online and quite good.  You'll also find a number of other books by Härdle on statistics in finance, nonparametrics, among other topics at the [same site][2].  \\n\\n\\n  [1]: http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/csa/\\n  [2]: http://fedc.wiwi.hu-berlin.de/xplore/ebooks/html/",,
998,16,469,ff62c928-21c7-471d-b782-e9fefdd74aed,2010-07-21 22:13:19.0,-1.0,,,
999,6,124,b26c5640-62e6-4c5f-bbda-aec4ca5f9612,2010-07-21 22:17:00.0,88.0,<classification><information-retrieval><text-mining>,edited tags,
1000,2,470,a6aea7ba-a79a-4089-a043-2f2f3792a49b,2010-07-21 22:35:38.0,36.0,"The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is a standard text for statistics and data mining, and is now free:\\n\\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn/\\n\\nWhy pay [$70 for it on amazon][1]?\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0387848576/ref=nosim/gettgenedone-20",,
1001,16,470,a6aea7ba-a79a-4089-a043-2f2f3792a49b,2010-07-21 22:35:38.0,-1.0,,,
1002,2,471,bbf2be15-bc9d-4b23-8a27-f8f51ad48a84,2010-07-21 22:57:58.0,168.0,Darrell Huff -- How to Lie with Statistics,,
1003,16,471,bbf2be15-bc9d-4b23-8a27-f8f51ad48a84,2010-07-21 22:57:58.0,-1.0,,,
1004,2,472,67c17969-9a61-4e2b-ad22-7f3f09e1a56a,2010-07-21 23:00:34.0,61.0,"For getting into stochastic processes and SDEs, Tom Kurtz's [lecture notes][1] are hard to beat.  It starts with a decent review of probability and some convergence results, and then dives right into continuous time stochastic processes in fairly clear, comprehensible language.  In general it's one of the best books on the topic -- free or otherwise -- I've found.\\n\\n\\n  [1]: http://www.math.wisc.edu/~kurtz/m735.htm",,
1005,16,472,67c17969-9a61-4e2b-ad22-7f3f09e1a56a,2010-07-21 23:00:34.0,-1.0,,,
1006,5,430,985d7cee-97ee-4980-ac45-1e40e6dd2637,2010-07-22 00:37:06.0,251.0,"I find that people find the solution more intuitive if you change it to 100 doors, close first, second, to 98 doors.  Similarly for 50 doors, etc.  ",deleted 2 characters in body,
1007,6,396,162b01f4-fe50-46dc-8b4f-7d095c7f8533,2010-07-22 00:39:48.0,159.0,<data-visualization><plotting><best-practices>,edited tags,
1008,5,457,e4b364e1-369d-4828-99da-fc041fba76b1,2010-07-22 01:21:24.0,,"I do not know the literature in the area well enough to offer a direct response. However, it seems to me that if the three tests differ then that is an indication that you need further research/data collection in order to definitively answer your question. \\n\\nYou may also want to look at [this][1] Google Scholar search\\n\\n**Update in response to your comment:**\\n\\nIf collecting additional data is not possible then there is one workaround. Do a simulation which mirrors your data structure, sample size and your proposed model. You can set the parameters to some pre-specified values. Estimate the model using the data generated and then check which one of the three tests points you to the right model. Such a simulation would offer some guidance as to which test to use for your real data. Does that make sense?\\n\\n  [1]: http://scholar.google.com/scholar?q=small%20sample%20properties%20of%20wald%20likelihood%20ratio&um=1&ie=UTF-8&sa=N&hl=en&tab=ws",added 494 characters in body,user28
1009,2,473,c7d165c3-d32d-4735-bdfb-0edbda47109e,2010-07-22 02:37:18.0,260.0,"The traditional solution to this problem is to use the [vector representation][1] for the news stories and then cluster the vectors.  The vectors are arrays where each entry represents a word or word class.  The value associated to each word will be the [tf-idf][2] weight.  This value goes up the more frequent the word in the document and down the more frequent the word is in the whole collection of documents.\\n\\nYou may think of the titles as the documents, but sticking to just the title for news stories may be a bit risky for clustering similar stories.  The problem is that by using word counts you are discarding all information on the order of the words. Longer texts compensate for that loss information by distinguishing documents by the vocabulary used (articles mentioning _finance_, _money_, ... are closer to each other than those mentioning _ergodic_, _Poincare_).\\n\\nIf you want to stick to titles, one idea is to think of word pairs as the words you use in the vector representation.  So for the title _The eagle has landed_, you would think of _the eagle_, _eagle has_, _has landed_. as the &ldquo;words.&rdquo;\\n\\nTo discover when a cluster has become much bigger or different from the others you will need to develop a decision procedure.\\n\\n[1]: http://dx.doi.org/10.1137/S0036144598347035\\n[2]: http://en.wikipedia.org/wiki/TF_IDF",,
1010,2,474,305a7e72-0754-4e2e-adb3-1c7bf6635a49,2010-07-22 03:21:18.0,260.0,"For testing the numbers produced by random number generators the [Diehard tests][1] are a practical approach. But those tests seem kind of arbitrary and one is may be left wondering if more should be included or if there is any way to really check the randomness.  \\n\\nThe best candidate for a definition of a random sequence seems to be the [Martin-Löf randomness][2].   The main idea for this kind of randomness, is beautifully developed in [Knuth, section 3.5][3], is to test for uniformity for all types of sub-sequences of the sequence of random numbers.  Getting that _all type of subsequences_ definition right turned out to be be really hard even when one uses notions of computability.\\n\\nThe Diehard tests are just some of the possible subsequences one may consider and their failure would exclude Martin-Löf randomness.\\n\\n\\n[1]: http://en.wikipedia.org/wiki/Diehard_tests\\n[2]: http://en.wikipedia.org/wiki/Algorithmically_random_sequence\\n[3]: http://en.wikipedia.org/wiki/The_Art_of_Computer_Programming",,
1011,2,475,643941cc-22e3-4d62-9f3f-3a7a1012442a,2010-07-22 03:55:48.0,253.0,There is also [Gephi][1] for plotting social networks.\\n\\n(p.s: Here is how to [connect it with R][2])\\n\\n\\n  [1]: http://gephi.org/\\n  [2]: http://www.r-bloggers.com/data-preparation-for-social-network-analysis-using-r-and-gephi/,,
1012,2,476,63bb4d01-3a1d-4955-91e7-457116b88b12,2010-07-22 04:34:32.0,251.0,"I won't give a definitive answer in terms of ranking the three.  Build 95% CIs around your parameters based on each, and if they're radically different, then your first step should be to dig deeper.  Transform your data (though the LR will be invariant), regularize your likelihood, etc.  In a pinch though, I would probably opt for the LR test and associated CI.  A rough argument follows.\\n\\nThe LR is invariant under the choice of parametrization (e.g. T versus logit(T)).  The Wald statistic assumes normality of (T - T0)/SE(T).  If this fails, your CI is bad.  The nice thing about the LR is that you don't need to find a transform f(T) to satisfy normality.  The 95% CI based on T will be the same.  Also, if your likelihood isn't quadratic, the Wald 95% CI, which is symmetric, can be kooky since it may prefer values with lower likelihood to those with higher likelihood.\\n\\nAnother way to think about the LR is that it's using more information, loosely speaking, from the likelihood function.  The Wald is based on the MLE and the curvature of the likelihood at null.  The Score is based on the slope at null and curvature at null.  The LR evaluates the likelihood under the null, and the likelihood under the union of the null and alternative, and combines the two.  If you're forced to pick one, this may be intuitively satisfying for picking the LR.\\n\\nKeep in mind that there are other reasons, such as convenience or computational, to opt for the Wald or Score.  The Wald is the simplest and, given a multivariate parameter, if you're testing for setting many individual ones to 0, there are convenient ways to approximate the likelihood.  Or if you want to add a variable at a time from some set, you may not want to maximize the likelihood for each new model, and the implementation of Score tests offers some convenience here.  The Wald and Score become attractive as your models and likelihood become unattractive.  (But I don't think this is what you were questioning, since you have all three available ...)",,
1013,2,477,65ab28a8-9e4e-4fd1-9e88-db5729dd00d9,2010-07-22 04:58:50.0,251.0,"I assume your friend prefers something that's biostatistics oriented.  Glantz's [Primer of Biostatistics][1] is a small book, an easy and quick read, and tends to get rave reviews from a similar audience.  If an online reference works, I like Gerard Dallal's [Handbook of Statitical Practice][2], which may do the trick if he's just refreshing previous knowledge.\\n\\n\\n  [1]: http://www.powells.com/biblio/62-9780071435093-1\\n  [2]: http://www.tufts.edu/~gdallal/LHSP.HTM\\n",,
1014,2,478,bdfd7861-2565-409b-ac5c-b78b60a5d017,2010-07-22 05:13:17.0,251.0,"I'm going to assume some basic statistics knowledge and recommend:\\n\\n- [The Statistical Sleuth][1] (Ramsey, Schafer) which contain a good deal of mini case studies as they cover the basic statistical tools for data analysis.  \\n\\n- [A First Course in Multivariate Statistics][2] (Flury) which covers the essential statistics required for data mining and the like.  \\n\\n",,
1015,16,478,bdfd7861-2565-409b-ac5c-b78b60a5d017,2010-07-22 05:13:17.0,-1.0,,,
1016,2,479,c0485573-7387-4e44-8de2-a9de97279f9d,2010-07-22 07:28:42.0,128.0,If it does not break your styleguide I would rather color the background of the plots red/(yellow/)green than just plotting a line. In my imagination this should make it pretty clear to a user that values are fine on green and to be checked on red. Just my 5&#xa2;.,,
1017,2,480,2adf0e2b-4013-4ff3-b104-e96328c95bcf,2010-07-22 08:58:36.0,223.0,I am not an expert of random forest but I clearly understand that the key issue with random forest is the (random) tree generation. Can you explain me how the trees are generated? (i.e. What is the used distribution for tree generation?)\\n\\nThanks in advance ! ,,
1018,1,480,2adf0e2b-4013-4ff3-b104-e96328c95bcf,2010-07-22 08:58:36.0,223.0,How does random forest generate the random forest ,,
1019,3,480,2adf0e2b-4013-4ff3-b104-e96328c95bcf,2010-07-22 08:58:36.0,223.0,<r><machine-learning><algorithms><cart>,,
1020,2,481,3f588222-1d5b-4592-90fc-9dcba2cd7d8f,2010-07-22 09:17:27.0,199.0,"Another question about time series from me.\\n\\nI have a dataset which gives daily records of violent incidents in a psychiatric hospital over three years. With the help from my previous question I have been fiddling with it and am a bit happier about it now.\\n\\nThe thing I have now is that the daily series is very noisy. It fluctuates wildly, up and down, from 0 at times up to 20. Using loess plots and the forecast package (which I can highly recommend for novices like me) I just get a totally flat line, with massive confidence intervals from the forecast.\\n\\nHowever, aggregating weekly or monthly the data make a lot more sense. They sweep down from the start of the series, and then increase again in the middle. Loess plotting and the forecast package both produce something that looks a lot more meaningful.\\n\\nIt does feel a bit like cheating though. Am I just preferring the aggregated versions because they look nice with no real validity to it?\\n\\nOr would it be better to compute a moving average and use that as the basis? I'm afraid I don't understand the theory behind all this well enough to be confident about what is acceptable",,
1021,1,481,3f588222-1d5b-4592-90fc-9dcba2cd7d8f,2010-07-22 09:17:27.0,199.0,Is it valid to aggregate a time series to make it look more meaningful?,,
1022,3,481,3f588222-1d5b-4592-90fc-9dcba2cd7d8f,2010-07-22 09:17:27.0,199.0,<time-series><forecasting>,,
1023,2,482,ed04540b-8993-49d1-990f-3e0a05bd7aa3,2010-07-22 09:24:52.0,190.0,"This totally depends on your time series and what effect you want to discover/proof etc.\\n\\nAn important thing here is, what kind of periods do you have in your data. Make a spectrum of you data and see what frequencies are common in you data.\\n\\nAnyway, you are not lying when you decide to display aggregated values. When you are looking to effects that are occurring over weeks (like, more violence in summer when it's hot weather) it is the right thing to do.\\n\\nMaybe you can also take a look at the Hilbert Huang Transform. This will give you Intrinsic Mode Functions that are very handy for visual analyses.",,
1024,2,483,f6e560f0-4f29-4f11-8883-de3a44d09c4f,2010-07-22 09:53:52.0,88.0,"The main idea is the bagging procedure, not making trees random. In detail, each tree is build on a sample of objects drawn with replacement from the original set; thus each tree has some objects that it haven't seen, what makes it more generalizing.  \\nFurthermore, trees are being weekend in such a way that on the each split only M randomly selected attributes are considered; M is usually a square root of the number of attributes in the set. This makes the trees less overfitted, because they are not pruned.\\n\\nYou can find more details [here][1]. \\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm",,
1026,5,483,e7232a29-6bea-4ea0-ac51-858f870898a3,2010-07-22 10:04:38.0,88.0,"The main idea is the bagging procedure, not making trees random. In detail, each tree is build on a sample of objects drawn with replacement from the original set; thus each tree has some objects that it haven't seen, what makes the whole ensemble more heterogeneous and thus better in generalizing.  \\nFurthermore, trees are being weekend in such a way that on the each split only M (or `mtry`) randomly selected attributes are considered; M is usually a square root of the number of attributes in the set. This makes the trees less overfitted, since they are not pruned.  \\nYou can find more details [here][1]. \\n\\nOn the other hand, there is a variant of RF called Extreme Random Forest, in which trees are made in more less random way (there is no optimization of splits) -- consult, I think, http://www.springerlink.com/index/10.1007/s10994-006-6226-1.\\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm",added 303 characters in body,
1027,2,485,a8357919-a99d-4836-a961-d6be46399262,2010-07-22 10:08:10.0,183.0,A question previously sought recommendations for [textbooks on mathematical statistics][1]\\n\\nDoes anyone know of any good online **video lectures** on mathematical statistics?\\nThe closest that I've found are:\\n\\n - [Machine Learning][2] \\n - [Econometrics][3] \\n\\n\\n  [1]: http://stats.stackexchange.com/questions/414/intro-to-statistics-for-mathematicians\\n  [2]: http://www.youtube.com/watch?v=UzxYlbK2c7E&feature=PlayList&p=A89DCFA6ADACE599&index=0\\n  [3]: http://economistsview.typepad.com/economics421/,,
1028,1,485,a8357919-a99d-4836-a961-d6be46399262,2010-07-22 10:08:10.0,183.0,Mathematical Statistics Videos,,
1029,3,485,a8357919-a99d-4836-a961-d6be46399262,2010-07-22 10:08:10.0,183.0,<mathematics><books><mathematical-statistics>,,
1030,2,486,69a2e70a-37de-44de-872c-08adba50a5df,2010-07-22 10:11:15.0,266.0,"I have calculated AIC and AICc to compare two general linear mixed models; The AICs are positive with model 1 having a lower AIC than model 2.  However, the values for AICc are both negative (model 1 is still < model 2).  Is it valid to use and compare negative AICc values? ",,
1031,1,486,69a2e70a-37de-44de-872c-08adba50a5df,2010-07-22 10:11:15.0,266.0,Negative values for AICc (corrected Akaike Information Criterion),,
1032,3,486,69a2e70a-37de-44de-872c-08adba50a5df,2010-07-22 10:11:15.0,266.0,<statistical-analysis><glm><model-choice><aic>,,
1033,2,487,79d39431-a9a7-4b45-b739-88fad43ca195,2010-07-22 10:15:28.0,266.0,"Not a book, but I recently discovered an article by Jacob Cohen in American Psychologist entitled ""Things I have learned (so far).""  It's available as a pdf [here][1].  \\n\\n\\n  [1]: http://www.uvm.edu/~bbeckage/Teaching/DataAnalysis/AssignedPapers/Cohen_1990.pdf",,
1034,16,487,79d39431-a9a7-4b45-b739-88fad43ca195,2010-07-22 10:15:28.0,-1.0,,,
1035,2,488,99fbfee4-2256-46a5-949e-ada7185f3667,2010-07-22 10:16:18.0,183.0,A lot of Social Science / Psychology students with minimal mathematical background like Andy Field's book: Discovering Statistics Using SPSS. He also has a website that shares a [lot of material][1].\\n\\n\\n  [1]: http://www.statisticshell.com/woodofsuicides.html,,
1036,16,488,99fbfee4-2256-46a5-949e-ada7185f3667,2010-07-22 10:16:18.0,-1.0,,,
1037,2,489,7d919442-3bf8-4669-9b1e-cbad4eee1d80,2010-07-22 11:05:55.0,214.0,It also depends on where you wan't to publish your plots. You'll save yourself a lot of trouble by consulting the guide for authors before making any plots for a journal. \\n\\nAlso save the plots in a format that is easy to modify or save the code you have used to create them. Chances are that you need to make corrections. ,,
1038,2,490,002db08a-661a-49c5-88e7-1cfb370dc1f5,2010-07-22 11:10:29.0,223.0,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n",,
1039,1,490,002db08a-661a-49c5-88e7-1cfb370dc1f5,2010-07-22 11:10:29.0,223.0,Variable selection procedure for binary classification,,
1040,3,490,002db08a-661a-49c5-88e7-1cfb370dc1f5,2010-07-22 11:10:29.0,223.0,<machine-learning><classification>,,
1041,2,491,e62bdd4c-1781-45a2-8f61-d640ea7c0f7b,2010-07-22 11:19:10.0,268.0,I recently found [Even You Can Learn Statistics][1] to be pretty useful.\\n\\n\\n  [1]: http://www.amazon.com/Even-You-Can-Learn-Statistics/dp/0131467573,,
1042,16,491,e62bdd4c-1781-45a2-8f61-d640ea7c0f7b,2010-07-22 11:19:10.0,-1.0,,,
1043,2,492,bfbcf975-25b9-4a39-bbfb-986a2770f579,2010-07-22 11:31:16.0,210.0,"I am proposing to try and find a trend in some very noisey long term data. The data is basically weekly measurements of something which moved about 5mm over a period of about 8 months. The data is to 1mm accuracey and is very noisey regularly changing +/-1 or 2mm in a week. We only have the data to the nearest mm. \\n\\nWe plan to use some basic singla processing with a fast fourier transform to seaparate out the noise from the raw data. The basic assumption is if we mirror our data set and add it to the end of our existing data set we can create a full wavelength of the data and therefore our data will show up in a fast fourier transform and we can hopefully then separate it out. \\n\\nGiven that this sounds a little dubious to me, is this a method worth purusing or is the method of mirroring and appending our data set somehow fundamentally flawed? We are looking at other approaches such as using a low pass filter as well. ",,
1044,1,492,bfbcf975-25b9-4a39-bbfb-986a2770f579,2010-07-22 11:31:16.0,210.0,Dubious use of signal processing principals to identify a trend,,
1045,3,492,bfbcf975-25b9-4a39-bbfb-986a2770f579,2010-07-22 11:31:16.0,210.0,<data-mining>,,
1046,2,493,59273ad3-0641-4805-9660-2d5f757969d7,2010-07-22 11:33:47.0,268.0,Statsoft's [Electronic Statistics Handbook][1] ('The only Internet Resource about Statistics Recommended by Encyclopedia Britannica') is worth checking out.\\n\\n\\n  [1]: http://www.statsoft.com/textbook/,,
1047,16,493,59273ad3-0641-4805-9660-2d5f757969d7,2010-07-22 11:33:47.0,-1.0,,,
1048,2,494,37404a21-781a-48b2-ae1e-b8e38c99f5da,2010-07-22 11:42:42.0,88.0,"Generally, it is assumed that AIC is defined up to adding a constant, so the fact if it is negative or positive is not meaningful at all. So the answer is yes, it is valid.",,
1049,2,495,1ed0da2c-cb47-410b-9edd-80b9e7e2f050,2010-07-22 11:43:42.0,214.0,"There is one called [Math and probability for life sciences][1], but I haven't followed it so I can't tell you if its good or not.\\n\\n\\n  [1]: http://www.academicearth.org/courses/math-and-proability-for-life-sciences",,
1050,2,496,3e2e451b-d3f8-4dee-a87f-2bc733979a0a,2010-07-22 11:44:13.0,190.0,I think you can get some distortion on the pasting point as not all the underlying waves will connect very well.\\n\\nI would suggest using a Hilbert Huang transform for this. Just do the split into intrinsic mode functions and see what is left over as residue when calculating them.,,
1051,2,497,05ba7635-4f1e-41d4-9bb5-575ac2d26c8d,2010-07-22 11:53:51.0,190.0,"Greedy forward selection.\\n\\nThe steps for this method are:\\n\\n- Make sure you have a train and validation set\\n- Repeat the following\\n    - Train a classifier with each single feature separately that is not selected yet and with all the previously selected features\\n    - If the result improves, add the best performing feature, else stop procedure\\n",,
1052,16,490,f25f1565-c142-4ead-ae8f-52a2fa59c91a,2010-07-22 11:58:13.0,223.0,,,
1053,2,498,ecb3fae1-79b8-4497-8673-27ed44117728,2010-07-22 11:58:21.0,62.0,"Sometimes, I just want to do a copy & paste from the output window in SAS. I can highlight text with a mouse-drag, but only SOMETIMES does that get copied to the clipboard. It doesn't matter if I use ""CTRL-C"" or right click -> copy, or edit -> copy\\n\\nAny other SAS users experience this, and do you know a workaround/option/technique that can fix it? \\n\\nSometimes, I can fix it by clicking in another window, and coming back to the output window, but sometimes I just have to save the output window as a .lst and get the text from another editor. \\n\\n",,
1054,1,498,ecb3fae1-79b8-4497-8673-27ed44117728,2010-07-22 11:58:21.0,62.0,"In PC SAS, how do you copy & paste from the output window?",,
1055,3,498,ecb3fae1-79b8-4497-8673-27ed44117728,2010-07-22 11:58:21.0,62.0,<sas><pc-sas>,,
1056,2,499,51043432-6abf-4e50-bbab-4c36a7670267,2010-07-22 12:06:28.0,267.0,"I've heard that when many regression model specifications (say, in OLS) are considered as possibilities for a dataset, this causes multiple comparison problems and the p-values and confidence intervals are no longer reliable. One extreme example of this is stepwise regression.\\n\\nWhen can I use the data itself to help specify the model, and when is this not a valid approach? Do you always need to have a subject-matter-based theory to form the model?\\n",,
1057,1,499,51043432-6abf-4e50-bbab-4c36a7670267,2010-07-22 12:06:28.0,267.0,When can you use data-based criteria to specify a regressionmodel?,,
1058,3,499,51043432-6abf-4e50-bbab-4c36a7670267,2010-07-22 12:06:28.0,267.0,<regression><frequentist><multiple-comparisons>,,
1059,2,500,4e422acb-4676-4de7-be89-0dc690a56d55,2010-07-22 12:16:29.0,190.0,"If I understand your question right, than the answer to your problem is to correct the p-values accordingly to the number of hypothesis.\\n\\nFor example Holm-Bonferoni corrections, where you sort the hypothesis (= your different models) by their p-value and reject those with a p samller than (desired p-value / index).\\n\\nMore about the topic can be found on [Wikipedia][1]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Multiple_comparisons",,
1060,2,501,969c7bea-636d-4f78-9151-cfd39cb5c8f9,2010-07-22 12:17:20.0,88.0,"Backward elimination.\\n\\nStart with the full set, then iteratively train the classifier on the remaining features and remove the feature wit the smallest importance, stop when the classifier error rapidly increases/becomes unacceptable high.\\n\\nImportance can be even obtained by removing iteratively each feature and check the error increase or adapted from the classifier if it produces it (like in case of Random Forest). \\n",,
1061,16,501,969c7bea-636d-4f78-9151-cfd39cb5c8f9,2010-07-22 12:17:20.0,-1.0,,,
1062,2,502,b0c56706-c7cf-4ec7-912d-9c2c4dfe8c19,2010-07-22 12:25:42.0,,I do not know at what level you want the videos to be but I have heard good things about Khan's Academy: http://www.khanacademy.org/#Statistics,,user28
1063,2,503,8a60f4d9-8290-46b3-a0cb-f7092b33190b,2010-07-22 12:25:48.0,256.0,"I have been using SAS a long time and have never had an issue with highlighting results from the output window.\\n\\nHowever since you are having an issue... there are alarge number of solutions!\\n\\nPerhaps the most i like... and probably a good habit to get into is to output your results into datasets... or into excel spread sheets directly (using the ODS) you can also output directly into pdf, rtf with 2 lines of the simplest code you can imagine!\\nif your code produces alot of output and you only have one table you want to copy you can specify the name of the table and it alone will be output using the ODS output.\\n\\nusually you just need to wrap your Procedure (like Proc Means for example)\\nwith \\n\\nods PDF;\\nProc Means Data = blah N NMISS MEAN STD;\\nclass upto you;\\nvar you name it;\\nrun;\\nods PDF close;\\n\\nof course there are many ways to get fancy with the way the output looks but that is a matter of trial and error and finding what you like or meets your needs.",,
1064,2,504,daf38661-3c55-4b6b-aa4a-e4bab9cab622,2010-07-22 12:33:49.0,5.0,Many of the **Berkeley** introductory statistics courses are available online (and on iTunes).  Here's an example: [**Stats 2**][1].  You can [find more here][2].\\n\\n\\n  [1]: http://webcast.berkeley.edu/course_details.php?seriesid=1906978493\\n  [2]: http://www.google.com/search?hl=en&rlz=1C1CHMP_en-USUS292US307&q=statistics+video+site:webcast.berkeley.edu+&aq=f&aqi=&aql=&oq=&gs_rfai=,,
1065,2,505,5585bce0-76c5-4f96-a957-3f86bd2b1bca,2010-07-22 12:42:13.0,88.0,"Metropolis scanning / MCMC\\n\\n - Select few features randomly for a\\n   start, train classifier only on them\\n   and obtain the error. \\n - Make some\\n   random change to this working set --\\n   either remove one feature, add\\n   another at random or replace some\\n   feature with one not being currently\\n   used.\\n - Train new classifier and get\\n   its error; store in `dE` the difference\\n   the error on the new set minus the error on the previous set. \\n - With probability `min(1;exp(-beta*dE))` accept this change, otherwise reject it and try another random change.\\n - Repeat it for a long time and finally return the working set that has globally achieved the smallest error.\\n\\nYou may extend it with some wiser control of `beta` parameter. Simpler way is to use simulated annealing when you increase `beta` (lower the temperature in physical analogy) over the time to reduce fluctuations and drive the algorithm towards minimum. Harder is to use [replica exchange][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Replica_exchange",,
1066,16,505,5585bce0-76c5-4f96-a957-3f86bd2b1bca,2010-07-22 12:42:13.0,-1.0,,,
1067,6,490,8553305d-48b1-41c5-8122-33ff20cded5d,2010-07-22 12:45:27.0,88.0,<machine-learning><classification><feature-selection>,edited tags,
1068,6,421,673b9773-839a-4584-a7a9-2c64642d1fd8,2010-07-22 12:56:00.0,223.0,<textbook><books><science>,edited tags,
1069,5,501,51e7fa20-309a-485e-80c3-6499f4fbbcb4,2010-07-22 13:05:25.0,223.0,"Backward elimination.\\n\\nStart with the full set, then iteratively train the classifier on the remaining features and remove the feature with the smallest importance, stop when the classifier error rapidly increases/becomes unacceptable high.\\n\\nImportance can be even obtained by removing iteratively each feature and check the error increase or adapted from the classifier if it produces it (like in case of Random Forest). \\n",added 1 characters in body,
1070,2,506,5664604c-087f-42cf-9dc2-636062bb025f,2010-07-22 13:10:57.0,215.0,"\\nBoth MOM and GMM are very general methods for estimating parameters of statistical models. GMM is - as the name suggests - a generalisation of MOM. It was developed by Lars Peter Hansen and first published in Econometrica [1]. As there are numerous textbooks on the subject (e.g. [2]) I presume you want a non-technical answer here.\\n\\n**Traditional or Classical Method of Moments Estimator**\\n\\nThe MOM estimator is a consistent but inefficient estimator. Assume a vector of data y which were generated by a probability distribution indexed by a parameter vector theta with k elements. In the method of moments, theta is estimated by computing k sample moments of y, setting them equal to population moments derived from the assumed probability distribution, and solving for theta. For example, the population moment of mu is the expectation of y, whereas the sample moment of mu is the sample mean of y. You would repeat this for each of the k elements of theta. As sample moments are generally consistent estimators of population moments, theta-hat will be consistent for theta.\\n\\n**Generalised Method of Moments**\\n\\nIn the example above, we had the same number of moment conditions as unknown parameters, so all we would have done is solved the k equations in k unknowns to obtain the parameter estimates. Hansen asked: What happens when we have more moment conditions than parameters as usually occurs in econometric models? How can we combine them optimally? That is the purpose of the GMM estimator. In GMM we estimate the parameter vector by minimising the sum of squares of the differences between the population moments and the sample moments, using the variance of the moments as a metric. This is the minimum variance estimator in the class of estimators that use these moment conditions. \\n\\n\\n[1] Hansen, L. P.  (1982): Large Sample Properties of Generalized Method of Moments Estimators, *Econometrica*, 50, 1029-1054\\n\\n[2] Hall, A. R. (2005). *Generalized Method of Moments (Advanced Texts in Econometrics).* Oxford University Press",,
1071,5,499,7d7faebe-eb4e-4208-aea7-c3a4326c5574,2010-07-22 13:26:10.0,267.0,"I've heard that when many regression model specifications (say, in OLS) are considered as possibilities for a dataset, this causes multiple comparison problems and the p-values and confidence intervals are no longer reliable. One extreme example of this is stepwise regression.\\n\\nWhen can I use the data itself to help specify the model, and when is this not a valid approach? Do you always need to have a subject-matter-based theory to form  the model?\\n",missing space in title,
1072,4,499,7d7faebe-eb4e-4208-aea7-c3a4326c5574,2010-07-22 13:26:10.0,267.0,When can you use data-based criteria to specify a regression model?,missing space in title,
1073,2,507,bc284b4a-1517-4807-b130-a5df8273d6ea,2010-07-22 13:40:30.0,215.0,"\\nWhat is your preferred method of checking for convergence when using Markov chain Monte Carlo for Bayesian inference, and why?",,
1074,1,507,bc284b4a-1517-4807-b130-a5df8273d6ea,2010-07-22 13:40:30.0,215.0,What is the best method for checking convergence in MCMC?,,
1075,3,507,bc284b4a-1517-4807-b130-a5df8273d6ea,2010-07-22 13:40:30.0,215.0,<bayesian><mcmc>,,
1076,2,508,0fedc8d5-ae99-4fc0-83b9-b5efb453d41a,2010-07-22 13:41:53.0,266.0,"I would add that the choice of plot should reflect the type of statistical test used to analyse the data.  In other words, whatever characteristics of the data were used for analysis should be shown visually - so you would show means and standard errors if you used a t-test but boxplots if you used a Mann-Whitney test.",,
1077,2,509,6150660b-fc75-43d4-9a36-5d1368a68d89,2010-07-22 13:43:18.0,,I like to do trace plots primarily and sometimes I use the Gelman-Rubin convergence diagnostic.,,user28
1078,2,510,b1734a00-65ab-4dd0-ad9d-7dd99583d308,2010-07-22 13:49:37.0,266.0,"More about design and power than analysis, but I like this one\\n\\n![alt text][1]\\n\\n\\n  [1]: http://imgs.xkcd.com/comics/experimentation.png",,
1079,16,510,b1734a00-65ab-4dd0-ad9d-7dd99583d308,2010-07-22 13:49:37.0,-1.0,,,
1080,5,485,69396aa3-51bd-4614-ab5c-c5e8763618b0,2010-07-22 13:51:20.0,183.0,A question previously sought recommendations for [textbooks on mathematical statistics][1]\\n\\nDoes anyone know of any good online **video lectures** on **mathematical statistics**?\\nThe closest that I've found are:\\n\\n - [Machine Learning][2] \\n - [Econometrics][3] \\n\\n  [1]: http://stats.stackexchange.com/questions/414/intro-to-statistics-for-mathematicians\\n  [2]: http://www.youtube.com/watch?v=UzxYlbK2c7E&feature=PlayList&p=A89DCFA6ADACE599&index=0\\n  [3]: http://economistsview.typepad.com/economics421/,emphasis on mathematical statistics,
1081,5,485,17ec5ee5-6bb6-47a0-8f78-9972a36e5961,2010-07-22 14:06:03.0,183.0,"A question previously sought recommendations for [textbooks on mathematical statistics][1]\\n\\nDoes anyone know of any good online **video lectures** on **mathematical statistics**?\\nThe closest that I've found are:\\n\\n - [Machine Learning][2] \\n - [Econometrics][3] \\n\\n**UPDATE:** A number of the suggestions mentioned below are good statistics-101 type videos.\\nHowever, I'm specifically wondering whether there are any videos that provide a rigorous mathematical presentation of statistics.\\ni.e., videos that might accompany a course that use a textbook mentioned in this [discussion on mathoverflow][4]\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/414/intro-to-statistics-for-mathematicians\\n  [2]: http://www.youtube.com/watch?v=UzxYlbK2c7E&feature=PlayList&p=A89DCFA6ADACE599&index=0\\n  [3]: http://economistsview.typepad.com/economics421/\\n  [4]: http://mathoverflow.net/questions/31655/statistics-for-mathematicians",clarification of question on mathematical statistics,
1082,16,485,17ec5ee5-6bb6-47a0-8f78-9972a36e5961,2010-07-22 14:06:03.0,183.0,,,
1083,2,511,532bab31-79d4-431a-b52b-c7be77a629b8,2010-07-22 14:32:20.0,3807.0,"From [xkcd][1]:\\n\\n![You don't use science to show that you are right, you use science to become right.][2]\\n\\n\\nIf some people who really believe that everything should be scientifically tested would actually walk their talk than they this comic might even show an event that actually happens.\\n\\n\\n  [1]: http://xkcd.com/701/\\n  [2]: http://imgs.xkcd.com/comics/science_valentine.png",,
1084,16,511,532bab31-79d4-431a-b52b-c7be77a629b8,2010-07-22 14:32:20.0,-1.0,,,
1085,2,512,40cf407b-e08c-46af-8a40-90184271db17,2010-07-22 14:52:54.0,33.0,"If you want to filter the long term trend out using signal processing, why not just use a low-pass? \\n\\nThe simplest thing I can think of would be an exponential moving average.\\n",,
1086,2,513,fb7cfafe-ae3d-44b6-8191-c2e7c53cee07,2010-07-22 15:22:20.0,260.0,"I like to use resampling: I repeat whatever method I used with a subsample of the data (say 80% or even 50% of the total).  By doing this with many different subsamples, I get a feel for how robust the estimates are.  For many estimation procedures this can be made into a real (meaning publishable) estimate of your errors.",,
1087,2,514,ad6969a3-be48-463e-8f44-50d6d181a70d,2010-07-22 15:36:53.0,266.0,"Here's how I would answer this question using a diagram.  \\n\\nLet's say we weigh 30 cats and calculate the mean weight.  Then we produce a scatter plot, with weight on the y axis and cat identity on the x axis.  The mean weight can be drawn in as a horizontal line.  We can then draw in vertical lines which connect each data point to the mean line - these are the deviations of each data point from the mean, and we call them residuals.  Now, these residuals can be useful because they can tell us something about the spread of the data: if there are many big residuals, then cats vary a lot in mass.  Conversely, if the residuals are mainly small, then cats are fairly closely clustered around the average weight.  So if we could have some metric which tells us the _average_ length of a residual in this data set, this would be a handy way of denoting how much spread there is in the data.  The standard deviation is, effectively, the length of the average residual. \\n\\nI would follow on on from this by giving the calculation for  s.d., explaining why we square and then square root (I like Vaibhav's short and sweet explanation).  Then I would mention the problems of outliers, as Graham does in his last paragraph.\\n\\n",,
1088,2,515,0357736d-1a28-464e-9e5c-76a97e795c52,2010-07-22 15:49:54.0,270.0,"1) A good demonstration of how ""random"" needs to be defined in order to work out probability of certain events:\\n\\nWhat is the chance that a random line drawn across a circle will be longer than the radius?\\n\\nThe question totally depends how you draw your line. Possibilities which you can describe in a real-world way for a circle drawn on the ground might include:\\n\\nDraw two random points inside the circle and draw a line through those. (see where two flies / stones fall...)\\n\\nChoose a fixed point on the circumference, then a random one elsewhere in the circle and join those. (in effect this is laying a stick across the circle at a variable angle through a given point and a random one eg where stone falls)\\n\\nDraw a diameter. Randomly choose a point along it and draw a perpendicular through that. (roll a stick along in a straight line so it rests across the circle)\\n\\nIt is relatively easy to show someone who can do some geometry (but not necessarily stats) the answer to the question can vary quite widely (from about 2/3 to about 0.866 or so)\\n\\n2) A reverse-engineered coin-toss: toss it (say) ten times and write down the result. Work out the probabilty of this exact sequence (1/2^10). A tiny chance, but you just saw it happen with your own eyes!... Every sequence *might* come up, including ten heads in a row, but it is hard for lay people to get their head round it. As an encore, try to convince them they have just as good a chance of winning the lottery with the numbers 1 through 6 as any other combination.\\n\\n3) Explaining why medical diagnosis may seem really flawed. A test for disease foo which is 99.9% accurate at identifying those who have it but .1% false-positively diagnoses those who don't really have it may seem to be wrong really so often when the prevalence of the disease is really low (eg 1 in 1000) but many patients are tested for it. \\n\\nThis is one that is best explained with real numbers - imagine 1 million people are tested, so 1000 have the disease, 999 are correctly identified, but 0.1% of 999,000 is 999 who are told they have it but don't. So half those who are told they have it actually do not, despite the high level of accuracy (99.9%) and low level of false posisitive (0.1%). A second (ideally different) test will then separate these groups out. \\n\\n[incidentally, I chose the numbers because they are easy to work with, of course they do not have to add up to 100% as the accuracy / fales positive rates are independent factors in the test]",,
1089,2,516,8fc128d8-5a6c-48fd-8a44-f1ae08936d1a,2010-07-22 16:00:44.0,251.0,"Yes it's valid to compare negative AICc values, in the same way as you would negative AIC values.  The correction factor in the AICc can become large with small sample size and relatively large number of parameters, and penalize heavier than the AIC.  So positive AIC values can correspond to negative AICc values.\\n",,
1090,5,478,45b2b15c-f2cb-4780-aa48-70e2a001eccc,2010-07-22 16:13:22.0,251.0,"I'm going to assume some basic statistics knowledge and recommend:\\n\\n- [The Statistical Sleuth][1] (Ramsey, Schafer) which contain a good deal of mini case studies as they cover the basic statistical tools for data analysis.  \\n\\n- [A First Course in Multivariate Statistics][2] (Flury) which covers the essential statistics required for data mining and the like.  \\n\\n  [1]: http://www.powells.com/biblio/62-9780071435093-1\\n  [2]: http://www.tufts.edu/~gdallal/LHSP.HTM\\n",added 104 characters in body,
1091,5,494,9c0509a3-288f-4509-80b4-d262d6251810,2010-07-22 16:15:51.0,88.0,"Generally, it is assumed that AIC is defined up to adding a constant, so the fact if it is negative or positive is not meaningful at all. So the answer is yes, it is valid.\\n\\nEdit: I thought it is obvious, but still --  the same holds for AICc.",added 73 characters in body,
1092,2,517,bd8740ac-88bd-464d-b3f8-890e57854238,2010-07-22 16:21:06.0,68.0,"In the context of machine learning, what is the difference between unsupervised learning, supervised learning and semi-supervised learning?",,
1093,1,517,bd8740ac-88bd-464d-b3f8-890e57854238,2010-07-22 16:21:06.0,68.0,"Unsupervised, supervised and semi-supervised learning",,
1094,3,517,bd8740ac-88bd-464d-b3f8-890e57854238,2010-07-22 16:21:06.0,68.0,<machine-learning>,,
1095,2,518,8717eb2a-81d5-47dd-8cff-1da4fa4b1f4d,2010-07-22 16:24:10.0,88.0,Try [R][1]. [Here][2] you have a list of clustering packages available.\\n\\n\\n  [1]: http://www.r-project.org/\\n  [2]: http://cran.at.r-project.org/web/views/Cluster.html,,
1096,2,519,acf50c30-fa2e-44b6-8418-53db8740029b,2010-07-22 16:32:57.0,270.0,"I read (a long time ago) of an interesting example about a decline in birth rates (or fertility rates if you prefer that measure) especially in the US, starting in the early 1960's,  as nuclear weapons testing was at an all-time high (in 1961 the biggest nuclear bomb ever detonated was tested in the USSR). Rates continued to deline until towards the end of the twentieth century when most countried finally stopped doing this.\\n\\nI can't find a reference which combines these figures now, but this Wikipedia article has figures on [nuclear weapons test][1] numbers by country.\\n\\nOf course, it might make better sense to look at the correlation of birth rate with the introduction and legalisation of the contraceptive pill 'coincidentally' starting in the early 1960s.  (In only some states first, then all states for married women only, then some for unmarried, then across the board), But even that could only be part of the cause; lots of other aspects of equality, economic changes and other factors play a significant part.\\n\\n  [1]: http://en.wikipedia.org/wiki/Nuclear_weapons_testing",,
1097,2,520,a4e42909-a7ee-490c-bb65-8ffb4cbf558e,2010-07-22 16:33:43.0,190.0,"Python will give you all the flexibility you need. With the NumPy and [SciPy cluster module][1] you have the tools you need, and the datatypes of NumPy give you a good insight in how much memory you will use.\\n\\n\\n  [1]: http://docs.scipy.org/doc/scipy/reference/cluster.html",,
1098,2,521,a23c8cd5-8c99-41bb-b8c1-6be92bf8c7a5,2010-07-22 16:39:05.0,190.0,"**Unsupervised Learning**\\n\\nUnsupervised learning is when you have no labeled data available for training. Examples of this are often clustering methods.\\n\\n**Supervised Learning**\\n\\nIn this case your training data exists out of labeled data. The problem you solve here is often predicting the labels for data points without label.\\n\\n**Semi-Supervised Learning**\\n\\nIn this case both labeled data and unlabeled data are used. This for example can be used in Deep belief networks, where some layers are learning the structure of the data (unsupervised) and one layer is used to make the classification (trained with supervised data)",,
1099,5,517,4ef5e91c-eb9b-4687-be59-3a5d48342624,2010-07-22 16:44:09.0,68.0,"In the context of machine learning, what is the difference between unsupervised learning, supervised learning and semi-supervised learning?\\n\\nAnd what are some of the main algorithmic approaches to look at?",added 68 characters in body,
1100,5,3,3179afaa-9c64-4275-b102-f3faa07ced3d,2010-07-22 17:33:17.0,18.0,"What are some valuable Statistical Analysis open source projects available right now?\\n\\nEdit: as pointed out by Sharpie, valuable could mean that helping you get things done faster and/or cheaper.",added 111 characters in body; added 1 characters in body,
1101,2,522,24979a9f-14d6-4250-8568-929392da1f8b,2010-07-22 18:03:43.0,39.0,"Generally, the problems of machine learning may be considered variations on function estimation for classification or prediction.\\n\\nIn **supervised learning** one is furnished with input (x1, x2, . .,) and output (y1, y2, . .,) and are challenged with finding a function that approximates this behavior in a generalizable fashion.  The output could be a class label (in classification) or a real number (in regression)-- these are the ""supervision"" in supervised learning. \\n\\nIn the case of **unsupervised learning**, in the base case, you receives inputs x1, x2, . ., but neither target outputs, nor rewards from its environment are provided.  Based on the problem (classify, or predict) and your background knowledge of the space sampled, you may use various methods: density estimation (estimating some underlying PDF for prediction), k-means clustering (classifying unlabeled real valued data), k-modes clustering (classifying unlabeled ordinal data), etc.\\n\\n**Semi-supervised learning** involves function estimation on labeled and unlabeled data.  This approach is motivated by the fact that labeled data is often costly to generate, whereas unlabeled data is generally not.  The challenge here mostly involves the technical question of how to treat data mixed in this fashion. \\n\\nIn addition to these kinds of learning, there are others, such as **reinforcement learning** whereby the learning method interacts with its environment by producing actions a1, a2, . . .. that produce rewards or punishments r1, r2, ...\\n\\n\\n",,
1102,5,522,32912819-328a-4516-9882-adbcc44dd5a7,2010-07-22 18:11:23.0,39.0,"Generally, the problems of machine learning may be considered variations on function estimation for classification or prediction.\\n\\nIn **supervised learning** one is furnished with input (x1, x2, . .,) and output (y1, y2, . .,) and are challenged with finding a function that approximates this behavior in a generalizable fashion.  The output could be a class label (in classification) or a real number (in regression)-- these are the ""supervision"" in supervised learning. \\n\\nIn the case of **unsupervised learning**, in the base case, you receives inputs x1, x2, . ., but neither target outputs, nor rewards from its environment are provided.  Based on the problem (classify, or predict) and your background knowledge of the space sampled, you may use various methods: density estimation (estimating some underlying PDF for prediction), k-means clustering (classifying unlabeled real valued data), k-modes clustering (classifying unlabeled ordinal data), etc.\\n\\n**Semi-supervised learning** involves function estimation on labeled and unlabeled data.  This approach is motivated by the fact that labeled data is often costly to generate, whereas unlabeled data is generally not.  The challenge here mostly involves the technical question of how to treat data mixed in this fashion. See this [Semi-Supervised Learning Literature Survey][1] for more details on semi-supervised learning methods.\\n\\nIn addition to these kinds of learning, there are others, such as **reinforcement learning** whereby the learning method interacts with its environment by producing actions a1, a2, . . .. that produce rewards or punishments r1, r2, ...\\n\\n\\n  [1]: http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf",added 170 characters in body,
1103,2,523,e80f9e28-daf9-4cac-b40a-95dedac45a0e,2010-07-22 18:16:08.0,74.0,"I don't think that supervised/unsupervised is the best way to think about it. For basic data mining, it's better to think about what you are trying to do. There are four main tasks:\\n\\n1. prediction. if you are predicting a real number, it is called regression. if you are predicting a whole number or class, it is called classification.\\n\\n2. modeling. modeling is the same as prediction, but the model is comprehensible by humans. Neural networks and support vector machines work great, but do not produce comprehensible models [1]. decision trees and classic linear regression are examples of easy-to-understand models. \\n\\n3. clustering. if you are trying to find natural groups of attributes, it is called factor analysis. if you are trying to find natural groups of observations, it is called clustering.\\n\\n4. association. it's much like correlation, but for enormous binary datasets. \\n\\n\\n[1] Apparently Goldman Sachs created tons of great neural networks for prediction, but then no one understood them, so they had to write other programs to try to explain the neural networks. ",,
1104,5,522,f71080c2-a8a7-4171-bab7-9476dd30b37f,2010-07-22 18:22:28.0,39.0,"Generally, the problems of machine learning may be considered variations on function estimation for classification, prediction or modeling.\\n\\nIn **supervised learning** one is furnished with input (x1, x2, . .,) and output (y1, y2, . .,) and are challenged with finding a function that approximates this behavior in a generalizable fashion.  The output could be a class label (in classification) or a real number (in regression)-- these are the ""supervision"" in supervised learning. \\n\\nIn the case of **unsupervised learning**, in the base case, you receives inputs x1, x2, . ., but neither target outputs, nor rewards from its environment are provided.  Based on the problem (classify, or predict) and your background knowledge of the space sampled, you may use various methods: density estimation (estimating some underlying PDF for prediction), k-means clustering (classifying unlabeled real valued data), k-modes clustering (classifying unlabeled categorical data), etc.\\n\\n**Semi-supervised learning** involves function estimation on labeled and unlabeled data.  This approach is motivated by the fact that labeled data is often costly to generate, whereas unlabeled data is generally not.  The challenge here mostly involves the technical question of how to treat data mixed in this fashion. See this [Semi-Supervised Learning Literature Survey][1] for more details on semi-supervised learning methods.\\n\\nIn addition to these kinds of learning, there are others, such as **reinforcement learning** whereby the learning method interacts with its environment by producing actions a1, a2, . . .. that produce rewards or punishments r1, r2, ...\\n\\n\\n  [1]: http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf",added 4 characters in body; added 10 characters in body,
1105,2,524,bd646b57-52b1-4230-8dac-971c5813c6dc,2010-07-22 18:25:54.0,,"Debugging MCMC programs is notoriously difficult. The difficulty arises because of several issues some of which are:\\n\\n(a) Cyclic nature of the algorithm\\n\\nWe iteratively draw parameters conditional on all other parameters. Thus, if a implementation is not working properly it is difficult to isolate the bug as the issue can be anywhere in the iterative sampler.\\n\\n(b) The correct answer is not necessarily known. \\n\\nWe have no way to tell if we have achieved convergence. To some extent this can be mitigated by testing the code on simulated data.\\n\\nIn light of the above issues, I was wondering if there is a standard technique that can be used to debug MCMC programs. ",,user28
1106,1,524,bd646b57-52b1-4230-8dac-971c5813c6dc,2010-07-22 18:25:54.0,,Is there a standard technique to debug MCMC programs?,,user28
1107,3,524,bd646b57-52b1-4230-8dac-971c5813c6dc,2010-07-22 18:25:54.0,,<mcmc><code>,,user28
1108,2,525,16dd3186-0cbb-4456-8c4f-b410bb33a8e1,2010-07-22 18:33:47.0,247.0,"Standard programming practice:\\n-when debugging run the simulation with fixed sources of randomness (i.e. same seed) so that any changes are due to code changes and not different random numbers\\n-try your code on a model (or several models) where the answer IS known\\n-adopt good programming habits so that you introduce fewer bugs \\n-think very hard & long about the answers you do get, whether they make sense, etc.\\n\\nI wish you good luck, and plenty of coffee!\\n",,
1109,2,526,d5103623-becb-4149-b987-69f503009053,2010-07-22 19:17:41.0,88.0,"As you know, there are two popular types of cross-validation, K-fold and random subsampling (as described in [Wikipedia][1]). Nevertheless, I know that some researchers are making and publishing papers where something that is described as a K-fold CV is indeed a random subsampling one, so in practice you never know what is really in the article you're reading.  \\nUsually of course the difference is unnoticeable, and so goes my question -- can you think of an example when the result of one type is significantly different from another? \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)",,
1110,1,526,d5103623-becb-4149-b987-69f503009053,2010-07-22 19:17:41.0,88.0,Does the cross validation implementation influence its results?,,
1111,3,526,d5103623-becb-4149-b987-69f503009053,2010-07-22 19:17:41.0,88.0,<machine-learning><cross-validation>,,
1112,5,390,50ba12a9-8849-4996-bad5-540306f26965,2010-07-22 19:26:47.0,74.0,Fooled By Randomness by Taleb,deleted 67 characters in body,
1113,5,494,7ce1523c-9f3f-452a-9aa2-b72898574eda,2010-07-22 20:16:08.0,88.0,"Generally, it is assumed that AIC (and so AICc) is defined up to adding a constant, so the fact if it is negative or positive is not meaningful at all. So the answer is yes, it is valid.\\n",Made it clearer.,
1114,2,527,96fec4ea-fb97-4925-994a-799598408575,2010-07-22 21:15:52.0,114.0,"Given two analytical methods to estimate some variable, where each has it's own error, what ways exist to show the two methods are equivalent (or not).\\n\\nI'm thinking that plotting the two results on a scatter graph is a good first step, but are there any good statistical methods ?",,
1115,1,527,96fec4ea-fb97-4925-994a-799598408575,2010-07-22 21:15:52.0,114.0,What ways are there to show two analytical methods are equivalent ?,,
1116,3,527,96fec4ea-fb97-4925-994a-799598408575,2010-07-22 21:15:52.0,114.0,<statistics><test><equivalence>,,
1117,2,528,163b7b0a-a339-4819-8092-f0042587d1b1,2010-07-22 22:33:37.0,3807.0,"Teaching ""Correlation does not mean causation"" doesn't really help anyone because at the end of the day all deductive arguments are based in part on correlation.\\n\\nHuman are very bad at learning not to do something. \\n\\nThe goal should rather be constructive: Always think about alternatives to your starting assumptions that might produce the same data.",,
1118,2,529,4c115cab-7de1-464c-9e30-6c15bc1d87b9,2010-07-22 22:38:05.0,,"Your use of the phrase 'analytical methods' is a bit confusing to me. I am going to assume that by 'analytical methods' you mean some specific model/estimation strategy. \\n\\nBroadly, speaking there are two types of metrics you could use to choose between estimators.\\n\\n**In-sample Metrics**\\n\\n - Likelihood ratio / Wald test / Score test\\n - R<sup>2</sup>\\n - In-sample Hit Rates (Percentage of correct predictions for sample data)\\n - (Lots of other metrics depending on model / estimation context)\\n\\n**Out-of-sample Metrics**\\n\\n - Out-of-sample Hit Rates (Percentage of correct predictions for out-of-sample data)\\n\\nIf the estimates are equivalent they would perform equally well on these metrics. You could also see if the estimates are not statistically different from one another (like the two-sample test of equality of means) but the methodology for this would depend on model and method specifics.",,user28
1119,2,530,0cda803f-4bb7-478e-82b8-d96b50d2aba6,2010-07-22 23:52:24.0,159.0,"It's very common in forecasting to aggregate data in order to increase the signal/noise ratio.  There are several papers on the effect of temporal aggregation on forecast accuracy in economics, for example. What you're probably seeing in the daily data is a weak signal that is being swamped by noise, whereas the weekly and monthly data are showing a stronger signal that is more visible.\\n\\nWhether you want to use temporal aggregation depends entirely on what your purpose is. If you need forecasts of daily incidents, then aggregation isn't going to be much use. If you are interested in exploring the effects of several covariates on the frequency of incidence, and all your data are available on a daily basis, then I would probably use the daily data as it will give a larger sample size and probably enable you to detect the effects more easily.\\n\\nSince you are using the forecast package, presumably you are interested in time series forecasting. So do you need daily forecasts, weekly forecasts or monthly forecasts? The answer will determine whether aggregation is appropriate for you.",,
1120,2,531,5771117d-5d91-48ca-acdb-82749d02efb8,2010-07-23 00:04:48.0,159.0,"I don't think it is possible to do Bonferoni or similar corrections to adjust for variable selection in regression because all the tests and steps involved in model selection are not independent.\\n\\nOne approach is to formulate the model using one set of data, and do inference on a different set of data. This is done in forecasting all the time where we have a training set and a test set. It is not very common in other fields, probably because data are so precious that we want to use every single observation for model selection and for inference. However, as you note in your question, the downside is that the inference is actually misleading.\\n\\nThere are many situations where a theory-based approach is impossible as there is no well-developed theory. In fact, I think this is much more common than the cases where theory suggests a model.\\n\\n",,
1121,5,162,22aafd5d-9a9b-41a4-b517-353d60e49cbc,2010-07-23 00:38:18.0,74.0,"These aren't exactly ""difficult"" concepts. Depends how lay the man is I suppose...\\n\\n 1. If you carved your distribution out\\n    of wood, and tried to balance it on\\n    your finger, the balance point would\\n    be the mean.\\n\\n 2. If you put a stick in the middle of\\n    your scatter plot, and attached the\\n    stick to each data point with a\\n    spring, the resting point of the\\n    stick would be your regression line.\\n\\n",added 43 characters in body,
1122,2,532,6d2cf595-13d0-4580-a235-91c2894c8a4b,2010-07-23 01:44:22.0,159.0,"I think Robin Girard's answer would work pretty well for 3 and possibly 4 dimensions, but the curse of dimensionality would prevent it working beyond that. However, his suggestion led me to a related approach which is to apply the cross-validated kernel density estimate to the first three principal component scores. Then a very high-dimensional data set can still be handled ok.\\n\\nIn summary, for i=1 to n\\n\\n1. Compute a density estimate of the first three principal component scores obtained from the data set without Xi. \\n2. Calculate the likelihood of Xi for the density estimated in step 1.\\n    call it Li. \\n\\nend for\\n\\nSort the Li (for i=1,..,n) and the outliers are those with likelihood below some threshold. I'm not sure what would be a good threshold -- I'll leave that for whoever writes the paper on this! One possibility is to do a boxplot of the log(Li) values and see what outliers are detected at the negative end.",,
1123,2,533,e533336b-b58a-40c6-9746-12671446a0c6,2010-07-23 01:48:56.0,251.0,"Along the lines of the mean as balance point, I like this view of the median as a balance point:\\n\\n- [A Pearl: a Balanced Median Necklace][1]\\n\\n\\n  [1]: http://statpics.blogspot.com/2009/08/pearl-balanced-median-necklace.html",,
1124,2,534,61c41668-aec0-4eae-8cb2-d042bdc3530f,2010-07-23 01:56:02.0,159.0,"We all know the mantra ""correlation does not imply causation"" which is drummed into all first year statistics students. There are some nice examples [here][1] to illustrate the idea.\\n\\nBut sometimes correlation *does* imply causation. The following example is taking from this [Wikipedia page][2]\\n\\n> For example, one could run an experiment on identical twins who were known to consistently get the same grades on their tests. One twin is sent to study for six hours while the other is sent to the amusement park. If their test scores suddenly diverged by a large degree, this would be strong evidence that studying (or going to the amusement park) had a causal effect on test scores. In this case, correlation between studying and test scores would almost certainly imply causation.\\n\\nAre there other situations where correlation implies causation?\\n\\n  [1]: http://stats.stackexchange.com/questions/36/correlation-does-not-mean-causation\\n  [2]: http://en.wikipedia.org/wiki/Correlation_does_not_imply_causation#Determining_causation",,
1125,1,534,61c41668-aec0-4eae-8cb2-d042bdc3530f,2010-07-23 01:56:02.0,159.0,Under what conditions does correlation imply causation?,,
1126,3,534,61c41668-aec0-4eae-8cb2-d042bdc3530f,2010-07-23 01:56:02.0,159.0,<correlation>,,
1127,2,535,e616ea5d-25b0-46b9-8891-c89aaca39be4,2010-07-23 02:00:12.0,251.0,Almost surely in a well designed experiment.,,
1128,5,535,3490ad72-c6f3-4cdf-890e-42e1063d4c8c,2010-07-23 02:06:23.0,251.0,"Almost surely in a well designed experiment.  (Designed, of course, to elicit such a [connexion][1].)\\n\\n\\n  [1]: http://18th.eserver.org/hume-enquiry.html#7",added 113 characters in body,
1132,12,535,66dabbf9-b5db-4fd5-bea5-8b3aae0fa77f,2010-07-23 02:18:01.0,251.0,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""}]}",,
1133,2,537,3c8438c4-df28-4999-855d-cae6b84313c5,2010-07-23 02:42:39.0,,"Your example is that of a [controlled experiment][1]. The only other context that I know of where a correlation can imply causation is that of a [natural experiment][2]. \\n\\nBasically, a natural experiment takes advantage of an assignment of some respondents to a treatment that happens naturally in the real world. Since assignment of respondents to treatment and control groups is not controlled by the experimenter the extent to which correlation would imply causation is perhaps weaker to some extent.\\n\\nSee the wiki links for more information controlled / natural experiments.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Controlled_experiment\\n  [2]: http://en.wikipedia.org/wiki/Natural_experiment",,user28
1134,2,538,e6186c27-5c1a-46f8-a18f-be2b7890ae6c,2010-07-23 04:49:41.0,260.0,"Correlation is not sufficient for causation.  The Wikipedia example can be dismissed by imagining that those twins always cheated in their tests by having a device that gives them the answers.  The twin that goes to the amusement park looses the device, hence the low grade.\\n\\nA good way to get this stuff straight is to think of the structure of Bayesian networks that may be generating the measured quantities, as done by  Pearl in his book [_Causality_][1].  His basic point is to look for hidden variables.  If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation.  Expose all hidden variables and one has causation.\\n\\n[1]: http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X",,
1135,5,538,76995466-235b-4d78-a073-346292463546,2010-07-23 05:55:44.0,260.0,"Correlation is not sufficient for causation.  One can get around the Wikipedia example by imagining that those twins always cheated in their tests by having a device that gives them the answers.  The twin that goes to the amusement park looses the device, hence the low grade.\\n\\nA good way to get this stuff straight is to think of the structure of Bayesian networks that may be generating the measured quantities, as done by  Pearl in his book [_Causality_][1].  His basic point is to look for hidden variables.  If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation.  Expose all hidden variables and one has causation.\\n\\n[1]: http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X",added 2 characters in body,
1136,2,539,1bdd4deb-f566-4d75-84f5-effcaec61513,2010-07-23 06:17:10.0,174.0,"In answering [this question on discrete and continuous data](http://stats.stackexchange.com/questions/206/discrete-and-continuous) I glibly asserted that it rarely makes sense to treat categorical data as continuous.\\n\\nOn the face of it that seems self-evident, but intuition is often a poor guide for statistics, or at least mine is. So now I'm wondering: is it true? Or are there established analyses for which a transform from categorical data to some continuum is actually useful? Would it make a difference if the data were ordinal?",,
1137,1,539,1bdd4deb-f566-4d75-84f5-effcaec61513,2010-07-23 06:17:10.0,174.0,Does it ever make sense to treat categorical data as continuous?,,
1138,3,539,1bdd4deb-f566-4d75-84f5-effcaec61513,2010-07-23 06:17:10.0,174.0,<categorical-data><continuous-data><data-transformation>,,
1139,2,540,b4bbb144-8f11-4f0a-8004-dfb06258de94,2010-07-23 06:17:56.0,183.0,"In my opinion the APA Statistical Task force summarised it quite well\\n\\n> ''Inferring causality from nonrandomized\\n> designs is a risky enterprise.\\n> Researchers using nonrandomized\\n> designs have an extra obligation to\\n> explain the logic behind covariates\\n> included in their designs and to alert\\n> the reader to plausible rival\\n> hypotheses that might explain their\\n> results. Even in randomized\\n> experiments, attributing causal\\n> effects to any one aspect of the\\n> treatment condition requires support\\n> from additional experimentation.''\\n- [APA Task Force][1]\\n\\n\\n  [1]: http://www.loyola.edu/library/ref/articles/Wilkinson.pdf",,
1140,2,541,51c9db74-130b-4fb6-b179-2f25918f3741,2010-07-23 06:22:04.0,159.0,"If there are only two categories, then transforming them to (0,1) makes sense. In fact, this is commonly done where the resulting dummy variable is used in regression models.\\n\\nIf there are more than two categories, then I think it only makes sense if the data are ordinal, and then only in very specific circumstances. For example, if I am doing regression and fit a nonparametric nonlinear function to the ordinal-cum-numeric variable, I think that is ok. But if I use linear regression, then I am making very strong assumptions about the relative difference between consecutive values of the ordinal variable, and I'm usually reluctant to do that.",,
1141,12,533,2237cbc2-88f6-4d4e-ac9b-f67f56d00376,2010-07-23 06:30:48.0,251.0,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""}]}",,
1142,2,542,0d4a0257-a552-4acb-8881-ab7161d5091b,2010-07-23 06:49:38.0,183.0,"In the social science context where I come from, the issue is whether you are interested in (a) prediction or (b) testing a focused research question.\\nIf the purpose is prediction then data driven approaches are appropriate.\\nIf the purpose is to examine a focused research question then it is important to consider which regression model specifically tests your question.\\n\\nFor example, if your task was to select a set of selection tests to predict job performance, the aim can in some sense be seen as one of maximising prediction of job performance.\\nThus, data driven approaches would be useful.\\n\\nIn contrast if you wanted to understand the relative role of personality variables and ability variables in influencing performance, then a specific model comparison approach might be more appropriate. \\n\\nTypically when exploring focussed research questions the aim is to elucidate something about the underlying causal processes that are operating as opposed to developing a model with optimal prediction. \\n\\nWhen I'm in the process of developing models about process based on cross-sectional data I'd be wary about:\\n(a) including predictors that could theoretically be thought of as consequences of the outcome variable. E.g., a person's belief that they are a good performer is a good predictor of job performance, but it is likely that this is at least partially caused by the fact that they have observed their own performance.\\n(b) including a large number of predictors that are all reflective of the same underlying phenomena. E.g., including 20 items all measuring satisfaction with life in different ways.\\n\\nThus, focused research questions rely a lot more on domain specific knowledge. \\nThis probably goes some way to explaining why data driven approaches are less often used in the social sciences.\\n\\n\\n\\n\\n",,
1143,2,543,cb2f84d4-4aad-443c-bfdf-1d8a3bfc65be,2010-07-23 07:13:03.0,183.0,"In addition to what has already been said above about summated scales, I'd also mention that the issue can change when analysing data at the group-level. For example, if you were examining\\n\\n - life satisfaction of states or countries,\\n - job satisfaction of organisations or departments,\\n - student satisfaction in subjects.\\n\\nIn all these cases each aggregate measure (perhaps the mean) is based on many individual responses (e.g., n=50, 100, 1000, etc.). In these cases the original Likert item begins to take on properties that resemble an interval scale at the aggregate level.",,
1144,2,544,f09f81e2-8f95-47c2-b7a1-e8202b66fe3f,2010-07-23 07:27:31.0,183.0,"It is common practice to treat ordered categorical variables with many categories as continuous. Examples of this:\\n\\n- Number of items correct on a 100 item test\\n- A summated psychological scale (e.g., that is the mean of 10 items each on a five point scale)\\n\\nAnd by ""treating as continuous"" I mean including the variable in a model that assumes a continuous random variable (e.g., as a dependent variable in a linear regression). I suppose the issue is how many scale points are required for this to be a reasonable simplifying assumption.\\n\\n\\nA few other thoughts:\\n\\n - **Polychoric correlations** attempt to model the relationship between two ordinal variables in terms of assumed latent continuous variables.\\n - **Optimal scaling** allows you to develop models where the scaling of a categorical variable is developed in a data driven way whilst respecting whatever scale constraints you impose (e.g., ordinality)\\n\\n",,
1146,2,545,5ab11780-1982-4ab1-9442-ef61698cf16a,2010-07-23 08:15:23.0,273.0,"The problem (dilemma) you face appears to be the one of selecting an optimal (or otherwise good) sampling interval for revising your forecasts. To start with, see [link text][1] of Brown's famous book, which would also qualify as a good reference. It all boils down to ""balancing the risk of not noticing a change quickly against the inherent variability of the data and the cost of revising plans frequently"". If you are not prepared to revise your forecast (and the decisions that motivated it) daily, you don't really need to use the (noisiest) daily data. An important point, often lost in the contemporary forecasting literature, is that forecasts are only necessary to assist with making a decision (unless one also knows how to derive fun from them).\\n\\n\\n  [1]: http://books.google.com.au/books?id=XXFNW_QaJYgC&pg=PA42 ""chapter 3""",,
1147,2,546,017367da-b316-4451-b7d7-f02dada5c807,2010-07-23 08:18:52.0,183.0,another from xkcd:\\n![alt text][1]\\n\\n\\n  [1]: http://imgs.xkcd.com/comics/fuck_grapefruit.png,,
1148,16,546,017367da-b316-4451-b7d7-f02dada5c807,2010-07-23 08:18:52.0,-1.0,,,
1149,2,547,9d6dfaaf-72aa-476f-bf0c-1b7d8fb0afd4,2010-07-23 08:33:04.0,88.0,"There is also a problem with the opposite case, when lack of correlation is used as a proof for the lack of causation. This problem is nonlinearity; when looking at correlation people usually check Pearson, which is only a tip of an iceberg. ",,
1150,5,443,c9db52ee-a580-4a51-aead-4009b3a7ed1d,2010-07-23 10:08:40.0,215.0,"There's two aspects to this *post hoc ergo propter hoc* problem that I like to cover: (i) reverse causality and (ii) endogeneity \\n\\nAn example of ""possible"" reverse causality:\\nSocial drinking and earnings - drinkers earn more money according to Bethany L. Peters & Edward Stringham, 2006. ""No Booze? You May Lose: Why Drinkers Earn More Money Than Nondrinkers,"" Journal of Labor Research, Transaction Publishers, vol. 27(3), pages 411-421, June. Or do people who earn more money drink more either because they have a greater disposable income or due to stress? This is a great paper to discuss for all sorts of reasons including measurement error, response bias, causality, etc.\\n\\nAn example of ""possible"" endogeneity:\\nThe Mincer Equation explains log earnings by education, experience and experience squared. There is a long literature on this topic. Labour economists want to estimate the causal relationship of education on earnings but perhaps education is endogenous because ""ability"" could increase the amount of education an individual has (by lowering the cost of obtaining it) and could lead to an increase in earnings, irrespective of the level of education. A potential solution to this could be an instrumental variable. Angrist and Pischke's book, Mostly Harmless Econometrics covers this and relates topics in great detail and clarity.\\n\\n\\nOther silly examples that I have no support for include:\\n- Number of televisions per capita and the numbers of mortality rate. So let's send TVs to developing countries. Obviously both are endogenous to something like GDP.\\n- Number of shark attacks and ice cream sales. Both are endogenous to the temperature perhaps?\\n\\nI also like to tell the terrible joke about the lunatic and the spider. A lunatic is wandering the corridors of an asylum with a spider he's carrying in the palm of his hand. He sees the doctor and says, ""Look Doc, I can talk to spiders. Watch this. ""Spider, go left!"" The spider duly moves to the left. He continues, ""Spider, go right."" The spider shuffles to the right of his palm. The doctor replies, ""Interesting, maybe we should talk about this in the next group session."" The lunatic retorts, ""That's nothing Doc. Watch this."" He pulls off each of the spider's legs one by one and then shouts, ""Spider, go left!"" The spider lies motionless on his palm and the lunatic turns to the doctor and concludes, ""If you pull off a spider's legs he'll go deaf."" \\n\\n\\n\\n",added 1 characters in body,
1151,2,548,bbbab809-705a-470e-ba50-df4654e1a3e1,2010-07-23 10:51:52.0,266.0,"In an answer to [this question about treating categorical data as continuous][1], optimal scaling was mentioned. How does this method work and how is it applied?\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/539/does-it-ever-make-sense-to-treat-categorical-data-as-continuous",,
1152,1,548,bbbab809-705a-470e-ba50-df4654e1a3e1,2010-07-23 10:51:52.0,266.0,How can I use optimal scaling to scale an ordinal categorical variable?,,
1153,3,548,bbbab809-705a-470e-ba50-df4654e1a3e1,2010-07-23 10:51:52.0,266.0,<scales><categorical-data><data-transformation>,,
1154,6,548,e234f4c9-92bf-4b01-b122-4749a2300eb6,2010-07-23 11:06:32.0,190.0,<categorical-data><data-transformation><optimal-scaling>,edited tags,
1155,5,538,fc3a0f88-33f6-4faf-8965-454b2b152725,2010-07-23 11:16:16.0,260.0,"Correlation is not sufficient for causation.  One can get around the Wikipedia example by imagining that those twins always cheated in their tests by having a device that gives them the answers.  The twin that goes to the amusement park looses the device, hence the low grade.\\n\\nA good way to get this stuff straight is to think of the structure of Bayesian network that may be generating the measured quantities, as done by  Pearl in his book [_Causality_][1].  His basic point is to look for hidden variables.  If there is a hidden variable that happens not to vary in the measured sample, then the correlation would not imply causation.  Expose all hidden variables and one has causation.\\n\\n[1]: http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X",Typo,
1158,2,549,d2bd3361-358f-49d6-93ea-2773d7d9fed8,2010-07-23 13:28:55.0,270.0,"In an analysis of ranking by frequency, as with a Pareto chart and associated values (eg how many categories make up the top 80% of product faults)",,
1159,2,550,db53a183-db33-4994-8ff2-9d9346871dc8,2010-07-23 13:58:07.0,5.0,"One of my favorite examples is the [Birthday Paradox][1] ([wikipedia entry][2]), which illustrates some important concepts of probability.\\n\\n\\n  [1]: http://mathworld.wolfram.com/BirthdayProblem.html\\n  [2]: http://en.wikipedia.org/wiki/Birthday_problem",,
1160,2,551,33588c98-71be-422a-ad8b-a480c04a5356,2010-07-23 14:43:14.0,215.0,"\\nI always tell students there are three reasons to transform a variable by taking the natural logarithm. The reason for logging the variable will determine whether you want to log the independent variable(s), dependent or both. To be clear throughout I'm talking about taking the natural logarithm. \\n\\nFirstly, to improve model fit as other posters have noted. For instance if your residuals aren't normally distributed then taking the logarithm of a skewed variable may improve the fit by altering the scale and making the variable more ""normally"" distributed. For instance, earnings is truncated at zero and often exhibits positive skew. If the variable has negative skew you could firstly invert the variable before taking the logarithm. I'm thinking here particularly of Likert scales that are inputted as continuous variables. While this usually applies to the dependent variable you occasionally have problems with the residuals (e.g. heteroscedasticity) caused by an independent variable which can be sometimes corrected by taking the logarithm of that variable. For example when running a model that explained lecturer evaluations on a set of lecturer and class covariates the variable ""class size"" (i.e. the number of students in the lecture) had outliers which induced heteroscedasticity because the variance in the lecturer evaluations was smaller in larger cohorts than smaller cohorts. Logging the student variable would help, although in this example either calculating Robust Standard Errors or using Weighted Least Squares may make interpretation easier.\\n\\nThe second reason for logging one or more variables in the model is for interpretation. I call this convenience reason. If you log both your dependent (Y) and independent (X) variable(s) your regression coefficients (beta) will be elasticities and interpretation would go as follows: a 1% increase in X would lead to a ceteris paribus beta % increase in Y (on average). Logging only one side of the regression ""equation"" would lead to alternative interpretations as outlined below:\\n\\nY and X -- a one unit increase in X would lead to a beta increase/decrease in Y\\n\\nLog Y and Log X -- a 1% increase in X would lead to a beta % increase/decrease in Y \\n\\nLog Y and X -- a one unit increase in X would lead to a beta*100 % increase/decrease in Y\\n\\nY and Log X -- a 1% increase in X would lead to a beta/100 increase/decrease in Y\\n\\n\\nAnd finally there could be a theoretical reason for doing so. For example some models that we would like to estimate are multiplicative and therefore nonlinear. Taking logarithms allows these models to be estimated by linear regression. Good examples of this include the Cobb-Douglas production function in economics and the Mincer Equation in education. The Cobb-Douglas production function explains how inputs are converted into outputs:\\n\\nY = AL^alphaK^beta \\n\\nwhere\\n\\nY is the total production or output of some entity e.g. firm, farm, etc.\\n\\nA is the total factor productivity (the change in output not caused by the inputs e.g. by technology change or weather)\\n\\nL is the labour input\\n\\nK is the capital input\\n\\nalpha & beta are output elasticities.\\n\\n\\nTaking logarithms of this makes the function easy to estimate using OLS linear regression as such:\\n\\nLn Y = A + alphaLn L + beta Ln K\\n\\n\\n\\n",,
1161,2,552,1fad6f71-653e-4f80-9540-a329dec45dd7,2010-07-23 14:53:50.0,215.0,"AIC = -2Ln(L)+ 2k \\n\\nwhere L is the maximised value of Likelihood function for that model and k is the number of parameters in the model.\\n\\nIn your example -2Ln(L)+ 2k <0 means that the log-likelihood at the maximum was > 0\\nwhich means that the likelihood at the maximum was > 1.\\n\\nThere is no problem with a positive log-likelihood. It is a common misconception that the log-likelihood must be negative. If the likelihood is derived from a probability density it can quite reasonably exceed 1 which means that log-likelihood is positive, hence the deviance and the AIC are negative. This is what occurred in your model.\\n\\nIf you believe that comparing AICs is a good way to choose a model then it would still be the case that the (algebraically) lower AIC is preferred not the one with the lowest absolute AIC value. To reiterate you want the most negative number in your example.\\n\\n",,
1162,2,553,951400eb-bfd2-4969-bf88-219534da7224,2010-07-23 15:02:40.0,215.0,"\\nThe trivial answer is that more data are always preferred to less data. \\n\\nThe problems of small sample size is clear. In linear regression (OLS) technically you can fit a model such as OLS where n = k+1 but you will get rubbish out of it i.e. very large standard errors. A common heuristic is that you should have 20 observations for every parameter you want to estimate. It is always a trade off between the size of your standard errors (and therefore significance testing) and the size of your sample. This is one reason some of us hate significance testing as you can get an incredibly small (relative) standard error with an enormous sample and therefore find pointless statistical significance on naive tests such as whether a regression coefficient is zero. There is a great paper by Arthur Goldberger called Micronumerocity on this topic which is summarized in chapter 23 of his book *A Course in Econometrics*.\\n\\nWhile sample size is important the quality of your sample is more important e.g. whether the sample is generalisable to the population, is it a Simple Random Sample or some other appropriate sampling methodology (and have this been accounted for during analysis), is their measurement error, response bias, selection bias, etc.",,
1163,2,554,387c9377-da96-45c6-a22e-36ad8525bf48,2010-07-23 15:09:02.0,215.0,"\\nI like to demonstrate sampling variation and essentially the Central Limit Theorem through an ""in-class"" exercise. Everybody in the class of say 100 students writes their age on a piece of paper. All pieces of paper are the same size and folded in the same fashion after I've calculated the average. This is the population and I calculate the average age. Then each student randomly selects 10 pieces of paper, writes down the ages and returns them to the bag. (S)he calculates the mean and passes the bag along to the next student. Eventually we have 100 samples of 10 students each estimating the population mean which we can describe through a histogram and some descriptive statistics. \\n\\nWe then repeat the demonstration this time using a set of 100 ""opinions"" that replicate some Yes/No question from recent polls e.g. If the (British General) election were called tomorrow would you consider voting for the British National Party. Students them sample 10 of these opinions.\\n\\nAt the end we've demonstrated sampling variation, the Central Limit Theorem, etc with both continuous and binary data.\\n\\n",,
1164,2,555,9ba6544f-eddd-49b9-834a-4a38974300ff,2010-07-23 15:17:56.0,,"ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression. \\n\\nIn light of their equivalence, is there any reason why ANOVA is used instead of linear regression? \\n\\nNote: I am particularly interested in hearing about **technical** reasons for the use of ANOVA instead of linear regression. ",,user28
1165,1,555,9ba6544f-eddd-49b9-834a-4a38974300ff,2010-07-23 15:17:56.0,,Why is ANOVA taught / used as if it is a different research methodology compared to linear regression? ,,user28
1166,3,555,9ba6544f-eddd-49b9-834a-4a38974300ff,2010-07-23 15:17:56.0,,<regression><anova>,,user28
1167,2,556,59b240c4-35a3-4402-b4dd-f066f182fd1c,2010-07-23 15:18:34.0,215.0,"\\nDiscrete data can take on only integer values whereas continuous data can take on any value. For instance the number of cancer patients treated by a hospital each year is discrete but your weight is continuous. Some data are continuous but measured in a discrete way e.g. your age. It is common to report your age as say, 31.",,
1168,5,553,ceef934c-8708-4e3d-a4ab-8e383d90f631,2010-07-23 15:20:15.0,215.0,"The trivial answer is that more data are always preferred to less data. \\n\\nThe problem of small sample size is clear. In linear regression (OLS) technically you can fit a model such as OLS where n = k+1 but you will get rubbish out of it i.e. very large standard errors. There is a great paper by Arthur Goldberger called Micronumerocity on this topic which is summarized in chapter 23 of his book *A Course in Econometrics*.\\n\\nA common heuristic is that you should have 20 observations for every parameter you want to estimate. It is always a trade off between the size of your standard errors (and therefore significance testing) and the size of your sample. This is one reason some of us hate significance testing as you can get an incredibly small (relative) standard error with an enormous sample and therefore find pointless statistical significance on naive tests such as whether a regression coefficient is zero. \\n\\nWhile sample size is important the quality of your sample is more important e.g. whether the sample is generalisable to the population, is it a Simple Random Sample or some other appropriate sampling methodology (and have this been accounted for during analysis), is there measurement error, response bias, selection bias, etc.",added 1 characters in body,
1169,5,455,984bb177-ec79-47d6-a203-35a772dfde0d,2010-07-23 15:27:06.0,3807.0,"Allright, I think this one is hilarious- but let's see if it passes the Statistical Analysis Miller test.\\n\\n## Fermirotica\\n\\n[![I love how Google handles dimensional analysis.  Stats are ballpark and vary wildly by time of day and whether your mom is in town.][1]](http://xkcd.com/563/)\\n\\n> I love how Google handles dimensional analysis.  Stats are ballpark and vary wildly by time of day and whether your mom is in town.\\n\\n  [1]: http://imgs.xkcd.com/comics/fermirotica.png",Added alt text,
1170,5,546,8c6d6e34-9c32-4a34-ba76-725a4f73fbd5,2010-07-23 15:31:59.0,190.0,another from [xkcd][1]:\\n![Coconuts are so far down to the left they couldn't be fit on the chart.  Ever spent half an hour trying to open a coconut with a rock?  Fuck coconuts.][2]\\n\\n\\n  [1]: http://xkcd.com/388/\\n  [2]: http://imgs.xkcd.com/comics/fuck_grapefruit.png,added 176 characters in body,
1171,2,557,604765be-29fa-42bb-8e40-6909a0f62e8f,2010-07-23 15:35:55.0,215.0,"As an economist, the analysis of variance (ANOVA) is taught and usually understood in relation to linear regression (e.g. in Arthur Goldberger's *A Course in Econometrics*). Economists/Econometricians typically view ANOVA as uninteresting and prefer to move straight to regression models. From the perspective of linear (or even generalised linear) models, ANOVA assigns coefficients into batches, with each batch corresponding to a ""source of variation"" in ANOVA terminology.\\n\\nGenerally you can replicate the inferences you would obtain from ANOVA using regression but not always OLS regression. Multilevel models are needed for analysing hierarchical data structures such as ""split-plot designs,"" where between-group effects are compared to group-level errors, and within-group effects are compared to data-level errors. Gelman's paper [1] goes into great detail about this problem and effectively argues that ANOVA is an important statistical tool that should still be taught for it's own sake.\\n\\nIn particular Gelman argues that ANOVA is a way of understanding and structuring multilevel models. Therefore ANOVA is not an alternative to regression but as a tool for summarizing complex high-dimensional inferences and for exploratory data analysis. \\n\\nGelman is a well-respected statistician and some credence should be given to his view. However, almost all of the empirical work that I do would be equally well served by linear regression and so I firmly fall into the camp of viewing it as a little bit pointless. Some disciplines with complex study designs (e.g. psychology) may find ANOVA useful.\\n\\n[1] Gelman, A. (2005). Analysis of variance: why it is more important than ever (with discussion). *Annals of Statistics* 33, 1–53.",,
1172,5,555,9e914d5f-91a3-4993-afe7-e09b9455d94a,2010-07-23 15:44:44.0,,"ANOVA is equivalent to linear regression with the use of suitable dummy variables. The conclusions remain the same irrespective of whether you use ANOVA or linear regression. \\n\\nIn light of their equivalence, is there any reason why ANOVA is used instead of linear regression? \\n\\nNote: I am particularly interested in hearing about **technical** reasons for the use of ANOVA instead of linear regression. \\n\\n**Edit**\\n\\nHere is one example using one-way ANOVA. Suppose, you want to know if the average height of male and females is the same. To test for your hypothesis you would collect data from a random   sample of male and females (say 30 each) and perform the ANOVA analysis (i.e., sum of squares for gender and error) to decide whether an effect exists.\\n\\nYou could also use linear regression to test for this as follows:\\n\\nDefine:\\n\\nGender = 1 if respondent is a male and 0 otherwise.\\n\\nHeight = Intercept + beta * Gender + error\\n\\nwhere\\n\\nerror ~ N(0,sigma^2)\\n\\nThen a test of whether beta = 0 is a an equivalent test for your hypothesis.",added 650 characters in body,user28
1173,6,155,9b8cf1cc-8993-4930-8b8d-a724c5aa48e6,2010-07-23 15:53:04.0,77.0,<teaching><layman>,edited tags,
1174,2,558,e3c8774f-5328-40b4-a940-edc3147fa917,2010-07-23 16:15:33.0,39.0,"I am trying to get a global perspective on some of the essential ideas in machine learning, and I was wondering if there is a comprehensive treatment of the different notions of loss (squared, log, hinge, proxy, etc.).  I was thinking something along the lines of a more comprehensive, formal presentation of John Langford’s excellent post on [Loss Function Semantics][1].\\n\\n\\n  [1]:  http://hunch.net/?p=269",,
1175,1,558,e3c8774f-5328-40b4-a940-edc3147fa917,2010-07-23 16:15:33.0,39.0,Comprehensive overview of loss functions?,,
1176,3,558,e3c8774f-5328-40b4-a940-edc3147fa917,2010-07-23 16:15:33.0,39.0,<loss-functions>,,
1177,5,550,a428bcbe-3970-4f4b-856a-531e83483db3,2010-07-23 16:19:17.0,5.0,"I have used the drunkard's walk before for random walk, and the drunk and her dog for cointegration; they're very helpful (partially because they're amusing).\\n\\nOne of my favorite common examples is the [Birthday Paradox][1] ([wikipedia entry][2]), which illustrates some important concepts of probability.  You can simulate this with a room full of people.\\n\\nIncidentally, I strongly recommend Andrew Gelman's [**""Teaching Statistics: A Bag of Tricks""**][3] for some examples of creative ways to teach statistical concepts (see the [table of contents][4]).  Also look at his paper about the course that he teaches on teaching statistics: [""A Course on Teaching Statistics at the University Level""][5].\\n\\n\\n  [1]: http://mathworld.wolfram.com/BirthdayProblem.html\\n  [2]: http://en.wikipedia.org/wiki/Birthday_problem\\n  [3]: http://www.stat.columbia.edu/~gelman/bag-of-tricks/\\n  [4]: http://www.stat.columbia.edu/~gelman/bag-of-tricks/contents.pdf\\n  [5]: http://www.stat.columbia.edu/~gelman/research/published/teachcourse3.pdf",added 395 characters in body; added 156 characters in body; added 228 characters in body,
1178,5,550,e4476fc6-f9ab-44b9-9d3c-4ff36fa8baaa,2010-07-23 16:25:03.0,5.0,"I have used the drunkard's walk before for random walk, and the drunk and her dog for cointegration; they're very helpful (partially because they're amusing).\\n\\nOne of my favorite common examples is the [Birthday Paradox][1] ([wikipedia entry][2]), which illustrates some important concepts of probability.  You can simulate this with a room full of people.\\n\\nIncidentally, I strongly recommend Andrew Gelman's [**""Teaching Statistics: A Bag of Tricks""**][3] for some examples of creative ways to teach statistical concepts (see the [table of contents][4]).  Also look at his paper about the course that he teaches on teaching statistics: [""A Course on Teaching Statistics at the University Level""][5].  And on [""Teaching Bayes to Graduate Students in Political Science, Sociology,\\nPublic Health, Education, Economics, ...""][6].\\n\\nFor describing Bayesian methods, using an unfair coin and flipping it multiple times is a pretty common/effective approach.\\n\\n\\n  [1]: http://mathworld.wolfram.com/BirthdayProblem.html\\n  [2]: http://en.wikipedia.org/wiki/Birthday_problem\\n  [3]: http://www.stat.columbia.edu/~gelman/bag-of-tricks/\\n  [4]: http://www.stat.columbia.edu/~gelman/bag-of-tricks/contents.pdf\\n  [5]: http://www.stat.columbia.edu/~gelman/research/published/teachcourse3.pdf\\n  [6]: http://www.stat.columbia.edu/~gelman/research/published/teachingbayes.pdf",added 209 characters in body; added 127 characters in body,
1179,2,559,cec8b54f-4847-4e8b-8711-a888634dce90,2010-07-23 16:30:41.0,88.0,"The general idea is that you should scale the categorical variable in such way that the resulting continuous variables will be just the most useful. So, it is always coupled with some regression or learning procedure and so the fitting of the model is accompanied by optimization (or trying various possibilities) of ordinal variables scaling.  \\nFor some more practical issues, consult the docks of R [aspect][1] and [homals][2] packages. \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/aspect/index.html\\n  [2]: http://cran.r-project.org/web/packages/homals/index.html",,
1180,2,560,237795a6-7283-4399-81a4-585218d93223,2010-07-23 16:36:50.0,5.0,"I use the Gelman-Rubin convergence diagnostic as well.  A potential problem with Gelman-Rubin is that it may mis-diagnose convergence if the shrink factor happens to be close to 1 by chance, in which case you can use a Gelman-Rubin-Brooks plot.  See the [""General Methods for Monitoring Convergence of Iterative Simulations""][2] paper for details.  This is supported in [the **coda** package][1] in R (for ""Output analysis and diagnostics for Markov Chain Monte Carlo simulations"").  `coda` also includes other functions (such as the Geweke’s convergence diagnostic).  \\n\\nYou can also have a look at [""boa: An R Package for MCMC Output Convergence\\nAssessment and Posterior Inference""][3]. \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/coda/index.html\\n  [2]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.2165\\n  [3]: http://www.jstatsoft.org/v21/i11/paper",,
1181,2,561,c108bd6f-b23d-4a2a-8301-1a1fe315995d,2010-07-23 16:49:47.0,215.0,"\\nAt the heart of your question is the question ""when is a relationship causal?"" It doesn't just need to be correlation implying (or not) causation.\\n\\nA good book on this topic is called *Mostly Harmless Econometrics* by Johua Angrist and Jorn-Steffen Pischke. They start from the experimental ideal where we are able to randomise the ""treatment"" under study in some fashion and then they move onto alternative methods for generating this randomisation in order to draw causal influences. This begins with the study of so called natural experiments. \\n\\nOne of the first examples of a natural experiment being used to identify causal relationships is Angrist's 1989 paper on [""Lifetime Earnings and the Vietnam Era Draft Lottery.""][1] This paper attempts to estimate the effect of military service on lifetime earnings. A key problem with estimating any causal effect is that certain types of people may be more likely to enlist, which may bias any measurement of the relationship. Angrist uses the natural experiment created by the Vietnam draft lottery to effectively ""randomly assign"" the treatment ""military service"" to a group of men. \\n\\nSo when do we have a causality? Under experimental conditions. When do we get close? Under natural experiments. There are also other techniques that get us close to ""causality"" i.e. they are much better than simply using statistical control. They include regression discontinuity, difference-in-differences, etc.\\n\\n\\n  [1]: http://www.irs.princeton.edu/pubs/pdfs/251.pdf",,
1182,2,562,6e3b7abe-fa44-43c3-a59c-aebd79b879b8,2010-07-23 16:51:11.0,5.0,This is a fairly general question:\\n\\nI have typically found that using multiple different models outperforms one model when trying to predict a time series out of sample.  Are there any good papers that demonstrate that the combination of models will outperform a single model?  Are there any best-practices around combining multiple models?,,
1183,1,562,6e3b7abe-fa44-43c3-a59c-aebd79b879b8,2010-07-23 16:51:11.0,5.0,When to use multiple models for prediction?,,
1184,3,562,6e3b7abe-fa44-43c3-a59c-aebd79b879b8,2010-07-23 16:51:11.0,5.0,<modeling><model-comparison>,,
1185,2,563,f7620b23-fdf6-44da-afb1-2520399fcc66,2010-07-23 16:53:20.0,215.0,"Instrumental variables are becoming increasingly common in applied economics and statistics. For the uninitiated, can we have some non-technical answers to the following questions:\\n\\n(1) What is an instrumental variable?\\n(2) When would one want to employ an instrumental variable?\\n(3) How does one find or choose an instrumental variable?",,
1186,1,563,f7620b23-fdf6-44da-afb1-2520399fcc66,2010-07-23 16:53:20.0,215.0,What is an instrumental variable?,,
1187,3,563,f7620b23-fdf6-44da-afb1-2520399fcc66,2010-07-23 16:53:20.0,215.0,<regression><experiment>,,
1188,2,564,73afddd8-a601-4126-bd66-a3a34f29105b,2010-07-23 16:57:50.0,215.0,"Difference in differences has long been popular as a non-experimental tool, especially in economics. Can somebody please provide a clear and non-technical answer to the following questions about difference-in-differences.\\n\\nWhat is a difference-in-difference estimator?\\nWhy is a difference-in-difference estimator any use?\\nCan we actually trust difference-in-difference estimates?\\n",,
1189,1,564,73afddd8-a601-4126-bd66-a3a34f29105b,2010-07-23 16:57:50.0,215.0,What is difference-in-differences?,,
1190,3,564,73afddd8-a601-4126-bd66-a3a34f29105b,2010-07-23 16:57:50.0,215.0,<regression><experiements>,,
1191,5,562,4bef9d02-f7cc-4e4d-8bbf-12abe2ee4c01,2010-07-23 16:59:31.0,5.0,"This is a fairly general question:\\n\\nI have typically found that using multiple different models outperforms one model when trying to predict a time series out of sample.  Are there any good papers that demonstrate that the combination of models will outperform a single model?  Are there any best-practices around combining multiple models?\\n\\nSome references:\\n\\n - Hui Zoua, Yuhong Yang [""Combining time series models for forecasting""][1] International Journal of Forecasting 20 (2004) 69– 84\\n\\n\\n  [1]: http://www.stat.umn.edu/~hzou/Papers/after.pdf",added 213 characters in body,
1192,6,564,c48533cd-4d9c-4b8f-9b3b-3c002e121223,2010-07-23 17:01:40.0,5.0,<regression><experiment>,edited tags,
1193,2,565,5143ec9f-d204-450b-b2a8-c051c90da1b6,2010-07-23 17:02:28.0,88.0,"The most spectacular example is the [Netflix challenge][1], which made really boosted blending popularity.\\n\\n\\n  [1]: http://www.netflixprize.com/",,
1194,2,566,ff590d2f-5547-404d-9145-7bc72b0adfe5,2010-07-23 17:05:52.0,190.0,Sometimes this kind of models are called an ensemble. For example [this page][1] gives a nice overview how it works. Also the references mentioned there are very useful.\\n\\n\\n  [1]: http://wapedia.mobi/en/Machine_learning_ensemble,,
1195,2,567,63de6f50-7912-4dce-9d3a-13c75bcf0ba1,2010-07-23 17:11:35.0,92.0,  I've often found the Engineering Statistics Handbook useful. It can be found [here][1] \\n\\n\\n  [1]: http://www.itl.nist.gov/div898/handbook/,,
1196,16,567,63de6f50-7912-4dce-9d3a-13c75bcf0ba1,2010-07-23 17:11:35.0,-1.0,,,
1197,2,568,b481814a-08cc-43e0-b356-2be9091d2034,2010-07-23 17:29:50.0,92.0,"In statistics you can never say something is absolutely certain, so statisticians use another approach to gauge whether a hypothesis is true or not. They try to reject all the other hypotheses that are not supported by the data. \\n\\nTo do this, statistical tests have a null hypothesis and an alternate hypothesis. The p-value reported from a statistical test is the likelihood of the result given that the null hypothesis was correct. That's why we want small p-values. The smaller they are, the less likely the result would be if the null hypothesis was correct. If the p-value is small enough (ie,it is very unlikely for the result to have occurred if the null hypothesis was correct), then the null hypothesis is rejected.   \\n\\nIn this fashion, null hypotheses can be formulated and subsequently rejected. If the null hypothesis is rejected, you accept the alternate hypothesis as the best explanation. Just remember though that the alternate hypothesis is never certain, since the null hypothesis could have, by chance, generated the results.\\n",,
1198,2,569,55de7b33-cd3c-4443-bc0f-9c2125223086,2010-07-23 17:32:44.0,5.0,"Following up on Peter's response on ensemble methods:\\n\\n - This is covered in [""The Elements of Statistical Learning""][1] (see page 288, for example).\\n - Witten and Frank [""Data Mining: Practical Machine Learning Tools and Techniques""][2] covers this in section 7.5, including a discussion of Bagging, Randomization, Boosting, Additive regression, Additive logistic regression, Option trees, Logistic model trees, and Stacking.\\n - This is covered in Chapter 14 of Christopher M. Bishop [""Pattern Recognition and Machine Learning""][3], including Bayesian Model Averaging, Boosting, Committees, Tree-based Models, and Conditional Mixture Models.\\n\\n\\n  [1]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/\\n  [2]: http://www.cs.waikato.ac.nz/~ml/weka/book.html\\n  [3]: http://research.microsoft.com/en-us/um/people/cmbishop/prml/",,
1199,2,570,7d174a9d-0614-4cf0-9820-c26ce50f8243,2010-07-23 17:59:27.0,251.0,"I'm curious if there are graphical techniques particular, or more applicable, to structural equation modeling.  I guess this could fall into categories for exploratory tools for covariance analysis or graphical diagnostics for SEM model evaluation.  (I'm not really thinking of path/graph models here.) \\n",,
1200,1,570,7d174a9d-0614-4cf0-9820-c26ce50f8243,2010-07-23 17:59:27.0,251.0,What graphical techniques do SEM methodologists use?,,
1201,3,570,7d174a9d-0614-4cf0-9820-c26ce50f8243,2010-07-23 17:59:27.0,251.0,<sem><data-visualization>,,
1202,2,571,34b7e967-2f87-4cd7-be05-f6082d4ac895,2010-07-23 18:29:41.0,217.0,One useful sufficient condition for some definitions of causation:\\n\\nCausation can be claimed when one of the correlated variables can be controlled (we can directly set its value) and correlation is still present. ,,
1203,2,572,a1bc9a32-81de-43a3-b616-f94b7979483c,2010-07-23 18:42:05.0,251.0,"I think Graham's second paragraph gets at the heart of the matter.  I suspect it's not so much technical than historical, probably due to the influence of ""Statistical Methods for Research Workers"", and the ease of teaching/applying the tool for non-statisticans in experimental analysis involving discrete factors, rather than delving into model building and associated tools.  In statistics, ANOVA is usually taught as a special case of regression.  (I think this is similar to why biostatistics is filled with a myriad of eponymous ""tests"" rather than emphasizing model building.)",,
1204,5,572,65778b96-0d6e-4e94-a004-af52f85f9f43,2010-07-23 18:54:03.0,251.0,"I think Graham's second paragraph gets at the heart of the matter.  I suspect it's not so much technical than historical, probably due to the influence of ""[Statistical Methods for Research Workers][1]"", and the ease of teaching/applying the tool for non-statisticans in experimental analysis involving discrete factors, rather than delving into model building and associated tools.  In statistics, ANOVA is usually taught as a special case of regression.  (I think this is similar to why biostatistics is filled with a myriad of eponymous ""tests"" rather than emphasizing model building.)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Ronald_Fisher",added 60 characters in body,
1205,6,563,13cc9b3f-a707-413a-af70-9d23a076cd1b,2010-07-23 19:05:09.0,88.0,<regression><experiment><econometrics>,edited tags,
1206,6,564,15aead3b-13c0-419b-9d74-601f5858c84e,2010-07-23 19:05:38.0,88.0,<regression><experiment><econometrics>,edited tags,
1207,2,573,146975c9-a714-4bbd-8870-2800eb46f32d,2010-07-23 19:45:45.0,190.0,"In ""[Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations][1]"" by Lee et. al.([PDF][2]) Convolutional DBN's are proposed and as a test images classified. This sound logical as you are interested in features that are small image that occur repeatedly, like small corners, edges etc.\\n\\nIn ""[Unsupervised feature learning for audio classification using convolutional deep belief networks][3]"" by Lee et. al. this method is applied for audio in different types of classifications. Speaker identification, gender indentification, phone classification and also some music genre / artist classification.\\n\\nHow can I the convolutional part of this network interpretated for audio, like it can be explained for images as edges?\\n\\n\\n  [1]: http://dx.doi.org/10.1145/1553374.1553453\\n  [2]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&rep=rep1&type=pdf\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.380&rep=rep1&type=pdf",,
1208,1,573,146975c9-a714-4bbd-8870-2800eb46f32d,2010-07-23 19:45:45.0,190.0,How to understand a convolutional deep belief network for audio classification,,
1209,3,573,146975c9-a714-4bbd-8870-2800eb46f32d,2010-07-23 19:45:45.0,190.0,,,
1210,2,574,21d906c6-cfc2-4fd8-b5da-be9a6a255727,2010-07-23 20:07:51.0,88.0,"Assuming there is some training involved, you may use some kind of cross-validation, or bootstrap of a train set.  \\nIf not, stick to the Srikant solution. I would do it even simpler, just assuming that the number of error is Poisson distributed.",,
1211,2,575,179ea86f-ddad-423e-94a7-7a13e1d1b940,2010-07-23 20:14:05.0,196.0,"What is the preferred method for for conducting post-hocs for within subjects tests?  I've seen published work where Tukey's HSD is employed but a review of Keppel and Maxwell & Delaney suggests that the likely violation of sphericity in these designs makes the error term incorrect and this approach problematic.  Maxwell & Delaney provide an approach to the problem in their book, but I've never seen it done that way in any stats package.  Does anyone know how to use their approach in R?  Is the approach they offer appropriate?  Would a Bonferoni or Sidak correction on multiple paired sample t-tests be reasonable?",,
1212,1,575,179ea86f-ddad-423e-94a7-7a13e1d1b940,2010-07-23 20:14:05.0,196.0,Post-hocs for within subjects tests?,,
1213,3,575,179ea86f-ddad-423e-94a7-7a13e1d1b940,2010-07-23 20:14:05.0,196.0,<post-hoc><sphericity><within-subjects>,,
1215,2,576,5286983a-498b-4927-af5d-6be6ed2bc317,2010-07-23 20:42:43.0,36.0,"[Wikipedia has a decent entry on this subject][1], but why not just use linear regression allowing for interactions between your independent variables of interest? This seems more interpretable to me. Then you might read up on [analysis of simple slopes (in the Cohen et al book free on Google Books)][2] if your variables of interest are quantitative.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Difference_in_differences\\n  [2]: http://books.google.com/books?id=fuq94a8C0ioC&lpg=PP1&dq=applied%20multiple%20regression%20correlation%20analysis%20for%20the%20behavioral%20sciences&pg=PA271#v=onepage&q=simple%20slopes&f=false",,
1216,2,577,717ee48b-72a9-45f9-bb72-82c5ece270c1,2010-07-23 20:49:12.0,196.0,"The AIC and BIC are both methods of assessing model fit penalized for the number of estimated parameters.  As I understand it, BIC penalizes models more for free parameters than does AIC.  Beyond a preference based on the stringency of the criteria, are there any other reasons to prefer AIC over BIC or vice versa?",,
1217,1,577,717ee48b-72a9-45f9-bb72-82c5ece270c1,2010-07-23 20:49:12.0,196.0,Is there any reason to prefer the AIC or BIC over the other?,,
1218,3,577,717ee48b-72a9-45f9-bb72-82c5ece270c1,2010-07-23 20:49:12.0,196.0,<modeling><aic><bic><model-selection>,,
1219,5,527,c3c3a18d-1852-4121-8b31-ab5ea0c68191,2010-07-23 20:57:17.0,114.0,"I have two different analytical methods that can measure the concentration of a particular molecule in a matrix (for instance measure the amount of salt in water)\\n\\nThe two methods are different, and each has it's own error.  What ways exist to show the two methods are equivalent (or not).\\n\\nI'm thinking that plotting the results from a number of samples measured by both methods on a scatter graph is a good first step, but are there any good statistical methods ?",Re-Worded,
1220,2,578,e3c11a03-6ac8-49ac-9fed-0fa1d1bd3fae,2010-07-23 21:18:52.0,196.0,"If you have no way of knowing the true concentration, the simplest approach would be a correlation.  A step beyond that might be to conduct a simple regression predicting the outcome on method 2 using method 1 (or vice versa). If the methods are identical the intercept should be 0; if the intercept is greater or less than 0 it would indicate the bias of one method relative to another.  The unstandardized slope should be near 1 if the methods on average produce results that are identical (after controlling for an upward or downward bias in the intercept).  The error in the unstandardized slope might serve as an index of the extent to which the two methods agree.  \\n\\nIt seems to me that the difficulty with statistical methods here that you are seeking to affirm what is typically posed as a null hypothesis, that is, that there are no differences between the methods.  This isn't a death blow for using statistical methods so long as you don't need a p value and you can quantify what you mean by ""equivalent"" and can decide how much deviation the two methods can have from one another before you no longer consider them equivalent.  In the regression approach I detailed above, you might consider the methods equivalent if confidence interval around the slope estimate included 1 and the CI around the intercept included 0.\\n",,
1221,2,579,0a276555-6a8d-4943-901e-def84bd78074,2010-07-23 21:23:18.0,88.0,"Indeed the only difference is that BIC is AIC extended to take number of objects (samples) into account. I would say that while both are quite weak (in comparison to for instance cross-validation) it is better to use AIC, than more people will be familiar with the abbreviation -- indeed I have never seen a paper or a program where BIC would be used (still I admit that I'm biased to problems where such criteria simply don't work).",,
1224,2,581,4b84007c-05d3-4de4-90fe-0c8b4adf9602,2010-07-23 22:12:36.0,99.0,I am currently using Viterbi training for an image segementation probelm.   I wanted to know what are the advantages/disadvantages of using the Baum-Welch algorithm instead of Viterbi training.   ,,
1225,1,581,4b84007c-05d3-4de4-90fe-0c8b4adf9602,2010-07-23 22:12:36.0,99.0,Differences between Baum Welch and Viterbi Training,,
1226,3,581,4b84007c-05d3-4de4-90fe-0c8b4adf9602,2010-07-23 22:12:36.0,99.0,<image><processing>,,
1227,2,582,e65be662-b81a-4ab8-a373-bd5151e13f1f,2010-07-23 23:38:20.0,108.0,"As you mentioned, AIC and BIC are methods to penalize models for having more regressor variables. A penalty function is used in these methods, which is a function of the number of parameters in the model. \\n\\n - When applying AIC, the penalty function is *z(p)* = 2 *p*.\\n\\n - When applying BIC, the penalty function is *z(p)* = p ln(*n*), which is based on interpreting the penalty as deriving from prior information (hence the name Bayesian Information Criterion).\\n\\nWhen *n* is large the two models will produce quite different results. Then the BIC applies a much larger penalty for complex models, and hence will lead to simpler models than AIC. However, as stated in [Wikipedia on BIC][1]: \\n\\n> it should be noted that in many\\n> applications..., BIC simply reduces to\\n> maximum likelihood selection because\\n> the number of parameters is equal for\\n> the models of interest.\\n\\n\\nFurther reading:\\n\\n- [Wikipedia: AIC][2]\\n- [Wikipedia: BIC][1]\\n\\n  [1]: http://en.wikipedia.org/wiki/Bayesian_information_criterion\\n  [2]: http://en.wikipedia.org/wiki/Akaike_information_criterion\\n",,
1228,5,582,19ac5337-78fc-48bb-80e4-daebecaec705,2010-07-23 23:43:40.0,108.0,"As you mentioned, AIC and BIC are methods to penalize models for having more regressor variables. A penalty function is used in these methods, which is a function of the number of parameters in the model. \\n\\n - When applying AIC, the penalty function is *z(p)* = 2 *p*.\\n\\n - When applying BIC, the penalty function is *z(p)* = p ln(*n*), which is based on interpreting the penalty as deriving from prior information (hence the name Bayesian Information Criterion).\\n\\nWhen *n* is large the two models will produce quite different results. Then the BIC applies a much larger penalty for complex models, and hence will lead to simpler models than AIC. However, as stated in [Wikipedia on BIC][1]: \\n\\n> it should be noted that in many\\n> applications..., BIC simply reduces to\\n> maximum likelihood selection because\\n> the number of parameters is equal for\\n> the models of interest.\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bayesian_information_criterion\\n",deleted 130 characters in body,
1229,2,583,5aff9b8a-9cb4-434a-b3bd-a76d36806281,2010-07-24 00:07:06.0,39.0,"Though AIC and BIC are both [maximum likelihood estimate][1] driven and penalize free parameters in an effort to combat overfitting, they do so in ways that result in significantly different behavior.  Lets look at one commonly presented version of the methods (which results form stipulating normally distributed errors and other well behaving assumptions):\\n\\n - **AIC** = -2*ln(likelihood) + 2*k,\\n\\nand \\n\\n - **BIC** = -2*ln(likelihood) + ln(N)*k,\\n\\nwhere:\\n\\n - k = model degrees of freedom\\n - N = number of observations\\n\\nThe best model in the group compared is the one that minimizes these scores, in both cases.  Claerly, AIC does not depend directly on sample size.  Moreover, generally speaking, AIC presents the danger that it might overfit, whereas BIC presents the danger that it might underfit, simply in virtue of how they penalize free parameters (2*k in AIC; ln(N)*k in BIC). Diachronically, as data is introduced and the scores are recalculated, at relatively low N (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher N (as the natural log of N overcomes 2).\\n\\nAdditionally, AIC is designed to is aimed at finding the best approximating model to the unknown data generating process (via minimizing expected estimated [K-L divergence][2]).  As such, it fails to converge in probability to the true model (assuming one is present in the group evaluated), whereas BIC does converge as n tends to infinity.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Maximum_likelihood\\n  [2]: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",,
1230,5,583,a391d36d-fe6b-4996-a704-727516ab7a14,2010-07-24 00:25:35.0,39.0,"Though AIC and BIC are both [maximum likelihood estimate][1] driven and penalize free parameters in an effort to combat overfitting, they do so in ways that result in significantly different behavior.  Lets look at one commonly presented version of the methods (which results form stipulating normally distributed errors and other well behaving assumptions):\\n\\n - **AIC** = -2*ln(likelihood) + 2*k,\\n\\nand \\n\\n - **BIC** = -2*ln(likelihood) + ln(N)*k,\\n\\nwhere:\\n\\n - k = model degrees of freedom\\n - N = number of observations\\n\\nThe best model in the group compared is the one that minimizes these scores, in both cases.  Claerly, AIC does not depend directly on sample size.  Moreover, generally speaking, AIC presents the danger that it might overfit, whereas BIC presents the danger that it might underfit, simply in virtue of how they penalize free parameters (2*k in AIC; ln(N)*k in BIC). Diachronically, as data is introduced and the scores are recalculated, at relatively low N (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher N (as the natural log of N overcomes 2).\\n\\nAdditionally, AIC is designed to is aimed at finding the best approximating model to the unknown data generating process (via minimizing expected estimated [K-L divergence][2]).  As such, it fails to converge in probability to the true model (assuming one is present in the group evaluated), whereas BIC does converge as n tends to infinity.\\n\\nSo, as in many methodological questions, which is to be preferred depends upon what you are trying to do, and whether or not any of the features outlined (convergence, relative tolerance for free parameters, minimizing expected K-L divergence) speak to your goals.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Maximum_likelihood\\n  [2]: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",added 268 characters in body,
1231,2,584,42b04e48-36d0-4c43-b8a5-06e19c7ed786,2010-07-24 00:44:43.0,240.0,"Forward-backward is used when you want to count 'invisible things'. For example, when using E-M to improve a model via unsupervised data. I think that Petrov's paper is an example. In the technique I'm thinking of, you first train a model with annotated data with fairly coarse annotations (e.g. a tag for 'Verb'). Then you arbitrarily split the probability mass for that state in two slightly unequal quantities, and retrain, running forward-backward to maximize likelihood by redistributing mass between the two states.",,
1232,2,585,998feb8f-8357-4efb-ad59-ca14ec9d5bc5,2010-07-24 00:46:15.0,240.0,You can certainly get different results simply because you train on different examples. I very much doubt that there's an algorithm or problem domain where the results of the two would differ in some predictable way.\\n,,
1233,6,581,15795aad-e561-4d44-989d-874da93ccb8d,2010-07-24 00:48:43.0,,<machine-learning><image><processing>,edited tags,user28
1234,5,583,f55323b6-d6da-401a-aad2-a11534fe8d59,2010-07-24 00:54:41.0,39.0,"Though AIC and BIC are both [maximum likelihood estimate][1] driven and penalize free parameters in an effort to combat overfitting, they do so in ways that result in significantly different behavior.  Lets look at one commonly presented version of the methods (which results form stipulating normally distributed errors and other well behaving assumptions):\\n\\n - **AIC** = -2*ln(likelihood) + 2*k,\\n\\nand \\n\\n - **BIC** = -2*ln(likelihood) + ln(N)*k,\\n\\nwhere:\\n\\n - k = model degrees of freedom\\n - N = number of observations\\n\\nThe best model in the group compared is the one that minimizes these scores, in both cases.  Claerly, AIC does not depend directly on sample size.  Moreover, generally speaking, AIC presents the danger that it might overfit, whereas BIC presents the danger that it might underfit, simply in virtue of how they penalize free parameters (2*k in AIC; ln(N)*k in BIC). Diachronically, as data is introduced and the scores are recalculated, at relatively low N (7 and less) BIC is more tolerant of free parameters than AIC, but less tolerant at higher N (as the natural log of N overcomes 2).\\n\\nAdditionally, AIC is designed to is aimed at finding the best approximating model to the unknown data generating process (via minimizing expected estimated [K-L divergence][2]).  As such, it fails to converge in probability to the true model (assuming one is present in the group evaluated), whereas BIC does converge as n tends to infinity.\\n\\nSo, as in many methodological questions, which is to be preferred depends upon what you are trying to do, what other methods are available, and whether or not any of the features outlined (convergence, relative tolerance for free parameters, minimizing expected K-L divergence) speak to your goals.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Maximum_likelihood\\n  [2]: http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence",added 34 characters in body,
1235,5,524,367253b5-7c61-4a5f-8fae-fecbe15eec82,2010-07-24 01:09:38.0,,"Debugging MCMC programs is notoriously difficult. The difficulty arises because of several issues some of which are:\\n\\n(a) Cyclic nature of the algorithm\\n\\nWe iteratively draw parameters conditional on all other parameters. Thus, if a implementation is not working properly it is difficult to isolate the bug as the issue can be anywhere in the iterative sampler.\\n\\n(b) The correct answer is not necessarily known. \\n\\nWe have no way to tell if we have achieved convergence. To some extent this can be mitigated by testing the code on simulated data.\\n\\nIn light of the above issues, I was wondering if there is a standard technique that can be used to debug MCMC programs. \\n\\n**Edit**\\n\\nI wanted to share the approach I use to debug my own programs. I, of course, do all of the things that PeterR mentioned. Apart from those, I perform the following tests using simulated data:\\n\\n1. Start all parameters from true values and see if the sampler diverges too far from the true values.\\n\\n2. I have flags for each parameter in my iterative sampler that determines whether I am drawing that parameter in the iterative sampler. For example, if a flag 'gen_param1' is set to true then I draw 'param1' from its full conditional in the iterative sampler. If this is set to false then 'param1' is set to its true value.\\n\\nOnce I finish writing up the sampler, I test the program using the following recipe:\\n\\n- Set the generate flag for one parameter to true and everything else to false and assess convergence with respect to true value.\\n- Set the generate flag for another parameter in conjunction with the first one and again assess convergence.\\n\\nThe above steps have been incredibly helpful to me.\\n",added 1029 characters in body,user28
1236,2,586,5d2396f0-2272-4d4d-92e8-e5738c861534,2010-07-24 01:41:28.0,251.0,"The [Tutorial on Energy-Based Learning][1] by LeCun et al. might get you a good part of the way there.  They describe a number of loss functions and discuss what makes them ""good or bad"" for energy based models.\\n\\n\\n  [1]: http://yann.lecun.com/exdb/publis/#lecun-06\\n",,
1237,2,587,330fc234-7455-424e-a831-b1d9907a91a7,2010-07-24 03:58:58.0,159.0,My quick explanation is\\n\\n - AIC is best for prediction as it is asymptotically equivalent to cross-validation.\\n - BIC is best for explanation as it is allows consistent estimation of the underlying data generating process.\\n,,
1238,2,588,7f47acb6-b2b3-4729-8e75-db733b53d5cd,2010-07-24 04:22:01.0,61.0,"The Baum-Welch algorithm and the Viterbi algorithm calculate different things.\\n\\nIf you know the transition probabilities for the hidden part of your model, and the emission probabilities for the visible outputs of your model, then the Viterbi algorithm gives you the most likely *complete* sequence of hidden states conditional on both your outputs and your model specification.\\n\\nThe Baum-Welch algorithm gives you both the mostl likely hidden transition probabilities as well as the most likely set of emission probabilities given only the observed states of the model (and, usually, an upper bound on the number of hidden states).  You also get the ""pointwise"" highest likelihood points in the hidden states, which is often slightly different from the single hidden sequence that is overall most likely.\\n\\nIf you know your model and just want the latent states, then there is no reason to use the Baum-Welch algorithm.  If you don't know your model, then you can't be using the Viterbi algorithm.",,
1239,2,589,a0036cee-f603-45fd-8887-4ed408a682f5,2010-07-24 04:54:53.0,61.0,"Computational issues are the strongest argument I've heard one way or the other.  The single biggest advantage of the Kolmogorov distance is that it's very easy to compute analytically for pretty much any CDF.  Most other distance metrics don't have a closed-form expression except, sometimes, in the Gaussian case.\\n\\nThe Kolmogorov distance of a sample also has a known sampling distribution given the CDF (I don't think most other ones do), which ends up being related to the Wiener process.  This is the basis for the Kolmogorov-Smirnoff test for comparing a sample to a distribution or two samples to each other.\\n\\nOn a more functional-analysis note, the sup norm is nice in that (as you mention) it basically defines uniform convergence.  This leaves you with norm convergence implying pointwise convergence, and so you if you're clever about how you define your function sequences you can work within a RKHS and use all of the nice tools that that provides as well.",,
1240,2,590,d257dc87-5845-4d05-a4c2-35fc95c7fc3d,2010-07-24 09:09:14.0,190.0,"In some papers, for example in [""The Geometric Density with Unknown Location Parameter""][1] by Klotz, a Geometric Distribution is called a Geometric Density.\\n\\nFor me, this claim looks erroneous, however Klotz is a serious statistician and a professor in the field.\\n\\nMy question is, to what extend is it legitimate to call a Geometric Distribution a Geometric Density?\\n\\n\\n  [1]: http://www.jstor.org/stable/2239262",,
1241,1,590,d257dc87-5845-4d05-a4c2-35fc95c7fc3d,2010-07-24 09:09:14.0,190.0,To what extent can we call a Geometric Distribution a Gemetric Density,,
1242,3,590,d257dc87-5845-4d05-a4c2-35fc95c7fc3d,2010-07-24 09:09:14.0,190.0,<distributions><discrete-data>,,
1243,2,591,91bf7c18-2102-4c8b-b5c6-041b282adc6b,2010-07-24 09:33:01.0,200.0,"The loss function is given by the problem. It could be anything. For example, you could also penalize the used CPU time and space.\\n\\nIn reinforcement learning, the loss function is an unknown non-deterministic function.\\nYou cannot redefine it without changing the problem.",,
1244,5,573,2a1c98f5-0c6f-4898-891e-7fb84422fe7f,2010-07-24 09:38:29.0,190.0,"In ""[Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations][1]"" by Lee et. al.([PDF][2]) Convolutional DBN's are proposed. Also the method is evaluated for image classification. This sounds logical, as there are natural local image features, like small corners and edges etc.\\n\\nIn ""[Unsupervised feature learning for audio classification using convolutional deep belief networks][3]"" by Lee et. al. this method is applied for audio in different types of classifications. Speaker identification, gender indentification, phone classification and also some music genre / artist classification.\\n\\nHow can the convolutional part of this network be interpretated for audio, like it can be explained for images as edges?\\n\\n\\n  [1]: http://dx.doi.org/10.1145/1553374.1553453\\n  [2]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&rep=rep1&type=pdf\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.380&rep=rep1&type=pdf",fixed a lot of bad grammar,
1245,2,592,0d48b5ef-0518-4a37-bdaa-9141f316d687,2010-07-24 09:52:52.0,57.0,"It is a bit old, but I have found Chris Chatfield's book,\\n\\n[Statistics for Technology: A Course in Applied Technology][1]\\n\\nto be an excellent introduction.\\n\\nIt was how I first learned about statistics for a conceptual point of view.\\n\\n\\n  [1]: http://www.amazon.com/Statistics-Technology-Applied-Chapman-Statistical/dp/0412253402/ref=ntt_at_ep_dpt_2",,
1246,16,592,0d48b5ef-0518-4a37-bdaa-9141f316d687,2010-07-24 09:52:52.0,-1.0,,,
1247,6,421,07dd87c5-5a69-4a97-b688-f7c2f19ef30a,2010-07-24 09:53:34.0,57.0,<textbook><books><subjective><science>,Added the 'subjective' tag to the question,
1248,9,421,cdbfc8ae-9930-4e8d-85fb-2e9dd9e5ad49,2010-07-24 09:58:03.0,190.0,<textbook><books><science>,Rollback to [673b9773-839a-4584-a7a9-2c64642d1fd8],
1249,5,470,84c904c3-23ec-4623-b3ef-f72f1e9175f7,2010-07-24 11:51:57.0,218.0,"The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman is a standard text for statistics and data mining, and is now free:\\n\\nhttp://www-stat.stanford.edu/~tibs/ElemStatLearn/\\n\\nAlso Available [here][1].\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0387848576/ref=nosim/gettgenedone-20",Edited out the dis-incentive to pay for physical book. ,
1250,2,593,470f90b9-f8aa-46fc-951c-f51243ed126b,2010-07-24 12:09:08.0,5.0,"I recall some discussion on this in the past; I'm not aware of any implementation of Maxwell & Delaney's approach, although it shouldn't be too difficult to do.  Have a look at ""[Repeated Measures ANOVA using R][1]"" which also shows one method of addressing the sphericity issue in [Tukey's HSD][2].\\n\\nYou might also find [this description of Friedman's test][3] of interest.\\n\\n\\n  [1]: http://gribblelab.org/2009/03/09/repeated-measures-anova-using-r/\\n  [2]: http://en.wikipedia.org/wiki/Tukey's_range_test\\n  [3]: http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/",,
1251,4,492,c56fe2f2-cde9-46fb-bcb4-c538aea5f497,2010-07-24 16:54:57.0,246.0,Dubious use of signal processing principles to identify a trend,edited title,
1253,5,456,54b747dd-4e41-44e6-872e-92dcddb52583,2010-07-24 17:16:47.0,39.0,"As the [Encyclopedia of GIS][1] states, the conditional autoregressive model (CAR) is appropriate for situations with first order dependency or relatively local spatial autocorrelation, and simultaneous autoregressive model (SAR) is  more suitable where there are second order dependency or a more global spatial autocorrelation.\\n\\nThis is made clear by the fact that CAR obeys the spatial version of the [Markov property][2], namely that the state of neighbors is due only to their neighbors (i.e. spatially “memoryless”, instead of temporally), whereas SAR does not.  This is due to the different ways in which they specify their variance-covariance matrixes.  So, when the spatial Markov property obtains, CAR provides a simpler way to model autocorrelated geo-referenced areal data.\\n\\nSee [Gis And Spatial Data Analysis: Converging Perspectives][3] for more details.\\n\\n\\n  [1]: http://books.google.com/books?id=6q2lOfLnwkAC&dq=Encyclopedia+of+GIS\\n  [2]:  http://en.wikipedia.org/wiki/Markov_property\\n  [3]: http://www.geog.ucsb.edu/~good/papers/387.pdf",deleted 1 characters in body,
1254,2,594,99198ae3-6f91-4edf-8612-9cb540c8b4e4,2010-07-24 19:25:59.0,240.0,"The E-M procedure appears, to the uninitiated, as more or less black magic. Estimate parameters of an HMM (for example) using supervised data. Then decode untagged data, using forward-backward to 'count' events as if the data were tagged, more or less. Why does this make the model better? I do know something about the math, but I keep wishing for some sort of mental picture of it.\\n",,
1255,1,594,99198ae3-6f91-4edf-8612-9cb540c8b4e4,2010-07-24 19:25:59.0,240.0,"E-M, is there an intuitive explanation?",,
1256,3,594,99198ae3-6f91-4edf-8612-9cb540c8b4e4,2010-07-24 19:25:59.0,240.0,<e-m><intuition>,,
1257,5,588,b6d0f834-7866-4808-9466-dca3a4d355e5,2010-07-24 19:31:34.0,61.0,"The Baum-Welch algorithm and the Viterbi algorithm calculate different things.\\n\\nIf you know the transition probabilities for the hidden part of your model, and the emission probabilities for the visible outputs of your model, then the Viterbi algorithm gives you the most likely *complete* sequence of hidden states conditional on both your outputs and your model specification.\\n\\nThe Baum-Welch algorithm gives you both the mostl likely hidden transition probabilities as well as the most likely set of emission probabilities given only the observed states of the model (and, usually, an upper bound on the number of hidden states).  You also get the ""pointwise"" highest likelihood points in the hidden states, which is often slightly different from the single hidden sequence that is overall most likely.\\n\\nIf you know your model and just want the latent states, then there is no reason to use the Baum-Welch algorithm.  If you don't know your model, then you can't be using the Viterbi algorithm.\\n\\nEdited to add: See Peter Smit's comment; there's some overlap/vagueness in nomenclature.  Some poking around led me to a chapter by Luis Javier Rodrıguez and Ines Torres in ""Pattern Recognition and Image Analysis"" (ISBN 978-3-540-40217-6, pp 845-857) which discusses the speed versus accuracy trade-offs of the two algorithms.\\n\\nBriefly, the Baum-Welch algorithm is essentially the Expectation-Maximization algorithm applied to a HMM; as a strict EM-type algorithm you're guaranteed to converge to at least a local maximum, and so for unimodal problems find the MLE.  It requires two passes over your data for each step, though, and the complexity gets very big in the length of the data and number of training samples.  However you do end up with the full conditional likelihood for your hidden parameters.\\n\\nThe Viterbi training algorithm (as opposed to the ""Viterbi algorithm"") approximates the MLE to achieve a gain in speed at the cost of accuracy.  It segments the data and then applies the Viterbi algorithm (as I understood it) to get the most likely state sequence in the segment, then uses that most likely state sequence to re-estimate the hidden parameters.  This, unlike the Baum-Welch algorithm, doesn't give the full conditional likelihood of the hidden parameters, and so ends up reducing the accuracy while saving significant (the chapter reports 1 to 2 orders of magnitude) computational time.\\n",added 1419 characters in body,
1258,2,595,8f3431d5-babc-4092-9676-e0cff17a21c3,2010-07-24 19:43:24.0,223.0,"You could use the (fast :) ) discrete wavelet transform. The package wavethresh under R will do all the work. \\nAnyway, I like the solution of @James because it is simple and seems to go straigh to the point.  ",,
1259,2,596,b347805b-5a58-4d24-9192-e7eff3275c16,2010-07-24 19:50:48.0,240.0,E-M provides a way to improve the estimation of a generative model with unannotated data. Is there anything out there that works the same way for discriminative models (e.g. perceptrons)?\\n,,
1260,1,596,b347805b-5a58-4d24-9192-e7eff3275c16,2010-07-24 19:50:48.0,240.0,Something like E-M for discriminative models?,,
1261,3,596,b347805b-5a58-4d24-9192-e7eff3275c16,2010-07-24 19:50:48.0,240.0,<machine-learning><e-m>,,
1262,2,597,3ee80d88-8cd1-45e2-995b-fcda0e712cce,2010-07-24 19:55:30.0,240.0,"Well, there's [this][1] and [that][2]. Two papers by Cramer and others discussing loss in the context of online learning algorithms.\\n\\n\\n  [1]: http://docs.google.com/viewer?a=v&q=cache:cfjRh4vE38kJ:books.nips.cc/papers/files/nips16/NIPS2003_LT21.pdf+passive-aggressive+machine+learning&hl=en&gl=us&pid=bl&srcid=ADGEEShVh5WierOOrVz8pTpfOPVUCm8HIqkUCm1rxCsUPm5UUapNONytwRMjMq63zcOye0L3djRjQCZW6YN_r12SvIEaWCtbx3b5La7EJ8nQuoWm2Jw8n8DuLEjS19K56fh8DZLOfrHu&sig=AHIEtbRBrb7Q7Lx6LSh_fKEFXyWaxW0RVw\\n  [2]: http://portal.acm.org/citation.cfm?id=1248547.1248566",,
1263,2,598,2fb66284-b3e8-4efc-9825-5faf2d2543e7,2010-07-24 20:15:20.0,22.0,"Few more on top of already mentioned:\\n\\n - [KNIME][1] together with R, Python and Weka integration extensions for data mining   \\n - [Mondrian][2] for quick EDA     \\n\\nAnd from spatial perspective:\\n\\n - [GeoDa][3] for spatial EDA and clustering of areal data\\n - [SaTScan][4] for clustering of point data\\n\\n\\n  [1]: http://www.knime.org/\\n  [2]: http://rosuda.org/mondrian/\\n  [3]: http://geodacenter.asu.edu/ogeoda\\n  [4]: http://www.satscan.org/",,
1264,16,598,2fb66284-b3e8-4efc-9825-5faf2d2543e7,2010-07-24 20:15:20.0,-1.0,,,
1265,2,599,24dceedc-2049-4d20-92bf-7182aab08627,2010-07-24 20:23:06.0,61.0,"Just to save some typing, call the observed data X, the missing data Z (e.g. the hidden states of the HMM), and the parameter vector we're trying to find Q (e.g. transition/emission probabilities).\\n\\nThe intuitive explanation is that we basically cheat, pretend for a moment we know Q so we can find a conditional distribution of Z that in turn lets us find the MLE for Q (ignoring for the moment the fact that we're basically making a circular argument), then admit that we cheated, put in our new, better value for Q, and do it all over again until we don't have to cheat anymore.\\n\\nSlightly more technically, by pretending that we know the real value Q, we can pretend we know something about the conditional distribution of Z|{X,Q}, which lets us improve our estimate for Q, which we now pretend is the real value for Q so we can pretend we know something about the conditional distribution of Z|{X,Q}, which lets us improve our estimate for Q, which... and so on.\\n\\nEven more technically, if we knew Z, we could maximize log(f(Q|X,Z)) and have the right answer.  The problem is that we don't know Z, and any estimate for Q must depend on it.   But if we want to find the best estimate (or distribution) for Z, then we need to know X and Q.  We're stuck in a chicken-and-egg situation if we want the unique maximizer analytically.\\n\\nOur 'out' is that -- for any estimate of Q (call it Q_n) -- we can find the distribution of Z|{Q_n,X}, and so we can maximize our *expected* joint log-likelihood of Q|{X,Z}, with respect to the conditional distribution of Z|{Q_n,X}.  This conditional distribution basically tells us how Z depends on the current value of Q given X, and lets us know how to change Q to increase our likelihood for both Q and Z at the same time for a particular value of Q (that we've called Q_n).  Once we've picked out a new Q_n+1, we have a different conditional distribution for Z|{Q_n+1, X} and so have to re-calculate the expectation.",,
1266,2,600,e8690a49-eb8f-4d0c-b307-5fc29dd8ef29,2010-07-24 20:36:13.0,30.0,"Mark,\\n\\nthe main reason of which I am aware for the use of K-S is because it arises naturally from Glivenko-Cantelli theorems in univariate empirical processes. The one reference I'd recommend is A.W.van der Vaart ""Asymptotic Statistics"", ch. 19. A more advanced monograph is ""Weak Convergence and Empirical Processes"" by Wellner and van der Vaart. \\n\\nI'd add two quick notes:\\n\\n 1. another measure of distance commonly used in univariate distributions is the Cramer-von Mises distance, which is an L^2 distance;\\n 2. in general vector spaces different distances are employed; the space of interest in many papers is polish. A very good introduction is Billingsley's ""Convergence of Probability Measures"". \\n\\nI apologize if I can't be more specific. I hope this helps.\\n\\n",,
1267,5,579,7e05dca9-e964-4647-beed-01152d227c3a,2010-07-24 21:03:17.0,88.0,"Indeed the only difference is that BIC is AIC extended to take number of objects (samples) into account. I would say that while both are quite weak (in comparison to for instance cross-validation) it is better to use AIC, than more people will be familiar with the abbreviation -- indeed I have never seen a paper or a program where BIC would be used (still I admit that I'm biased to problems where such criteria simply don't work).\\n\\nEdit: AIC and BIC are equivalent to cross-validation provided two important asumptions -- when they are defined, so when the model is a maximum likelihood one and when you are only interested in model performance on a training data. In case of collapsing some data into some kind of consensus they are perfectly ok.   \\nIn case of making a prediction machine for some real-world problem the first is false, since your training set represent only a scrap of information about the problem you are dealing with, so you just can't optimize your model; the second is false, because you expect that your model will handle the new data for witch you can't even expect that the training set will be representative. \\nAnd to this end CV was invented; to simulate the behavior of the model when confronted with an independent data. In case of model selection, CV gives you not only the quality approximate, but also quality approximation distribution, so it has this great advantage that it can say ""I don't know, whatever the new data will come, either of them can be better.""  ",Extension.,
1269,2,602,aa18db30-d199-4919-a2f9-251fb9fee99f,2010-07-24 22:29:40.0,88.0,"Do you know any good heuristics for finding optimal value of ν in case of ν-SVM classification? In this particular problem I have a radial basis kernel, if it helps.",,
1270,1,602,aa18db30-d199-4919-a2f9-251fb9fee99f,2010-07-24 22:29:40.0,88.0,Heuristics for optimizing ν-SVM?,,
1271,3,602,aa18db30-d199-4919-a2f9-251fb9fee99f,2010-07-24 22:29:40.0,88.0,<machine-learning><svm>,,
1272,2,603,68277491-2a92-414c-a58a-4866a649fc0a,2010-07-24 22:54:13.0,30.0,"> Usually of course the difference is\\n> unnoticeable, and so goes my question\\n> -- can you think of an example when the result of one type is\\n> significantly different from another?\\n\\nI am not sure at all the difference is unnoticeable, and that only in ad hoc example it will be noticeable. Both cross-validation and bootstrapping (sub-sampling) methods depend critically on their design parameters, and this understanding is not complete yet. In general, results *within* k-fold cross-validation depend critically on the number of folds, so you can expect always different results from what you would observe in sub-sampling.\\n\\nCase in point: say that you have a true linear model with a fixed number of parameters. If you use k-fold cross-validation (with a given, fixed k), and let the number of observations go to infinity, k-fold cross validation will be asymptotically inconsistent for model selection, i.e., it will identify an incorrect model with probability greater than 0. This surprising result is due to Jun Shao, ""Linear Model Selection by Cross-Validation"", *Journal of the American Statistical Association*, **88**, 486-494 (1993), but more papers can be found in this vein.\\n\\nIn general, respectable statistical papers specify the cross-validation protocol, exactly because results are not invariant. In the case where they choose a large number of folds for large datasets, they remark and try to correct for biases in model selection.",,
1273,2,604,5f4df81f-e23b-4824-a466-1cc449ecb361,2010-07-24 23:09:05.0,165.0,"I am puzzled by something I found using Linear Discriminant Analysis. Here is the problem - I first ran the Discriminant analysis using 20 or so independent variables to predict 5 segments. Among the outputs, I asked for the Predicted Segments, which are the same as the original segments for around 80% of the cases. Then I ran again the Discriminant Analysis with the same independent variables, but now trying to predict the Predicted Segments. I was expecting I would get 100% of correct classification rate, but that did not happen and I am not sure why. It seems to me that if the Discriminant Analysis cannot predict with 100% accuracy it own predicted segments then somehow it is not a optimum procedure since a rule exist that will get 100% accuracy. I am missing something?\\n\\nNote - This situation seems to be the same as in Regression Analysis when you run the model trying to predict the predicted dependent variable. But in regression you will get R2 = 1 if you do that.\\n\\nNote 2 - I run this test with Discriminant Analysis in SPSS.",,
1274,1,604,5f4df81f-e23b-4824-a466-1cc449ecb361,2010-07-24 23:09:05.0,165.0,Discriminant Analysis,,
1275,3,604,5f4df81f-e23b-4824-a466-1cc449ecb361,2010-07-24 23:09:05.0,165.0,<regression><analysis><discriminant>,,
1276,2,605,097b191c-c440-4e7d-bbf3-bc2af743fcf8,2010-07-24 23:25:48.0,88.0,"This is quite normal in case of machine learning -- it does not need to be optimal, it must be general. ",,
1277,5,35,1de47bc7-dd3e-4c30-b3c9-4b47ba719806,2010-07-25 01:44:14.0,54.0,"I have a data set that I'd expect to follow a Poisson distribution, but it is overdispersed by about 3-fold. At the present, I'm modelling this overdispersion using something like the following code in R.\\n\\n    ## assuming a median value of 1500\\n    med = 1500\\n    rawdist = rpois(1000000,med)\\n    oDdist = rawDist + ((rawDist-med)*3)\\n\\nVisually, this seems to fit my empirical data very well. If I'm happy with the fit, is there any reason that I should be doing something more complex, like using a [negative binomial distribution, as described here](http://en.wikipedia.org/wiki/Overdispersion#Poisson)? (If so, any pointers or links on doing so would be much appreciated).\\n\\nOh, and I'm aware that this creates a slightly jagged distribution (due to the multiplication by three), but that shouldn't matter for my application.\\n\\n\\n----------\\n\\n\\n**Update:**  For the sake of anyone else who searches and finds this question, here's a simple R function to model an overdispersed poisson using a negative binomial distribution. Set d to the desired mean/variance ratio:\\n\\n    rpois.od<-function (n, lambda,d=1) {\\n      if (d==1)\\n        rpois(n, lambda)\\n      else\\n         rnbinom(n, size=(lambda/(d-1)), mu=lambda)\\n    }\\n\\n(via the R mailing list: https://stat.ethz.ch/pipermail/r-help/2002-June/022425.html)",added followup with solution,
1278,2,606,7b5a72e6-a62f-4c50-8be0-8665b80de6f9,2010-07-25 02:37:06.0,30.0,"A very popular approach is penalized logistic regression, in which one maximizes the sum of the log-likelihood and a penalization term consisting of the L1-norm (""lasso""), L2-norm (""ridge""), a combination of the two (""elastic""), or a penalty associated to groups of variables (""group lasso""). This approach has several advantages:\\n\\n 1. It has strong theoretical properties, e.g., [see this paper by Candes & Plan][1] and close connections to compressed sensing;\\n 2. It has accessible expositions, e.g., in [Elements of Statistical Learning][2] by Friedman-Hastie-Tibshirani (available online);\\n 3. It has readily available software to fit models. R has the [glmnet][3] package which is very fast and works well with pretty large datasets. Python has [scikit-learn][4], which includes L1- and L2-penalized logistic regression;\\n 4. It works very well in practice, as shown in many application papers in image recognition, signal processing, biometrics, and finance.\\n\\n\\n  [1]: http://www-stat.stanford.edu/~candes/papers/LassoPredict.pdf\\n  [2]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html\\n  [3]: http://cran.r-project.org/web/packages/glmnet/index.html\\n  [4]: http://scikit-learn.sourceforge.net",,
1279,16,606,7b5a72e6-a62f-4c50-8be0-8665b80de6f9,2010-07-25 02:37:06.0,-1.0,,,
1280,2,607,58474ad0-c485-4210-a7d1-f290df9404cf,2010-07-25 03:29:41.0,30.0,"I think the answer to your question is simply in the affirmative. Take any issue of Statistical Science, JAMA, Annals of Statistics of the past 10 years and you'll find papers on boosting, SVM, and neural networks, although this area is less active now. Statisticians have appropriated the work of Valiant and Vapnick, but on the other side, computer scientists have absorbed the work of Donoho and Talagrand. I don't think there is much difference in scope and methods anymore. I have never bought Breiman's argument that CS people were only interested in minimizing loss using whatever works. That view was heavily influenced by his participation in Neural Networks conferences and his consulting work; but PAC, SVMs, Boosting have all solid foundations. And today, unlike in 2001, Statistics is more concerned with finite-sample properties, algorithms and massive datasets.\\n\\nBut I think that there are still three important differences that are not going away soon. \\n\\n 1. Methodological Statistics papers are still overwhelmingly formal and deductive, whereas Machine Learning researchers are more tolerant of new approaches even if they don't come with a proof attached;\\n 2. The ML community primarily shares new results and publications in conferences and related proceedings, whereas statisticians use journal papers. This slows down progress in Statistics and identification of star researchers. John Langford a [nice post][1] on the subject from a while back;\\n 3. Statistics still covers areas that are (for now) of little concern to ML, such as survey design, sampling, industrial Statistics etc.\\n\\n\\n  [1]: http://hunch.net/?p=318",,
1281,2,608,fa535df5-b48d-4ec7-849a-4224fa5f6060,2010-07-25 05:08:20.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other",,
1282,1,608,fa535df5-b48d-4ec7-849a-4224fa5f6060,2010-07-25 05:08:20.0,196.0,How to empirically demonstrate in R what cross-validation methods the AIC and BIC are equivalent to?,,
1283,3,608,fa535df5-b48d-4ec7-849a-4224fa5f6060,2010-07-25 05:08:20.0,196.0,<r><aic><cross-validation><bic>,,
1284,6,577,6ad1bf62-f2ec-4bd8-ac38-2d15b3e0f0ce,2010-07-25 05:09:18.0,196.0,<modeling><aic><cross-validation><bic><model-selection>,Added the cross-validation tag,
1285,4,608,494e4bb1-684a-47b1-98d6-c216a7e2eabe,2010-07-25 05:17:00.0,196.0,How can one empirically demonstrate in R which cross-validation methods the AIC and BIC are equivalent to?,Grammar changes in title to make it more readable.,
1286,2,609,8164bf83-7c82-4fad-987c-5d7759ce0c8f,2010-07-25 07:18:44.0,223.0,"**As a summary**, my answer is : if you have an explicit expression or can figure out some how what your distance measure, then you can say if it gives more importance to some part of the distribution. The other way to analyse and compare such test is the minimax theory. \\n\\nAt the end some test will be good for some alternatives and some for other. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. \\n\\n\\n----------\\n **Some details** \\n\\n Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their ""optimal detection boudary"" http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492.\\n\\n\\nLet me go distance by distance:\\n\\n 1. KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.\\n\\n \\n - Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  http://en.wikipedia.org/wiki/Wasserstein_metric is ubiquitous and hence hard to compare. \\n   - For the particular case of W2 it has been uses in http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1017938923  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. \\n   - For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a ""balanced test"" not giving importance to tails. \\n\\n\\n\\n\\n----------\\n\\n\\nTo recall and extend the comment I made which complete the answer: \\n\\nI know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be ** minimax** for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. \\n\\n",,
1287,2,610,2c1ac0bd-0bd9-4a8b-b5f6-805b14fc7b25,2010-07-25 09:13:59.0,159.0,It sounds dodgy to me as the trend estimate will be biased near the point where you splice on the false data. An alternative approach is a nonparametric regression smoother such as loess or splines.,,
1289,6,411,aa1f139a-c7da-4ebf-b780-4971cf2b764e,2010-07-25 09:41:58.0,223.0,<distributions><hypothesis-testing><probability><mathematical-statistics>,edited tags,
1290,5,609,e8627fe6-2113-456f-953d-1c4cf926c7fa,2010-07-25 09:44:12.0,223.0,"**As a summary**, my answer is : if you have an explicit expression or can figure out some how what your distance is measuring (what ""differences"" it gives weigth to), then you can say what it is better for. An other complementary way to analyse and compare such test is the minimax theory. \\n\\nAt the end some test will be good for some alternatives and some for other. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. \\n\\n\\n----------\\n **Some details** \\n\\n Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their ""optimal detection boudary"" http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492.\\n\\n\\nLet me go distance by distance:\\n\\n 1. KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.\\n\\n \\n - Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  http://en.wikipedia.org/wiki/Wasserstein_metric is ubiquitous and hence hard to compare. \\n   - For the particular case of W2 it has been uses in http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1017938923  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. \\n   - For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a ""balanced test"" not giving importance to tails. \\n\\n\\n\\n\\n----------\\n\\n\\nTo recall and extend the comment I made which complete the answer: \\n\\nI know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be minimax for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. \\n\\n",added 19 characters in body; deleted 5 characters in body,
1291,4,590,68ba9381-b746-400e-8281-23c1318a4f84,2010-07-25 10:09:23.0,190.0,To what extent can we call a Geometric Distribution a Geometric Density,edited title,
1292,5,596,1d477130-bc7f-4323-9417-4316d2c0d173,2010-07-25 12:15:12.0,240.0,"E-M provides a way to improve the estimation of a generative model with unannotated data. Is there anything out there that works the same way for discriminative models (e.g. perceptrons)?\\n\\nFor example, consider averaged perceptron tagger. It would be handy to be able to throw the entire Gigaword through some process of unsupervised model improvement.\\n.",added 168 characters in body,
1293,5,604,421fce24-49dc-46cf-981b-7ef0ac29412b,2010-07-25 12:20:27.0,165.0,"I am puzzled by something I found using Linear Discriminant Analysis. Here is the problem - I first ran the Discriminant analysis using 20 or so independent variables to predict 5 segments. Among the outputs, I asked for the Predicted Segments, which are the same as the original segments for around 80% of the cases. Then I ran again the Discriminant Analysis with the same independent variables, but now trying to predict the Predicted Segments. I was expecting I would get 100% of correct classification rate, but that did not happen and I am not sure why. It seems to me that if the Discriminant Analysis cannot predict with 100% accuracy it own predicted segments then somehow it is not a optimum procedure since a rule exist that will get 100% accuracy. I am missing something?\\n\\nNote - This situation seems to be similar to that in Linear Regression Analysis (If you fit the model y = a + bX + error and use the estimated equation with the same data you will get y(hat) [= a(hat) + b(hat)X]. Now if you estimate the model y(hat) = a + bX + error, you will find the same a(hat) and b(hat) as before, and no error and R2 = 100% - perfect fit. I though this would also happen with Linear Discriminant Analysis, but it does not.)\\n\\nNote 2 - I run this test with Discriminant Analysis in SPSS.",added 249 characters in body,
1294,2,611,f85ce618-5d4a-44db-93b0-309941331e9e,2010-07-25 12:36:23.0,223.0,"The concept of density is much wider than you may think. A density of a probability measure $P$ can be defined with respect to a measure $\\lambda$ that dominates $P$ by the Radon Nikodym Theorem (see http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem). Here density should be understood as a density with respect to the counting measure defined on the mentionned countable set. I agree that it is not extremly rigorous not to mention the reference when talking about a density (but who mention density  wrt lesbesgue measure?), but it pose no problem while reading the paper in question so .... ",,
1295,5,611,6515ee9c-0675-42a3-be1e-e60086fdf7ce,2010-07-25 12:45:06.0,223.0,"The concept of density is much wider than you may think. A density of a probability measure $P$ can be defined with respect to a measure $\\lambda$ that dominates $P$ by the Radon Nikodym Theorem (see http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem). Here density should be understood as a density with respect to the counting measure defined on the mentionned countable set. I agree that it is not extremely rigorous not to mention the reference when talking about a density (but who mention density  wrt lesbesgue measure?), but it pose no problem while reading the paper in question so .... \\n\\n\\n----------\\n\\n\\nAdditional Annex Note\\n I have seen a certain number of machine learning notes (I won't do delation) where  the reference measure is not the counting measure and we see things such as $P(X=x|Y=y)$ with X being a continuous variable (with a density wrt Lebesgues) (to apply Bayes principle and derive the Bayes rule). I guess people want to be pedagogic and do not want to bother students with technical details :) ",added 437 characters in body,
1296,2,612,f21ed174-2034-4741-9ee7-15960db62286,2010-07-25 14:31:31.0,144.0,"I have tried to reproduce some research (using PCA) from SPSS in R. In my experience, <a href=""http://www.personality-project.org/R/html/principal.html"">`principal()`</a> function from package `psych` was the only function that came close (or if my memory serves me right, dead on) to match the output. To match the same results as in SPSS, I had to use parameter `principal(..., rotate = ""varimax"")`. I have seen papers talk about how they did PCA, but based on the output of SPSS and use of rotation, it sounds more like Factory analysis.\\n\\nQuestion: Is PCA, even after rotation (using `varimax`), still PCA? I was under the impression that this might be in fact Factor analysis... In case it's not, what details am I missing?",,
1297,1,612,f21ed174-2034-4741-9ee7-15960db62286,2010-07-25 14:31:31.0,144.0,is psych::principal function still PCA when using rotation?,,
1298,3,612,f21ed174-2034-4741-9ee7-15960db62286,2010-07-25 14:31:31.0,144.0,<r><pca><fa><spss><rotation>,,
1299,2,613,d88905b6-bf0e-4cf7-a0d7-4d0e50404936,2010-07-25 14:33:25.0,30.0,"I have been a heavy R user for the past 6-7 years. As a language, it has several design limitations. Yet, for work in econometrics and in data analysis, I still wholeheartedly recommend it. It has a large number of packages that would be relevant to you for econometrics, time series, consumer choice modeling etc. and of course excellent visualization, good algebra and numerical libraries etc. I would not worry too much about data size limitations. Although R was not designed for ""big data"" (unlike, say, SAS) there are ways around it. The availability of packages is what makes the difference, really.\\n\\nI've only read Clojure's language specs, and it's beautiful and clean. It addresses in a natural way issues of parallelization and scale.  And if you have some basic java or OOP knowledge, you can benefit from the large number of high-quality java libraries.\\n\\nThe issue I have with Clojure is that is a recent one-man (R.Hickey) operation, therefore 1) very risky 2) very immature 3) with niche adoption. Great for enthusiasts, early adopters, CS/ML people who want to try new things. For a user who sees a language as a means to an end and who needs very robust code that can be shared code with others, established languages seem a safer choice. Just know who you are.",,
1300,2,614,a44663e9-2a59-4085-b4db-ae9e7de9def3,2010-07-25 14:53:50.0,107.0,"There have been a few questions about statistical [textbooks][1], such as the question [Free statistical textbooks][2]. However, I am looking for textbooks that are Open Source, for example, having an [Creative Commons][3] license. The reason is that in course material in other domains, you still want to include some text about basic statistics. In this case, it would be interesting to reuse existing material, instead of rewriting that material.\\n\\nTherefore, what Open Source textbooks on statistics (and perhaps machine learning) are available?\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/tagged/textbook\\n  [2]: http://stats.stackexchange.com/questions/170/free-statistical-textbooks\\n  [3]: http://creativecommons.org/",,
1301,1,614,a44663e9-2a59-4085-b4db-ae9e7de9def3,2010-07-25 14:53:50.0,107.0,Open Source statistical textbooks?,,
1302,3,614,a44663e9-2a59-4085-b4db-ae9e7de9def3,2010-07-25 14:53:50.0,107.0,<textbook><open-source>,,
1303,2,615,74f99d0b-dda0-429a-bd02-00f5e405b3d2,2010-07-25 14:56:32.0,88.0,Thanks to the chaos in definitions of both they are effectively a synonyms. Don't believe words and look deep into the docks to find the equations.,,
1304,2,616,394784f2-7a5d-4385-b96d-1d31e475c651,2010-07-25 15:25:13.0,5.0,"The [""Statistics""][1] book on wikibooks.\\n\\n\\n  [1]: http://en.wikibooks.org/wiki/Statistics",,
1305,5,616,b7d37899-256f-43f0-9f9a-3a557f5e7e9e,2010-07-25 15:32:14.0,5.0,"The [""Statistics""][1] book on wikibooks\\n\\n\\n  [1]: http://en.wikibooks.org/wiki/Statistics\\n",added 121 characters in body; deleted 120 characters in body,
1306,2,617,f1b2b7e2-ca69-4613-90c1-2df5857646fb,2010-07-25 15:32:59.0,5.0,[Multivariate statistics with R][2]\\n\\n\\n  [2]: http://www.opentextbook.org/2009/04/03/multivariate-statistics-with-r/,,
1307,2,618,4ae6c38e-079b-43cd-b7db-ffec276f360d,2010-07-25 18:02:06.0,54.0,I've always liked this one:\\n\\n![lemons vs deaths][1]\\n\\n\\n  [1]: http://i.imgur.com/dxfWK.jpg,,
1308,5,618,a476e44e-c9ce-4c75-82be-5d3ded2b79bb,2010-07-25 18:10:35.0,54.0,I've always liked this one:\\n\\n![lemons vs deaths][1]\\n\\n\\n  [1]: http://i.imgur.com/dxfWK.jpg\\n\\n\\nsource: http://pubs.acs.org/doi/abs/10.1021/ci700332k,added 59 characters in body,
1309,5,609,6fdaade9-202c-4a20-8d23-9c720c3cab44,2010-07-25 19:34:50.0,223.0,"**As a summary**, my answer is : if you have an explicit expression or can figure out some how what your distance is measuring (what ""differences"" it gives weigth to), then you can say what it is better for. An other complementary way to analyse and compare such test is the minimax theory. \\n\\nAt the end some test will be good for some alternatives and some for other. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. \\n\\n\\n----------\\n **Some details** \\n\\n Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their ""optimal detection boudary"" http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492.\\n\\n\\nLet me go distance by distance:\\n\\n 1. KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.\\n\\n \\n 2. Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  http://en.wikipedia.org/wiki/Wasserstein_metric is ubiquitous and hence hard to compare. \\n   - For the particular case of W2 it has been uses in http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1017938923  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. \\n   - For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a ""balanced test"" not giving importance to tails. \\n\\n\\n\\n\\n----------\\n\\n\\nTo recall and extend the comment I made which complete the answer: \\n\\nI know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be minimax for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. \\n\\n",added 1 characters in body,
1310,2,619,5e7d1322-a953-4672-a4d1-6ad9d3b7be06,2010-07-25 20:08:35.0,240.0,"I have a depressing and not-very-specific anecdote to share here. I spent some time as a co-worker of a statistical MT researcher. If you want to see a really big, complex, model, look no further.\\n\\nHe was putting me through NLP bootcamp for his own amusement. I am, in general, the sort of programmer who lives and dies by the unit test and the debugger. As a young person at Symbolics, I was struck by the aphorism, 'programming is debugging an empty editor buffer.' (Sort of like training a perceptron model.)\\n\\nSo, I asked him, 'how do you test and debug this stuff.' He answer was, ""You get it right the first time. You think it through (in his case, often on paper) very carefully, and you code it very carefully. Because when you get it wrong, the chances of isolating the problem are very slim.""\\n\\n\\n",,
1311,2,620,1ee1f2e9-9dc8-46a0-8fb1-fa12b530c947,2010-07-25 22:04:16.0,253.0,[R programming wiki book][1]\\n\\n\\n  [1]: http://en.wikibooks.org/wiki/R_Programming,,
1312,2,621,020a9498-ac42-4fd8-ba20-329835931241,2010-07-25 23:56:43.0,88.0,"Some googling found http://collegeopentextbooks.org/statisticsprobbooks.html . Still, be aware that most of CC-ed material is share-aliked (so you must also publish your work on CC) or at least attributed (so you must add info that certain part was copied and from whom). The same works with GFDL (both SA & A), it is even worse since in principle you should print it along with the document.",,
1313,5,596,10fc6f26-212b-4e20-9b43-8c4ad04e5a02,2010-07-26 00:32:30.0,240.0,"E-M provides a way to improve the estimation of a generative model with unannotated data. Is there anything out there that works the same way for discriminative models (e.g. perceptrons)?\\n\\nFor example, consider averaged perceptron tagger. It would be handy to be able to throw the entire Gigaword through some process of unsupervised model improvement.\\n\\n**EDIT**:\\n\\nSo, I was pleasantly surprise to note that this site has the ambition of dealing with machine learning, but I'm learning by experiment what vocabulary is generic and what is very domain-specific. Apologies.\\n\\nConsider a sequence classification problem, like part-of-speech tagging or named entity extraction. You can train a generative model (e.g. an HMM). Thats's a probability model, and you can apply E-M. However, the number of states grows prohibitive if you want to look at many features, and so the fashion tends toward things like CRFs (batch) or Perceptron (online).\\n\\nFor example, [this][1] paper talks about unsupervised learning in for a perceptron POS tagger, but the details are that they add the output of several pre-existing taggers to the training set of their model.\\n\\n\\n  [1]: http://docs.google.com/viewer?a=v&q=cache:lFSlhiI9L5cJ:www.aclweb.org/anthology/E/E09/E09-1087.pdf+unsupervised+perceptron+training&hl=en&gl=us&pid=bl&srcid=ADGEESjBE8kls2rWhD_U1fL9lz0Wq_elscJ6orH7KNx0ryr0hh37jnzp53sOA-jzJrSI0gea3AwN4lLkq7jsNFbAUcooyDgyYRO2BSqZdpBH4x84KlzLlsccaMcQNbk-1zLvFDAIZVy8&sig=AHIEtbTx9N6TyrKV_7dza3xG9G_dhNxfVg",added 1150 characters in body,
1314,5,608,8af4de57-fe4f-45d9-9b94-452177aba96f,2010-07-26 06:17:03.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n    set.seed(386) #Set a seed so the results are consistent (I hope)\\n    a <- rnorm(60) #predictor\\n    b <- rnorm(60) #predictor\\n    c <- rnorm(60) #predictor\\n    y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    data <- data.frame(y,a,b,c) \\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other",Added sample dataset,
1315,2,622,a245e1e9-96b8-4e4c-84fb-0c3723f252f2,2010-07-26 09:12:21.0,159.0,I see these terms being used and I keep getting them mixed up. Is there a simple explanation of the differences between them?,,
1316,1,622,a245e1e9-96b8-4e4c-84fb-0c3723f252f2,2010-07-26 09:12:21.0,159.0,"What is the difference between a partial likelihood, profile likelihood and marginal likelihood?",,
1317,3,622,a245e1e9-96b8-4e4c-84fb-0c3723f252f2,2010-07-26 09:12:21.0,159.0,<estimation><maximum-likelihood>,,
1318,2,623,086bc7b9-6a80-4c6e-99ea-91881649c2c0,2010-07-26 09:46:48.0,87.0,"Garbage in, garbage out....\\n\\nImplicit in getting the full benefit of linear regression is that the noise follows a normal distribution.  Ideally you have mostly data and a little noise.... not mostly noise and a little data.  You can test for normality of residuals after the linear fit by looking at the residuals.  You can also filter input data before the linear fit for obvious, glaring errors. \\n\\nHere are some types of noise in garbage input data that do not typically fit a normal distribution:\\n\\n - Digits missing or added with hand-entered data (off by a factor of 10 or more)\\n - Wrong or incorrectly converted units (grams vs kilos vs pounds; meters, feet, miles, km), possibly from merging multiple data sets (Note: The Mars Orbiter was thought to be lost in this way, so even NASA rocket scientists can make this mistake)\\n - Use of codes like 0, -1, -99999 or 99999 to mean something non-numeric like ""not applicable"" or ""column unavailable"" and just dumping this into a linear model along with valid data\\n\\nWriting a spec for what is ""valid data"" for each column can help you tag invalid data. For instance, a person's height in cm should be in a range, say, 100-300cm.  If you find 1.8 for height thats a typo, and while you can assume it was 1.8m and alter it to 180 -- I'd say it is usually safer to throw it out and best to document as much of the filtering as possible.  \\n\\n",,
1319,6,421,51620419-30bc-4850-8129-9357dfc28496,2010-07-26 09:48:59.0,190.0,<textbook><science>,edited tags,
1320,6,220,2e3dc9ae-09e1-47e6-bc0c-7eb4f5b4dece,2010-07-26 09:51:11.0,190.0,<distributions><random-variable><minimum>,edited tags,
1321,9,421,6b52e104-aad0-4e04-bad5-8a5c25a64489,2010-07-26 10:35:37.0,219.0,<books><science>,Rollback to [cafcf180-f874-4b98-a9a4-8167bc6c8e35],
1322,6,223,f4baa8a6-0252-46d7-8725-c54414b00c3d,2010-07-26 11:21:33.0,190.0,<online><introductory><textbooks>,edited tags,
1323,6,125,5d40f388-65ba-426a-b6c2-0ef8ea91fc68,2010-07-26 11:22:08.0,190.0,<bayesian><best-of><textbooks>,edited tags,
1324,6,414,effc71af-ca70-4338-bce8-b622a56f963b,2010-07-26 11:23:21.0,190.0,<textbooks>,edited tags,
1325,6,170,bcc55f6b-8606-4ba0-a470-f59fe4b3205a,2010-07-26 11:23:38.0,190.0,<teaching><books>,edited tags,
1326,6,614,cb674f69-50e1-4954-8a17-c321f4d24afb,2010-07-26 11:23:59.0,190.0,<open-source><books>,edited tags,
1327,6,375,bd50aca6-57c3-4fac-a3bb-d36552586312,2010-07-26 11:26:49.0,190.0,<random-generation><proof><randomness>,edited tags,
1328,6,573,0adfc741-cac4-4632-a8ec-d30798628dc3,2010-07-26 11:34:02.0,190.0,<intuition>,edited tags,
1329,2,624,a5741691-ed9f-435d-8c3b-ad698d9d6f5c,2010-07-26 11:53:11.0,59.0,"In engineering, we usually have Handbooks that pretty much dictate the state of the practice. These books are usually devoid of theory and focus on the applied methodology. Is there a forecasting Handbook out there? that solely focuses on the technique and not the background?",,
1330,1,624,a5741691-ed9f-435d-8c3b-ad698d9d6f5c,2010-07-26 11:53:11.0,59.0,forecasting handbooks,,
1331,3,624,a5741691-ed9f-435d-8c3b-ad698d9d6f5c,2010-07-26 11:53:11.0,59.0,<forecasting>,,
1332,2,625,83223950-0f4a-40b6-a720-6cb7f362e727,2010-07-26 12:01:58.0,159.0,"There are two that I know of:\\n\\n 1. [Handbook of economic forecasting][1]. Relatively theoretical. Not for undergraduates. A narrow look at forecasting --- specifically about economic forecasting.\\n 2. [Principles of forecasting][2]. Simpler, broader. Widely used by forecasting practitioners. Often reflects the idiosyncratic opinions of the editor which are presented as established facts.\\n\\n  [1]: http://www.amazon.com/dp/0444513957\\n  [2]: http://www.amazon.com/dp/0792379306/",,
1333,5,625,69485432-f430-4b65-8fd8-c5453044eb61,2010-07-26 12:20:12.0,159.0,"There are two that I know of:\\n\\n 1. [Handbook of economic forecasting][1]. Relatively theoretical. Not for undergraduates. A narrow look at forecasting --- specifically about economic forecasting.\\n 2. [Principles of forecasting][2]. Simpler, broader. Widely used by forecasting practitioners. Often reflects the idiosyncratic opinions of the editor which are presented as established facts.\\n\\nAlternatively, you could use an intro textbook. [My own textbook][3] is often used as a sort of handbook by forecasting practitioners working in a business environment.\\n\\n\\n  [1]: http://www.amazon.com/dp/0444513957\\n  [2]: http://www.amazon.com/dp/0792379306/\\n  [3]: http://robjhyndman.com/forecasting",added 217 characters in body,
1334,2,626,ba7a16fa-b9ca-4b9a-8a7d-102adb033909,2010-07-26 12:38:40.0,30.0,"""Bayesian Core: A Practical Approach to Computational Bayesian Statistics"" by Marin and Robert, *Springer-Verlag* (2007)",,
1335,16,626,ba7a16fa-b9ca-4b9a-8a7d-102adb033909,2010-07-26 12:38:40.0,-1.0,,,
1336,2,627,1408faa0-a9fc-4aec-999e-cdaa2e9fbfb8,2010-07-26 12:53:22.0,236.0,I really like  [The Little Handbook of Statistical Practice][1] by Gerard E. Dallal\\n\\n\\n  [1]: http://www.tufts.edu/~gdallal/LHSP.HTM,,
1337,16,627,1408faa0-a9fc-4aec-999e-cdaa2e9fbfb8,2010-07-26 12:53:22.0,-1.0,,,
1338,6,305,2c35bafd-67a8-410b-94cb-83c62b1585c7,2010-07-26 13:05:38.0,159.0,<variance><t-test><homogeneity>,edited tags,
1339,6,25,8f5c609d-2f4f-4327-bfbb-50d4027a9347,2010-07-26 13:07:00.0,159.0,<time-series><modeling><finance><software-rec>,edited tags,
1340,6,25,b7bdca0a-6c5c-40bc-8126-745eeba6094a,2010-07-26 13:07:28.0,159.0,<time-series><modeling><finance><software>,edited tags,
1341,6,581,a659d1a7-35a6-4f91-acfb-222e45f5150d,2010-07-26 13:48:06.0,88.0,<machine-learning><image-processing>,edited tags,
1342,6,486,09e6bf51-c7ed-4139-ba7a-d45b97bd5e3a,2010-07-26 13:48:54.0,88.0,<statistical-analysis><aic><glm><model-selection>,edited tags,
1343,5,131,ab5b859a-9f7e-4927-b97d-37313a1fc0b1,2010-07-26 14:06:09.0,5.0,"Let me start by saying that I love both languages: you can't go wrong with either, and they are certainly better than something like C++ or Java for doing data analysis.\\n\\nFor basic data analysis I would suggest R (especially with plyr).  IMO, R is a little easier to learn than Clojure, although this isn't completely obvious since Clojure is based on Lisp and there are numerous fantastic Lisp resources available (such as [SICP][1]).  There are less keywords in Clojure, but the libraries are much more difficult to install and work with.  Also, keep in mind that R (or S) is largely derived from Scheme, so you would benefit from Lisp knowledge when using it.\\n\\nIn general:\\n\\nThe main advantage of R is the community on CRAN (over 2461  packages and counting).  Nothing will compare with this in the near future, not even a commercial application like matlab.\\n\\nClojure has the big advantage of running on the JVM which means that it can use any Java based library immediately.\\n\\nI would add that I gave [a talk relating Clojure/Incanter to R][2] a while ago, so you may find it of interest.  In my experience around creating this, Clojure was generally slower than R for simple operations.\\n\\n\\n  [1]: http://mitpress.mit.edu/sicp/\\n  [2]: http://bit.ly/dhDZkp",added 243 characters in body,
1344,6,414,089adf76-0dd0-4751-8436-b3e3aab74b0e,2010-07-26 14:13:48.0,5.0,<textbooks><introductory>,edited tags,
1345,6,125,400fa447-8ac8-4807-ba29-c777977e9cdb,2010-07-26 14:14:39.0,5.0,<bayesian><textbooks><best-of><introductory>,edited tags,
1346,2,628,1225f0d2-c3a9-42dc-9490-1f2ef8c37351,2010-07-26 14:40:27.0,,"The likelihood function usually depends on many parameters. Depending on the application, we are usually interested in only a subset of these parameters. For example, in linear regression, interest typically lies in the slope coefficients and not on the error variance. \\n\\nDenote the parameters we are interested in as &beta; and the parameters that are not of primary interest as &theta;. The standard way to approach the estimation problem is to maximize the likelihood function so that we obtain estimates of &beta; and &theta;. However, since the primary interest lies in &beta; partial, profile and marginal likelihood offer alternative ways to estimate &beta; without estimating &theta;\\n\\nIn order to see the difference denote the standard likelihood by L(&beta;, &theta;|data). \\n\\n**Maximum Likelihood**\\n\\nFind &beta; and &theta; that maximizes L(&beta;, &theta;|data).\\n\\n**Partial Likelihood**\\n\\nIf we can write the likelihood function as:\\n\\nL(&beta;, &theta;|data) = L1(&beta;|data) L2(&theta;|theta)\\n\\nThen we simply maximize L1(&beta;|data).\\n\\n**Profile Likelihood**\\n\\nIf we can express &theta; as a function of &beta; then we replace &theta; with the corresponding function. \\n\\nSay, &theta; = g(&beta;). Then, we maximize:\\n\\nL(&beta;, g(&beta;)|data)\\n\\n**Marginal Likelihood**\\n\\nWe integrate out &theta; from the likelihood equation by exploiting the fact that we can identify the probability distribution of &theta; conditional on &beta;.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Sufficient_statistic",,user28
1352,16,624,0dc955d6-05df-40ba-96ee-aae850252de9,2010-07-26 15:00:23.0,59.0,,,
1353,6,223,090f6217-4c89-4a2f-9c89-a1c737c9b85c,2010-07-26 15:11:22.0,71.0,<textbooks><introductory><online>,edited tags,
1354,16,614,78ed5f38-ce57-4e3d-a1b3-32067d985acb,2010-07-26 15:31:39.0,107.0,,,
1355,6,614,78ed5f38-ce57-4e3d-a1b3-32067d985acb,2010-07-26 15:31:39.0,107.0,<books><open-source>,edited tags,
1356,16,617,426ea7ea-d23e-484f-9040-c151134cf9b9,2010-07-26 15:37:10.0,5.0,,,
1357,16,616,310a6eb1-6835-459e-b305-932dd45f2907,2010-07-26 15:37:25.0,5.0,,,
1358,16,621,7d37bc6c-75f2-48aa-8542-1092f5868f77,2010-07-26 15:48:09.0,88.0,,,
1359,16,620,496f99ef-5a7b-496f-a2b6-2862efc29651,2010-07-26 15:52:05.0,253.0,,,
1364,2,631,1bfbbe28-e10e-4c43-b797-1c5f2a056771,2010-07-26 16:10:45.0,88.0,What is an estimator of standard deviation of standard deviation if normality of data can be assumed?,,
1365,1,631,1bfbbe28-e10e-4c43-b797-1c5f2a056771,2010-07-26 16:10:45.0,88.0,Standard deviation of standard deviation,,
1366,3,631,1bfbbe28-e10e-4c43-b797-1c5f2a056771,2010-07-26 16:10:45.0,88.0,<standard-deviation>,,
1368,2,632,bb3f46c7-cf74-409f-a8d6-767f51338b25,2010-07-26 16:23:25.0,223.0,"Assume you observe $X1,..,Xn$ iid from a normal with mean zero and variance $\\sigma^2$. The (empirical) standard deviation is the square root of the estimation $\\hat{sigma}^2$ of $\\sigma^2$ (unbiased or not that is not the question). As an estimator (obtained with $X1,..,Xn$), $\\hat{sigma}$ has a variance that can be calculated theoretically. Maybe what you call the standard deviation of standard deviation is actually the square root of the variance of the  standard deviation, i.e. $\\sqrt{E[(\\sigma-\\hat{sigma})^2]}$?  It is not an estimator, it is a theoretical quantity (something like $\\sigma/\\sqrt{n}$ to be confirmed) that can be calculated explicitely !",,
1369,6,222,56a6acba-0a0e-4c77-b650-a9f061594149,2010-07-26 16:51:56.0,88.0,<fundamentals><pca><scores>,edited tags,
1370,6,480,976771cc-cde3-4014-9714-027121345301,2010-07-26 16:57:59.0,217.0,<machine-learning><r><algorithms><cart><random-forest>,edited tags,
1371,2,633,16d61e88-24bc-4bad-ba97-4ef5170e6fe1,2010-07-26 17:29:19.0,59.0,"I have a data set of about 3,000 field observations. \\n\\nThe data collected is divided into 20 variables (real numbers), 30 boolean variables, and 10 or so look up variables and one ""answer"" variable\\n\\nWe have about 20,000 objects in the field, and i'm trying to produce an ""answer"" for the 20,000 objects based on the 3,000 observations.\\n\\nWhat are some of the available methods that incorporate booleans and look up tables?\\n\\nany suggestions on how i should proceed?",,
1372,1,633,16d61e88-24bc-4bad-ba97-4ef5170e6fe1,2010-07-26 17:29:19.0,59.0,incorporating boolean data into analysis,,
1373,3,633,16d61e88-24bc-4bad-ba97-4ef5170e6fe1,2010-07-26 17:29:19.0,59.0,<statistical-analysis>,,
1374,2,634,4e875aa3-40b4-491f-944f-e1351a60ed6d,2010-07-26 17:46:45.0,5.0,"You are decribing ""categorical variables"" (represented in R a factors). These can be incorporated into almost any statistical model by being assigned levels.  You would need to give more detail about your particular problem in order to be advised on a particular method.  \\n\\nIncidentally boolean is often referred to as binomial or logical in statistics.\\n\\n*Edit*\\n\\nif the response variable has two possible outcomes, you might consider a binomial regression. http://en.m.wikipedia.org/wiki/Binomial_regression",,
1375,5,633,56ee00ee-84be-4969-ae5a-9c237f13fe19,2010-07-26 17:47:51.0,59.0,"I have a data set of about 3,000 field observations. \\n\\nThe data collected is divided into 20 variables (real numbers), 30 boolean variables, and 10 or so look up variables and one ""answer"" variable\\n\\nWe have about 20,000 objects in the field, and i'm trying to produce an ""answer"" for the 20,000 objects based on the 3,000 observations.\\n\\nWhat are some of the available methods that incorporate booleans and look up tables?\\n\\nany suggestions on how i should proceed?\\n\\n**EDIT** \\nthe answer variable is a boolean as well",added 55 characters in body,
1376,2,635,f36a232b-ec02-4457-98fe-30ce875bdfca,2010-07-26 17:50:21.0,30.0,"Robin,\\n\\nimplementations of RF differ slightly. I know that Salford Systems' [proprietary implementation][1] is supposed to be better than the [vanilla one][2] in R. A description of the algorithm is in [ESL by Friedman-Hastie-Tibshirani, 2nd ed, 3rd printing][3]. An entire chapter (15th) is devoted to RF, and I find it actually clearer than the original paper. The tree construction algorithm is detailed on p.588; no need for me to reproduce it here, since the book is available online. I hope this helps.\\n\\n\\n  [1]: http://salford-systems.com/products/randomforests/overview.html\\n  [2]: http://cran.r-project.org/web/packages/randomForest/\\n  [3]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/download.html",,
1377,2,636,17948391-df33-4b3e-a5a6-9371a0b6ee81,2010-07-26 17:50:37.0,88.0,Try Random Forest.,,
1378,6,612,3814cace-ea99-4392-b010-91ead437885e,2010-07-26 17:52:30.0,190.0,<r><pca><spss><factor-analysis>,edited tags,
1379,5,633,f977d153-bf71-4534-9ad2-8d980b788275,2010-07-26 17:53:06.0,59.0,"I have a data set of about 3,000 field observations. \\n\\nThe data collected is divided into 20 variables (real numbers), 30 boolean variables, and 10 or so look up variables and one ""answer"" variable\\n\\nWe have about 20,000 objects in the field, and i'm trying to produce an ""answer"" for the 20,000 objects based on the 3,000 observations.\\n\\nWhat are some of the available methods that incorporate booleans and look up tables?\\n\\nany suggestions on how i should proceed?\\n\\n**EDIT**\\n\\nthe answer variable is a boolean as well\\n\\n\\n**EDIT 2**\\n\\na sample of the variable data:\\n\\n - Age of specimen\\n - length, area, volume\\n - time since last inspection\\n - height\\n - design life\\n\\nLookup table\\n\\n - material type \\n - coating type\\n - design standard\\n - design effectiveness\\n\\n\\na sample of the boolean\\n\\n - is it inspected?\\n - is it in bad shape\\n - does it need repairs soon\\n\\nthe answer variable which is my f(x) is:\\n\\n - is it useable\\n",added 402 characters in body; added 24 characters in body,
1380,5,608,53228e72-fa66-4b24-9a78-042769f5254d,2010-07-26 17:53:41.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n    set.seed(76) #Set a seed so the results are consistent (I hope)\\n    a <- rnorm(60) #predictor\\n    b <- rnorm(60) #predictor\\n    c <- rnorm(60) #predictor\\n    y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    data <- data.frame(y,a,b,c) \\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other",changed the seed so the example is useful,
1381,6,633,39df0988-f3c6-4fdc-a39a-f0b935693613,2010-07-26 18:04:49.0,71.0,<modeling><categorical-data><model-selection><binary-data>,edited tags,
1382,5,634,c1d834ae-1491-480b-b1ee-07a97b02b05e,2010-07-26 18:11:19.0,5.0,"You are decribing ""[**categorical** variables][1]"" (represented in R a factors). These can be incorporated into almost any statistical model by being assigned levels.  You would need to give more detail about your particular problem in order to be advised on a particular method.  \\n\\n*Edit*\\n\\nIf the response variable has two possible outcomes, you might consider [a binomial regression][2]. \\n\\nNote: If you're not familiar with the different kinds of variables in statistics, I suggest reading the first few chapters of Andrew Gelman's ""[Data Analysis Using Regression and Multilevel/Hierarchical Models][3]"" which covers this in a very understandable manner.\\n\\n\\n  [1]: http://www.oswego.edu/~srp/stats/variable_types.htm\\n  [2]: http://en.m.wikipedia.org/wiki/Binomial_regression\\n  [3]: http://www.stat.columbia.edu/~gelman/arm/",added 4 characters in body; added 319 characters in body,
1383,5,636,c758613b-06f1-4751-98e4-9c571c851f6a,2010-07-26 18:21:26.0,88.0,"Try Random Forest; from my experience it may perform well on such kind of data, and gives you a some additional interesting information, like variable importance and object similarity measure. ",added 175 characters in body,
1384,2,637,6d118029-c3d2-428a-9c5d-b10b34b947a0,2010-07-26 18:22:33.0,74.0,"It sounds like you are trying to predict your boolean response, yes?\\n\\nThis is called classification.\\n\\nLogistic Regression is the obvious choice here, but there are other methods too. You can't do traditional regression, because the response is not a real number.\\n\\nThe lookup variables are called nominals, and can be dealt with in regression by using ""dummy"" variables.\\n\\nFor example, if your lookup variable is type=[steel, aluminum, plastic] (N=3), then your dummy variables would look like this:\\n\\nIsSteel = [1,0]\\nIsAlum = [1,0]\\n\\nThere would only be two (N-1) dummy variables, as IsSteel=0 AND IsAlum=0 represents ""IsPlastic""=1\\n\\nBut any good stats program should handle this.\\n\\nIf you need a book, I recommend Multivariate Data Analysis by Hair.",,
1385,2,638,a67bd103-c6b0-48c0-90c5-7579fd0f0402,2010-07-26 18:24:35.0,59.0,"Provided a sample size S that I plan on using to forecast data. What are some of the ways to subdivide the data so that I use some of it to establish a model, and the remainder data to validate the model?\\n\\nI know there is no black and white answer to this but it would be interesting to know  some ""rules of thumb"" or usually used ratios. I know back at university, one of our professors used to say model on 60% and validate on 40%.\\n\\n",,
1386,1,638,a67bd103-c6b0-48c0-90c5-7579fd0f0402,2010-07-26 18:24:35.0,59.0,"How to calculate the ratio of ""data used in analysis"" and ""saved data for validation"" from a sample set?",,
1387,3,638,a67bd103-c6b0-48c0-90c5-7579fd0f0402,2010-07-26 18:24:35.0,59.0,<statistical-analysis><forecasting>,,
1390,5,634,fc54c7f3-4c4c-4a90-99c3-43981c33de60,2010-07-26 18:25:45.0,5.0,"You are decribing ""[**categorical** variables][1]"" (represented in R a factors). These can be incorporated into almost any statistical model by being assigned levels.  You would need to give more detail about your particular problem in order to be advised on a particular method.  \\n\\n*Edit*\\n\\nIf the response variable has two possible outcomes, you might consider [binomial][2] or [logistic][3] regression. \\n\\nNote: If you're not familiar with the different kinds of variables in statistics, I suggest reading the first few chapters of Andrew Gelman's ""[Data Analysis Using Regression and Multilevel/Hierarchical Models][4]"" which covers this in a very understandable manner.\\n\\n\\n  [1]: http://www.oswego.edu/~srp/stats/variable_types.htm\\n  [2]: http://en.m.wikipedia.org/wiki/Binomial_regression\\n  [3]: http://en.wikipedia.org/wiki/Logistic_regression\\n  [4]: http://www.stat.columbia.edu/~gelman/arm/",added 72 characters in body,
1392,5,162,db5016d2-b86b-46ac-8f5d-87c1308f36ca,2010-07-26 18:29:44.0,74.0," 1. If you carved your distribution out\\n    of wood, and tried to balance it on\\n    your finger, the balance point would\\n    be the mean.\\n\\n 2. If you put a stick in the middle of\\n    your scatter plot, and attached the\\n    stick to each data point with a\\n    spring, the resting point of the\\n    stick would be your regression line.\\n\\n",deleted 86 characters in body,
1393,2,640,0254bf04-be5d-43b5-bbbc-79cc0fd16e0b,2010-07-26 18:40:54.0,,I suppose that you are looking for the [distribution of the sample variance][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance#Distribution_of_the_sample_variance,,user28
1394,6,328,6d3d6310-d617-4e21-bb51-99f38eaf5ac6,2010-07-26 18:48:37.0,190.0,<analysis><finance><beginner>,edited tags,
1395,6,138,8ecc9190-ece3-4dc6-bff9-3b25f08d0277,2010-07-26 18:49:10.0,190.0,<r><beginner>,edited tags,
1396,6,31,30cd18e8-4c71-4219-acec-957eafb47b89,2010-07-26 18:50:36.0,190.0,<hypothesis-testing><beginner><p-value><t-value>,edited tags,
1397,6,222,ec67aaa1-0e6d-4b28-ac23-2453fee5beda,2010-07-26 18:50:54.0,190.0,<pca><beginner><scores>,edited tags,
1398,6,26,96305781-941c-4cd5-843f-2893dbedb1ce,2010-07-26 18:51:29.0,190.0,<standard-deviation><beginner><statistics>,edited tags,
1399,6,269,ea4257ac-c350-47b6-93fd-11d36cc604fd,2010-07-26 18:53:09.0,190.0,<standard-deviation><beginner><variance><population><sample>,edited tags,
1400,6,50,7df50fbc-df07-4cd0-83b6-501e0136b1d6,2010-07-26 18:53:44.0,190.0,<beginner><random-variable>,edited tags,
1401,6,2,d17fa84b-64e7-49a9-9b72-5d8ed832aa24,2010-07-26 18:55:26.0,190.0,<distributions><beginner><normality>,edited tags,
1402,5,608,3b7da46a-fc4a-4364-b228-8b16e4d6e946,2010-07-26 18:59:19.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n    set.seed(76) #Set a seed so the results are consistent (I hope)\\n    a <- rnorm(60) #predictor\\n    b <- rnorm(60) #predictor\\n    c <- rnorm(60) #predictor\\n    y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    data <- data.frame(y,a,b,c) \\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\nPer earlier comments, below I have provided a list of seeds from 1 to 10000 in which AIC and BIC disagree.  This was done by a simple search through the available seeds, but if someone could provide a way to generate data which would tend to produce divergent answers from these two information criteria it may be particularly informative.\\n\\n    notable.seeds <- read.csv(""http://student.ucr.edu/~rpier001/res.csv"")$seed\\n\\nAs an aside, I thought about ordering these seeds by the extent to which the AIC and BIC disagree which I've tried quantifying as the sum of the absolute differences of the AIC and BIC.  For example, \\n\\n    AICDiff <- AIC(bad.model) - AIC(good.model) \\n    BICDiff <- BIC(logLik(bad.model)) - BIC(logLik(good.model))\\n    disagreement <- sum(abs(c(AICDiff,BICDiff)))\\n\\nwhere my disagreement metric only reasonably applies when the observations are notable.  For example,\\n\\n    are.diff <- sum(sign(c(AICDiff,BICDiff)))\\n    notable <- ifelse(are.diff == 0 & AICDiff != 0,TRUE,FALSE)\\n\\nHowever in cases where AIC and BIC disagreed, the calculated disagreement value was always the same (and is a function of sample size).  Looking back at how AIC and BIC are calculated I can see why this might be the case computationally, but I'm not sure why it would be the case conceptually.  If someone could elucidate that issue as well, I'd appreciate it.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other",added 1378 characters in body,
1403,2,641,b6fc33d4-86b9-4876-9a54-9c054d11341b,2010-07-26 19:20:53.0,74.0,"Sites like eMarketer offer general survey results about internet usage.\\n\\nWho else has a big set of survey results, or regularly releases them?\\n\\nThanks!\\n\\n",,
1404,1,641,b6fc33d4-86b9-4876-9a54-9c054d11341b,2010-07-26 19:20:53.0,74.0,Where is a good place to find survey results?,,
1405,3,641,b6fc33d4-86b9-4876-9a54-9c054d11341b,2010-07-26 19:20:53.0,74.0,<data><surveys>,,
1406,2,642,37aac51d-8de9-4423-9a56-ecd291f69076,2010-07-26 19:23:19.0,59.0,government websites usually .... I use the [RITA][1] a lot\\n\\n\\n  [1]: http://www.rita.dot.gov/,,
1407,2,643,4f6408f6-e488-4520-9937-33ba49c4be19,2010-07-26 19:26:37.0,7.0,"My father is a math enthusiast, but not interested in statistics much. It would be neat to *try* to illustrate some of the wonderful bits of statistics, and the CLT is a prime candidate. How would you convey the mathematical beauty and impact of the central limit theorem to a non-statistician? ",,
1408,1,643,4f6408f6-e488-4520-9937-33ba49c4be19,2010-07-26 19:26:37.0,7.0,How do you convey the beauty of the Central Limit Theorem to a non-statistician?,,
1409,3,643,4f6408f6-e488-4520-9937-33ba49c4be19,2010-07-26 19:26:37.0,7.0,<theory>,,
1410,2,644,d121530f-16a9-48c0-8697-2de1261ae728,2010-07-26 19:27:49.0,288.0,Well as you said there is no black and white answer. I generally don't divide the data in 2 parts but use methods like k-fold cross validation instead. \\n\\nIn k-fold cross validation you divide your data randomly into k parts and fit your model on k-1 parts and test the errors on the left out part. You repeat the process k times leaving each part out of fitting one by one. You can take the mean error from each of the k iterations as an indication of the model error. This works really well if you want to compare the predictive power of different models. \\n\\nOne extreme form of k-fold cross validation is the generalized cross validation where you just leave out one data point for testing and fit the model to all the remaining points. Then repeat the process n times leaving out each data point one by one. I generally prefer k-fold cross validation over the generalized cross validation ... just a personal choice,,
1411,2,645,402e3460-ab0f-4846-93b4-b34fc1764553,2010-07-26 19:28:53.0,9426.0,Having just recently started teaching myself Machine Learning and Data Analysis I'm finding myself hitting a brick wall on the need for creating and querying large sets of data. I would like to take data I've been aggregating in my professional and personal life and analyze it but I'm uncertain of the best way to do the following:\\n\\n1) How should I be storing this data? Excel? SQL? ??\\n\\n2) What is a good way for a beginner to begin trying to analyze this data? I am a professional computer programmer so the complexity is not in writing programs but more or less specific to the domain of data analysis. ,,
1412,1,645,402e3460-ab0f-4846-93b4-b34fc1764553,2010-07-26 19:28:53.0,9426.0,Best Way to Aggregate and Analyze Data,,
1413,3,645,402e3460-ab0f-4846-93b4-b34fc1764553,2010-07-26 19:28:53.0,9426.0,<analysis><data>,,
1414,2,646,60e5a630-ffde-417c-9498-71138c681641,2010-07-26 19:31:36.0,302.0,"The best place to find survey data related to the social sciences is the ICPSR data clearinghouse: http://www.icpsr.umich.edu/icpsrweb/ICPSR/access/index.jsp\\n\\nAlso, the 'survey' tag on Infochimps has many interesting and free data sets: http://infochimps.org/tags/survey",,
1415,2,647,70b5649b-2179-4a28-8a7f-1ab733a9dbf7,2010-07-26 19:34:07.0,25.0,"I think both the concept and the terminology of ""SD of SD"" is too slippery to tackle. But it is easier to think about the confidence interval of a SD. You compute the SD from a sample of data and want to compute a confidence interval that is 95% (or some other confidence level) likely to contain the true SD of the population from which the data were sampled.\\n\\n[Detailed explanation][1]\\n\\n[Free web calculator][2]\\n\\n\\n  [1]: http://www.graphpad.com/faq/viewfaq.cfm?faq=1381 ""Here""\\n  [2]: http://www.graphpad.com/quickcalcs/CISD1.cfm ",,
1416,2,648,b2fb14a3-bf7a-4ca4-a33f-4954f6b10985,2010-07-26 19:34:09.0,68.0,"To fully appreciate the CLT, it should be seen.\\n\\nHence the notion of the [bean machine][1] and plenty of youtube [videos][2] for illustration.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Galton_board\\n  [2]: http://www.youtube.com/results?search_query=galton+board&aq=0",,
1418,2,650,5b76edf8-f932-4edb-b96b-2e15af139c80,2010-07-26 19:34:52.0,190.0,"Your question is so broad that the answer is: it depends. Still, to give some more useful answer I'll indicate what I think are common in Research.\\n\\nStoring of data is very often done in text files. When doing statistical analyses you mostly work with a collection of one type of vectors. This can be seen as a table and written in csv format. The reason thins are often stored in plain-text, is because simply every tool can read them and it is easy to transform them.\\n\\nAbout analyzing, this is a bit harder to be specific. If it is 2 dimensional, make a scatterplot. If it is high-dimensional, do PCA and see where the first principal components exist of to discover important variables. If you have time data, plot it. This is all so general that to be useful you have to really indicate better what your data is.\\n",,
1419,6,645,8a5b23e7-2dc2-4c7a-9481-f1cf955f404c,2010-07-26 19:36:20.0,190.0,<beginner><analysis><data>,edited tags,
1420,2,651,bb3c4e45-fa56-4243-86c9-b131c192af46,2010-07-26 19:39:05.0,25.0,"My book, [*Intuitive Biostatistics*][1], is written partly from a medical point of view. It focusses on the practical parts of interpreting statistical results, with almost no math.\\n\\n\\n  [1]: http://www.intuitivebiostatistics.com",,
1421,2,652,080f85c7-27bc-4519-8688-4e82999b6aeb,2010-07-26 19:39:49.0,9426.0,"I bought this book:\\n\\nHow to Measure Anything: Finding the Value of Intangibles in Business\\n\\nhttp://rcm.amazon.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=justibozon-20&o=1&p=8&l=as1&m=amazon&f=ifr&md=10FE9736YVPPT7A0FBG2&asins=0470539399\\n\\nand\\n\\nHead First Data Analysis: A Learner's Guide to Big Numbers, Statistics, and Good Decisions\\n\\nhttp://rcm.amazon.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=justibozon-20&o=1&p=8&l=as1&m=amazon&f=ifr&md=10FE9736YVPPT7A0FBG2&asins=0596153937\\n\\nWhat other books would you recommend?",,
1422,1,652,080f85c7-27bc-4519-8688-4e82999b6aeb,2010-07-26 19:39:49.0,9426.0,Best Books for an Introduction to Statistical Data Analysis?,,
1423,3,652,080f85c7-27bc-4519-8688-4e82999b6aeb,2010-07-26 19:39:49.0,9426.0,<machine-learning><bayesian><books>,,
1424,16,641,2b5c7efb-05a2-40f7-8080-4bd32f9092db,2010-07-26 19:41:11.0,74.0,,,
1425,5,641,2b5c7efb-05a2-40f7-8080-4bd32f9092db,2010-07-26 19:41:11.0,74.0,"Sites like eMarketer offer general survey results about internet usage.\\n\\nWho else has a big set of survey results, or regularly releases them?\\n\\n* Preferably marketing research focused.\\n\\nThanks!\\n\\n",added 44 characters in body,
1426,2,653,acc10f23-bd5b-4ad8-91fe-7011e031c890,2010-07-26 19:42:50.0,190.0,"It really depends on the amount of data you have, the specific cost of methods and how exactly you want your result to be.\\n\\nSome examples:\\n\\nIf you have little data, you probably want to use cross-validation (k-fold, leave-one-out, etc.) Your model will probably not take much resources to train and test anyway. It are good ways to get the most out of your data\\n\\nYou have a lot of data: you probably want to take a reasonably large test-set, ensuring that there will be little possibility that some strange samples will give to much variance to your results. How much data you should take? It depends completely on your data and model. In speech recognition for example, if you would take too much data (let's say 3000 sentences), your experiments would take days, as a realtime factor of 7-10 is common. If you would take too little, it is too much dependent on the speakers that you are choosing (which are not allowed in the training set).\\n\\nRemember also, in a lot of cases it is good to have a validation/development set too!",,
1427,2,654,2a0f6f85-e089-4dd8-b445-0d898eabf8c4,2010-07-26 19:44:44.0,88.0,"1:10 test:train ratio is popular because it looks round, 1:9 is popular because of 10-fold CV, 1:2 is popular because it is also round and reassembles bootstrap. Sometimes one gets a test from some data-specific criteria, for instance last year for testing, years before for training. \\n\\nThe general rule is such: the train must be large enough to so the accuracy won't drop significantly, and the test must be large enough to silence random fluctuations.\\n\\nStill I prefer CV, since it gives you also a distribution of error.",,
1428,5,645,65eaff74-4d3a-401c-95fe-d01a73e2db85,2010-07-26 19:45:28.0,9426.0,"Having just recently started teaching myself Machine Learning and Data Analysis I'm finding myself hitting a brick wall on the need for creating and querying large sets of data. I would like to take data I've been aggregating in my professional and personal life and analyze it but I'm uncertain of the best way to do the following:\\n\\n1) How should I be storing this data? Excel? SQL? ??\\n\\n2) What is a good way for a beginner to begin trying to analyze this data? I am a professional computer programmer so the complexity is not in writing programs but more or less specific to the domain of data analysis. \\n\\nEDIT: Apologies for my vagueness, when you first start learning about something it's hard to know what you don't know, ya know? ;)\\n\\nHaving said that, my aim is to apply this to two main topics:\\n\\n1) Software team metrics (think Agile velocity, quantifying risk, likelihood of a successfully completed iteration given x number of story points)\\n\\n2) Machine learning (ex. system exceptions have occurred in a given set of modules what is the likelihood that a module will throw an exception in the field, how much will that cost, what can the data tell me about key modules to improve that will get me the best bang for my buck, predict what portion of the system the user will want to use next in order to start loading data, etc)",Updated with some examples of how I might use these techniques.; added 2 characters in body,
1429,2,655,f829ec19-3981-4239-a800-afdbe03a59b6,2010-07-26 19:49:53.0,315.0,"You might find useful this one: [The Elements of Statistical Learning: Data Mining, Inference, and Prediction][1]\\n\\n\\n  [1]: http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_1?ie=UTF8&s=books&qid=1280173685&sr=1-1",,
1430,2,656,664a12ea-4835-4496-abba-864b50873d42,2010-07-26 19:51:45.0,25.0,![alt text][1]\\n\\n\\n  [1]: http://bp1.blogger.com/_x7QjiZypFj0/Rp9dcGTsBKI/AAAAAAAAAA0/VWwfWDv6nzM/s400/Outlier.jpg,,
1431,16,656,664a12ea-4835-4496-abba-864b50873d42,2010-07-26 19:51:45.0,-1.0,,,
1432,16,652,3203ff69-8bea-4327-b256-9bb59cedd333,2010-07-26 19:53:10.0,9426.0,,,
1433,5,652,3203ff69-8bea-4327-b256-9bb59cedd333,2010-07-26 19:53:10.0,9426.0,"I bought this book:\\n\\n[How to Measure Anything: Finding the Value of Intangibles in Business][1] \\n\\nand \\n\\n[Head First Data Analysis: A Learner's Guide to Big Numbers, Statistics, and Good Decisions][2]\\n\\nWhat other books would you recommend?\\n\\n\\n  [1]: http://rcm.amazon.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=justibozon-20&o=1&p=8&l=as1&m=amazon&f=ifr&md=10FE9736YVPPT7A0FBG2&asins=0470539399\\n  [2]: http://rcm.amazon.com/e/cm?lt1=_blank&bc1=000000&IS2=1&bg1=FFFFFF&fc1=000000&lc1=0000FF&t=justibozon-20&o=1&p=8&l=as1&m=amazon&f=ifr&md=10FE9736YVPPT7A0FBG2&asins=0596153937",Cleaned up the way the links looked.,
1434,2,657,253bdf99-6c15-49c3-849f-fa123f22a563,2010-07-26 19:53:45.0,315.0,I liked these lectures: [Statistical Aspects of Data Mining][1]. The lecturer is solving example problems using R.\\n\\n\\n  [1]: http://www.youtube.com/results?search_query=statistical+aspects+of+data+mining&aq=0,,
1435,2,658,2f4e0758-d13b-4b57-8446-1a5b7502741c,2010-07-26 19:56:23.0,88.0,What I loved most with CLT is the cases when it fails -- this gives me a hope that the life is a bit more interesting that Gauss curve suggests. So show him the Cauchy distribution.,,
1437,6,652,f4c267c6-65a3-4b68-a6c5-8985e740ed0d,2010-07-26 19:57:33.0,190.0,<machine-learning><bayesian><beginner><books>,edited tags,
1438,2,659,939e9306-9055-4bb7-811f-d6bc52750dc9,2010-07-26 19:58:28.0,317.0,"The principal components of a data matrix are the eigenvector-eigenvalue pairs of its variance-covariance matrix.  In essence, they are the decorrelated pieces of the variance.  Each one is a linear combination of the variables for an observation -- suppose you measure w, x, y,z on each of a bunch of subjects.  Your first PC might work out to be something like\\n\\n0.5w + 4x + 5y - 1.5z\\n\\nThe loadings (eigenvectors) here are (0.5, 4, 5, -1.5).  The score (eigenvalue) for each observation is the resulting value when you substitute in the observed (w, x, y, z) and compute the total.\\n\\nThis comes in handy when you project things onto their principal components (for, say, outlier detection) because you just plot the scores on each like you would any other data.  This can reveal a lot about your data if much of the variance is correlated (== in the first few PCs).\\n",,
1439,12,384,e763c94a-7352-447b-9eb3-8af28db2233a,2010-07-26 20:01:54.0,190.0,"{""Voters"":[{""Id"":190,""DisplayName"":""Peter Smit""}]}",,
1440,13,384,b936c6e4-604c-4bb9-acae-c710df7b9434,2010-07-26 20:01:58.0,190.0,"{""Voters"":[{""Id"":190,""DisplayName"":""Peter Smit""}]}",,
1441,2,660,ec00e7fe-6028-4679-9517-9bdadfc8973b,2010-07-26 20:03:02.0,292.0,"I am collecting textual data surrounding press releases, blog posts, reviews, etc of certain companies' products and performance.\\n\\nSpecifically, I am looking to see if there are correlations between certain *types* and/or *sources* of such ""textual"" content with market valuations of the companies' stock symbols.\\n\\nSuch *apparent* correlations can be found by the human mind fairly quickly - but that is not scalable. How can I go about automating such analysis of disparate sources?",,
1442,1,660,ec00e7fe-6028-4679-9517-9bdadfc8973b,2010-07-26 20:03:02.0,292.0,"Automating statistical correlation between ""texts"" and ""data""",,
1443,3,660,ec00e7fe-6028-4679-9517-9bdadfc8973b,2010-07-26 20:03:02.0,292.0,<finance><correlation><text-mining>,,
1444,2,661,ddc4d6fb-a3b9-4c39-915d-203741b5bb33,2010-07-26 20:06:09.0,319.0,"Sam Savage's book [Flaw of Averages][1] is filled with good layman explanations of statistical concepts.  In particular, he has a good explanation of Jensen's inequality.  If the graph of your return on an investment is convex, i.e. it ""smiles at you"", then randomness is in your favor: your average return is greater than your return at the average.\\n\\n\\n  [1]: http://www.amazon.com/gp/product/0471381977?ie=UTF8&tag=theende-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0471381977",,
1445,2,662,285626dd-61db-4014-a153-6160b3c970dd,2010-07-26 20:08:09.0,319.0,[Introduction to Statistical Thought][1]\\n\\n\\n  [1]: http://www.math.umass.edu/~lavine/Book/book.html,,
1446,16,662,285626dd-61db-4014-a153-6160b3c970dd,2010-07-26 20:08:09.0,-1.0,,,
1447,2,663,1eeac6ba-2130-4c93-9bec-fd873832feed,2010-07-26 20:09:20.0,319.0,[Introduction to Statistical Thought][1]\\n\\n\\n  [1]: http://www.math.umass.edu/~lavine/Book/book.html,,
1448,16,663,1eeac6ba-2130-4c93-9bec-fd873832feed,2010-07-26 20:09:20.0,-1.0,,,
1449,2,664,fc0c08c8-c394-4dbe-8e82-77ec6734f18e,2010-07-26 20:12:15.0,319.0,"It's seldom useful to conclude that something is ""random"" in the abstract.  More often you want to test whether it has a certain kind of random structure.  For example, you might want to test whether something has a uniform distribution, with all values in a certain range equally likely.  Or you might want to test whether something has a normal distribution, etc.  To test whether data has a particular distribution, you can use a goodness of fit test such as the chi square test or the Kolmogorov-Smirnov test.",,
1450,2,665,64006c55-dc37-4f64-897c-70aeb046e0a1,2010-07-26 20:17:17.0,327.0,"What's the difference between probability and statistics, and why are they studied together ?",,
1451,1,665,64006c55-dc37-4f64-897c-70aeb046e0a1,2010-07-26 20:17:17.0,327.0,What's the difference between probability and statistics,,
1452,3,665,64006c55-dc37-4f64-897c-70aeb046e0a1,2010-07-26 20:17:17.0,327.0,<probability>,,
1454,2,667,0b32407f-e7d2-41eb-bb22-2b48caf19e13,2010-07-26 20:18:46.0,88.0,"Probability is a pure science (math), statistics is about data. They are connected since probability forms some kind of fundament for statistics, providing basic ideas.",,
1455,2,668,d5c23095-bea3-44bb-b0d5-419bccf9e755,2010-07-26 20:19:12.0,319.0,"If you already know another programming language, [these notes][1] may help point out some of the ways R might surprise you.\\n\\n\\n  [1]: http://www.johndcook.com/R_language_for_programmers.html",,
1456,6,665,edd9557e-8459-4b9d-9e91-9d9e75e50597,2010-07-26 20:19:19.0,190.0,<beginner><probability>,edited tags,
1457,2,669,b1a851d8-1fd1-4a0e-8141-b78359284cfd,2010-07-26 20:22:30.0,319.0,[The Flaw of Averages][1] by Sam Savage.\\n\\n\\n  [1]: http://www.amazon.com/gp/product/0471381977?ie=UTF8&tag=theende-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0471381977,,
1458,16,669,b1a851d8-1fd1-4a0e-8141-b78359284cfd,2010-07-26 20:22:30.0,-1.0,,,
1460,2,671,73b86f3f-a7f2-4510-9b56-d4cc2c705447,2010-07-26 20:25:16.0,302.0,"If you have experience in other languages, these ""R Rosetta Stone"" videos may be useful:\\n\\n 1. [Python][1]\\n 2. [MATLAB][2]\\n 3. [SQL][3]\\n\\nThese are all included in the [video list added by Jeromy][4], so big +1 for his list.\\n\\n\\n  [1]: http://www.vcasmo.com/video/drewconway/7183\\n  [2]: http://www.vcasmo.com/video/drewconway/7211\\n  [3]: http://www.vcasmo.com/video/drewconway/7210\\n  [4]: http://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html",,
1462,2,672,4191a7d6-ecab-46ed-950f-0aecdb85b8f2,2010-07-26 20:30:36.0,333.0,If someone could explain main ideas i.e. concepts here?\\nNot asking for any derivations of complex mathematical notation.,,
1463,1,672,4191a7d6-ecab-46ed-950f-0aecdb85b8f2,2010-07-26 20:30:36.0,333.0,What is Bayes theorem all about?,,
1464,3,672,4191a7d6-ecab-46ed-950f-0aecdb85b8f2,2010-07-26 20:30:36.0,333.0,<probability>,,
1465,6,672,e2a87c03-7177-4438-ad94-5331b3bfe454,2010-07-26 20:32:51.0,88.0,<beginner><bayesian><probability><theory>,edited tags,
1466,2,673,6b171a14-0041-48c3-8e05-cdec2504c40d,2010-07-26 20:34:45.0,25.0,"Table 3.1 of [*Intuitive Biostatistics*][1] answers this question with the diagram shown below. Note that all the arrows point to the right for probability, and point to the left for statistics.\\n\\nPROBABILITY\\n\\n \\n\\n> General ---> Specific\\n> \\n>  Population ---> Sample\\n> \\n>  Model ---> Data\\n\\n\\nSTATISTICS\\n\\n \\n\\n> General <--- Specific\\n> \\n>  Population <--- Sample\\n> \\n>  Model <--- Data\\n\\n\\n  [1]: http://www.intuitivebiostatistics.com",,
1467,2,674,0c7bf34d-d9ef-4742-8e2a-4b94f030014d,2010-07-26 20:45:36.0,,"Probability is about quantifying uncertainty whereas statistics is explaining the variation in some measure of interest (e.g., why do income levels vary?) that we observe in the real world. \\n\\nWhen we are trying to explain the variation that we observe we measure some observable factors (e.g., gender, education level, age etc for the income example). However, since we cannot possibly take into account all possible factors that affect income, we leave any unexplained variation to random errors (which is where quantifying uncertainty comes in).\\n\\nSince, we attribute ""Variation = Effect of Observable Factors + Effect of Random Errors"" we need the tools provided by probability to account for the effect of random errors on the variation that we observe.\\n\\nSome examples follow:\\n\\n**Quantifying Uncertainty**\\n\\nExample 1: You roll a 6-sided dice. What is the probability of obtaining a 1?\\n\\nExample 2: What is the probability that the annual income of an adult person selected at random from the United States is less than $40,000?\\n\\n**Explaining Variation**\\n\\nExample 1: We observe that the annual income of a person varies. What factors explain the variation in a person's income? \\n\\nClearly, we cannot account for all factors. Thus, we attribute a person's income to some observable factors (e.g, education level, gender, age etc) and leave any remaining variation to uncertainty (or in the language of statistics: to random errors). \\n\\nExample 2: We observe that some consumers choose Tide most of the time they buy a detergent whereas some other consumers choose detergent brand xyz. What explains the variation in choice? We attribute the variation in choices to some observable factors such as price, brand name etc and leave any unexplained variation to random errors (or uncertainty).\\n\\n\\n",,user28
1468,2,675,aa5ab12e-08b4-4863-b6d5-5a7da86dd979,2010-07-26 20:47:19.0,89.0,"The short answer to this I've heard from Persi Diaconis is the following: the problems considered by probability and statistics are inverse to each other.  In probability theory we consider some underlying process which has some randomness or uncertainty modeled by random variables, and we figure out what happens.  In statistics we observe something that has happened, and try to figure out what underlying process would explain those observations.",,
1469,2,676,a290c874-c6c9-4786-97ed-5be1dae99415,2010-07-26 20:49:33.0,288.0,Well my Prof. used these in Introductory probability class:\\n\\n1) Shoe size are correlated with reading ability\\n\\n2) Shark attack is correlated with sale of ice cream. ,,
1470,2,677,cf23672f-fd54-4b14-8657-3c41b6b9b4a2,2010-07-26 20:56:22.0,74.0,"Let's say you have some data like this:\\n\\n    Return Symbol News Text\\n    -4%	DELL   Centegra and Dell Services recognized with Outsourcing Center's...\\n    7%	 MSFT	Rising Service Revenues Benefit VMWare\\n    1%	 CSCO	Cisco Systems (CSCO) Receives 5 Star Strong Buy Rating From S&P\\n    4%	 GOOG	Summary Box: Google eyes more government deals\\n    7%	 AAPL	Sohu says 2nd-quarter net income rises 10 percent on higher...\\n\\nYou want to predict the return based on the text, or the source of the text.\\n\\nThis is called Text Mining. \\n\\nWhat you do ultimately is create an enormous matrix like this:\\n\\n    Return Centegra Rising Services Recognized...\\n    -4%    0.23     0      0.11     0.34\\n    -7%    0        0      0.23     0\\n    ...\\n\\nThat has one column for every word, and one row for each return, and a weighted score for each word. Then you run a regression and see if you can predict which words predict the return.\\n\\nBook: Fundamentals of Predictive Text Mining, Weiss\\n\\nSoftware: GATE or RapidMiner with Text Plugin\\n\\nYou should also do a search on Google Scholar and read up on the ins and outs. ",,
1471,2,678,fdc8280e-2c58-4b1d-99b3-1081780ea7bc,2010-07-26 21:00:00.0,81.0,"The **probability** of an event is its long-run relative frequency. So it's basically telling you the ***chance*** of, for example, getting a 'head' on the next flip of a coin, or getting a '3' on the next roll of a die.\\n\\nAny numerical measure computed from a sample of the population is a **statistic**. For example, the sample mean is a statistic which estimates the population mean, which is a parameter. So basically it's giving you some kind of ***summary*** of a sample.\\n\\n\\n - *You can only get a statistic from a\\n   sample, otherwise if you compute a\\n   numerical measure on a population, it\\n   is called a population parameter.*\\n\\n\\n\\n",,
1475,5,674,5708cc14-c754-42c3-ab61-ca6d80308412,2010-07-26 21:01:25.0,,"Probability is about quantifying uncertainty whereas statistics is explaining the variation in some measure of interest (e.g., why do income levels vary?) that we observe in the real world. \\n\\nWe explain the variation by using some observable factors (e.g., gender, education level, age etc for the income example). However, since we cannot possibly take into account all possible factors that affect income, we leave any unexplained variation to random errors (which is where quantifying uncertainty comes in).\\n\\nSince, we attribute ""Variation = Effect of Observable Factors + Effect of Random Errors"" we need the tools provided by probability to account for the effect of random errors on the variation that we observe.\\n\\nSome examples follow:\\n\\n**Quantifying Uncertainty**\\n\\nExample 1: You roll a 6-sided dice. What is the probability of obtaining a 1?\\n\\nExample 2: What is the probability that the annual income of an adult person selected at random from the United States is less than $40,000?\\n\\n**Explaining Variation**\\n\\nExample 1: We observe that the annual income of a person varies. What factors explain the variation in a person's income? \\n\\nClearly, we cannot account for all factors. Thus, we attribute a person's income to some observable factors (e.g, education level, gender, age etc) and leave any remaining variation to uncertainty (or in the language of statistics: to random errors). \\n\\nExample 2: We observe that some consumers choose Tide most of the time they buy a detergent whereas some other consumers choose detergent brand xyz. What explains the variation in choice? We attribute the variation in choices to some observable factors such as price, brand name etc and leave any unexplained variation to random errors (or uncertainty).\\n\\n\\n",deleted 37 characters in body,user28
1476,5,678,af596bac-b956-47b6-b4e1-ad08026ae814,2010-07-26 21:05:06.0,81.0,"The **probability** of an event is its long-run relative frequency. So it's basically telling you the ***chance*** of, for example, getting a 'head' on the next flip of a coin, or getting a '3' on the next roll of a die.\\n\\nA **statistic** is any numerical measure computed from a sample of the population. For example, the sample mean. We use this as a statistic which estimates the population mean, which is a parameter. So basically it's giving you some kind of ***summary*** of a sample.\\n\\n\\n - *You can only get a statistic from a\\n   sample, otherwise if you compute a\\n   numerical measure on a population, it\\n   is called a population parameter.*\\n\\n\\n\\n",edited body; added 13 characters in body,
1477,2,680,0024fc0f-3bc2-4726-9b73-ea13735d5941,2010-07-26 21:11:25.0,74.0,"If you have enormous data sets - ones that make Excel or Notepad load slowly, then a database is a good way to go. MS SQL Server Express is free, and it's easy to connect with JMP and other programs. You may want to sample in this case. You don't have to normalize the data in the database. Otherwise, CSV is sharing-friendly. \\n\\nIn terms of analysis, here are some starting points:\\n\\n- Describe one variable:\\n\\nHistogram  \\nSummary statistics (mean etc)\\n\\n- Describe relationship between variables:\\n\\nScatter Plot  \\nMosaic plot for categorical  \\nCorrelation  \\nContingency table for categorical  \\nAssociation Analysis (for huge binary datasets)  \\n\\n- Predict a real number: regression (or other machine learning regression techniques)\\n\\n- Predict a whole number: classification (logistic regression or other machine learning techniques)\\n\\n- Put observations into ""natural"" groups: clustering\\n\\n- Put attributes into ""natural"" groups: factoring\\n\\n- Quantifying Risk = Standard Deviation\\n\\n- Likelihood of a successfully completed iteration given x number of story points = Logistic Regression\\n\\nGood luck!",,
1478,2,681,c0d42613-bfde-4d65-8ab4-a002fa80a0c4,2010-07-26 21:13:43.0,279.0,"You don't necessarily have to go Bayesian on your model, plain maximum likelihood estimation works just fine (though has no explicit solution). Multiple R packages (eg. aod or VGAM) will fit the distribution for you.\\n\\nAlternatively, you can use the quasi-likelihood based overdispersed binomial model that does not assume a beta-binomial distribution, just adjusts for the overdispersion. The `glm` function with the `quasibinomial` family will fit this model in R.",,
1479,2,682,40a82b6e-2b16-420d-8fe2-441b376e4510,2010-07-26 21:17:29.0,74.0,"I didn't find How To Measure Anything, nor Head First particularly good.\\n\\nStatistics In Plain English is a good starter book.\\n\\nOnce you finish that, Multivariate Data Analysis by Hair is fantastic.\\n\\nGood Luck!",,
1480,16,682,40a82b6e-2b16-420d-8fe2-441b376e4510,2010-07-26 21:17:29.0,-1.0,,,
1482,2,683,d0fe7dbf-18ca-4eb2-a080-ff6069135eb9,2010-07-26 21:34:14.0,223.0,"If your interested in the mathematical statistic around entropy, you may consult this book http://www.renyi.hu/~csiszar/Publications/Information_Theory_and_Statistics:_A_Tutorial.pdf\\nit is freely available. ",,
1483,2,684,14901ffb-8eb3-49d8-b696-e10a41bb6a4b,2010-07-26 21:35:29.0,81.0,There are two schools of thought is Statistics: [Frequentest and Bayesian][1]. \\n\\nBayes theorem is to do with the latter and can be seen as a way of understanding how the probability that a theory is true is affected by a new piece of evidence. This is known as conditional probability. You might want to look at [this][2] to get a handle on the math.\\n\\nThis differs from the Frequentest approach which relies only on the long-run relative frequency and does not take the new piece of evidence into account.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/22/bayesian-and-frequentist-reasoning-in-plain-english\\n  [2]: http://stattrek.com/Lesson1/Bayes.aspx,,
1484,2,685,344d121a-079e-4493-aad6-119195d0b3f7,2010-07-26 21:40:20.0,327.0,"Is there something about statistics that lends itself to this sort of saying, or is it just that people will say anything to support their case, and this includes citing irrelevant or incomplete statistics?",,
1485,1,685,344d121a-079e-4493-aad6-119195d0b3f7,2010-07-26 21:40:20.0,327.0,"Lies, Damn Lies and Statistics",,
1486,3,685,344d121a-079e-4493-aad6-119195d0b3f7,2010-07-26 21:40:20.0,327.0,<theory>,,
1487,2,686,779493a8-17b3-410c-b483-e87369fcfab5,2010-07-26 21:58:37.0,339.0,Check out the following links. I'm not sure what exactly are you looking for.\\n\\n[Monte Carlo Simulation for Statistical Inference][1]\\n\\n[Kernel methods and Support Vector Machines][2]\\n\\n[Introduction to Support Vector Machines][3]\\n\\n[Monte Carlo Simulations][4]\\n\\n[Free Science and Video Lectures Online!][5]\\n\\n[Video lectures on Machine Learning][6]\\n\\n\\n  [1]: http://videolectures.net/mlss08au_freitas_asm/\\n  [2]: http://videolectures.net/mlss08au_smola_ksvm/\\n  [3]: http://videolectures.net/epsrcws08_campbell_isvm/\\n  [4]: http://academicearth.org/lectures/monte-carlo-simulations-application-to-lattice-models\\n  [5]: http://freescienceonline.blogspot.com/2009_09_01_archive.html\\n  [6]: http://videolectures.net/Top/Computer_Science/Machine_Learning/,,
1488,16,686,779493a8-17b3-410c-b483-e87369fcfab5,2010-07-26 21:58:37.0,-1.0,,,
1489,2,687,98f697c8-5ae9-42d4-8217-bce424051ccc,2010-07-26 22:00:01.0,81.0,"\\nStatistics is about inferring something about a population, and that  requires some level of interpretation.\\n\\nMore intuitively, ""is the glass half full or half empty?"". They both mean the same thing, but  may have a different effect on the person who hears it.\\n\\nSo I would say it's the interpretation aspect which is the problem\\n\\nP.S. There's an interesting article on the [BBC website][1] which may be worth a read.\\n\\nP.P.S. If you meant this more generally, then there could be a case for saying that the frequentest approach to statistics can give a different result to the Bayesian approach.\\n\\n\\n  [1]: http://www.bbc.co.uk/dna/h2g2/A1091350",,
1493,2,689,51eb45b5-6469-48e8-b08b-9077cf8b6471,2010-07-26 22:12:58.0,,See [Videos on data analysis using R][1] on Jeromy Anglim's blog.  There are many links at that page and he updates it.  He has another post with many links to videos on probability and statistics as well as linear algebra and calculus.\\n\\n\\n  [1]: http://jeromyanglim.blogspot.com/2010/05/videos-on-data-analysis-with-r.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed:+jeromyanglim+(Jeromy+Anglim's+Blog:+Psychology+and+Statistics),,Michael Bishop
1494,16,689,51eb45b5-6469-48e8-b08b-9077cf8b6471,2010-07-26 22:12:58.0,-1.0,,,
1498,2,691,7a0f30d3-ad3f-4cf9-8f7b-31c2a8b99fc8,2010-07-26 22:22:21.0,352.0,"Just so people know, there is a Math Overflow question on the same topic.\\n\\n[Why-is-it-so-cool-to-square-numbers-in-terms-of-finding-the-standard-deviation][1]\\n\\nThe take away message is that using the square root of the variance leads to easier maths. A similar response is given by Rich and Reed above. \\n\\n\\n  [1]: http://mathoverflow.net/questions/1048/why-is-it-so-cool-to-square-numbers-in-terms-of-finding-the-standard-deviation",,
1499,2,692,53f6d25f-3acb-4942-a1cf-530b433732a4,2010-07-26 22:29:48.0,72.0,"Oversimplifying a bit, I have about a million records that record the entry time and exit time of people in a system spanning about ten years.  Every record has an entry time, but not every record has an exit time.  The mean time in the system is ~1 year. \\n\\nThe missing exit times happen for two reasons:\\n\\n 1. The person has not left the system at the time the data was captured.\\n 2. The person's exit time was not recorded. This happens to say 50% of the records\\n\\nThe questions of interest are:\\n\\n 1. Are people spending less time in the system, and how much less time.\\n 2. Are more exit times being recorded, and how many.\\n\\nWe can model this by saying that the probability that an exit gets recorded varies linearly with time, and that the time in the system has a Weibull whose parameters vary linearly with time.  We can then make a maximum likelihood estimate of the various parameters and eyeball the results and deem them plausible.  We chose the Weibull distribution because it seems to be used in measuring lifetimes and is fun to say as opposed to fitting the data better than say a gamma distribution.\\n\\nWhere should I look to get a clue as to how to do this correctly?  We are somewhat mathematically savvy, but not extremely statistically savvy.   ",,
1500,1,692,53f6d25f-3acb-4942-a1cf-530b433732a4,2010-07-26 22:29:48.0,72.0,How do I determine if a model is appropriate?,,
1501,3,692,53f6d25f-3acb-4942-a1cf-530b433732a4,2010-07-26 22:29:48.0,72.0,<modeling>,,
1502,2,693,1ec1dcfe-8e88-4739-b274-2aec72d300ed,2010-07-26 22:48:51.0,319.0,I like the example of a jar of red and green jelly beans.  \\n\\nA probabilist starts by knowing the proportion of each and asks the probability of drawing a red jelly bean.  A statistician infers the proportion of red jelly beans by sampling from the jar.,,
1503,4,692,edf1af15-7ac0-4b28-9d84-e569db2173aa,2010-07-26 22:50:02.0,72.0,How do I determine if a survival model with missing data is appropriate?,edited title,
1504,2,694,608b8286-9e7f-4c05-a3dc-9cf17041cadf,2010-07-26 23:04:39.0,74.0,"This isn't technically a cartoon, but close enuf:\\n\\n![alt text][1]\\n\\n\\n  [1]: http://www.austinthirdgen.org/upload/piechart.jpg",,
1505,16,694,608b8286-9e7f-4c05-a3dc-9cf17041cadf,2010-07-26 23:04:39.0,-1.0,,,
1506,2,695,ae86f4b9-e6e9-4d5a-ac77-c46537695f6a,2010-07-26 23:10:16.0,173.0,My favourite book on Statistics is is David William's [Weighing the Odds][1]. Davison's [Statistical Models][2] is good too.\\n\\n\\n  [1]: http://amzn.to/aRoxQq\\n  [2]: http://amzn.to/90gOnD,,
1507,16,695,ae86f4b9-e6e9-4d5a-ac77-c46537695f6a,2010-07-26 23:10:16.0,-1.0,,,
1508,2,696,14f7f79d-d0b4-4261-ad0a-04e624b8cc5d,2010-07-26 23:22:39.0,173.0,I think that my question is subsumed by this more general discussion: http://stats.stackexchange.com/questions/411/motivation-for-kolmogorov-distance-between-distributions,,
1509,5,608,3c816794-6d0e-4838-aaa3-b5573ecb30ee,2010-07-26 23:36:58.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n\\n    generate.data <- function(seed)\\n    {\\n    	set.seed(seed) #Set a seed so the results are consistent (I hope)\\n    	a <- rnorm(60) #predictor\\n    	b <- rnorm(60) #predictor\\n    	c <- rnorm(60) #predictor\\n    	y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    	data <- data.frame(y,a,b,c) \\n    	return(data)	\\n    }\\n   \\n    data <- generate.data(76)\\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\nPer earlier comments, below I have provided a list of seeds from 1 to 10000 in which AIC and BIC disagree.  This was done by a simple search through the available seeds, but if someone could provide a way to generate data which would tend to produce divergent answers from these two information criteria it may be particularly informative.\\n\\n    notable.seeds <- read.csv(""http://student.ucr.edu/~rpier001/res.csv"")$seed\\n\\nAs an aside, I thought about ordering these seeds by the extent to which the AIC and BIC disagree which I've tried quantifying as the sum of the absolute differences of the AIC and BIC.  For example, \\n\\n    AICDiff <- AIC(bad.model) - AIC(good.model) \\n    BICDiff <- BIC(logLik(bad.model)) - BIC(logLik(good.model))\\n    disagreement <- sum(abs(c(AICDiff,BICDiff)))\\n\\nwhere my disagreement metric only reasonably applies when the observations are notable.  For example,\\n\\n    are.diff <- sum(sign(c(AICDiff,BICDiff)))\\n    notable <- ifelse(are.diff == 0 & AICDiff != 0,TRUE,FALSE)\\n\\nHowever in cases where AIC and BIC disagreed, the calculated disagreement value was always the same (and is a function of sample size).  Looking back at how AIC and BIC are calculated I can see why this might be the case computationally, but I'm not sure why it would be the case conceptually.  If someone could elucidate that issue as well, I'd appreciate it.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other",Restructured how I provided sampled datasets so future examples would be easier...,
1510,16,25,22014a2b-58a4-404d-bca4-d03a667ebe7d,2010-07-26 23:38:29.0,69.0,,,
1511,6,25,22014a2b-58a4-404d-bca4-d03a667ebe7d,2010-07-26 23:38:29.0,69.0,<modeling><time-series><finance><software>,edited tags,
1512,2,697,ed0b9079-e5aa-4063-9666-44593615938d,2010-07-26 23:48:24.0,367.0,"As per above, you need a set of articles and responses, and then you train eg. a Neural Net to them. RapidMiner will let you do this but there are many other tools out there that will let you do regressions of this size. Ideally your response variable will be consistent (ie % change after 1 hour exactly, or % change after 1 day exactly etc). \\n\\nYou may also want to apply some sort of filtering or classification to your training variables ie the words in the article. This could be as simple as filtering some words (eg prepositions, pronouns) or more complex like using syntax to choose which words should go into the regression. Note that any filtering you do risks biasing the result.\\n\\nSome folks at University of Arizona already made a system that does this - their paper is on acm here and you may find it interesting. http://www.computer.org/portal/web/csdl/doi/10.1109/MC.2010.2 (you'll need a subscription to access if you're not eg at university). The references may also help point you in the right direction.",,
1514,2,699,e7967cc3-a162-412d-b589-aa67b1fafd2e,2010-07-27 00:01:58.0,352.0,"The [Rivest-Tarjan-Selection algorithm][1] (sometimes also called the median-of-medians algorithm) will let you compute the median element in linear-time without any sorting. For large data sets this is can be quite a bit faster than log-linear sorting. However, it won't solve your memory storage problem.\\n\\n  [1]: http://en.wikipedia.org/wiki/Selection_algorithm#Linear_general_selection_algorithm_-_Median_of_Medians_algorithm",,
1516,5,608,36592323-ee43-4b35-972d-d91f9e70be61,2010-07-27 00:06:02.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n\\n    generate.data <- function(seed)\\n    {\\n    	set.seed(seed) #Set a seed so the results are consistent (I hope)\\n    	a <- rnorm(60) #predictor\\n    	b <- rnorm(60) #predictor\\n    	c <- rnorm(60) #predictor\\n    	y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    	data <- data.frame(y,a,b,c) \\n    	return(data)	\\n    }\\n   \\n    data <- generate.data(76)\\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\nPer earlier comments, below I have provided a list of seeds from 1 to 10000 in which AIC and BIC disagree.  This was done by a simple search through the available seeds, but if someone could provide a way to generate data which would tend to produce divergent answers from these two information criteria it may be particularly informative.\\n\\n    notable.seeds <- read.csv(""http://student.ucr.edu/~rpier001/res.csv"")$seed\\n\\nAs an aside, I thought about ordering these seeds by the extent to which the AIC and BIC disagree which I've tried quantifying as the sum of the absolute differences of the AIC and BIC.  For example, \\n\\n    AICDiff <- AIC(bad.model) - AIC(good.model) \\n    BICDiff <- BIC(logLik(bad.model)) - BIC(logLik(good.model))\\n    disagreement <- sum(abs(c(AICDiff,BICDiff)))\\n\\nwhere my disagreement metric only reasonably applies when the observations are notable.  For example,\\n\\n    are.diff <- sum(sign(c(AICDiff,BICDiff)))\\n    notable <- ifelse(are.diff == 0 & AICDiff != 0,TRUE,FALSE)\\n\\nHowever in cases where AIC and BIC disagreed, the calculated disagreement value was always the same (and is a function of sample size).  Looking back at how AIC and BIC are calculated I can see why this might be the case computationally, but I'm not sure why it would be the case conceptually.  If someone could elucidate that issue as well, I'd appreciate it.\\n\\nIn an attempt to partially answer my own question, I read [Wikipedia's][1] description of leave-one-out cross validation \\n\\n> involves using a single observation\\n> from the original sample as the\\n> validation data, and the remaining\\n> observations as the training data.\\n> This is repeated such that each\\n> observation in the sample is used once\\n> as the validation data.\\n\\nIn R code, I suspect that that would mean something like this...\\n\\n    resid <- rep(NA, Nobs) \\n    for (lcv in 1:Nobs)\\n    	{\\n    		data.loo <- data[-lcv,] #drop the data point that will be used for validation\\n    		loo.model <- lm(y ~ a+b,data=data.loo) #construct a model without that data point\\n                resid[lcv] <- data[lcv,""y""] - (coef(loo.model)[1] + coef(loo.model)[2]*data[lcv,""a""]+coef(loo.model)[3]*data[lcv,""b""]) #compare the observed value to the value predicted by the loo model for each possible observation, and store that value\\n        }\\n\\n... is supposed to yield values in resid that is related to the AIC.  In practice the sum of squared residuals from each iteration of the LOO loop detailed above is a good predictor of the AIC for the notable.seeds, r^2 = .9776.  But, [elsewhere][1] a stats exchange contributor suggested that LOO should be asymptotically equivalent to the AIC (at least for linear models), so I'm a little disappointed that r^2 isn't closer to 1.  Obviously this isn't really an answer - more like additional code to try to encourage someone to try to provide a better answer.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other\\n  [2]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation",Added my attempt at implementing LOO.,
1517,6,692,31d8d1c1-739f-46e5-be9c-0126727e0549,2010-07-27 00:20:03.0,,<modeling><survival-analysis><censoring>,edited tags,user28
1518,6,692,5bbae674-ace8-46b3-8a48-b14ee07e7ca0,2010-07-27 00:20:27.0,,<modeling><survival-analysis><censoring>,edited tags,user28
1519,2,700,3510f1da-1f29-4a13-9173-c85b3a8e9e0f,2010-07-27 00:21:32.0,368.0,These [lecture notes][1] on information theory by O. Johnson contain a good introduction to different kinds of entropy.\\n\\n\\n  [1]: http://www.statslab.cam.ac.uk/~yms/ICL0706.ps,,
1520,2,701,3dbc6db8-68ad-41ab-a842-9175b8dc2b40,2010-07-27 00:24:09.0,369.0,"Because squares can allow use of many other mathematical operations or functions more easily than absolute values.\\n\\nExample: squares can be integrated, differentiated, can be used in trigonometric, logarithmic and other functions, with ease.",,
1521,5,608,31fb1920-5db2-46c9-9686-bbc225261b27,2010-07-27 00:32:13.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n\\n    generate.data <- function(seed)\\n    {\\n    	set.seed(seed) #Set a seed so the results are consistent (I hope)\\n    	a <- rnorm(60) #predictor\\n    	b <- rnorm(60) #predictor\\n    	c <- rnorm(60) #predictor\\n    	y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    	data <- data.frame(y,a,b,c) \\n    	return(data)	\\n    }\\n   \\n    data <- generate.data(76)\\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\nPer earlier comments, below I have provided a list of seeds from 1 to 10000 in which AIC and BIC disagree.  This was done by a simple search through the available seeds, but if someone could provide a way to generate data which would tend to produce divergent answers from these two information criteria it may be particularly informative.\\n\\n    notable.seeds <- read.csv(""http://student.ucr.edu/~rpier001/res.csv"")$seed\\n\\nAs an aside, I thought about ordering these seeds by the extent to which the AIC and BIC disagree which I've tried quantifying as the sum of the absolute differences of the AIC and BIC.  For example, \\n\\n    AICDiff <- AIC(bad.model) - AIC(good.model) \\n    BICDiff <- BIC(logLik(bad.model)) - BIC(logLik(good.model))\\n    disagreement <- sum(abs(c(AICDiff,BICDiff)))\\n\\nwhere my disagreement metric only reasonably applies when the observations are notable.  For example,\\n\\n    are.diff <- sum(sign(c(AICDiff,BICDiff)))\\n    notable <- ifelse(are.diff == 0 & AICDiff != 0,TRUE,FALSE)\\n\\nHowever in cases where AIC and BIC disagreed, the calculated disagreement value was always the same (and is a function of sample size).  Looking back at how AIC and BIC are calculated I can see why this might be the case computationally, but I'm not sure why it would be the case conceptually.  If someone could elucidate that issue as well, I'd appreciate it.\\n\\nIn an attempt to partially answer my own question, I read [Wikipedia's][1] description of leave-one-out cross validation \\n\\n> involves using a single observation\\n> from the original sample as the\\n> validation data, and the remaining\\n> observations as the training data.\\n> This is repeated such that each\\n> observation in the sample is used once\\n> as the validation data.\\n\\nIn R code, I suspect that that would mean something like this...\\n\\n    resid <- rep(NA, Nobs) \\n    for (lcv in 1:Nobs)\\n    	{\\n    		data.loo <- data[-lcv,] #drop the data point that will be used for validation\\n    		loo.model <- lm(y ~ a+b,data=data.loo) #construct a model without that data point\\n                resid[lcv] <- data[lcv,""y""] - (coef(loo.model)[1] + coef(loo.model)[2]*data[lcv,""a""]+coef(loo.model)[3]*data[lcv,""b""]) #compare the observed value to the value predicted by the loo model for each possible observation, and store that value\\n        }\\n\\n... is supposed to yield values in resid that is related to the AIC.  In practice the sum of squared residuals from each iteration of the LOO loop detailed above is a good predictor of the AIC for the notable.seeds, r^2 = .9776.  But, [elsewhere][1] a contributor suggested that LOO should be asymptotically equivalent to the AIC (at least for linear models), so I'm a little disappointed that r^2 isn't closer to 1.  Obviously this isn't really an answer - more like additional code to try to encourage someone to try to provide a better answer.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other\\n  [2]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation",name of site was given incorrectly,
1522,2,702,b110ebfe-b7a0-42a1-afe7-fe5fdbd15079,2010-07-27 00:37:33.0,,You could use the estimated model to predict the exit times for all the people in your system. You could then compare the estimated exit times with the actual exit times (where you have this data) and compute a metric such as [RMSE][1] to assess how good your predictions are which will in turn give you a sense of model fit. See also this [link][2].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Root_mean_square_deviation\\n  [2]: http://www.statsoft.com/textbook/survival-failure-time-analysis/#dgoodness,,user28
1523,2,703,2700ed5f-b68a-4a3d-8d1f-211d6ba13110,2010-07-27 00:47:25.0,251.0,"All three are used when dealing with nuisance parameters in the completely specified likelihood function.  \\n\\nThe marginal likelihood  is the primary method to eliminate nuisance parameters in theory.  It's a true likelihood function (i.e. it's proportional to the (marginal) probability of the observed data).\\n\\nThe partial likelihood is not a true likelihood in general.  However, in some cases it can be treated as a likelihood for asymptotic inference.  For example in Cox proportional hazards models, where it originated, we're interested in the observed rankings in the data (T1 > T2 > ..) without specifying the baseline hazard.  Efron showed that the partial likelihood loses little to no information for a variety of hazard functions.\\n\\nThe profile likelihood is convenient when we have a multidimensional likelihood function and a single parameter of interest.  It's specified by replacing the nuisance S by its MLE at each fixed T (the parameter of interest), i.e. L(T) = L(T, S(T)).  This can work well in practice, though there is a potential bias in the MLE obtained in this way; the marginal likelihood corrects for this bias.",,
1524,5,570,96c8d2e4-9971-4282-8170-71a23b810576,2010-07-27 00:49:21.0,251.0,"I'm curious if there are graphical techniques particular, or more applicable, to structural equation modeling.  I guess this could fall into categories for exploratory tools for covariance analysis or graphical diagnostics for SEM model evaluation.  (I'm not really thinking of path/graph diagrams here.) \\n",elaborate acronym in title,
1525,4,570,96c8d2e4-9971-4282-8170-71a23b810576,2010-07-27 00:49:21.0,251.0,What graphical techniques are used in Structural Equation Modeling?,elaborate acronym in title,
1526,2,704,3dd7f094-c3fb-43fa-9b4c-686ff996c627,2010-07-27 00:54:58.0,,Here's an answer to the question asked on stackoverflow: http://stackoverflow.com/questions/1058813/on-line-iterator-algorithms-for-estimating-statistical-median-mode-skewness/2144754#2144754\\n\\nThe iterative update median += eta * sgn(sample - median) sounds like it could be a way to go.,,Guy Srinivasan
1527,2,705,36250b79-4b5a-453e-9026-c129a17e7024,2010-07-27 01:07:48.0,,"This is a very interesting question. Suppose that we have a 2 dimensional covariance matrix (very unrealistic example for SEM but please bear with me). Then you can plot the iso-contours for the observed covariance matrix vis-a-vis the estimated covariance matrix to get a sense of model fit.\\n\\nHowever, in reality you will a high-dimensional covariance matrix. In such a situation, you could probably do several 2 dimensional plots taking 2 variables at a time. Not the ideal solution but perhaps may help to some extent.  ",,user28
1528,2,706,eab6f260-8b09-42b8-86a4-584042d9ed65,2010-07-27 01:51:15.0,378.0,"Yet another reason (in addition to the excellent ones above) comes from Fisher himself, who showed that the standard deviation is more ""efficient"" than the absolute deviation. Here, efficient has to do with how much a statistic will fluctuate in value on different samplings from a population. If your population is normally distributed, the standard deviation of various samples from that population will, on average, tend to give you values that are pretty similar to each other, whereas the absolute deviation will give you numbers that spread out a bit more. Now, obviously this is in ideal circumstances, but this reason convinced a lot of people (along with the math being cleaner), so most people worked with standard deviations.",,
1529,5,705,284acc9a-f1c1-4365-825b-ba0daef9f679,2010-07-27 02:14:37.0,,"This is a very interesting question. Suppose that we have a 2 dimensional covariance matrix (very unrealistic example for SEM but please bear with me). Then you can plot the iso-contours for the observed covariance matrix vis-a-vis the estimated covariance matrix to get a sense of model fit.\\n\\nHowever, in reality you will a high-dimensional covariance matrix. In such a situation, you could probably do several 2 dimensional plots taking 2 variables at a time. Not the ideal solution but perhaps may help to some extent.  \\n\\n**Edit**\\n\\nA slightly better method is to perform [Principal Component Analysis (PCA)][1] on the observed covariance matrix. Save the projection matrix from the PCA analysis on the observed covariance matrix. Use this projection matrix to transform the estimated covariance matrix. \\n\\nWe then plot iso-contours for the two highest variances of the rotated observed covariance matrix vis-a-vis the estimated covariance matrix. Depending on how many plots we want to do we can take the second and the third highest variances etc. We start from the highest variances as we want to explain as much variation in our data as possible. \\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Principal_component_analysis",added 709 characters in body,user28
1530,2,707,5264ac78-6d72-4e4b-a985-7cf14e728e77,2010-07-27 02:19:56.0,188.0,"As an extension on the k-fold answer, the ""usual"" choice of k is either 5 or 10. The leave-one-out method has a tendency to produce models that are too conservative. FYI, here is a reference on that fact: \\n\\nShao, J. (1993), Linear Model Selection by Cross-Validation, Journal of the American\\nStatistical Association, Vol. 88, No. 422, pp. 486-494",,
1531,2,708,dce7d99a-3775-43fb-a37a-33825ca110b1,2010-07-27 02:31:38.0,25692.0,"let me give you a very very intuitional insight.  suppose you are tossing a coin 10 times and you get 8 heads and 2 tails. the question that would come to your mind is whether this coin is biased towards heads or not. now if you go by conventional definitions or the frequentist approach of probability you might say that the coin is unbiased and this is an exceptional occurrence. hence you would conclude that the possibility of getting a head next toss is also 50%. but suppose you are a bayesian.  you would actually think that since you have got exceptionally high number of heads, the coin has a bias towards the head side. there are methods to calculate this possible bias. you would calculate them and then when you toss the coin next time, you would definitely call a heads. \\n\\nso , bayesian probability is about the belief that you develop based on the data you observe. i hope that was simple enough.",,
1532,2,709,a468e9f5-8f55-4a9a-b8bd-2e4b360fc26e,2010-07-27 03:46:45.0,401.0,"<a href=""http://cgibbons.berkeley.edu/Courses/ECON141_S10/IV.pdf"">Here are some slides that I prepared for an econometrics course at UC Berkeley.</a> I hope that you find them useful---I believe that they answer your questions and provide some examples.\\n\\nCharlie",,
1533,2,710,a54eff50-6cf4-46d4-aae9-297356c9832f,2010-07-27 03:54:35.0,196.0,"My understanding is that the distinction between PCA and Factor analysis primarily is in whether there is an error term. Thus PCA can, and will, faithfully represent the data whereas factor analysis is less faithful to the data it is trained on but attempts to represent underlying trends or communality in the data.  Under a standard approach PCA is not rotated, but it is mathematically possible to do so, so people do it from time to time.  I agree with the commenters in that the ""meaning"" of these methods is somewhat up for grabs and that it probably is wise to be sure the function you are using does what you intend - for example, as you note R has some functions that perform a different sort of PCA than users of SPSS are familiar with.",,
1534,2,711,491f58f1-950e-47fa-b080-16cf94eafa5c,2010-07-27 03:54:51.0,401.0,"Using robust standard errors has become common practice in economics. Robust standard errors are typically larger than non-robust (standard?) standard errors, so the practice can be viewed as an effort to be conservative. \\n\\nIn large samples (<i>e.g.,</i> if you are working with Census data with millions of observations or data sets with ""just"" thousands of observations), heteroskedasticity tests will almost surely turn up positive, so this approach is appropriate. \\n\\nAnother means for combating heteroskedasticity is weighted least squares, but this approach has become looked down upon because it changes the estimates for parameters, unlike the use of robust standard errors. If your weights are incorrect, your estimates are biased. If your weights are right, however, you get smaller (""more efficient"") standard errors than OLS with robust standard errors.\\n\\nCharlie",,
1535,2,712,fe800725-1cdd-4ce0-afb2-3c05e56098b5,2010-07-27 03:58:22.0,402.0,Is anyone aware of good data anonymization software? Or perhaps a package for R that does data anonymization? Obviously not expecting uncrackable anonymization - just want to make it difficult. ,,
1536,1,712,fe800725-1cdd-4ce0-afb2-3c05e56098b5,2010-07-27 03:58:22.0,402.0, Data anonymization software,,
1537,3,712,fe800725-1cdd-4ce0-afb2-3c05e56098b5,2010-07-27 03:58:22.0,402.0,<data>,,
1538,2,713,fdf0014b-ce5a-4eae-83d5-c8b0c3dcd08f,2010-07-27 04:04:15.0,,"Naturally you can describe dispersion of a distribution in any way meaningful (absolute deviation, quantiles, etc.). \\n\\nOne nice fact is that the variance is the second central moment, and every distribution is uniquely described by its moments if they exist.\\nAnother nice fact is that the variance is much more tractable mathematically than any comparable metric.\\nAnother fact is that the variance is one of two parameters of the normal distribution for the usual parametrization, and the normal distribution only has 2 non-zero central moments which are those two very parameters. Even for non-normal distributions it can be helpful to think in a normal framework.\\n\\nAs I see it, the reason the standard deviation exists as such is that in applications the square-root of the variance regularly appears (such as to standardize a random varianble), which necessitated a name for it.",,arik
1539,2,714,da57f5c9-fa66-4a46-85dd-0adf32ef0118,2010-07-27 04:27:33.0,,data.table is my favorite now! Very look forward to the new version with the more wishlist implemented. ,,Branson
1540,16,714,da57f5c9-fa66-4a46-85dd-0adf32ef0118,2010-07-27 04:27:33.0,-1.0,,,
1541,2,715,6499c9ee-0eda-4d53-85dd-740600a09db3,2010-07-27 04:42:25.0,130.0,I am developing a multi-class perceptron algorithm and was wondering if there are any datasets that could be used to test a multi-class perceptron? - A dataset where the classes are linearly separable and have at least 100 or more instances for training?,,
1542,1,715,6499c9ee-0eda-4d53-85dd-740600a09db3,2010-07-27 04:42:25.0,130.0,Dataset for multi class perceptron,,
1543,3,715,6499c9ee-0eda-4d53-85dd-740600a09db3,2010-07-27 04:42:25.0,130.0,<data><multivariable><classification>,,
1545,6,53,9afd362b-f3f6-4a78-b8d5-2c811ac457c5,2010-07-27 04:56:52.0,190.0,<modeling><statistical-analysis><multivariable>,edited tags,
1546,6,270,ffc9aa31-8559-4315-ba04-3eeb089f8f47,2010-07-27 04:57:34.0,190.0,<modeling><data><poisson>,edited tags,
1547,6,39,06d47563-0831-4e5d-b865-8ad8f6f4d2a4,2010-07-27 04:57:47.0,190.0,<modeling><bayesian><logit><transportation>,edited tags,
1548,2,717,dc50810c-171c-4bd7-9ed9-85acb36d04c6,2010-07-27 05:04:45.0,196.0,"I asked about why there was a difference between the average of the maximum of 100 draws from a random normal distribution and the 98th percentile of the normal distribution.  The answer I received from Rob Hyndman was mostly acceptable, but too technically dense to accept without revision.  I was left wondering whether it was possible to provide an answer that explains in intuitively understandable plain language why these two values are not equal.  \\n\\nUltimately, my answer may be unsatisfyingly circular; but conceptually, the reason max(rnorm(100)) tends to be higher than qnorm(.98) is, in short, because on average the highest of 100 random normally distributed scores will on occasion exceed its expected value.  However this distortion is non-symmetrical, since when low scores are drawn they are unlikely to end up being the highest out of the 100 scores.  Each independent draw is a new chance to exceed the expected value, or to be ignored because the obtained value isn't the maximum of the 100 drawn values.  For a visual demonstration compare the histogram of the maximum of 20 values to the histogram of the maximum of 100 values, the difference in skew, especially in the tails, is stark.\\n\\nI arrived at this answer indirectly while working through a related problem/question I had asked in the comments.  Specifically, if I found that someone's test scores were ranked in the 95th percentile, I'd expect that on average if I put them in a room with 99 other test takers that their rank would average out to be 95.  This turns out to be more or less the case (R code)...\\n\\n    for (i in 1:NSIM)\\n    {\\n        rank[i] <- seq(1,100)[order(c(qnorm(.95),rnorm(99)))==1]\\n    }\\n    summary(rank)\\n\\nAs an extension of that logic, I had likewise been expecting that if I took 100 people in a room and selected the person with 95th highest score, then took another 99 people and had them take the same test, that _on average_ the selected person would be ranked 95th in the new group. But this is not the case (R code)...\\n\\n    for (i in 1:NSIM)\\n    {\\n        testtakers <- rnorm(100)\\n        testtakers <- testtakers[order(testtakers)]\\n        testtakers <- testtakers[order(testtakers)]\\n        ranked95 <- testtakers[95]\\n        rank[i] <- seq(1,100)[order(c(ranked95,rnorm(99)))==1]\\n    }\\n    summary(rank)\\n\\nWhat makes the first case different from the second case is that in the first case the individual's score places them at exactly the 95th percentile.  In the second case their score may turn out to be somewhat higher or lower than the true 95th percentile.  Since they can not possibly rank higher than 100, groups that produce a rank 95 score that is actually at the 99th percentile or higher can not offset (in terms of average rank) those cases where the rank 95 score is much lower than the true 90th percentile.  If you look at the histograms for the two rank vectors provided in this answer it is easy to see that there is a restriction of range in the upper ends that is a consequence of this process I have been describing.",,
1549,5,575,e7384291-8802-453a-badf-27971a1727ac,2010-07-27 05:14:47.0,196.0,"What is the preferred method for for conducting post-hocs for within subjects tests?  I've seen published work where Tukey's HSD is employed but a review of Keppel and Maxwell & Delaney suggests that the likely violation of sphericity in these designs makes the error term incorrect and this approach problematic.  Maxwell & Delaney provide an approach to the problem in their book, but I've never seen it done that way in any stats package.  Is the approach they offer appropriate?  Would a Bonferoni or Sidak correction on multiple paired sample t-tests be reasonable?  An acceptable answer will provide general R code which can conduct post-hocs on a simple, multiple-way, and mixed designs as produced by the ezANOVA function in the ez package.",Further changes for bounty.,
1550,6,575,e7384291-8802-453a-badf-27971a1727ac,2010-07-27 05:14:47.0,196.0,<r><post-hoc><sphericity><within-subjects>,Further changes for bounty.,
1551,5,575,147700e6-fe30-4dff-8a25-226c74fd0df3,2010-07-27 05:20:08.0,196.0,"What is the preferred method for for conducting post-hocs for within subjects tests?  I've seen published work where Tukey's HSD is employed but a review of Keppel and Maxwell & Delaney suggests that the likely violation of sphericity in these designs makes the error term incorrect and this approach problematic.  Maxwell & Delaney provide an approach to the problem in their book, but I've never seen it done that way in any stats package.  Is the approach they offer appropriate?  Would a Bonferoni or Sidak correction on multiple paired sample t-tests be reasonable?  An acceptable answer will provide general R code which can conduct post-hocs on a simple, multiple-way, and mixed designs as produced by the ezANOVA function in the ez package and appropriate citations that are likely to pass muster with reviewers.",added citation request,
1552,2,718,81c83c6b-0621-422a-ac24-0598201e2202,2010-07-27 05:23:38.0,25.0,[This article][1] by David Howell explains the problems and several solutions.\\n\\n\\n  [1]: http://www.uvm.edu/~dhowell/StatPages/More_Stuff/RepMeasMultComp/RepMeasMultComp.html,,
1554,2,720,a031f985-bd04-43e7-8018-ef7feed1cc81,2010-07-27 05:36:14.0,25.0,"All that matters is the difference between two AIC (or, better, AICc) values, representing the fit to two models.  The actual value of the AIC (or AICc), and whether it is positive or negative, means nothing. If you simply changed the units the data are expressed in, the AIC (and AICc) would change dramatically. But the difference between the AIC of the two alternative models would not change at all.\\n\\nBottom line: Ignore the actual value of AIC (or AICc) and whether it is positive or negative. Ignore also the ratio of two AIC (or AICc) values. Pay attention only to the difference.",,
1555,2,721,c2bf41cf-d4ea-4627-ad89-c31f013e735b,2010-07-27 05:40:44.0,251.0,"The [UCLA Statistical Computing][1] site has a number of examples in various languages (SAS, R, etc).  In particular, see the following pages (look among the links titled logistic regression, categorical data analysis and generalized linear models):\\n\\n- [Data Analysis Examples][2]\\n- [Textbook Examples][3]\\n\\n\\n\\n  [1]: http://www.ats.ucla.edu/stat/\\n  [2]: http://www.ats.ucla.edu/stat/dae/\\n  [3]: http://www.ats.ucla.edu/stat/examples/default.htm\\n",,
1556,2,722,41dafc8a-e59e-41e6-9776-eaa6e7a62895,2010-07-27 05:41:11.0,25.0,"I've published a method for identifying outliers in nonlinear regression, and it can be also used when fitting a linear model.\\n\\nHJ Motulsky and RE Brown. [Detecting outliers when fitting data with nonlinear regression – a new method based on robust nonlinear regression and the false discovery rate][1]. BMC Bioinformatics 2006, 7:123\\n\\n\\n  [1]: http://www.biomedcentral.com/1471-2105/7/123/abstract/",,
1558,2,723,332b015d-116b-4438-87b9-7b7dde02455a,2010-07-27 06:00:07.0,190.0,"I'm doing shopping cart analyses my dataset is set of transaction vectors, with the items the products being bought.\\n\\nWhen applying k-means on the transactions, I will always get *some* result. A random matrix would probably also show some clusters.\\n\\nIs there a way to test whether the clustering I find is a significant one, or that is can be very well be a coincidence. If yes, how can I do it.",,
1559,1,723,332b015d-116b-4438-87b9-7b7dde02455a,2010-07-27 06:00:07.0,190.0,How can I test whether my clustering of binary data is significant,,
1560,3,723,332b015d-116b-4438-87b9-7b7dde02455a,2010-07-27 06:00:07.0,190.0,<clustering><statistical-significance><binary-data>,,
1561,2,724,c662f381-59c0-4f72-a194-9edc5825d086,2010-07-27 06:01:10.0,251.0,The [Cornell Anonymiaztion Tookit][1] is open source.  Their [research page][2] has links to associated publications.\\n\\n\\n  [1]: http://sourceforge.net/projects/anony-toolkit/\\n  [2]: http://www.cs.cornell.edu/bigreddata/privacy/,,
1562,2,725,b1ffc62f-3711-4915-9be2-09960107a864,2010-07-27 06:10:43.0,223.0,"An **hyperspectral image is** a multidimensional image with more than 200 spectral bands i.e. and image for which each pixel is a vector of dimension 200 (and most often is considered as a curve). \\n\\n\\nWhat are the **implemented package** (I am especially interested in R packages but if other free algorithm exist, I will try them) for **frontier detection** and (unsupervised ) **segmentation** of this type of images?  \\n",,
1563,1,725,b1ffc62f-3711-4915-9be2-09960107a864,2010-07-27 06:10:43.0,223.0,Frontier estimation - or Segmentation - R package for hyperspectral images ,,
1564,3,725,b1ffc62f-3711-4915-9be2-09960107a864,2010-07-27 06:10:43.0,223.0,<machine-learning><image-processing>,,
1565,2,726,2b2e2e8a-6e1a-4bf8-a91e-4371a6c6fec9,2010-07-27 06:20:38.0,223.0,"What is your favorite statistician quote ? \\nThis is community wiki, so please one quote per answer.  ",,
1566,1,726,2b2e2e8a-6e1a-4bf8-a91e-4371a6c6fec9,2010-07-27 06:20:38.0,223.0,famous statistician quote,,
1567,3,726,2b2e2e8a-6e1a-4bf8-a91e-4371a6c6fec9,2010-07-27 06:20:38.0,223.0,<statistical-analysis>,,
1568,16,726,2b2e2e8a-6e1a-4bf8-a91e-4371a6c6fec9,2010-07-27 06:20:38.0,223.0,,,
1569,2,727,4157d018-c060-49e9-a9ad-bff8cee856d9,2010-07-27 06:23:57.0,223.0,> The combination of some data and an\\n> aching desire for an answer does not\\n> ensure that a reasonable answer can be\\n> extracted from a given body of data\\n\\nTukey,,
1570,16,727,4157d018-c060-49e9-a9ad-bff8cee856d9,2010-07-27 06:23:57.0,-1.0,,,
1571,2,728,216ac302-859d-47d2-b841-d0864853ee7a,2010-07-27 06:26:04.0,223.0,"> All we know about the world etaches us that the effects of A and B are always different -in some decimale place- for any A and B. Thus asking ""are the effecs different? "" is foolish\\n\\nTukey (again but this one is my favorite)",,
1572,16,728,216ac302-859d-47d2-b841-d0864853ee7a,2010-07-27 06:26:04.0,-1.0,,,
1573,2,729,cb1c5efb-9a0f-4756-b41d-f316652ce2e0,2010-07-27 06:36:26.0,159.0,In God we trust. All others must have data. (W. Edwards Deming),,
1574,16,729,cb1c5efb-9a0f-4756-b41d-f316652ce2e0,2010-07-27 06:36:26.0,-1.0,,,
1575,2,730,c8c84bee-6813-4e06-baf0-5bce2802dcf3,2010-07-27 06:37:30.0,159.0,"All models are wrong, but some are useful. (George E. P. Box)",,
1576,16,730,c8c84bee-6813-4e06-baf0-5bce2802dcf3,2010-07-27 06:37:30.0,-1.0,,,
1577,5,728,c48a1610-73e7-40fe-b2cd-b1cead43ef42,2010-07-27 06:38:11.0,159.0,"> All we know about the world teaches us that the effects of A and B are always different---in some decimal place---for any A and B. Thus asking ""are the effects different?"" is foolish.\\n\\nTukey (again but this one is my favorite)",added 2 characters in body,
1578,5,728,9cc85d77-f433-47fd-b621-ad0d14db9826,2010-07-27 06:38:18.0,223.0,"> All we know about the world teaches us that the effects of A and B are always different -in some decimale place- for any A and B. Thus asking ""are the effecs different? "" is foolish\\n\\nTukey (again but this one is my favorite)",edited body,
1579,8,728,6473bebe-3858-475f-bb6f-1971b318d1e7,2010-07-27 06:42:07.0,159.0,"> All we know about the world teaches us that the effects of A and B are always different---in some decimal place---for any A and B. Thus asking ""are the effects different?"" is foolish.\\n\\nTukey (again but this one is my favorite)",Rollback to [c48a1610-73e7-40fe-b2cd-b1cead43ef42],
1580,2,731,b100462c-2046-4ec8-95e1-b9911f574a04,2010-07-27 06:45:31.0,251.0,"If you're looking at system faults, you might be interested in the following paper employing machine learning techniques for fault diagnosis at eBay.  It may give you a sense of what kind of data to collect or how one team approached a specific problem in a similar domain.\\n\\n- [Fault Diagnosis Using Decision Trees][1]\\n\\nIf you're just getting started, something like [RapidMiner][2] or [Orange][3] might be a good software system to start playing with your data pretty quickly.  Both of them can access the data in a variety of formats (file csv, database, among others).  \\n\\n\\n  [1]: http://www.cs.berkeley.edu/~brewer/papers/icac2004_chen_diagnosis.pdf\\n  [2]: http://rapid-i.com/content/view/181/190/\\n  [3]: http://www.ailab.si/orange/\\n",,
1581,2,732,50413be3-d88d-476c-bd26-c63ef861a3d3,2010-07-27 06:51:24.0,251.0,"""Strange events permit themselves the luxury of occurring.""\\n\\n-- [Charlie Chan][1]\\n\\n\\n  [1]: http://gutenberg.net.au/ebooks02/0200691.txt",,
1582,16,732,50413be3-d88d-476c-bd26-c63ef861a3d3,2010-07-27 06:51:24.0,-1.0,,,
1583,2,733,57528ad2-8482-4b56-80a0-619065eea6a3,2010-07-27 06:52:09.0,88.0,"There is something like [silhouette][1], which to some extent defines statistic that determines the cluster quality (for instance it is used in optimizing k). Now a possible Monte Carlo would go as follows: you generate a lot of random dataset similar to your original (for instance by shuffling values between rows in each column), cluster and obtain a distribution of mean silhouette that then may be used to test significance of silhouette in real data. Still I admin that I have never tried this idea.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Silhouette_(clustering)",,
1584,2,734,e4ff4724-9827-4e84-bd26-47530e2ee0d8,2010-07-27 06:55:06.0,88.0,Maybe the good old [iris][1]? It suits your needs and is good for start. \\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Iris,,
1585,2,735,0569fe31-8d3f-4dff-9e44-cc27386d9e8a,2010-07-27 07:19:13.0,56.0,"The one thing [ROOT][1] is really good at is storing enourmous amounts of data. ROOT is a C++ library used in particle physics; it also comes with Ruby and Python bindings, so you could use packages in these languages (e.g. NumPy or Scipy) to analyze the data when you find that ROOT offers to few possibilities out-of-the-box.\\n\\nThe ROOT fileformat can store trees or tuples, and entries can be read sequentially, so you do not need to keep all data in memory at the same time. This allows to analyze petabytes of data, something you wouldn't want to try with Excel or R.\\n\\nThe ROOT I/O documentation can be reached from [here][2].\\n\\n[1]: http://root.cern.ch\\n[2]: http://root.cern.ch/drupal/content/root-files-1",,
1586,2,736,988b8ab9-8558-4546-91c4-f4f41f0389ee,2010-07-27 07:24:19.0,300.0,zoo and xts are a must in my work!,,
1587,16,736,988b8ab9-8558-4546-91c4-f4f41f0389ee,2010-07-27 07:24:19.0,-1.0,,,
1588,2,737,89db57f6-7511-4546-8cfb-e0c71fc535e5,2010-07-27 07:26:34.0,,"There are no rotuine statistical questions, only questionable statistical rotuines.\\n\\n(I don't remember the name)",,Tzippy
1589,16,737,89db57f6-7511-4546-8cfb-e0c71fc535e5,2010-07-27 07:26:34.0,-1.0,,,
1591,2,738,f4ef572e-9a5f-420c-94a8-4cb7488c2018,2010-07-27 07:38:19.0,188.0,"Say you were standing with one foot in the oven and one foot in an ice bucket.  According to the percentage people, you should be perfectly comfortable.  \\n\\n-Bobby Bragan, 1963",,
1592,16,738,f4ef572e-9a5f-420c-94a8-4cb7488c2018,2010-07-27 07:38:19.0,-1.0,,,
1593,2,739,ce707f85-d0c3-41c3-9210-1cc33acdd1a5,2010-07-27 07:41:56.0,,"""To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.""\\n\\nRonald Fisher (1938)",,sjcockell
1594,16,739,ce707f85-d0c3-41c3-9210-1cc33acdd1a5,2010-07-27 07:41:56.0,-1.0,,,
1595,4,726,b6ffd395-3a34-43b5-a403-7de0743b4d6c,2010-07-27 07:51:17.0,190.0,Famous statistician quotes,edited title,
1596,5,737,caaad69c-a4b7-4f67-9abb-853154106653,2010-07-27 07:52:58.0,190.0,"There are no routine statistical questions, only questionable statistical routines.\\n\\nD.R. Cox",deleted 19 characters in body,
1600,2,741,162c3c52-954f-4aa1-994c-5d0ce8377105,2010-07-27 07:56:42.0,251.0,"The basic way to see if your data is Weibull is to plot the log of cumulative hazards versus log of times and see if a straight line might be a good fit.  The cumulative hazard can be found using the non-parametric Nelson-Aalen estimator.  There are similar graphical diagnostics for Weibull regression if you fit your data with covariates and some references follow.  \\n\\nThe [Klein & Moeschberger][1] text is pretty good and covers a lot of ground with model building/diagnostics for parametric and semi-parametric models (though mostly the latter).  If you're working in R, [Theneau's book][2] is pretty good (I believe he wrote the [survival][3] package).  It covers a lot of Cox PH and associated models, but I don't recall if it has much coverage of parametric models, like the one you're building.\\n\\nBTW, is this a million subjects each with one entry/exit or recurrent entry/exit events for some smaller pool of people? Are you conditioning your likelihood to account for the censoring mechanism?  \\n\\n\\n  [1]: http://www.powells.com/biblio/72-9780387239187-0\\n  [2]: http://www.powells.com/biblio/61-9780387987842-1\\n  [3]: http://cran.r-project.org/web/views/Survival.html\\n  \\n",,
1602,2,743,34fb539a-fc68-41a6-be2b-4844902c9cfd,2010-07-27 08:13:26.0,210.0,"I was having a look round a few things yesturday and came across [Bayesian Search Theory][1]. Thinking about this theory led me to think about a problem I was working on a few years ago regarding geological interpretation. \\n\\nWe were looking at the geology at one specific site and it was essentially made up from two different types of rocks. Boreholes had been drilled at different locations and showed differing amounts of the two different types of rocks at different levels in the ground along with different amounts of weathering of the rock. A number of geologists looked at the available data and all came up with different interpretations. It seems to me that Bayesian Search Theory could have been used in this case, particualrly where extra data was gathered with time, to give some indication of how likely the different interpretations were. \\n\\nHas anyone encountered a case where Bayesian Search Theory has been used in this case. Is there a standard frameowrk for doing this? I would have thought this may be something that the oil industry may have a lot of research on because it would be applicable to the search for oil. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bayesian_search_theory",,
1603,1,743,34fb539a-fc68-41a6-be2b-4844902c9cfd,2010-07-27 08:13:26.0,210.0,Use of Bayesian Search Theory in geological interpretation,,
1604,3,743,34fb539a-fc68-41a6-be2b-4844902c9cfd,2010-07-27 08:13:26.0,210.0,<bayesian><search-theory>,,
1605,2,744,c0ca8a8d-8e7a-4000-a7e7-f830b1b7fc78,2010-07-27 08:42:23.0,319.0,"""An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem."" -- John Tukey",,
1606,16,744,c0ca8a8d-8e7a-4000-a7e7-f830b1b7fc78,2010-07-27 08:42:23.0,-1.0,,,
1607,2,745,2a6a2980-91e0-43ed-b8be-5ff7594b641f,2010-07-27 08:42:25.0,,"I use \\n\\ncar, doBy, Epi, ggplot2, gregmisc (gdata, gmodels, gplots, gtools), Hmisc, plyr, RCurl, RDCOMClient, reshape, RODBC, TeachingDemos, XML.\\n\\na lot.",,esco
1608,16,745,2a6a2980-91e0-43ed-b8be-5ff7594b641f,2010-07-27 08:42:25.0,-1.0,,,
1609,2,746,dea7f618-b705-4d0c-8419-ce2f4d53859c,2010-07-27 08:43:15.0,319.0,"""efficiency = statistical efficiency x usage"" -- John Tukey",,
1610,16,746,dea7f618-b705-4d0c-8419-ce2f4d53859c,2010-07-27 08:43:15.0,-1.0,,,
1611,2,747,eb0e2937-13c5-4d31-a952-377f221802bd,2010-07-27 09:00:42.0,144.0,"Packages I often use are [raster][1], [sp][2], [spatstat][3], [vegan][4] and [splancs][5]. I sometimes use ggplot2, tcltk and lattice.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/raster/index.html\\n  [2]: http://cran.r-project.org/web/packages/sp/index.html\\n  [3]: http://cran.r-project.org/web/packages/spatstat/index.html\\n  [4]: http://cran.r-project.org/package=vegan\\n  [5]: http://cran.r-project.org/web/packages/splancs/index.html",,
1612,16,747,eb0e2937-13c5-4d31-a952-377f221802bd,2010-07-27 09:00:42.0,-1.0,,,
1613,2,748,31431255-dd7d-40df-bee0-17b07af10c41,2010-07-27 09:09:25.0,434.0,"""A big computer, a complex algorithm and a long time does not equal science.""\\n\\nRobert Gentleman",,
1614,16,748,31431255-dd7d-40df-bee0-17b07af10c41,2010-07-27 09:09:25.0,-1.0,,,
1615,2,749,b79c81d9-23b1-42f5-815e-9f5631d4d759,2010-07-27 09:10:20.0,339.0,"Regarding shopping cart analysis, I think that the main objective is to individuate the most frequent combinations of products bought by the customers. The `association rules` represent the most natural methodology here (indeed they were actually developed for this purpose). Analysing the combinations of products bought by the customers, and the number of times these combinations are repeated, leads to a rule of the type ‘if condition, then result’ with a corresponding interestingness measurement. You may also consider `Log-linear models` in order to investigate the associations between the considered variables.\\n\\nNow as for clustering, here are some information that may come in handy:\\n\\nAt first consider `Variable clustering`. Variable clustering is used for assessing collinearity, redundancy, and for separating variables into clusters that can be scored as a single variable, thus resulting in data reduction. Look for the `varclus` function (package Hmisc in R)\\n\\nAssessment of the clusterwise stability: function `clusterboot` {R package  fpc}\\n\\nDistance based statistics for cluster validation: function `cluster.stats` {R package  fpc}\\n\\nAs mbq have mentioned, use the silhouette widths for assessing the best number of clusters. Watch [this][1]. Regarding silhouette widths, see also the [optsil][2] function. \\n\\nEstimate the number of clusters in a data set via the [gap statistic][3]\\n\\nFor calculating Dissimilarity Indices and Distance Measures see [dsvdis][4] and [vegdist][5]\\n\\nEM clustering algorithm can decide how many clusters to create by cross validation, (if you can't specify apriori how many clusters to generate). *Although the EM algorithm is guaranteed to converge to a maximum, this is a local maximum and may not necessarily be the same as the global maximum. For a better chance of obtaining the global maximum, the whole procedure should be repeated several times, with different initial guesses for the parameter values. The overall log-likelihood figure can be used to compare the different final configurations obtained: just choose the largest of the local maxima*.\\nYou can find an implementation of the EM clusterer in the open-source project [WEKA][6]\\n\\n[This][7] is also an interesting link.\\n\\nAlso search [here][8] for `Finding the Right Number of Clusters in k-Means and EM Clustering: v-Fold Cross-Validation`\\n\\nFinally, you may explore clustering results using [clusterfly][9] \\n\\n\\n  [1]: http://www.google.com/codesearch/p?hl=en#sTQFIWS4uR8/afs/sipb/project/r-project/arch/sun4x_510/lib/R/library/cluster/R-ex/pam.object.R&q=lang:r%20%22optimal%20number%20of%20clusters%22&sa=N&cd=6&ct=rc\\n  [2]: http://finzi.psych.upenn.edu/R/library/optpart/html/optsil.html\\n  [3]: http://finzi.psych.upenn.edu/R/library/clusterSim/html/index.GAP.html\\n  [4]: http://finzi.psych.upenn.edu/R/library/labdsv/html/dsvdis.html\\n  [5]: http://finzi.psych.upenn.edu/R/library/vegan/html/vegdist.html\\n  [6]: http://www.cs.waikato.ac.nz/~ml/weka/\\n  [7]: http://zoonek2.free.fr/UNIX/48_R/06.html\\n  [8]: http://metalperencanaan.blogspot.com/2009_02_08_archive.html\\n  [9]: http://had.co.nz/model-vis/",,
1616,6,726,4e327c4f-8b82-429d-b0a2-0985bc2c3b33,2010-07-27 09:13:56.0,127.0,<statistical-analysis><subjective>,edited tags,
1617,2,750,207dc3b2-963a-4b82-9740-a501720b5089,2010-07-27 09:16:48.0,127.0,"> There are three kinds of lies: lies,\\n> damned lies, and statistics.\\n\\n-- [probably: Charles Wentworth Dilke (1843–1911).][1]\\n\\n\\n  [1]: https://secure.wikimedia.org/wikipedia/en/wiki/Lies,_damned_lies,_and_statistics",,
1618,16,750,207dc3b2-963a-4b82-9740-a501720b5089,2010-07-27 09:16:48.0,-1.0,,,
1619,2,751,f9121545-8b53-4b66-be7c-69d01e1b2d54,2010-07-27 09:19:53.0,127.0,> The Median Isn't the Message\\n\\n--[Stephen Jay Gould][1]\\n\\n\\n  [1]: http://cancerguide.org/median_not_msg.html,,
1620,16,751,f9121545-8b53-4b66-be7c-69d01e1b2d54,2010-07-27 09:19:53.0,-1.0,,,
1621,2,752,08eb81a0-8200-4357-83b2-73b8344e770b,2010-07-27 09:20:27.0,127.0,"> Figures don't lie, but liars do figure\\n\\n--Mark Twain",,
1622,16,752,08eb81a0-8200-4357-83b2-73b8344e770b,2010-07-27 09:20:27.0,-1.0,,,
1623,2,753,0905cb43-c77e-4dbb-af6f-ff557aa59bbe,2010-07-27 09:22:04.0,127.0,"> Statistics are like bikinis.  What\\n> they reveal is suggestive, but what\\n> they conceal is vital.\\n\\n--Aaron Levenstein",,
1624,16,753,0905cb43-c77e-4dbb-af6f-ff557aa59bbe,2010-07-27 09:22:04.0,-1.0,,,
1625,2,754,77520744-0995-439a-9ce8-20d26f4f2a44,2010-07-27 09:25:00.0,223.0,"The matematician, carried along on his flood of symbols, dealing apparently with purely formal thruths, may still reach results of endless importance for our description of physical universe\\n\\nKarl Pearson",,
1626,16,754,77520744-0995-439a-9ce8-20d26f4f2a44,2010-07-27 09:25:00.0,-1.0,,,
1627,2,755,6f3f0962-a7f3-421e-80ab-ccd6172b24a3,2010-07-27 09:44:12.0,183.0,"I don't know about famous, but the following is one of my favourites:\\n\\n""Conducting data analysis is like drinking a fine wine. It is important to swirl and sniff\\nthe wine, to unpack the complex bouquet and to appreciate the experience. Gulping\\nthe wine doesn’t work."" - Daniel B. Wright (2003), see [PDF of Article][1].\\n\\n\\n  [1]: http://www2.fiu.edu/~dwright/pdf/makefriends.pdf",,
1628,16,755,6f3f0962-a7f3-421e-80ab-ccd6172b24a3,2010-07-27 09:44:12.0,-1.0,,,
1629,5,741,52bc7d28-2804-47bb-a5af-9e1ea2587504,2010-07-27 09:45:01.0,251.0,"The basic way to see if your data is Weibull is to [plot][1] the log of cumulative hazards versus log of times and see if a straight line might be a good fit.  The cumulative hazard can be found using the non-parametric Nelson-Aalen estimator.  There are similar graphical [diagnostics][2] for Weibull regression if you fit your data with covariates and some references follow.  \\n\\nThe [Klein & Moeschberger][3] text is pretty good and covers a lot of ground with model building/diagnostics for parametric and semi-parametric models (though mostly the latter).  If you're working in R, [Theneau's book][4] is pretty good (I believe he wrote the [survival][5] package).  It covers a lot of Cox PH and associated models, but I don't recall if it has much coverage of parametric models, like the one you're building.\\n\\nBTW, is this a million subjects each with one entry/exit or recurrent entry/exit events for some smaller pool of people? Are you conditioning your likelihood to account for the censoring mechanism?  \\n\\n\\n  [1]: http://www.itl.nist.gov/div898/handbook/eda/section3/weibplot.htm\\n  [2]: http://www.weibull.com/hotwire/issue71/relbasics71.htm\\n  [3]: http://www.powells.com/biblio/72-9780387239187-0\\n  [4]: http://www.powells.com/biblio/61-9780387987842-1\\n  [5]: http://cran.r-project.org/web/views/Survival.html",added 73 characters in body; added 68 characters in body,
1632,2,757,8bfd2cac-c35a-4837-8236-cb1db95536dc,2010-07-27 09:51:19.0,439.0,"I find [lattice][1] along with the companion book ""Lattice: Multivariate Data Visualization with R"" by Deepayan Sarkar invaluable.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/lattice/index.html",,
1633,16,757,8bfd2cac-c35a-4837-8236-cb1db95536dc,2010-07-27 09:51:19.0,-1.0,,,
1634,2,758,c2ff6e16-119c-4f93-96f8-f1dc8dda7ec0,2010-07-27 10:00:23.0,190.0,"This is a method using Monte Carlo to show whether a result is correct.\\n\\nOur Null Hypothesis H_0 is that our dataset does not have an interesting clustering. Our alternative hypothesis H_1 is that our dataset contains an interesting clustering.\\n\\nHereby we think of interesting as, more interesting than the clustering structure of a random dataset with the same row and column margins. Of course other constraints could be chosen, but to loose constraints will make our result too general, and to narrow constraints will fix the clustering to much, therefore making our result insignificant automatically. The margins, as we will see, are a good choice because of the methods existing for randomizing with it.\\n\\nLet's define as our test statistic the clustering error (squared in-cluster distance), T from Π_0. The value for our original dataset is *t*.\\n\\nWe don't know anything about this distribution, except that we can draw samples from it. Which makes it a good candidate for Monte Carlo.\\n\\nNow we draw *n* (i.i.d) random samples from Π_0 and calculate the empirical *p*-value with the formula p_emp = 1 / (n+1) * (Σ_i=1-n I(t_i >= t)  + 1)\\n\\nThe random sampling can be done by swap randomization. In simple words, a square is searched with on two opposite corners an 1 and on the other two corners a 0. Then the corners are flipped. This is keeping the column and row margins. The procedure is repeated enough times until the dataset is randomized enough (this will take some experiments). More info about this can be found in [Assessing Data Mining Results via Swap Randomization by Gionis et. al.][1]\\n\\nOne method to do this is defining the distribution of your data and taking the clustering error as test-statistic t.\\n\\nFor example, if we consider all data sets with the same row and column margins as being our data distribution, than we can take n random matrices Xi from this distribution and calculate the clustering error for them. Then we can calculate the emperical p-value by the formula \\n\\n\\n  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.3286&rep=rep1&type=pdf ",,
1635,2,759,eb2bd260-a4f3-42a4-b9e9-b7815ad60822,2010-07-27 10:07:42.0,88.0,"I am afraid there is no; during my little adventure with such data we have just converted it to a data frame form, added some extra attributes made from neighborhoods of pixels and used standard methods. Still, packages [ripa][1] and [hyperSpec][2] might be useful.  \\nFor other software, I've got an impression that most of sensible applications are commercial.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/ripa/index.html\\n  [2]: http://cran.r-project.org/web/packages/hyperSpec/index.html",,
1636,2,760,9df87949-de01-4136-af92-b4154b41d029,2010-07-27 10:24:44.0,442.0,"I am currently writing a paper in which I have the pleasure to conduct both between and within subjects comparisons. After discussion with my supervisor we decided to run *t*-tests and use the pretty simple `Holm-Bonferroni method` ([wikipedia][1]) for correcting for alpha error cumulation. It controls for familwise error rate but has a greater power than the ordinary Bonferroni procedure.\\nProcedure:\\n\\n 1. You run the *t*-tests for all comparisons you want to do.\\n 2. You order the *p*-values according to their value.\\n 3. You test the smallest *p*-value against *alpha* / *k*, the second smallest against *alpha* /( *k* - 1), and so forth until the first test turns out non-significant in this sequence of tests.\\n\\nCite Holm (1979) which can be downloaded via the link in the Wikipedia.\\n\\n  [1]: http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method\\n  [2]: http://www.jstor.org/stable/4615733",,
1637,2,761,7ce452d4-fca0-484d-865c-194a83bd84b8,2010-07-27 10:26:36.0,251.0,"I like this from Steve Skienna's [Calculated Bets][1] (see the link for complete discussion):\\n\\n> In summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to to measure the extent to which our world is ideal.\\n\\n\\n  [1]: http://books.google.com/books?id=UvWGgaE4ZA8C&lpg=PA86&ots=RXQyDddYcu&dq=In%20summary%2C%20probability%20theory%20enables%20us%20to%20find%20the%20consequences%20of%20a%20given%20ideal%20world%2C%20while%20statistical%20theory%20enables%20us%20to%20to%20measure%20the%20extent%20to%20which%20our%20world%20is%20ideal.&pg=PA86#v=onepage&q&f=false",,
1638,2,762,f325544f-8ecb-4b17-af7e-f50da9a0f4db,2010-07-27 11:04:03.0,214.0,Have a look at the [multcomp][1]-package and its vignette [Simultaneous Inference in General Parametric Models][2]. I think it should do what wan't and the vignette has very good examples and extensive references.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/multcomp/index.html\\n  [2]: http://cran.r-project.org/web/packages/multcomp/vignettes/generalsiminf.pdf,,
1639,2,763,9bdeb4a4-46f4-4f99-b70c-759dc5d3e8f6,2010-07-27 11:04:08.0,438.0,"The last six months or so of the NetFlix Prize seemed to me to sharply change the general community-wide presumption against algorithm combining. For instance, my formal training (university courses) and later on-the-job oversight/mentoring taught us to avoid algorithm combination unless we had an explicit reason to do so--and ""to improve resolution of my current algorithm"", wasn't really deemed a good reason. (Others might have a different experience--of course i'm inferring a community-wide view based solely on my own experience, though my experience in coding poorly-performing ML algorithms is substantial.) \\n\\nStill, there were a few ""patterns"" in which combining algorithms in one way or another was accepted, and actually improved performance. For me, the most frequent example involved some ML algorithm configured in machine mode (assigning a class label to each data point) and in which there were more than two classes (usually many more). When for instance, using a supervised-learning algorithm  to resolve four classes, and we would see excellent separation *except for* let's say Class III versus Class IV. So out of those six decision boundaries, only one resolved below the required threshold. Particularly when classes III and IV together accounted for a small percent of the data, adding an additional algorithm *optimized just on the resolution of those two classes*, was a fairly common solution to this analytical problem type. (Usually that 'blind spot' was an inherent limitation of the primary algorithm--e.g., it was a linear classifier and the III/IV decision boundary was non-linear. \\n\\nIn other words, when we had a reliable algorithm suited to the processing environment (which was usually streaming data) and that performed within the spec except for a single blind spot that caused it to fail to resolve two (or more) classes that accounted for a small fraction of the data, then it was always better to 'bolt-on' another specialized algorithm to catch what the main algorithm was systematically missing.\\n",,
1640,2,764,fcf010c8-9d22-4661-b2ea-5ed1081af2a3,2010-07-27 11:29:20.0,445.0,"I have commonly heard that LME models are more sound in the analysis of accuracy data (i.e., in psychology experiments), in that they can work with binomial and other non-normal distributions that traditional approaches (e.g., ANOVA) can't.\\n\\nWhat is the mathematical basis of LME models that allow them to incorporate these other distributions, and what are some not-overly-technical papers describing this?",,
1641,1,764,fcf010c8-9d22-4661-b2ea-5ed1081af2a3,2010-07-27 11:29:20.0,445.0,Linear Mixed Effects Models,,
1642,3,764,fcf010c8-9d22-4661-b2ea-5ed1081af2a3,2010-07-27 11:29:20.0,445.0,<modeling><analysis>,,
1643,2,765,e751d527-56fe-4277-bdc1-ce7c6a3bd152,2010-07-27 11:56:42.0,442.0,"A model is saturated if and only if it has as many parameters as it has data points (observations). Or put otherwise, in non-saturated models the degrees of freedom are bigger than zero.\\n\\nThis basically means that this model is useless, because it does not describe the data more parsimoniously than the raw data does (and describing data parsimoniously is generally the idea behind using a model). Furthermore, saturated models can (but don't necessarily) provide a perfect fit because they just interpolate or iterate the data.\\n\\nTake for example the mean as a model for some data. If you have only one data point (e.g., 5) using the mean (i.e., 5; note that the mean is a saturated model for only one data point) does not help at all. However if you already have two data points (e.g., 5 and 7) using the mean (i.e., 6) as a model provides you with a more  parsimonious description than the original data.",,
1644,2,766,adfbed0d-f18c-4222-bf1a-ebb9d0c9458c,2010-07-27 12:10:34.0,449.0,"Most statistical packages have a function to calculate the natural logarithm of the factorial directly (e.g. the lfactorial() function in R, the lnfactorial() function in Stata). This allows you to include the constant term in the log-likelihood if you want.",,
1645,2,767,ca299122-3003-4357-85b0-49452ab9f529,2010-07-27 12:31:57.0,447.0,"Your question implies that AIC and BIC try to answer the same question, which is not true.\\nAIC tries to select the model that most adequately describes an unknown, high dimensional reality. This means that reality is never in the set of candidate models that are being considered.  On the contrary, BIC tries to find the TRUE model among the set of candidates. If find it quite odd the assumption that reality is instantiated in one of the model that the researchers built along the way. This is a real issue for BIC.\\n\\n  \\nNevertheless, there are a lot of researchers who say BIC is better than AIC, using model recovery simulations as an argument. These simulations consist of generating data from models A and B, and then fitting both datasets with the two models. Overfitting occurs when the wrong model fits the data better than the generating. The point of these simulations is to see how well AIC and BIC correct these overfits. Usually, the results point to the fact that AIC is too liberal and still frequently prefers a more complex, wrong model over a simpler, true model. At first glance these simulations seem to be really good arguments, but the problem with them is that they are meaningless for AIC. As I said before, AIC does not consider that any of the candidate models being tested is actually true. According to AIC, all models are approximations to reality, and reality should never have a low dimensionality. At least lower than some of the candidate models. \\n\\n\\nmy recommendation: use both AIC and BIC. Most of the times they will agree on the preferred model, when they dont, just report it.\\n\\n\\nIf you are unhappy with both AIC and BIC, and you have free time to invest, look up for Minimum Description Length (MDL), a totally different approach that overcomes the limitations of AIC and BIC. There are several measures stemming from MDL, like normalized maximum likelihood or the Fisher Information approximation. The problem with MDL is that its mathematically demanding and/or computationally intensive. \\n\\nStill, if you wanna stick to simple solutions, a nice way for assessing model flexibility (especially when the number of parameters are equal, rendering AIC and BIC useless) is doing Parametric Bootstrap, which is quite easy to implement. here is a link to a paper on it:\\n[link text][1]\\n\\n\\n  [1]: http://www.ejwagenmakers.com/2004/PBCM.pdf\\n\\nsome people here advocate the use of cross-validation. I personally have used it, and dont have anything against it, but the issue with it is that the choice among the sample-cutting rule (leave-one-out, K-fold, etc) is an unprincipled one. \\n\\ncheers",,
1646,5,765,267fe83d-17a8-4cf8-9836-651602131b01,2010-07-27 12:32:15.0,442.0,"A model is saturated if and only if it has as many parameters as it has data points (observations). Or put otherwise, in non-saturated models the degrees of freedom are bigger than zero.\\n\\nThis basically means that this model is useless, because it does not describe the data more parsimoniously than the raw data does (and describing data parsimoniously is generally the idea behind using a model). Furthermore, saturated models can (but don't necessarily) provide a (useless) perfect fit because they just interpolate or iterate the data.\\n\\nTake for example the mean as a model for some data. If you have only one data point (e.g., 5) using the mean (i.e., 5; note that the mean is a saturated model for only one data point) does not help at all. However if you already have two data points (e.g., 5 and 7) using the mean (i.e., 6) as a model provides you with a more  parsimonious description than the original data.",added 10 characters in body,
1647,6,764,da7632dd-e0d5-4043-9ae0-256b731cb48e,2010-07-27 12:35:32.0,88.0,<beginner><mixed-model>,edited tags,
1648,2,768,ac81f1d3-4d4d-4a39-a424-350801666697,2010-07-27 12:36:47.0,,"In probability theory, we are given random variables X1, X2, ... in some way, and then we study their properties, i.e. calculate probability P{ X1 \\in B1 }, study the convergence of X1, X2, ... etc.\\n\\nIn mathematical statistics, we are given n realizations of some random variable X, and set of distributions D; the problem is to find amongst distributions from D one which is most likely to generate the data we observed. \\n",,zoran
1649,5,767,d14f9dba-c24d-4e92-9b06-43dd827c73b8,2010-07-27 12:39:36.0,447.0,"Your question implies that AIC and BIC try to answer the same question, which is not true.\\nAIC tries to select the model that most adequately describes an unknown, high dimensional reality. This means that reality is never in the set of candidate models that are being considered.  On the contrary, BIC tries to find the TRUE model among the set of candidates. I find it quite odd the assumption that reality is instantiated in one of the model that the researchers built along the way. This is a real issue for BIC.\\n\\n  \\nNevertheless, there are a lot of researchers who say BIC is better than AIC, using model recovery simulations as an argument. These simulations consist of generating data from models A and B, and then fitting both datasets with the two models. Overfitting occurs when the wrong model fits the data better than the generating. The point of these simulations is to see how well AIC and BIC correct these overfits. Usually, the results point to the fact that AIC is too liberal and still frequently prefers a more complex, wrong model over a simpler, true model. At first glance these simulations seem to be really good arguments, but the problem with them is that they are meaningless for AIC. As I said before, AIC does not consider that any of the candidate models being tested is actually true. According to AIC, all models are approximations to reality, and reality should never have a low dimensionality. At least lower than some of the candidate models. \\n\\n\\nmy recommendation: use both AIC and BIC. Most of the times they will agree on the preferred model, when they dont, just report it.\\n\\n\\nIf you are unhappy with both AIC and BIC, and you have free time to invest, look up for Minimum Description Length (MDL), a totally different approach that overcomes the limitations of AIC and BIC. There are several measures stemming from MDL, like normalized maximum likelihood or the Fisher Information approximation. The problem with MDL is that its mathematically demanding and/or computationally intensive. \\n\\nStill, if you wanna stick to simple solutions, a nice way for assessing model flexibility (especially when the number of parameters are equal, rendering AIC and BIC useless) is doing Parametric Bootstrap, which is quite easy to implement. here is a link to a paper on it:\\n[link text][1]\\n\\n\\n  [1]: http://www.ejwagenmakers.com/2004/PBCM.pdf\\n\\nsome people here advocate the use of cross-validation. I personally have used it, and dont have anything against it, but the issue with it is that the choice among the sample-cutting rule (leave-one-out, K-fold, etc) is an unprincipled one. \\n\\ncheers",deleted 1 characters in body,
1650,2,769,8a6bf17d-a0bd-4398-b080-657ace1859a3,2010-07-27 12:46:48.0,327.0,"When does data analysis cease to be statistics ?\\n\\nAre the following examples all applications of statistics ?: computer vision, face recognition, compressed sensing, lossy data compression, signal processing.",,
1651,1,769,8a6bf17d-a0bd-4398-b080-657ace1859a3,2010-07-27 12:46:48.0,327.0,What types of data analysis do not count as statistics?,,
1652,3,769,8a6bf17d-a0bd-4398-b080-657ace1859a3,2010-07-27 12:46:48.0,327.0,<theory>,,
1653,2,770,78fd2067-5d0d-43d5-b272-b27c40e716e3,2010-07-27 12:53:30.0,453.0,"During every machine learning tutorial you'll find, there is the common ""You will need to know x amount of stats before starting this tutorial"".  As such, using your knowledge of stats, you will learn about machine learning.\\n\\nMy question is whether this can be reversed.  Can a computer science student learn statistics through studying machine learning algorithms?  Has this been tested, at all?  Are there examples where this is the case already?\\n",,
1654,1,770,78fd2067-5d0d-43d5-b272-b27c40e716e3,2010-07-27 12:53:30.0,453.0,"Is it possible to use machine learning as a method for learning stats, rather than vice-versa?",,
1655,3,770,78fd2067-5d0d-43d5-b272-b27c40e716e3,2010-07-27 12:53:30.0,453.0,<machine-learning><statistics>,,
1656,2,771,db146166-14b9-431c-a307-92351066a95a,2010-07-27 13:02:58.0,88.0,"I think that learning machine learning requires only an elementary subset of statistics; too much may be dangerous, since some intuitions are in conflict. Still, the answer to the question can it be reversed is no.",,
1657,2,772,55cd6a67-499a-4371-aa6b-3647206e9174,2010-07-27 13:07:38.0,5.0,"One major benefit of mixed-effects models is that they don't assume independence amongst observations, and there can be a correlated observations within a unit or cluster.\\n\\nThis is covered concisely in ""Modern Applied Statistics with S"" (MASS) in the first section of chapter 10 on ""Random and Mixed Effects"".  V&R walk through an example with gasoline data comparing ANOVA and lme in that section, so it's a good overview.  The R function to be used in `lme` in the `nlme` package.  \\n\\nThe model formulation is based on Laird and Ware (1982), so you can refer to that as a primary source although it's certainly not good for an introduction.\\n\\n* Laird, N.M. and Ware, J.H. (1982) ""Random-Effects Models for Longitudinal Data"", Biometrics, 38, 963–974.\\n* Venables, W.N. and Ripley, B.D. (2002) ""[Modern Applied Statistics with S][1]"", 4th Edition, Springer-Verlag.\\n\\nYou can also have a look at the [""Linear Mixed Models""][2] (PDF) appendix to John Fox's ""An R and S-PLUS Companion to Applied Regression"".  And [this lecture by Roger Levy][3] (PDF) discusses mixed effects models w.r.t. a multivariate normal distribution.\\n\\n\\n  [1]: http://www.stats.ox.ac.uk/pub/MASS4/\\n  [2]: http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf\\n  [3]: http://idiom.ucsd.edu/~rlevy/lign251/fall2007/lecture_14.pdf",,
1658,5,760,fdd8e287-a932-4652-8308-9da1fb20b3be,2010-07-27 13:08:54.0,442.0,"I am currently writing a paper in which I have the pleasure to conduct both between and within subjects comparisons. After discussion with my supervisor we decided to run *t*-tests and use the pretty simple `Holm-Bonferroni method` ([wikipedia][1]) for correcting for alpha error cumulation. It controls for familwise error rate but has a greater power than the ordinary Bonferroni procedure.\\nProcedure:\\n\\n 1. You run the *t*-tests for all comparisons you want to do.\\n 2. You order the *p*-values according to their value.\\n 3. You test the smallest *p*-value against *alpha* / *k*, the second smallest against *alpha* /( *k* - 1), and so forth until the first test turns out non-significant in this sequence of tests.\\n\\nCite Holm (1979) which can be downloaded via the link at [wikipedia][1].\\n\\n  [1]: http://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method\\n  [2]: http://www.jstor.org/stable/4615733",added 1 characters in body; edited body,
1659,2,773,8b5a1b4c-9a3e-44f5-a2a8-a8e5ab669be3,2010-07-27 13:09:54.0,447.0,"As everybody else said before, it means that you have as much parameters have you have data points. So, no goodness of fit testing. But this does not mean that ""by definition"", the model can perfectly fit any data point. I can tell you by personal experience of working with some saturated models that could not predict specific data points. It is quite rare, but possible.\\n\\n\\nAnother important issue is that saturated does not mean useless. For instance, in mathematical models of human cognition, model parameters are associated with specific cognitive processes that have a theoretical background. If a model is saturated, you can test its adequacy by doing focused experiments with manipulations that should affect only specific parameters. If the theoretical predictions match the observed differences (or lack of) in parameter estimates, then one can say that the model is valid. \\n\\nAn example: Imagine for instance a model that has two sets of parameters, one for cognitive processing, and another for motor responses. Imagine now that you have an experiment with two conditions, one in which the participants ability to respond is impaired (they can only use one hand instead of two), and in the other condition there is no impairment. If the model is valid, differences in parameter estimates for both conditions should only occur for the motor response parameters.\\n\\nAlso, be aware that even if one model is non-saturated, it might still be non-identifiable, which means that different combinations of parameter values produce the same result, which compromises any model fit.\\n\\nIf you wanna find more information on these issues in general, you might wanna take look at these papers:\\n\\nBamber, D., & van Santen, J. P. H. (1985). How many parameters can a model have and still be testable? Journal of Mathematical Psychology, 29, 443-473. \\n\\nBamber, D., & van Santen, J. P. H. (2000). How to Assess a Model's Testability and Identifiability. Journal of Mathematical Psychology, 44, 20-40. \\n\\n\\ncheers",,
1660,2,774,2370183c-ac44-48f3-877a-cb709e44315e,2010-07-27 13:14:19.0,5.0,"I really wouldn't suggest using machine learning in order to learn statistics.  The mathematics employed in machine learning is often different because there's a real emphasis on the computational algorithm.  Even treatment of the same concept will be different.  \\n\\nA simple example of this would be to compare the treatment of linear regression between a basic statistics textbook and a machine learning textbook.  Most machine learning texts give a heavy treatment to concepts like ""[gradient descent][1]"" and other optimzation techniques, while a statistics textbook will typically just cover [ordinary least squares][2] (if even that).\\n\\nLastly, machine learning generally doesn't cover the same material when it comes to things like model comparison, sampling, etc.  So while some of the basic models are the same, the conceptual frameworks can be very different.\\n\\n  [1]: http://en.wikipedia.org/wiki/Gradient_descent\\n  [2]: http://en.wikipedia.org/wiki/Ordinary_least_squares",,
1661,2,775,58c83e2b-a513-4fe9-90c0-35ef8f8a3fb1,2010-07-27 13:14:54.0,460.0,What is the difference between Operations Research and Statistical Analysis?\\n,,
1662,1,775,58c83e2b-a513-4fe9-90c0-35ef8f8a3fb1,2010-07-27 13:14:54.0,460.0,Operations Research V/s Statistical Analysis?,,
1663,3,775,58c83e2b-a513-4fe9-90c0-35ef8f8a3fb1,2010-07-27 13:14:54.0,460.0,<statistical-analysis>,,
1664,2,776,88284b98-2c45-4573-84b4-d77a96c0aa20,2010-07-27 13:25:13.0,5.0,"This may be slightly controversial, but certainly most of the examples that you give are not statistics (some of these would more properly fall under machine learning):\\n\\nStatistics usually covers the scenario where you are making inferences something when you only have a subset of the data (hence the use of things like [the p-value][1]).  The goal is to come to a deeper understanding of the underlying process that generates the data.  \\n\\nIn the examples that you provide, the goal is to perform some kind of action; whether the model itself is an accurate depiction of the underlying process is completely irrelevant so long as the predictions are accurate.  See [my related question][2] about the difference between machine learning and statistics.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/31/what-is-the-meaning-of-p-values-and-t-values-in-statistical-tests\\n  [2]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning",,
1665,2,777,129446c6-f118-4edf-86b4-ec4da245d7fa,2010-07-27 13:35:21.0,334.0,"Those are entire academic discplines so I do not think you can expect much more here than pointers to further, and more extensive, documentation as e.g. Wikipedia on [Operations Research](http://en.wikipedia.org/wiki/Operations_research) and [Statistics](http://en.wikipedia.org/wiki/Statistics).\\n\\nLet me try a personal definition which may be grossly simplifying:\\n\\n - Operations Research is concerned with process modeling and optimisation\\n - Statistical Modeling is concerning with describing the so-called 'data generating process': find a model that describes something observed, and then do estimation, inference and possibly prediction.",,
1666,2,778,0102e354-b81a-490d-918b-3c33d82d4bd5,2010-07-27 13:39:35.0,447.0,"\\n\\nIm sorry, but there seems to be a confusion here: \\nBayes' Theorem  is not up for discussion of the neverending Bayesian-Frequentist debate. It is a theorem that is consistent with both schools of thought (given that it is consistent with Kolmogorov's probability axioms).\\n\\nOf course that Bayes' theorem is the core of Bayesian statistics, bit the theorem itself is universal. The clash between frequentists and Bayesians mostly pertains to how prior distributions can be defined or not.\\n\\n\\nSo, if the question is about Bayes' theorem (and not Bayesian statistics):\\n\\nBayes' theorem defines how one can calculate specific conditional probabilities. Imagine for instance that you know: the probability of somebody having symptom A, given that they have disease X p(A|X); the probability of somebody in general having disease X p(X); the probability of somebody in general having symptom A p(A). with these 3 pieces of information you can calculate the probability of somebody having disease X, given that they have sympotm A p(X|A).  \\n\\n\\ncheers\\n",,
1667,2,779,1a5297e3-0b5a-4078-93f6-8f9a7f8f95d8,2010-07-27 13:49:41.0,463.0,"I've heard that a lot of quantities that occur in nature are normally distributed. This is typically justified using the central limit theorem, which says that when you average a large number of iid random variables, you get a normal distribution. So, for instance, a trait that is determined by the additive effect of a large number of genes may be approximately normally distributed since the gene values may behave roughly like iid random variables.\\n\\nNow, what confuses me is that the property of being normally distributed is clearly not invariant under monotonic transformations. So, if there are two ways of measuring something that are related by a monotonic transformation, they are unlikely to both be normally distributed (unless that monotonic transformation is linear). For instance, we can measure the sizes of raindrops by diameter, by surface area, or by volume. Assuming similar shapes for all raindrops, the surface area is proportional to the square of the diameter, and the volume is proportional to the cube of the diameter. So all of these ways of measuring cannot be normally distributed.\\n\\nSo my question is whether the particular way of scaling (i.e., the particular choice of monotonic transformation) under which the distribution does become normal, must carry a physical significance. For instance, should heights be normally distributed or the square of height, or the logarithm of height, or the square root of height? Is there a way of answering that question by understanding the processes that affect height?",,
1668,1,779,1a5297e3-0b5a-4078-93f6-8f9a7f8f95d8,2010-07-27 13:49:41.0,463.0,Normal distribution and monotonic transformations,,
1669,3,779,1a5297e3-0b5a-4078-93f6-8f9a7f8f95d8,2010-07-27 13:49:41.0,463.0,<normality>,,
1670,2,780,4afc72c8-527f-4c07-a58c-743f8240d38d,2010-07-27 13:55:18.0,172.0,"Operations Research (OR), sometimes called ""Management Science"", consists of three main topics, Optimization, Stochastic Processes, Process and Production Methodologies. \\n\\nOR uses statistical analysis in many contexts (for example discrete event simulations) but they should not be considered the same, additionally one of the main topics in OR is the optimization (linear, and nonlinear) which can make it more clear why these two fields should be considered different \\n\\nThere is [another exchange website for OR][1] if you are interested \\n\\n\\n  [1]: http://www.or-exchange.com/questions",,
1671,2,781,640fc2c5-e28b-4f81-a046-ac02564334fa,2010-07-27 13:59:02.0,22.0,There is an interesting approach to anaonymization of strings in the context of [record linkage][1] using n-grams developed by [ANU Data Mining Group][2].\\n\\nThe paper with description and sample Python code is available [here][3]. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Record_linkage\\n  [2]: http://datamining.anu.edu.au/projects/linkage.html\\n  [3]: http://www.biomedcentral.com/1472-6947/4/9/,,
1672,2,782,ca4252a4-a65a-4954-97b6-307c1144b869,2010-07-27 14:02:37.0,223.0,I think you missunderstood (half of) the use statistician make of the normal distribution but I really like your quesstion. \\n\\nI don't think it is a good idea to assume directly normality and I  admit it is done sometime (maybe because the normal distribution is tractable unimodal ...) without verification. Hence your remark about monotonic map is excellent ! \\n\\n However the powerfull use of normality comes when you construct yourself new statistics such as the one that appears when you apply the empiriral counter part of expectation: **the empirical mean**. Hence empirical mean and more generally smoothing is what makes normality appear everywhere....  ,,
1673,2,783,d307586a-c137-4dd4-ada6-02f69063558c,2010-07-27 14:03:52.0,442.0,"""... we want to underscore that, surely, God loves the .06 nearly as much as the .05. Can there be any doubt that God views the strength of evidence for or against the null as a fairly continuous function of the magnitude of p?"" (p.1277) \\n\\nRosnow, R. L., & Rosenthal, R. (1989). Statistical procedures and the justification of knowledge in psychological science. American Psychologist, 44(10), 1276-1284. [pdf][1]\\n\\n\\n  [1]: http://socrates.berkeley.edu/~maccoun/PP279_Rosnow.pdf",,
1674,16,783,d307586a-c137-4dd4-ada6-02f69063558c,2010-07-27 14:03:52.0,-1.0,,,
1675,2,784,82d3c02a-ffe4-4555-8187-1d0d77e7fa75,2010-07-27 14:04:13.0,22.0,[Street-Fighting Mathematics][1]. The Art of Educated Guessing and Opportunistic Problem Solving  \\nby Sanjoy Mahajan from MIT\\n\\n\\n  [1]: http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=12156,,
1676,16,784,82d3c02a-ffe4-4555-8187-1d0d77e7fa75,2010-07-27 14:04:13.0,-1.0,,,
1677,2,785,080ae7d6-05f4-43c5-a42e-b4f99d4945ff,2010-07-27 14:10:31.0,447.0,“The statistician cannot evade the responsibility for understanding the\\nprocess he applies or recommends.” – Sir Ronald A. Fisher,,
1678,16,785,080ae7d6-05f4-43c5-a42e-b4f99d4945ff,2010-07-27 14:10:31.0,-1.0,,,
1679,2,786,20967d68-6321-467e-8b9a-6c6534abcde4,2010-07-27 14:14:10.0,460.0,"    The death of one man is a tragedy.  The death of millions is a statistic.\\n\\n  ~Joe Stalin, comment to Churchill at Potsdam, 1945",,
1680,16,786,20967d68-6321-467e-8b9a-6c6534abcde4,2010-07-27 14:14:10.0,-1.0,,,
1681,2,787,164e3306-8efc-4917-9313-a16de130a686,2010-07-27 14:20:42.0,460.0,"> Statistics are the triumph of the\\n> quantitative method, and the\\n> quantitative method is the victory of\\n> sterility and death.\\n\\n~ **Hillaire Belloc** *in* The Silence of the Sea",,
1682,16,787,164e3306-8efc-4917-9313-a16de130a686,2010-07-27 14:20:42.0,-1.0,,,
1683,5,786,28439662-319a-429b-8b1b-c35fe03e636d,2010-07-27 14:21:06.0,460.0,"> The death of one man is a tragedy. \\n> The death of millions is a statistic.\\n\\n  ~Joe Stalin, comment to Churchill at Potsdam, 1945",added 1 characters in body,
1684,2,788,628f2b0d-999b-49d8-a72d-8aecfc5401de,2010-07-27 14:25:39.0,460.0,"> A witty statesman said, you might\\n> prove anything by figures.\\n\\n~ Thomas Carlyle, Chartism (1839) ch. 2",,
1685,16,788,628f2b0d-999b-49d8-a72d-8aecfc5401de,2010-07-27 14:25:39.0,-1.0,,,
1686,2,789,086682bc-692a-4ccd-bbe2-330fa145be9d,2010-07-27 14:38:34.0,447.0,"[Statistical Analysis with the General Linear Model][1]\\n\\n\\n  [1]: http://psy.otago.ac.nz/miller/index.htm#GLMBook\\n\\nI covers basic linear models (ANOVA, ANCOVA, multiple regression). I can tell by personal experience that it is really really good book to get into the general framework of linear models, which are very useful in many advanced approaches (e.g., hierarchical modeling) \\n\\ncheers",,
1687,16,789,086682bc-692a-4ccd-bbe2-330fa145be9d,2010-07-27 14:38:34.0,-1.0,,,
1688,5,749,acf70106-0c49-4ebe-94c4-cfb1ab389e90,2010-07-27 14:44:54.0,339.0,"Regarding shopping cart analysis, I think that the main objective is to individuate the most frequent combinations of products bought by the customers. The `association rules` represent the most natural methodology here (indeed they were actually developed for this purpose). Analysing the combinations of products bought by the customers, and the number of times these combinations are repeated, leads to a rule of the type ‘if condition, then result’ with a corresponding interestingness measurement. You may also consider `Log-linear models` in order to investigate the associations between the considered variables.\\n\\nNow as for clustering, here are some information that may come in handy:\\n\\nAt first consider `Variable clustering`. Variable clustering is used for assessing collinearity, redundancy, and for separating variables into clusters that can be scored as a single variable, thus resulting in data reduction. Look for the `varclus` function (package Hmisc in R)\\n\\nAssessment of the clusterwise stability: function `clusterboot` {R package  fpc}\\n\\nDistance based statistics for cluster validation: function `cluster.stats` {R package  fpc}\\n\\nAs mbq have mentioned, use the silhouette widths for assessing the best number of clusters. Watch [this][1]. Regarding silhouette widths, see also the [optsil][2] function. \\n\\nEstimate the number of clusters in a data set via the [gap statistic][3]\\n\\nFor calculating Dissimilarity Indices and Distance Measures see [dsvdis][4] and [vegdist][5]\\n\\nEM clustering algorithm can decide how many clusters to create by cross validation, (if you can't specify apriori how many clusters to generate). *Although the EM algorithm is guaranteed to converge to a maximum, this is a local maximum and may not necessarily be the same as the global maximum. For a better chance of obtaining the global maximum, the whole procedure should be repeated several times, with different initial guesses for the parameter values. The overall log-likelihood figure can be used to compare the different final configurations obtained: just choose the largest of the local maxima*.\\nYou can find an implementation of the EM clusterer in the open-source project [WEKA][6]\\n\\n[This][7] is also an interesting link.\\n\\nAlso search [here][8] for `Finding the Right Number of Clusters in k-Means and EM Clustering: v-Fold Cross-Validation`\\n\\nFinally, you may explore clustering results using [clusterfly][9] \\n\\n\\n  [1]: http://www.google.com/codesearch/p?hl=en#sTQFIWS4uR8/afs/sipb/project/r-project/arch/sun4x_510/lib/R/library/cluster/R-ex/pam.object.R&q=lang:r%20%22optimal%20number%20of%20clusters%22&sa=N&cd=6&ct=rc\\n  [2]: http://finzi.psych.upenn.edu/R/library/optpart/html/optsil.html\\n  [3]: http://finzi.psych.upenn.edu/R/library/clusterSim/html/index.GAP.html\\n  [4]: http://finzi.psych.upenn.edu/R/library/labdsv/html/dsvdis.html\\n  [5]: http://finzi.psych.upenn.edu/R/library/vegan/html/vegdist.html\\n  [6]: http://www.cs.waikato.ac.nz/~ml/weka/\\n  [7]: http://zoonek2.free.fr/UNIX/48_R/06.html\\n  [8]: http://www.statsoft.com/textbook/cluster-analysis/#vfold\\n  [9]: http://had.co.nz/model-vis/",deleted 4 characters in body,
1689,2,790,1bf652e8-e97c-4a03-b69b-1c4117255bf0,2010-07-27 14:46:49.0,228.0,I am looking at a scientific paper in which a single measurement is calculated using a logarithmic mean\\n\\n> 'triplicate spots were combined to\\n> produce one signal by taking the\\n> logarithmic mean of reliable spots'\\n\\n**Why choose the log-mean?**\\n\\nAre the authors making an assumption about the underlying distribution? \\n\\nThat it is what...Log-Normal? \\n\\nHave they just picked something they thought was reasonable... i.e. between a a mean and a geometric mean?\\n\\nAny thoughts?,,
1690,1,790,1bf652e8-e97c-4a03-b69b-1c4117255bf0,2010-07-27 14:46:49.0,228.0,Why (or when) to use the log-mean?,,
1691,3,790,1bf652e8-e97c-4a03-b69b-1c4117255bf0,2010-07-27 14:46:49.0,228.0,<distributions><logarithm><mean>,,
1692,2,791,850893fc-7334-432a-a31b-cad9e7c6e12c,2010-07-27 14:52:19.0,447.0,"I really dont think so, as there are fundamental aspects in statistics that are simply overlooked in machine learning. For instance, in statistics, when fitting a model to data, the discrpeancy function that is used (e.g., G^2, RMSEA) is essential because they have different statistical properties. In machine learning, it just doesnt matter, so it is not covered at all. \\n\\nOf course one could argue that you could learn those things later, but IMHO, it is better to undestand and care about some issues, and possibly in the future not care about them, then the other way around. \\n\\ncheers",,
1693,2,792,31bae185-0ccf-4e39-882c-8beec88c51e2,2010-07-27 15:03:28.0,88.0,"Simply CLT (nor any other theorem) does not state that every quantity in the universe is normally distributed. Indeed, statisticians often use monotonic transformations to improve normality, so they could use their favorite tools.",,
1694,2,793,c605e78d-2c55-4b15-9fe4-142c5f180759,2010-07-27 15:04:58.0,3807.0,> Are the authors making an assumption\\n> about the underlying distribution?\\n\\nYou are making an assumption whether you choose to use it or whether you choose against using it.\\nFor Power Law distributions it usually makes sense to look at the logarithms.,,
1695,2,794,bba7faeb-c25c-4284-8c7f-05e0907af8d1,2010-07-27 15:05:38.0,7620.0,"> The subjectivist (i.e. Bayesian)\\n> states his judgements, whereas the\\n> objectivist sweeps them under the\\n> carpet by calling assumptions\\n> knowledge, and he basks in the\\n> glorious objectivity of science.\\n\\nI.J. Good",,John A. Ramey
1696,16,794,bba7faeb-c25c-4284-8c7f-05e0907af8d1,2010-07-27 15:05:38.0,-1.0,,,
1697,2,795,ff8b0df6-4a56-4ee1-a840-f42f6c2237c6,2010-07-27 15:08:11.0,474.0,"I have data compiled by someone else where score averages have been computed over time- averages range from 0-100. The original scores have negative values in many cases and the average would have been negative also, raw average ranges from -30 to 90.  How is this 'normalization' accomplished?  \\nThanks",,
1698,1,795,ff8b0df6-4a56-4ee1-a840-f42f6c2237c6,2010-07-27 15:08:11.0,474.0,Normalization of series ,,
1699,3,795,ff8b0df6-4a56-4ee1-a840-f42f6c2237c6,2010-07-27 15:08:11.0,474.0,<mean>,,
1700,2,796,3f5cee0c-0725-4d34-81e3-ee9f0435d830,2010-07-27 15:14:22.0,442.0,"I would like to oppose the other two answers based on a paper (in German) by [Kubinger, Rasch and Moder (2009)][1].\\n\\nThey argue, based on ""extensive"" simulations from distributions either meeting or not meeting the assumptions imposed by a t-test, (normality and homogenity of variance) that the welch-tests performs equally well when the assumptions are met (i.e., basically same probability of committing alpha and beta errors) but outperforms the t-test if the assumptions are not met, especially in terms of power. Therefore, they recommend to always use the welch-test if the sample size exceeds 30.\\n\\nAs a meta-comment: For people interested in statistics (like me and probably most other here) an argument based on data (as mine) should at least count equally as arguments solely based on theoretical grounds (as the others here).\\n\\n  [1]: http://www.psycontent.com/content/t5726m72644gq457/",,
1701,2,797,fb81f957-db1f-4746-b5d5-2c7c207b90d8,2010-07-27 15:19:43.0,,"Very good question. I feel that the answer depends on the whether you can identify the underlying process that gives rise to the measurement in question. If for example, you have evidence feel that height is a linear combination of several factors (e.g., height of parents, height of grandparents etc) then it would be natural to assume that height is normally distributed. On the other hand if you have evidence or perhaps even theory that the log of height is a linear combination of several variables (e.g., log parents heights, log of grandparents heights etc) then the log of height will be normally distributed.\\n\\nIn most situations, we do not know the underlying process that drives the measurement of interest. Thus, we can do one of several things:\\n\\n(a) If the empirical distribution of heights looks normal then we use a the normal density fur further analysis which implicitly assumes that height is a linear combination of several variables.\\n\\n(b) If the empirical distribution does not look normal then we can try some transformation as suggested by [mbq][1] (e.g. log(height)). In this case we implicitly assume that the transformed variable (i.e., log(height)) is a linear combinatio of several variables.\\n\\n(c) If (a) or (b) do not help then we have to abandon the advantages that CLT and an assumption of normality give us and model the variable using some other distribution.\\n\\n\\n  [1]: http://stats.stackexchange.com/users/88/mbq",,user28
1702,2,798,1f0fa65b-f189-47fd-bd6f-8f7c8e52e6fd,2010-07-27 15:21:48.0,476.0,"I'm interested in finding as optimal of a method as I can for determining how many bins I should use in a histogram. My data should range from 30 to 350 objects at most, and in particular I'm trying to apply thresholding (like Otsu's method) where ""good"" objects, which I should have fewer of and should be more spread out, are separated from ""bad"" objects, which should be more dense in value. A concrete value would have a score of 1-10 for each object. I'd had 5-10 objects with scores 6-10, and 20-25 objects with scores 1-4. I'd like to find a histogram binning pattern that generally allows something like Otsu's method to threshold off the low scoring objects. However, in the implementation of Otsu's I've seen, the bin size was 256, and often I have many fewer data points that 256, which to me suggests that 256 is not a good bin number. With so few data, what approaches should I take to calculating the number of bins to use?",,
1703,1,798,1f0fa65b-f189-47fd-bd6f-8f7c8e52e6fd,2010-07-27 15:21:48.0,476.0,"Calculating optimal number of bins in a histogram for n, where n ranges from 30-350",,
1704,3,798,1f0fa65b-f189-47fd-bd6f-8f7c8e52e6fd,2010-07-27 15:21:48.0,476.0,<statistics>,,
1705,5,782,33985f98-4ddb-4235-be5a-ba76730ad3f7,2010-07-27 15:24:19.0,223.0,"I think you missunderstood (half of) the use statistician make of the normal distribution but I really like your question. \\n\\nI don't think it is a good idea to assume directly normality and I  admit it is done sometime (maybe because the normal distribution is tractable, unimodal ...) without verification. Hence your remark about monotonic map is excellent ! \\n\\n However the powerfull use of normality comes when you construct yourself new statistics such as the one that appears when you apply the empiriral counter part of expectation: **the empirical mean**. Hence empirical mean and more generally smoothing is what makes normality appear everywhere...  ",deleted 1 characters in body; added 1 characters in body; deleted 1 characters in body,
1706,5,797,c9758fc1-fb30-42cc-b157-11b560ef631a,2010-07-27 15:25:00.0,,"Very good question. I feel that the answer depends on the whether you can identify the underlying process that gives rise to the measurement in question. If for example, you have evidence that height is a linear combination of several factors (e.g., height of parents, height of grandparents etc) then it would be natural to assume that height is normally distributed. On the other hand if you have evidence or perhaps even theory that the log of height is a linear combination of several variables (e.g., log parents heights, log of grandparents heights etc) then the log of height will be normally distributed.\\n\\nIn most situations, we do not know the underlying process that drives the measurement of interest. Thus, we can do one of several things:\\n\\n(a) If the empirical distribution of heights looks normal then we use a the normal density for further analysis which implicitly assumes that height is a linear combination of several variables.\\n\\n(b) If the empirical distribution does not look normal then we can try some transformation as suggested by [mbq][1] (e.g. log(height)). In this case we implicitly assume that the transformed variable (i.e., log(height)) is a linear combination of several variables.\\n\\n(c) If (a) or (b) do not help then we have to abandon the advantages that CLT and an assumption of normality give us and model the variable using some other distribution.\\n\\n\\n  [1]: http://stats.stackexchange.com/users/88/mbq",fixed typos,user28
1707,2,799,b8433666-dde9-4bd9-bb73-7b99a5ca4920,2010-07-27 15:26:34.0,447.0,"The main advantage of LME for analysing accuracy data is the ability to account for a series of random effects. In psychology experiments, researchers usually aggregate items and/or participants. Not only are people different from each other, but items also differ (some words might be more distinctive or memorable, for instance). Ignoring these sources of variability usually leads to underestimations of accuracy (for instance lower d' values). Although the participant aggregation issue can somehow be dealt with  individual estimation, the item effects are still there, and are commonly larger than participant effects. LME not only allows you to tackle both random effects simultaneously, but also to add specificy additional predictor variables (age, education level, word length, etc.) to them.\\n\\nA really good reference for LMEs, especially focused in the fields of linguistics and experimental psychology, is \\n[Analyzing Linguistic Data: A Practical Introduction to Statistics using R][1] \\n\\n\\n  [1]: http://www.amazon.com/Analyzing-Linguistic-Data-Introduction-Statistics/dp/0521709180\\n\\ncheers\\n",,
1708,2,800,3c6c74c8-9f91-40ea-9b83-7ae8d1958581,2010-07-27 15:27:18.0,,"The question is a bit unclear. But, perhaps the normalization you are looking for is this:\\n\\nNormalized Score = 100 * (Raw Score - min(Raw_score)) / (max(Raw Score) - min(Raw Score))",,user28
1709,2,801,9e62a2eb-b830-48c0-bf90-d7672a959a83,2010-07-27 15:30:31.0,210.0,"I'm not sure this counts as strictly good practise, but I tend to produce more than one histogram with different bin widths and pick the histogram which histgram to use based on which histgram fits the interpretation I'm trying to communicate best. Whilst this introduces some objectivity into the choice of histogram I justify it on the basis I have had much more time to understnad the data than the person I'm giving the histogram to so I need to give them a very concise message. \\n\\nI'm also a big fan of presenting histograms with the same number of points in each bin rather than the same bin width. I usually find these represent the data far better then the constant bin width although they are mopre difficult to produce. ",,
1710,2,802,9b25d9d3-26b7-4fba-9c20-2388808c9a36,2010-07-27 15:35:14.0,196.0,"The rescaling of a particular variable should, when possible, relate to some comprehensible scale for the reason that it helps make the resulting model interpretable.  However, the resulting transformation need not absolutely carry a physical significance.  Essentially you have to engage in a trade off between the violation of the normality assumption and the interpretability of your model.  What I like to do in these situations is have the original data, data transformed in a way that makes sense, and the data transformed in a way that is most normal.  If the data transformed in a way that makes sense is the same as the results when the data is transformed in a way that makes it most normal, I report it in a way that is interpretable with a side note that the results are the same in the case of the optimally transformed (and/or untransformed) data.  When the untransformed data is behaving particularly poorly, I conduct my analyses with the transformed data but do my best to report the results in untransformed units.\\n\\nAlso, I think you have a misconception in your statement that ""quantities that occur in nature are normally distributed"".  This only holds true in cases where the value is ""determined by the additive effect of a large number"" of independent factors.  That is, means and sums are normally distributed regardless of the underlying distribution from which they draw, where as individual values are not expected to be normally distributed.  As was of example, individual draws from a binomial distribution do not look at all normal, but a distribution of the sums of 30 draws from a binomial distribution does look rather normal.",,
1711,2,803,ff37a1b0-d29a-4b71-8f4f-bf14d41a0ea5,2010-07-27 15:36:41.0,257.0,"Or more generically,\\n\\n`Index = ( ((RawValue - Min(Raw))/(Max(Raw)-Min(Raw)) * (Max(Out)-Min(Out) ) + Min(Out)`\\n\\n\\nWhere `Raw` is the input vector, `Out` is the output vector, and `RawValue` is the value in question.  Srikant's answer is the same for an output range of 0 to 100.\\n\\nTo convert back, rearrange to get:\\n\\n`RawValue = (((Index - Min(Out))*(Max(Raw)-Min(Raw)) / (Max(Out)-Min(Out)) + Min(Raw)`",,
1712,2,804,3dc068c3-8249-41c0-bf4e-5d62bd34cedc,2010-07-27 15:43:51.0,447.0,"of course, one could ditch both tests, and start using a Bayesian t-test (Savage-Dickey ratio test), which can account for unequal and unequal variances, and best of all, it allows for a quantification of evidence in favor of the null hypothesis (which means, no more of old ""failure to reject"" talk)\\n\\nThis test is very simple (and fast) to implement, and there is a paper that clearly explains to readers unfamiliar with Bayesian statistics how to use it, along with an R script. you basically can just insert your data send the commands to the R console:\\n\\n[Wetzels, R., Raaijmakers, J. G. W., Jakab, E., & Wagenmakers, E.-J. (2009). How to Quantify Support For and Against the Null Hypothesis: A Flexible WinBUGS Implementation of a Default Bayesian t-test.][1] \\n\\nthere is also a tutorial for all this, with example data:\\n\\nhttp://www.ruudwetzels.com/index.php?src=SDtest\\n\\nI know this is not a direct response to what was asked, but I thought readers might enjoy having this nice alternative\\n\\ncheers\\n\\n  [1]: http://www.ruudwetzels.com/articles/Wetzelsetal2009_SDtest.pdf",,
1713,2,805,682dca44-ff93-47be-b59b-ff4f892042be,2010-07-27 15:43:55.0,,"I am attempting to calculate the standard error (SE) for the positive predictive value (PPV), negative predictive value (NPV), and diagnostic odds ratio (DOR) that I have obtained using the rates of true positives, false positives, true negatives, and false negatives in a sample. I am able to get 95% CIs but not SE. \\n\\nThank you!",,Jay
1714,1,805,682dca44-ff93-47be-b59b-ff4f892042be,2010-07-27 15:43:55.0,,"How do I calculated the SE for PPV, NPV, and DOR?",,Jay
1715,3,805,682dca44-ff93-47be-b59b-ff4f892042be,2010-07-27 15:43:55.0,,<error><standard>,,Jay
1716,5,782,6ce007ff-e374-4c50-ac36-701543d5667f,2010-07-27 15:44:37.0,223.0,"I think you missunderstood (half of) the use statistician make of the normal distribution but I really like your question. \\n\\nI don't think it is a good idea to assume systematically normality and I  admit it is done sometime (maybe because the normal distribution is tractable, unimodal ...) without verification. Hence your remark about monotonic map is excellent ! \\n\\n However the powerfull use of normality comes when you construct yourself new statistics such as the one that appears when you apply the empiriral counter part of expectation: **the empirical mean**. Hence empirical mean and more generally smoothing is what makes normality appear everywhere...  ",added 6 characters in body,
1717,2,806,a54d156c-eae4-4b79-8059-45d0846fa9df,2010-07-27 15:56:38.0,59.0,"Lately, there have been numerous questions about [normalization][1]\\n\\nWhat are some of the situations where you never ever ever should normalize your data, and what are the alternatives?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normalization_(statistics)",,
1718,1,806,a54d156c-eae4-4b79-8059-45d0846fa9df,2010-07-27 15:56:38.0,59.0,When should normalization never be used?,,
1719,3,806,a54d156c-eae4-4b79-8059-45d0846fa9df,2010-07-27 15:56:38.0,59.0,<normality>,,
1720,2,807,56729623-0fef-40c2-8bef-798ca23a4aa3,2010-07-27 16:00:22.0,196.0,"A statistic is any sample estimation of a population characteristic is it not?  Thus, I would suggest so long as inference from a characterized sample to a population is taking place what is occurring is, at least in part, statistics.  Under my definition machine learning would be a discipline that makes use of statistics.  For example, under many examples of machine learning it seems like part of what is happening is an attempt to take information provided to the system (a sample) in order to guess from which distribution (a population) it arose.",,
1721,2,808,60de5f06-60dd-48d6-a404-ffec08530166,2010-07-27 16:04:42.0,482.0,"A statistical analysis, properly conducted, is a delicate dissection of uncertainties, a surgery of suppositions.   - M.J. Moroney\\n",,
1722,16,808,60de5f06-60dd-48d6-a404-ffec08530166,2010-07-27 16:04:42.0,-1.0,,,
1723,4,805,a6832c77-63a7-427d-8c69-c37ce74b0195,2010-07-27 16:07:59.0,,"How do I calculate the SE for PPV, NPV, and DOR?",edited title,Jay
1724,2,809,04cc32a4-b5b8-4962-a379-e6f87ae8b2ee,2010-07-27 16:14:26.0,,"> An argument over the meaning of words\\n> is a matter of law, an argument\\n> grounded in empirical data and\\n> quantitative estimates is an argument\\n> about science.\\n\\n~ Razib Khan (though he is not a statistician or famous)\\n",,Michael Bishop
1725,16,809,04cc32a4-b5b8-4962-a379-e6f87ae8b2ee,2010-07-27 16:14:26.0,-1.0,,,
1726,5,783,1b052275-d1d6-44f2-b093-8fb375009616,2010-07-27 16:17:34.0,442.0,"> ""... we want to underscore that,\\n> surely, God loves the .06 nearly as\\n> much as the .05. Can there be any\\n> doubt that God views the strength of\\n> evidence for or against the null as a\\n> fairly continuous function of the\\n> magnitude of p?"" (p.1277)\\n\\nRosnow, R. L., & Rosenthal, R. (1989). Statistical procedures and the justification of knowledge in psychological science. American Psychologist, 44(10), 1276-1284. [pdf][1]\\n\\n\\n  [1]: http://socrates.berkeley.edu/~maccoun/PP279_Rosnow.pdf",added 19 characters in body,
1727,2,810,8004745a-c6a6-45e1-9fc9-69d861313bb6,2010-07-27 16:17:43.0,25.0,"If you use too few bins, the histogram doesn't really portray the data very well. If you have too many bins, you get a broken comb look, which also doesn't give a sense of the distribution.\\n\\nOne solution is to create a graph that shows every value. Either a dot plot, or a cumulative frequency distribution, which doesn't require any bins.  \\n\\nIf you want to create a frequency distribution with equally spaced bins, you need to decide how many bins (or the width of each). The decision clearly depends on the number of values. If you have lots of values, your graph will look better and be more informative if you have lots of bins. [This wikipedia page][1] lists several methods for deciding bin width from the number of observations. The simplest method is to set the number of bins equal to the square root of the number of values you are binning.\\n\\n[This page from Hideaki Shimazaki][2] explains an alternative method. It is a bit more complicated to calculate, but seems to do a great job. The top part of the page is a Java app. Scroll past that to see the theory and explanation, then keep scrolling to find links to the papers that explain the method.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width\\n  [2]: http://web.mit.edu/hshimaza/www/res/histogram.html#WebApp",,
1728,2,811,f3a581ab-ae9a-4eb6-89be-80fab406ef6c,2010-07-27 16:18:28.0,,> It is the mark of a truly intelligent\\n> person to be moved by statistics.\\n\\nGeorge Bernard Shaw,,Michael Bishop
1729,16,811,f3a581ab-ae9a-4eb6-89be-80fab406ef6c,2010-07-27 16:18:28.0,-1.0,,,
1730,2,812,324d2ec2-3be1-4663-9ffb-08c4e91a9144,2010-07-27 16:19:42.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two categories. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n",,
1731,1,812,324d2ec2-3be1-4663-9ffb-08c4e91a9144,2010-07-27 16:19:42.0,223.0,What is the difference between functional data analysis and high dimensional data analysis,,
1732,3,812,324d2ec2-3be1-4663-9ffb-08c4e91a9144,2010-07-27 16:19:42.0,223.0,<statistical-analysis><data-mining>,,
1733,2,813,65ad8599-06fc-4e85-b5c5-38128aab54d0,2010-07-27 16:23:12.0,,> Statistical thinking will one day be\\n> as necessary a qualification for\\n> efficient citizenship as the ability\\n> to read and write.\\n\\n--H.G. Wells,,Michael Bishop
1734,16,813,65ad8599-06fc-4e85-b5c5-38128aab54d0,2010-07-27 16:23:12.0,-1.0,,,
1735,2,814,3ad78cfd-ed1c-4ff4-9c51-d0eb905f35a7,2010-07-27 16:26:53.0,,"> These days the statistician is often\\n> asked such questions as ""Are you a\\n> Bayesian?"" ""Are you a frequentist?""\\n> ""Are you a data analyst?"" ""Are you a\\n> designer of experiments?"". I will\\n> argue that the appropriate answer to\\n> ALL of these questions can be (and\\n> preferably should be) ""yes"", and that\\n> we can see why this is so if we\\n> consider the scientific context for\\n> what statisticians do.\\n\\n--G.E.P. Box",,Michael Bishop
1736,16,814,3ad78cfd-ed1c-4ff4-9c51-d0eb905f35a7,2010-07-27 16:26:53.0,-1.0,,,
1737,2,815,1ae5fd27-07e0-49f7-939a-5978025a2555,2010-07-27 16:34:22.0,368.0,"I must admit that I do not really understand your question:\\n\\n- your raindrops example is not very satisfying since this is not illustrating the fact that the Gaussian behaviour comes from the ""average of a large number of iid random variables"".\\n\\n- if the quantity $X$ that you are interested in is an average $\\frac{Y_1+\\ldots+Y_N}{N}$ that fluctuates around its mean in a Gaussian way, you can also expect that $\\frac{f(Y_1)+\\ldots+f(Y_N)}{N}$ has a Gaussian behaviour.\\n\\n- if the fluctuation of $X$ around its mean are approximately Gaussian and small, then so are the fluctuation of $f(X)$ around its mean (by Taylor expansion)\\n\\n- could you cite some true examples of (real life) Gaussian behaviour coming from averaging: this is not very common! Gaussian behaviour is often used in statistics as a first rough approximation because the computations are very tractable. As physicists uses the harmonic approximation, statisticians uses the Gaussian approximation.",,
1738,2,816,036bb489-87cb-468d-b635-8958d9572c2a,2010-07-27 16:34:55.0,56.0,"Of course one should never try to blindly normalize data if the data does not follow a (single) normal distribution.\\n\\nFor example one might want to rescale observables X to all be normal with ![alt text][1], but this can only work if the data is normal and if both &mu; and &sigma; are the same for all data points (e.g. &sigma; doesn't depend on &mu; in a particular X range).\\n\\n\\n  [1]: http://upload.wikimedia.org/math/9/6/f/96f8910920015d409a29dfaec21daeb9.png",,
1739,2,817,e99376c0-7778-4d2c-9a38-151c4c6c7175,2010-07-27 16:37:40.0,62.0,"Vipul, you're not being totally precise in your question. \\n\\n> This is typically justified using the\\n> central limit theorem, which says that\\n> when you average a large number of iid\\n> random variables, you get a normal\\n> distribution.\\n\\n\\nI'm not entirely sure this is what you're saying, but keep in mind that the raindrops in your example are not iid random variables. The mean calculated by sampling a certain number of those raindrops is a random variables, and as the means are calculated using a large enough sample size, the distribution of that sample mean is normal. \\n\\nThe law of large numbers says that the value of that sample mean converges to the average value of the population (strong or weak depending on type of convergence). \\n\\nThe CLT says that the sample mean, call it XM(n), which is a random variable, has a distribution, say G(n). As n approaches infintity, that distribution is the normal distribution. CLT is all about *convergence in distribution*, not a basic concept. \\n\\nThe observations you draw (diameter, area, volume) don't have to be normal at all. They probably won't be if you plot them. But, the sample mean from taking all three observations will have a normal distribution. And, the volume won't be the cube of the diameter, nor will the area be the square of the diameter. The square of the sums is not going to be the sum of the squares, unless you get oddly lucky. \\n",,
1740,2,818,eee26d55-5275-40fd-b40a-90f68a9cb26a,2010-07-27 16:37:42.0,480.0,we mostly use\\n\\nggplot - for charts <br>\\nstats <br>\\ne1071 - for svm <br>\\n,,
1741,16,818,eee26d55-5275-40fd-b40a-90f68a9cb26a,2010-07-27 16:37:42.0,-1.0,,,
1742,2,819,6624cb40-8c52-4ed5-8ec3-c4ced2f03b73,2010-07-27 16:40:37.0,25.0,"The PPV and NPV are proportions. You know the numerator and denominator, which both are positive integers, so you can calculate the proportion. Your goal, I presume, is to quantify how well you have determined those values. If your sample size is huge, then those values are likely to be very close to their true population values. If the sample size is small, then there is more random variability and those values are more likely to be far from their true value. \\n\\nThe confidence intervals you calculated tell you what you want to know. Given a bunch of assumptions, you can be 95% confident that the confidence interval includes the population value. \\n\\nWhat about the standard error? The problem is that the uncertainty of a proportion is not symmetrical. The asymmetry is very noticeable when the proportion is far from 0.50 and the sample size is small. Since the confidence interval is not symmetrical around the observed proportion, trying to express this uncertainty as a single plus/minus standard error won't be very informative. It makes more sense to report the confidence interval. \\n\\nIf you really want to calculate a standard error, even knowing it isn't very helpful, here is the formula to compute an approximate standard error from p (the proportion) and n (sample size; the denominator): \\n\\n![][1]\\n\\n\\n  [1]: http://upload.wikimedia.org/math/2/8/5/285265cbedc09f97b3c98762a3433393.png",,
1743,2,820,2025463d-ada8-472b-9e8d-f0485a31c294,2010-07-27 16:42:39.0,488.0,"I am building a web application for used book trading and I am adding a feature to propose other book that would be interesting when they view an offer.\\n\\nCurrently the data that I store are the following (they are updated each time someone visit an offer) :\\n\\nISBN (book of the offer)  \\nSessId (a unique id that everyone has when visiting the website)  \\nNumberOfVisit (The number of time someone has view an offer of that book)\\n\\nI also have access to some user update data which categorize the book by subject and course. It isn't necessarily up-to-date and precise, but it's nonetheless data.\\n\\nWhat would be the best approach to list the most interesting books for a book ?",,
1744,1,820,2025463d-ada8-472b-9e8d-f0485a31c294,2010-07-27 16:42:39.0,488.0,How can I link item by relevance ?,,
1745,3,820,2025463d-ada8-472b-9e8d-f0485a31c294,2010-07-27 16:42:39.0,488.0,<algorithms>,,
1746,2,821,e6e62dcc-7e95-4e23-972e-16fd84c31e25,2010-07-27 16:46:45.0,,You probably want to look at [recommender systems][1]. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Recommender_system,,user28
1747,2,822,cf78a45d-2ded-43ce-825a-8330963c9807,2010-07-27 16:50:45.0,247.0,"There are many,many ways to do this....I'd suggest googling for some search terms to get started, such as ""market basket analysis"", or having  a look at Toby Segaran's ""Programming Collective Intelligence"" if you know python (even if you don't - it is pretty easy to understand).",,
1748,2,823,c6f0066c-ef78-4850-8c66-6ed21acd91c5,2010-07-27 16:50:51.0,25.0,"> Let’s be clear: the work of science\\n> has nothing whatever to do with\\n> consensus. Consensus is the business\\n> of politics. Science, on the contrary,\\n> requires only one investigator who\\n> happens to be right, which means that\\n> he or she has results that are\\n> verifiable by reference to the real\\n> world. In science consensus is\\n> irrelevant. What is relevant is\\n> reproducible results. The greatest\\n> scientists in history are great\\n> precisely because they broke with the\\n> consensus.\\n> \\n> There is no such thing as consensus\\n> science. If it’s consensus, it isn’t\\n> science. If it’s science, it isn’t\\n> consensus. Period.\\n\\nMichael Crichton",,
1749,16,823,c6f0066c-ef78-4850-8c66-6ed21acd91c5,2010-07-27 16:50:51.0,-1.0,,,
1750,2,824,f7f53ba5-1e14-402e-a738-bbde37cb6b7a,2010-07-27 16:56:35.0,39.0,"Bayes' theorem is a relatively simple, but fundamental result of probability theory that allows for the calculation of certain conditional probabilities.  Conditional probabilities are just those probabilities that reflect the influence of one event on the probability of another. \\n\\nSimply put, in its most famous form, it states that the probability of a theory given new evidence (called the posterior probability) is equal to the following equation of prior probabilities: the probability of the observed data given the theory, times the probability of the theory being true (prior to new evidence), divided by the probability of seeing that data.  \\n\\nThe significance of Bayes theorem is largely due to its proper use being a point of contention between schools of thought on probability.  To a subjective Bayesian (that interprets probability as being subjective degrees of belief) Bayes' theorem provides the cornerstone for theory testing, theory selection and other practices, by plugging their subjective probability judgments into the equation, and running with it.  To a frequentist (that interprets probability as [limiting relative frequencies][1]), this use of Bayes' theorem is an abuse, and they strive to instead use meaningful (non-subjective) priors.  it should be noted that there are objective Bayesians that takes priors to be objectively determinable. \\n\\nSecondarily, Bayes theorem, as well as other notions of conditional probability regularly thwart our intuitions about probability.  See the [Monty Hall Problem][2] for a classic example.\\n\\n  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Frequency_%28statistics%29\\n  [2]: http://en.wikipedia.org/wiki/Monty_Hall_problem",,
1751,5,824,a89d27e8-2709-4afc-ad96-adbf19f271b2,2010-07-27 17:02:16.0,39.0,"Bayes' theorem is a relatively simple, but fundamental result of probability theory that allows for the calculation of certain conditional probabilities.  Conditional probabilities are just those probabilities that reflect the influence of one event on the probability of another. \\n\\nSimply put, in its most famous form, it states that the probability of a theory given new evidence (called the posterior probability) is equal to the following equation: the probability of the observed data given the theory, times the probability of the theory being true (prior to new evidence), divided by the probability of seeing that data.  \\n\\nFormally, the equation looks like this:  \\n\\n![alt text][1]\\n\\nwhere,\\n\\n - H is a hypothesis, and D is the data.\\n - P(H) is the prior probability of H\\n - P(D | H) is the conditional probability of seeing the data D given that the hypothesis H is true.\\n - P(D) is the marginal probability of D.\\n - P(H | D) is the posterior probability.\\n\\n\\nThe significance of Bayes theorem is largely due to its proper use being a point of contention between schools of thought on probability.  To a subjective Bayesian (that interprets probability as being subjective degrees of belief) Bayes' theorem provides the cornerstone for theory testing, theory selection and other practices, by plugging their subjective probability judgments into the equation, and running with it.  To a frequentist (that interprets probability as [limiting relative frequencies][2]), this use of Bayes' theorem is an abuse, and they strive to instead use meaningful (non-subjective) priors.  it should be noted that there are objective Bayesians that takes priors to be objectively determinable. \\n\\nSecondarily, Bayes theorem, as well as other notions of conditional probability regularly thwart our intuitions about probability.  See the [Monty Hall Problem][3] for a classic example.\\n\\n  \\n\\n\\n  [1]: http://upload.wikimedia.org/math/3/1/3/313f04bd948b0c467d53cfc822a0fc8a.png\\n  [2]: http://en.wikipedia.org/wiki/Frequency_%28statistics%29\\n  [3]: http://en.wikipedia.org/wiki/Monty_Hall_problem",added 407 characters in body,
1752,2,825,e6d83bfb-c925-472b-874a-5b0ba5c42a6e,2010-07-27 17:04:37.0,480.0,I have R-scripts for reading large amounts of csv data from different files and then perform machine learning tasks such as svm for classification.\\n<br>\\nAre there any libraries for making use of multiple cores on the server for R.\\n<br>\\nor \\n<br>\\nWhat is most suitable way to achieve that?,,
1753,1,825,e6d83bfb-c925-472b-874a-5b0ba5c42a6e,2010-07-27 17:04:37.0,480.0,Any suggestions for making R code use multiple processors?,,
1754,3,825,e6d83bfb-c925-472b-874a-5b0ba5c42a6e,2010-07-27 17:04:37.0,480.0,<r>,,
1755,2,826,f9138837-4fa6-4089-b505-d23c0f37d5ae,2010-07-27 17:06:17.0,215.0," like to demonstrate sampling variation and essentially the Central Limit Theorem through an ""in-class"" exercise. Everybody in the class of say 100 students writes their age on a piece of paper. All pieces of paper are the same size and folded in the same fashion after I've calculated the average. This is the population and I calculate the average age. Then each student randomly selects 10 pieces of paper, writes down the ages and returns them to the bag. (S)he calculates the mean and passes the bag along to the next student. Eventually we have 100 samples of 10 students each estimating the population mean which we can describe through a histogram and some descriptive statistics.\\n\\nWe then repeat the demonstration this time using a set of 100 ""opinions"" that replicate some Yes/No question from recent polls e.g. If the (British General) election were called tomorrow would you consider voting for the British National Party. Students them sample 10 of these opinions.\\n\\nAt the end we've demonstrated sampling variation, the Central Limit Theorem, etc with both continuous and binary data.",,
1756,5,820,2df4f2c3-ba67-4450-b49b-95ffb729698d,2010-07-27 17:06:26.0,488.0,"I am building a web application for used book trading and I am adding a feature to propose other book that would be interesting when they view an offer.\\n\\nCurrently the data that I store are the following (they are updated each time someone visit an offer) :\\n\\nISBN (book of the offer)  \\nSessId (a unique id that everyone has when visiting the website)  \\nNumberOfVisit (The number of time someone has view an offer of that book)\\n\\nI also have access to some user update data which categorize the book by subject and course. It isn't necessarily up-to-date and precise, but it's nonetheless data.\\n\\nWhat are the approach to list the most interesting books for a book ?",deleted 10 characters in body; edited tags,
1757,6,820,2df4f2c3-ba67-4450-b49b-95ffb729698d,2010-07-27 17:06:26.0,488.0,<beginner><algorithms>,deleted 10 characters in body; edited tags,
1758,2,827,46e0e2bd-9d9f-462b-9a0b-8a6d52a8e0ed,2010-07-27 17:06:30.0,5.0,"If it's on Linux, then the most straight-forward is [**multicore**][1].  Beyond that, I suggest having a look at [MPI][2] (especially with the [**snow**][3] package).\\n\\nMore generally, have a look at:\\n\\n 1. The [High-Performance Computing view][4] on CRAN.\\n 2. [""State of the Art in Parallel Computing with R""][5]\\n\\nLastly, I recommend using the [foreach][6] package to abstract away the parallel backend in your code.  That will make it more useful in the long run.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/multicore/index.html\\n  [2]: http://www.stats.uwo.ca/faculty/yu/Rmpi/\\n  [3]: http://cran.r-project.org/web/packages/snow/index.html\\n  [4]: http://cran.r-project.org/web/views/HighPerformanceComputing.html\\n  [5]: http://cran.r-project.org/web/views/HighPerformanceComputing.html\\n  [6]: http://cran.r-project.org/web/packages/foreach/index.html",,
1759,5,824,0a0c623e-5f34-4eda-a1c8-ad339e643121,2010-07-27 17:09:08.0,39.0,"Bayes' theorem is a relatively simple, but fundamental result of probability theory that allows for the calculation of certain conditional probabilities.  Conditional probabilities are just those probabilities that reflect the influence of one event on the probability of another. \\n\\nSimply put, in its most famous form, it states that the probability of a hypothesis given new data (**P(H|D)**; called the posterior probability) is equal to the following equation: the probability of the observed data given the hypothesis (**P(D|H)**; called the conditional probability), times the probability of the theory being true prior to new evidence (**P(H)**; called the prior probability of H), divided by the probability of seeing that data, period (**P(D**); called the marginal probability of D).  \\n\\nFormally, the equation looks like this:  \\n\\n![alt text][2]\\n\\nThe significance of Bayes theorem is largely due to its proper use being a point of contention between schools of thought on probability.  To a subjective Bayesian (that interprets probability as being subjective degrees of belief) Bayes' theorem provides the cornerstone for theory testing, theory selection and other practices, by plugging their subjective probability judgments into the equation, and running with it.  To a frequentist (that interprets probability as [limiting relative frequencies][3]), this use of Bayes' theorem is an abuse, and they strive to instead use meaningful (non-subjective) priors.  It should be noted that there are also objective Bayesians that takes priors to be objectively determinable. \\n\\n\\n  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bayesian_probability\\n  [2]: http://upload.wikimedia.org/math/3/1/3/313f04bd948b0c467d53cfc822a0fc8a.png\\n  [3]: http://en.wikipedia.org/wiki/Frequency_%28statistics%29\\n  [4]: http://en.wikipedia.org/wiki/Monty_Hall_problem",deleted 71 characters in body; added 12 characters in body; deleted 188 characters in body; added 5 characters in body,
1760,2,828,a4d4d96f-d5c1-47ff-9439-ad18bf2852c4,2010-07-27 17:12:15.0,334.0,"Shane is correct. Both [multicore](http://cran.r-project.org/package=multicore) and [Rmpi](http://cran.r-project.org/package=Rmpi) are winners.\\n\\nSlightly broader coverage of the topic is in the [CRAN Task View on High-Performance Computing](http://cran.r-project.org/web/views/HighPerformanceComputing.html).  This also links to a fairly recent survey article on [Parallel Computing with R](http://www.jstatsoft.org/v31/i01/) from JSS.  \\n\\nLastly, a few hands-on examples and tips are in the *Intro to HPC with R* tutorial I give once in a while -- see my [presentations page](http://dirk.eddelbuettel.com) for the most recent copy from last week at useR.",,
1761,2,829,a5f0821c-0785-4ec0-b2d0-a8746b5832e4,2010-07-27 17:15:37.0,491.0,"Whether one can normalize a non-normal data set depends on the application.  For example, data normalization is required for many statistical tests (i.e. calculating a z-score, t-score, etc.)  Some tests are more prone to failure when normalizing non-normal data, while some are more resistant (""robust"" tests).  \\n\\nOne less-robust statistic is the mean, which is sensitive to outliers (i.e. non-normal data).  Alternatively, the median is less sensitive to outliers (and therefore more robust).\\n\\nA great example of non-normal data when many statistics fail is bi-modally distributed data.  Because of this, it's always good practice to visualize your data as a frequency distribution (or even better, test for normality!)",,
1762,5,824,09692473-81f0-4fbc-a7c0-f93bccd210d8,2010-07-27 17:15:57.0,39.0,"Bayes' theorem is a relatively simple, but fundamental result of probability theory that allows for the calculation of certain conditional probabilities.  Conditional probabilities are just those probabilities that reflect the influence of one event on the probability of another. \\n\\nSimply put, in its most famous form, it states that the probability of a hypothesis given new data (**P(H|D)**; called the posterior probability) is equal to the following equation: the probability of the observed data given the hypothesis (**P(D|H)**; called the conditional probability), times the probability of the theory being true prior to new evidence (**P(H)**; called the prior probability of H), divided by the probability of seeing that data, period (**P(D**); called the marginal probability of D).  \\n\\nFormally, the equation looks like this:  \\n\\n![alt text][2]\\n\\nThe significance of Bayes theorem is largely due to its proper use being a point of contention between schools of thought on probability.  To a subjective Bayesian (that interprets probability as being subjective degrees of belief) Bayes' theorem provides the cornerstone for theory testing, theory selection and other practices, by plugging their subjective probability judgments into the equation, and running with it.  To a frequentist (that interprets probability as [limiting relative frequencies][3]), this use of Bayes' theorem is an abuse, and they strive to instead use meaningful (non-subjective) priors (as do objective Bayesians under yet another interpretation of probability). \\n\\n\\n  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bayesian_probability\\n  [2]: http://upload.wikimedia.org/math/3/1/3/313f04bd948b0c467d53cfc822a0fc8a.png\\n  [3]: http://en.wikipedia.org/wiki/Frequency_%28statistics%29\\n  [4]: http://en.wikipedia.org/wiki/Monty_Hall_problem",deleted 34 characters in body,
1763,2,830,14c1a944-8d2f-41b5-86f5-10f28132bb8e,2010-07-27 17:18:36.0,447.0,"Both Shane and Dirk's responses  are spot on. \\n\\nNevertheless, you might wanna take a look at a commercial version of R, called [Revolution R][1] which is built to deal with big datasets and run on multiple cores. This software is free for academics (which might be your case, I dont know)\\n\\n\\n  [1]: http://www.revolutionanalytics.com/",,
1764,2,831,af3a02df-e317-42cb-a630-e347071a1ba3,2010-07-27 17:19:27.0,494.0,In the case of log-normally distributed data .... the geometric mean is a better measure of central tendancy than the arithmetic mean. I mean I would guess they look at the paper and have seen a log-normal distribution. \\n\\nSpots ... makes me think its referring to probes from a microarray .. in which case they do tend to do log-normally distributed (can't find the reference though sorry).,,
1765,5,812,056a8cb8-a3de-4628-bb54-c35f01e8c6ea,2010-07-27 17:36:21.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n",added 2 characters in body,
1766,2,832,8f45c166-aaff-4d81-ad10-cd4bf2bc182a,2010-07-27 17:37:35.0,,"If you use Stata, you can use the -clt- command that creates graphs of sampling distributions, see\\n\\nhttp://www.ats.ucla.edu/stat/stata/ado/teach/clt.htm\\n",,Michael Mitchell
1767,2,833,8e71e908-bfe4-44a6-a90f-22d6ff2d11fc,2010-07-27 17:44:35.0,495.0,"Yes and no. At the theoretical level, both cases can use similar techniques and frameworks (an excellent example being Gaussian process regression).\\n\\nThe critical difference is the assumptions used to prevent overfitting (regularization):\\n\\n - In the functional case, there is usually some assumption of smoothness, in other words, values occurring close to each other should be similar in some systematic way. This leads to the use of techniques such as splines, loess, Gaussian processes, etc.\\n\\n - In the high-dimensional case, there is usually an assumption of sparsity: that is, only a subset of the dimensions will have any signal. This leads to techniques aiming at identifying those dimensions (Lasso, LARS, slab-and-spike priors, etc.)",,
1768,2,834,9ae61f9d-6ed1-4a2e-b444-fc2a24aac210,2010-07-27 17:46:25.0,498.0,"I've heard that AIC can be used to choose among several models (which regressor to use).\\n\\nBut i would like to understand formally what it is in a kind of ""advanced undergraduated"" level,which i think would be something formal but with intuition arising from the formula.\\n\\nAnd is it possible to implement AIC in stata with complex survey data?\\n\\nThank you very much in advance!!\\n\\nRegards!\\n",,
1769,1,834,9ae61f9d-6ed1-4a2e-b444-fc2a24aac210,2010-07-27 17:46:25.0,498.0,What is AIC? Looking for a formal but intuitive answer...,,
1770,3,834,9ae61f9d-6ed1-4a2e-b444-fc2a24aac210,2010-07-27 17:46:25.0,498.0,<aic><surveys>,,
1771,2,835,871a8db2-ec84-4377-b2c9-9c5f79bfc980,2010-07-27 17:57:48.0,62.0,"[Data is the sword of the 21st century, those who wield it well, the Samurai.][1]\\n\\n\\n  [1]: http://googleblog.blogspot.com/2009/02/from-height-of-this-place.html",,
1772,16,835,871a8db2-ec84-4377-b2c9-9c5f79bfc980,2010-07-27 17:57:48.0,-1.0,,,
1773,5,729,a48a2abe-31b0-4466-82ce-9f74e8c4f3d6,2010-07-27 18:06:47.0,218.0,In God we trust. All others must bring data. (W. Edwards Deming),corrected quote,
1774,5,658,879278e9-b26d-41ff-bda7-3b58621eddcd,2010-07-27 18:06:52.0,88.0,What I loved most with CLT is the cases when it is not applicable -- this gives me a hope that the life is a bit more interesting that Gauss curve suggests. So show him the Cauchy distribution.,Fixed due to Baltimark suggestion. ,
1775,5,732,353bc74d-64f3-458c-84b0-452cdcdb0646,2010-07-27 18:08:19.0,223.0,"""Strange events permit themselves the luxury of occurring. ""\\n\\n-- [Charlie Chan][1]\\n\\n\\n  [1]: http://gutenberg.net.au/ebooks02/0200691.txt",added 1 characters in body,
1776,2,836,1c10e40f-6db4-428c-bc5d-4c92cbd6b81a,2010-07-27 18:14:44.0,334.0,"It is a heuristic, and as such, has been subjected to *extensive* testing.  So when to trust it or not is not simple clear-cut and always-true decision.\\n\\nAt a rough approximation, it trades off goodness of fit and number of variables (""degrees of freedom""). Much more, as usual, [at the Wikipedia article about AIC](http://en.wikipedia.org/wiki/Akaike_information_criterion).",,
1777,5,812,79733546-2c41-4794-819f-f0ff6286b93b,2010-07-27 18:16:34.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT: Simon Byrne mentioned something interesting in his answer: \\n\\n - sparsity (S-sparse assumption, l^p ball and weak l^p ball for p<1) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used in functional data analysis. \\n\\nthe point  (that I forgot to mention, I admit) is  that inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity... ",added 481 characters in body,
1778,6,834,e3120062-f493-415f-be63-24b96450144b,2010-07-27 18:21:09.0,88.0,<beginner><aic>,edited tags,
1779,2,837,ebdfb401-e480-44d0-b17e-ed0326957b36,2010-07-27 18:25:40.0,501.0,"I am creating multiple logistic regression models using lrm from Harrell's Design package in R.  One model I would like to make is the model with no predictors.  eg I want to predict a constant c such that \\n\\nlogit(Y) ~ c\\n\\nI know I how to compute c (divide the number of ""1""s by the total), what I would like is to use lrm so I can manipulate it as a model in a consistent way with the other models I am making.  Is this possible, and if so how?  \\n\\nI have tried so far\\n\\n    library(Design)\\n    data(mtcars)\\n    lrm(am ~ 1, data=mtcars)\\n\\nwhich gives the error:\\n\\n    Error in dimnames(stats) <- list(names(cof), c(""Coef"", ""S.E."", ""Wald Z"",  :\\n        length of 'dimnames' [1] not equal to array extent\\n\\nand I have tried\\n\\n    lrm(am ~ ., data=mtcars)\\n\\nBut this uses all the predictors, rather then none of the predictors.\\n\\nThanks in advance\\n    \\n\\n\\nTAGS: R logistic-regression Design",,
1780,1,837,ebdfb401-e480-44d0-b17e-ed0326957b36,2010-07-27 18:25:40.0,501.0,R lrm model with no predictors,,
1781,3,837,ebdfb401-e480-44d0-b17e-ed0326957b36,2010-07-27 18:25:40.0,501.0,<r><linear-regression>,,
1782,2,838,5d5740ca-09c6-43c9-9c63-da84621e5481,2010-07-27 18:28:36.0,288.0,"Basically one needs a loss function in order to optimize anything. AIC provides the loss function which when minimized gives a ""optimal""* model which fits the given data. The AIC loss function (2k-2*log(L)) tries to formulate the bias variance trade off that every statistical modeler faces when fitting a model to finite set of data.\\n\\nIn other words while fitting a model if you increase the number of parameters you will improve the log likelihood but will run into the danger of over fitting. The AIC penalizes for increasing the number of parameters thus minimizing the AIC selects the model where the improvement in log likelihood is not worth the penalty for increasing the number of parameters.\\n\\n* Note that when I say optimal model it is optimal in the sense that the model minimizes the AIC. There are other criteria (e.g. BIC) which may give other ""optimal"" models.  \\n\\nI don't have any experience with stata so cannot help you with the other part of the question. ",,
1783,6,837,fb8bdfd5-2d60-4587-9279-1effd30d162c,2010-07-27 18:31:23.0,,<r><logistic>,edited tags,user28
1784,5,812,d6db494a-c6ba-4585-baf7-21b3c1d7b6f9,2010-07-27 18:32:56.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT: Simon Byrne mentioned something interesting in his answer: \\n\\n - sparsity (S-sparse assumption, l^p ball and weak l^p ball for p<1) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used as a structural assumption in functional data analysis. \\n\\nthe point  (that I forgot to mention, I admit) is  that inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity... ",added 27 characters in body,
1785,2,839,82971066-a720-4722-ba5a-8b17d7003283,2010-07-27 18:33:51.0,39.0,"Though it is not generally labeled as Bayesian search theory, such methods are pretty widely used in oil exploration.  There are, however, important differences in the standard examples that drive different features of their respective modeling problems.  \\n\\nIn the case of lost vessel exploration (in Bayesian search theory), we are looking for a specific point on the sea floor (one elevation), with a  distributions modeling the likelihood of its resting location, and another distribution modeling the likelihood of finding the boat were it at that depth.  These distributions are then guide search, and are continuously updated through the results of the guided search.\\n\\nThough similar, oil exploration is fraught with complicating features (multiple sampling depths, high sampling costs, variable yields, multiple geological indicators, drilling cost, etc.) that necessitate methods that go beyond what is considered in the prior example.  See [Learning through Oil and Gas Exploration][1] for an overview of these complicating factors and a way to model them.\\n\\nSo, yes, it may be said that the oil exploration problem is different in magnitude, but not kind from lost vessel exploration, and thus similar methods may be fruitfully applied.   \\n  [1]: http://www.nek.lu.se/ryde/NordicEcont09/Papers/levitt.pdf",,
1786,2,840,2210cd89-ca24-40a4-b9a2-9f9bc4499119,2010-07-27 18:33:56.0,288.0,I work with both R and Matlab and I use [R.matlab](http://cran.r-project.org/web/packages/R.matlab/index.html) a lot to transfer data between the two.,,
1787,16,840,2210cd89-ca24-40a4-b9a2-9f9bc4499119,2010-07-27 18:33:56.0,-1.0,,,
1788,2,841,cb49f841-a96c-4353-bb07-62d237586ec8,2010-07-27 18:38:02.0,30.0,"I have N paired observations (X_i, Y_i) drawn from a common unknown distribution, which has finite first and second moments, and is symmetric around the mean.\\n\\nLet sigma_X the standard deviation of X (unconditional on Y), and sigma_Y the same for Y. I would like to test the hypothesis  \\n\\nH_0: sigma_X = sigma_Y\\n\\nH_a: sigma_X != sigma_Y\\n\\n\\nDoes anyone know of such a test? I can assume in first analysis that the distribution is normal, although the general case is more interesting. I am looking for a closed-form solution. Bootstrap is always a last resort.",,
1789,1,841,cb49f841-a96c-4353-bb07-62d237586ec8,2010-07-27 18:38:02.0,30.0,Comparing the variance of paired observations,,
1790,3,841,cb49f841-a96c-4353-bb07-62d237586ec8,2010-07-27 18:38:02.0,30.0,<standard-deviation><hypothesis-testing><distributions>,,
1791,5,839,26cc8dbc-d9ca-4f3e-8059-4f2659032fc9,2010-07-27 18:39:12.0,39.0,"Though it is not generally labeled as Bayesian search theory, such methods are pretty widely used in oil exploration.  There are, however, important differences in the standard examples that drive different features of their respective modeling problems.  \\n\\nIn the case of lost vessel exploration (in Bayesian search theory), we are looking for a specific point on the sea floor (one elevation), with a  distributions modeling the likelihood of its resting location, and another distribution modeling the likelihood of finding the boat were it at that depth.  These distributions are then guide search, and are continuously updated through the results of the guided search.\\n\\nThough similar, oil exploration is fraught with complicating features (multiple sampling depths, high sampling costs, variable yields, multiple geological indicators, drilling cost, etc.) that necessitate methods that go beyond what is considered in the prior example.  See [Learning through Oil and Gas Exploration][1] for an overview of these complicating factors and a way to model them.\\n\\nSo, yes, it may be said that the oil exploration problem is different in magnitude, but not kind from lost vessel exploration, and thus similar methods may be fruitfully applied. Finally, a quick literature search reveals many different modeling approaches, which is not too surprising, given the complicated nature of the problem.\\n\\n  [1]: http://www.nek.lu.se/ryde/NordicEcont09/Papers/levitt.pdf",added 152 characters in body,
1792,2,842,2ec8fb67-4565-4b2f-9427-9e9b327a1ad2,2010-07-27 18:39:32.0,88.0,"I noticed that the previous answers lack some general HPC considerations.  \\nFirst of all, neither of those packages will enable you to run **one** SVM in parallel. So what you can speed up is parameter optimization or cross-validation, still you must write your own functions for that. Or of course you may run the job for different datasets in parallel, if it is a case.  \\nThe second issue is memory; if you want to spread calculation over a few physical computers, there is no free lunch and you must copy the data -- here you must consider if it makes sense to predistribute a copy of data across computers to save some communication. On the other hand if you wish to use multiple cores on one computer, than the multicore is especially appropriate because it enables all child processes to access the memory of the parent process, so you can save some time and a lot of memory space.",,
1793,2,843,56ed4e38-04ce-4649-a874-0e829f304065,2010-07-27 18:47:25.0,56.0,"If I need to determine the number of bins programmatically I usually start out with a histogram that has way more bins than needed. Once the histogram is filled I then combine bins until I have enough entries per bin for the method I am using, e.g. if I want to model Poisson-uncertainties in a counting experiment with uncertainties from a normal distribution until I have more than something like 10 entries.",,
1794,2,844,c0cdef8c-13d9-4330-be78-321e4f819906,2010-07-27 18:48:54.0,,"You could use the fact that the [distribution of the sample variance][1] is a chi square distribution centered at the true variance. Under your null hypothesis, your test statistic would be the difference of two chi squared random variates centered at the same unknown true variance. I do not know whether the difference of two chi-squared random variates is an identifiable distribution but the above may help you to some extent.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance#Distribution_of_the_sample_variance",,user28
1795,2,845,266a0db8-1e60-4735-b746-bdae95d33ec8,2010-07-27 19:02:09.0,8.0,If you want to go down the non-parametric route you could always try the squared ranks test. The assumptions for this test (taken from [here][1]) are:\\n\\n1. Both samples are random samples from their respective populations.\\n1. In addition to independence within each sample there is mutual independence between\\nthe two samples.\\n1. The measurement scale is at least interval.\\n\\n\\nThese [lecture notes][1] should get started.\\n\\n\\n  [1]: http://www.stat.wvu.edu/~xiawang/courses/stat551/hwk/hw14.pdf,,
1796,5,780,702c20e2-9a18-4a40-b51c-527a965407b8,2010-07-27 19:10:25.0,172.0,"Operations Research (OR), sometimes called ""Management Science"", consists of three main topics, Optimization, Stochastic Processes, Process and Production Methodologies. \\n\\nOR uses statistical analysis in many contexts (for example discrete event simulations) but they should not be considered the same, additionally one of the main topics in OR is optimization (linear, and nonlinear) which can make it more clear why these two fields should be considered different \\n\\nThere is [another exchange website for OR][1] if you are interested \\n\\n\\n  [1]: http://www.or-exchange.com/questions",deleted 4 characters in body,
1797,2,846,7d537deb-b89e-4dfc-a980-6271fa3a969b,2010-07-27 19:10:53.0,503.0,I'm working on regression models in STATISTICA application and I need to know what is Fisher-Snedecor distribution for and how to analyze my regression model in this distribution. What the significance level means? What is v1 and v2? I need an explanation and little tutorial on real data.\\n\\nThanks in advance!\\n,,
1798,1,846,7d537deb-b89e-4dfc-a980-6271fa3a969b,2010-07-27 19:10:53.0,503.0,Regression output and Fisher-Snedecor distribution,,
1799,3,846,7d537deb-b89e-4dfc-a980-6271fa3a969b,2010-07-27 19:10:53.0,503.0,<untagged>,,
1800,5,456,47e8b8b8-19b2-4c19-8d42-5b4fd2770875,2010-07-27 19:11:57.0,39.0,"As the [Encyclopedia of GIS][1] states, the conditional autoregressive model (CAR) is appropriate for situations with first order dependency or relatively local spatial autocorrelation, and simultaneous autoregressive model (SAR) is  more suitable where there are second order dependency or a more global spatial autocorrelation.\\n\\nThis is made clear by the fact that CAR obeys the spatial version of the [Markov property][2], namely it assumes that the state of a particular area is influenced its neighbors and not neighbors of neighbors, etc. (i.e. it is spatially “memoryless”, instead of temporally), whereas SAR does not assume such.  This is due to the different ways in which they specify their variance-covariance matrixes.  So, when the spatial Markov property obtains, CAR provides a simpler way to model autocorrelated geo-referenced areal data.\\n\\nSee [Gis And Spatial Data Analysis: Converging Perspectives][3] for more details.\\n\\n\\n  [1]: http://books.google.com/books?id=6q2lOfLnwkAC&dq=Encyclopedia+of+GIS\\n  [2]:  http://en.wikipedia.org/wiki/Markov_property\\n  [3]: http://www.geog.ucsb.edu/~good/papers/387.pdf",added 71 characters in body,
1801,2,847,a356e4a3-7f58-40cc-982f-34f2e03c9c39,2010-07-27 19:12:53.0,500.0,"GWO = Google Website Optimizer\\n\\nGWO is a tool provided by Google to do [A/B](http://en.wikipedia.org/wiki/A/B_testing) and [MVT](http://en.wikipedia.org/wiki/Multivariate_testing) experiments on websites.\\n\\nThis has been an unanswered question for a long time so I thought I'd ask it here and see if I can get any help. Here is some documentation (clues) that Google has published about the statistics used:\\n\\n - [All about statistics](http://www.google.com/support/websiteoptimizer/bin/answer.py?hl=en&answer=61146#stats)\\n - [Fractional versus Full Factorial analysis](http://www.google.com/support/websiteoptimizer/bin/answer.py?hl=en&answer=74818)\\n\\nExample Data ([summary pdf](http://dl.dropbox.com/u/2511052/gwoTestResults/WebOptimizer-summary.pdf)):\\n\\n - [XML](http://dl.dropbox.com/u/2511052/gwoTestResults/WebsiteOptimizer.xml)\\n - [CSV](http://dl.dropbox.com/u/2511052/gwoTestResults/WebsiteOptimizer.csv)\\n\\nI (as well as many others) are interested in discivering the math used by Google, becuase no one has been able to reproduce their calculations yet.\\n\\nThank you in advance for any help that you can provide.",,
1802,1,847,a356e4a3-7f58-40cc-982f-34f2e03c9c39,2010-07-27 19:12:53.0,500.0,What statistical model is used to calculate the test results for GWO?,,
1803,3,847,a356e4a3-7f58-40cc-982f-34f2e03c9c39,2010-07-27 19:12:53.0,500.0,<statistical-analysis><statistics>,,
1804,2,848,9eba3f3f-ba62-440c-a4cd-e758e5336309,2010-07-27 19:19:46.0,,"As an addition to Harvey's response if you have a small sample of data then there are alternatives to the standard error approximation formula shown above. I would recommend using software, one example would be to use the `binom.test` function in R, to calculate these confidence intervals rather than doing it ""by hand"".",,Ralph
1805,2,849,72fb939c-1619-425f-8f28-c4432b0c704d,2010-07-27 19:22:33.0,485.0,"Principal Components Analysis (PCA) and Common Factor Analysis (CFA) are distinct methods.  Often, they produce similar results and PCA is used as the default extraction method in the SPSS Factor Analysis routines.  This undoubtedly results in a lot of confusion about the distinction between the two.\\n\\nThe bottom line is, these are two different models, conceptually.  In PCA, the components are actual orthogonal linear combinations that maximize the total variance.  In FA, the factors are linear combinations that maximize the shared portion of the variance--underlying ""latent constructs"".  That's why FA is often called ""common factor analysis"".  FA uses a variety of optimization routines and the result, unlike PCA, depends on the optimization routine used and starting points for those routines.  Simply there is not a single unique solution.\\n\\nIn R, the factanal() function provides CFA with a maximum likelihood extraction.  So, you shouldn't expect it to reproduce and SPSS result which is based on a PCA extraction.  It's simply not the same model or logic.  I'm not sure if you would get the same result if you used SPSS's Maximum Likelihood extraction either as they may no both use the same algorithm.  \\n\\nFor better or for worse in R, you can, however, reproduce the mixed up ""factor analysis"" that SPSS provides as its default.  Here's the process in R.  With this code, I'm able to reproduce the SPSS Principal Component ""Factor Analysis"" result using this dataset.  (With the exception of the sign, which is indeterminant).  That result could also then be rotated using any of Rs available rotation methods.\\n\\n    # Load the base dataset attitude to work with.\\n    data(attitude)\\n    # Compute eigenvalues and eigen vectors of the correlation matrix.\\n    pfa.eigen<-eigen(cor(attitude))\\n    # Print and note that eigen values are those produced by SPSS.\\n    # Also note that SPSS will extract 2 components as eigen values > 1 = 2\\n    pfa.eigen$values\\n    # set a value for the number of factors (for clarity)\\n    factors<-2\\n    # Extract and transform two components.\\n    pfa.eigen$vectors [ , 1:factors ]  %*% \\n    + diag ( sqrt (pfa.eigen$values [ 1:factors ] ),factors,factors )\\n\\n\\n    \\n      ",,
1806,5,849,9c4b5ced-5192-444c-babc-c4b8c29387cf,2010-07-27 19:27:45.0,485.0,"Principal Components Analysis (PCA) and Common Factor Analysis (CFA) are distinct methods.  Often, they produce similar results and PCA is used as the default extraction method in the SPSS Factor Analysis routines.  This undoubtedly results in a lot of confusion about the distinction between the two.\\n\\nThe bottom line is, these are two different models, conceptually.  In PCA, the components are actual orthogonal linear combinations that maximize the total variance.  In FA, the factors are linear combinations that maximize the shared portion of the variance--underlying ""latent constructs"".  That's why FA is often called ""common factor analysis"".  FA uses a variety of optimization routines and the result, unlike PCA, depends on the optimization routine used and starting points for those routines.  Simply there is not a single unique solution.\\n\\nIn R, the factanal() function provides CFA with a maximum likelihood extraction.  So, you shouldn't expect it to reproduce an SPSS result which is based on a PCA extraction.  It's simply not the same model or logic.  I'm not sure if you would get the same result if you used SPSS's Maximum Likelihood extraction either as they may not use the same algorithm.  \\n\\nFor better or for worse in R, you can, however, reproduce the mixed up ""factor analysis"" that SPSS provides as its default.  Here's the process in R.  With this code, I'm able to reproduce the SPSS Principal Component ""Factor Analysis"" result using this dataset.  (With the exception of the sign, which is indeterminant).  That result could also then be rotated using any of Rs available rotation methods.\\n\\n    # Load the base dataset attitude to work with.\\n    data(attitude)\\n    # Compute eigenvalues and eigen vectors of the correlation matrix.\\n    pfa.eigen<-eigen(cor(attitude))\\n    # Print and note that eigen values are those produced by SPSS.\\n    # Also note that SPSS will extract 2 components as eigen values > 1 = 2\\n    pfa.eigen$values\\n    # set a value for the number of factors (for clarity)\\n    factors<-2\\n    # Extract and transform two components.\\n    pfa.eigen$vectors [ , 1:factors ]  %*% \\n    + diag ( sqrt (pfa.eigen$values [ 1:factors ] ),factors,factors )\\n\\n\\n    \\n      ",deleted 1 characters in body; deleted 4 characters in body,
1807,2,850,d0e420ed-f9e3-4902-b219-f70cab6371df,2010-07-27 19:28:38.0,88.0,Try creating a dummy variable with so many ones as the number of rows in mtcars and put it formula instead of one.,,
1808,6,846,05ccbdcc-12a9-493d-9e62-42bc0fe76dbc,2010-07-27 19:30:35.0,88.0,<beginner>,edited tags,
1809,2,851,d56780b3-c78e-4636-be66-4399ca1d6fb8,2010-07-27 19:34:06.0,8.0,"The Fisher Snedecor distribution is another name for the [F-distribution][1]. The F-distribution comes from the ratio of two chi-squared variables.\\n\\nIn regression, you use the F-distribution in the [ANOVA][2] table. This table gives you information about which variables/covariates should be in your model.\\n\\nThe v1 and v2 are called the degrees of freedom. In simple linear regression,o.e.\\n\\nY = a + bx + epsilon\\n\\nwe have a single extra variable. So v1 = p = 1 and v2 = n-p-1 = n-2.\\n\\nI don't use STATISTICA, so I can't give you more detailed information.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/F-distribution\\n  [2]: http://en.wikipedia.org/wiki/Analysis_of_variance",,
1810,5,812,03033cf3-97b9-4900-9fe8-3aa246641438,2010-07-27 19:35:36.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT: Simon Byrne mentioned something interesting in his answer: \\n\\n - sparsity (S-sparse assumption, l^p ball and weak l^p ball for p<1) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used as a structural assumption in functional data analysis. \\n\\nthe point  (that I did not mention, I admit) is  that inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity... this make the critical difference mentionned by Simon not so critical?",added 68 characters in body,
1812,6,847,c2fc2ad0-2201-43b6-afb4-95bcf450245d,2010-07-27 19:48:13.0,8.0,<statistical-analysis><statistics><google>,edited tags,
1813,2,852,ec081c15-7db7-45c4-958e-c88a18a95143,2010-07-27 19:52:26.0,,The coefficient on a logged explanatory variable when the dependent variable also is in log form is an elasticity (or the percentage change in the dependent variable if the explanatory variable changes by one percent).  Suppose I estimate a regression without logging the dependent variable but I use a log link in a General Linear Model (and family gausian) while the explanatory variable remains in log form.  Is the coefficient on that explanatory variable still an elasticity?,,David Jacobs
1814,1,852,ec081c15-7db7-45c4-958e-c88a18a95143,2010-07-27 19:52:26.0,,Elasticities Using GLM,,David Jacobs
1815,3,852,ec081c15-7db7-45c4-958e-c88a18a95143,2010-07-27 19:52:26.0,,<econometrics>,,David Jacobs
1816,2,853,d56f2be2-37dd-4593-9938-77f7923dc0b9,2010-07-27 19:55:30.0,74.0,It sounds like they simply use Choice Modeling. You would use conditional logistic regression to analyze it. \\n\\nThe people from JMP (SAS) have a nice (free) book about Design of Experiments and Choice Modeling.,,
1817,2,854,fac7f14e-e78f-4022-8b48-36a986518610,2010-07-27 19:58:44.0,506.0,"I would use **Fisher's Exact Test**, even for large N. I wouldn't know why not. Any performance argument predates today's fast computers.",,
1818,2,855,76a8f98a-00c6-4073-8335-a2c8e0fe2f0f,2010-07-27 20:05:20.0,8.0,"I thought this was too obvious, until I saw this [question][1]!\\n\\nWhen you normalise data, make sure you always have access to the raw data after normalisation. Of course, you could break this rule if you have a good reason, e.g. storage.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/795/normalization-of-series",,
1819,6,852,406c8985-3e1c-4fe2-97e3-7bd865bc1087,2010-07-27 20:14:28.0,88.0,<econometrics><generalized-linear-model>,edited tags,
1820,2,856,0f17fd2d-8c80-4e9f-a08f-466a3ae420d8,2010-07-27 20:24:45.0,506.0,"Does anyone know of a variation of Fisher's Exact Test which takes weights into account?  \\nSo instead of the usual 2x2 cross table, every data point has a ""mass"" or ""size"" value weighing the point.\\n\\nExample data:\\n\\n    A B weight\\n    N N 1\\n    N N 3\\n    Y N 1\\n    Y N 2\\n    N Y 6\\n    N Y 7\\n    Y Y 1\\n    Y Y 2\\n    Y Y 3\\n    Y Y 4\\n\\nFisher's Exact Test then uses this 2x2 cross table:\\n\\n    A\\B  N  Y All\\n     N   2  2   4\\n     Y   2  4   6\\n    All  4  6  10\\n\\nIf we would take the weight as an 'actual' number of data points, this would result in:\\n\\n    A\\B  N  Y All\\n     N   4 13  17\\n     Y   3 10  13\\n    All  7 23  30\\n\\nBut that would result in much too high a confidence. One data point changing from N/Y to N/N would make a very large difference in the statistic.\\n\\nAny pointer would be appreciated.",,
1821,1,856,0f17fd2d-8c80-4e9f-a08f-466a3ae420d8,2010-07-27 20:24:45.0,506.0,Fisher's Exact Test with weights?,,
1822,3,856,0f17fd2d-8c80-4e9f-a08f-466a3ae420d8,2010-07-27 20:24:45.0,506.0,<hypothesis-testing>,,
1823,2,857,7ef2a590-cf12-4b70-b699-7a92d43cbfb2,2010-07-27 20:25:16.0,482.0,"Operations Research began during wartime in the 1940s with scientists and others addressing problems in Radar operations, Anti-Submarine Warfare (ASW), and air operations. It is really a methodology to help decision makers choose a course of action by using an analytic framework that includes statistics, linear and non-linear programming, game theory, decision theory, etc. Statistics is one of many tools it uses.\\n",,
1824,2,858,89839ef9-cccb-4d80-9024-90a499d063d1,2010-07-27 20:33:48.0,485.0,"I'm not sure why this doesn't work with lrm.  However, R does Logistic Regression just fine with its own internal functions.  See GLM.  Here's your model, working...\\n\\nsummary(glm(am~1, data = mtcars, family=binomial(link=logit)))\\n\\nSo, unless you need something that lrm() from design provides, then use GLM with the binomial logit link.",,
1825,5,856,63d35723-70cd-4b78-8a4a-4e72db36e93a,2010-07-27 20:42:24.0,506.0,"Does anyone know of a variation of Fisher's Exact Test which takes weights into account?  \\nSo instead of the usual 2x2 cross table, every data point has a ""mass"" or ""size"" value weighing the point.\\n\\nExample data:\\n\\n    A B weight\\n    N N 1\\n    N N 3\\n    Y N 1\\n    Y N 2\\n    N Y 6\\n    N Y 7\\n    Y Y 1\\n    Y Y 2\\n    Y Y 3\\n    Y Y 4\\n\\nFisher's Exact Test then uses this 2x2 cross table:\\n\\n    A\\B  N  Y All\\n     N   2  2   4\\n     Y   2  4   6\\n    All  4  6  10\\n\\nIf we would take the weight as an 'actual' number of data points, this would result in:\\n\\n    A\\B  N  Y All\\n     N   4 13  17\\n     Y   3 10  13\\n    All  7 23  30\\n\\nBut that would result in much too high a confidence. One data point changing from N/Y to N/N would make a very large difference in the statistic.  \\nPlus, it wouldn't work if any weight contained fractions.\\n\\nAny pointer would be appreciated.",added 61 characters in body,
1826,5,680,a7775a55-9e75-4133-b917-a8585506706b,2010-07-27 20:57:38.0,74.0,"If you have enormous data sets - ones that make Excel or Notepad load slowly, then a database is a good way to go. MS SQL Server Express is free, and it's easy to connect with JMP and other programs. You may want to sample in this case. You don't have to normalize the data in the database. Otherwise, CSV is sharing-friendly. \\n\\nIn terms of analysis, here are some starting points:\\n\\n- Describe one variable:\\n\\nHistogram  \\nSummary statistics (mean etc)\\n\\n- Describe relationship between variables:\\n\\nScatter Plot  \\nCorrelation  \\nMosaic plot for categorical  \\nContingency table for categorical  \\nAssociation Analysis (for huge binary datasets)  \\n\\n- Predict a real number: regression (OLS regression or other machine learning regression techniques)\\n\\n- Predict a whole number: classification (logistic regression or other machine learning techniques)\\n\\n- Put observations into ""natural"" groups: clustering\\n\\n- Put attributes into ""natural"" groups: factoring\\n\\n- Quantifying Risk = Standard Deviation, or proportion of times that ""bad things"" happen\\n\\n- Likelihood of a successfully completed iteration given x number of story points = Logistic Regression\\n\\nGood luck!",added 64 characters in body,
1827,2,859,3fcd7071-81f6-41b5-ba0f-096ffdd1e51a,2010-07-27 21:07:20.0,445.0,"Sorry for the verbose background to this question:\\n\\nOccasionally in investigations of animal behaviour, an experimenter is interested in the amount of time that a subject spends in different, pre-defined zones in a test apparatus. I've often seen this sort of data analyzed using ANOVA; however, I have never been entirely convinced of the validity of such analyses, given that ANOVA assumes the observations are independent, and they never actually are independent in these analyses (since more time spent in one zone means that less is spent in other zones!). \\n\\nFor example,\\nD. R. Smith, C. D. Striplin, A. M. Geller, R. B. Mailman, J. Drago, C. P. Lawler, M. Gallagher, Behavioural assessment of mice lacking D1A dopamine receptors, Neuroscience, Volume 86, Issue 1, 21 May 1998, Pages 135-146\\n\\n(http://www.sciencedirect.com/science/article/B6T0F-3THY168-F/2/007a74da6fbe693e297e50ec91dec141)\\n\\nIn the above article, they reduce the degrees of freedom by 1 in order to compensate for the non-independence. However, I am not sure how such a manipulation can actually ameliorate this violation of ANOVA assumptions. \\n\\nPerhaps a chi-squared procedure might be more appropriate? What would you do to analyze data like this (preference for zones, based on time spent in zones)?\\n\\nThanks!",,
1828,1,859,3fcd7071-81f6-41b5-ba0f-096ffdd1e51a,2010-07-27 21:07:20.0,445.0,ANOVA with Non-Independent Observations,,
1829,3,859,3fcd7071-81f6-41b5-ba0f-096ffdd1e51a,2010-07-27 21:07:20.0,445.0,<anova>,,
1830,2,860,2463f148-f2d8-4903-b6e6-52969f84af67,2010-07-27 21:51:43.0,511.0,"Let f be your true distribution, and g the family from which you are trying to fit your data. Then theta, maximum likelihood estimator of parameters of g, is a random variable. You could formulate model selection as finding distribution family g that minimizes expected KL divergence between f and g(\\theta), which can be written as\\n\\nEntropy(f)-E_x E_y[log(g(x|theta(y)))]\\n\\nSince you are minimizing over g's, Entropy(f) term doesn't matter and you look for g that maximizes E_x E_y[log(g(x|theta(y)))]\\n\\nLet L(\\theta(y)|y) be the likelihood of data y according to g(\\theta). You could estimate E_x E_y[log(g(x|theta(y)))] as log(L(\\theta(y)|y)) but that estimator is biased\\n\\nAkaike's showed that when f belongs to family g with dimension k, the following estimator is unbiased\\n\\nlog(L(theta(y)|y))-k\\n\\nBurnham has more details in this <a href=""http://faculty.washington.edu/skalski/classes/QERM597/papers_xtra/Burnham%20and%20Anderson.pdf"">paper</a>",,
1831,2,861,ed482cdf-20f5-42be-9a90-44be030eaaec,2010-07-27 22:33:05.0,481.0,There is a free book on Geostatistical Mapping with R [here][1] it might help your problem.\\n\\n  [1]: http://spatial-analyst.net/book/\\n,,
1832,5,845,ecd06ed1-8124-4104-8da6-9aa887077bc6,2010-07-27 23:40:02.0,8.0,If you want to go down the non-parametric route you could always try the squared ranks test. The assumptions for this test (taken from [here][1]) are:\\n\\n1. Both samples are random samples from their respective populations.\\n1. In addition to independence within each sample there is mutual independence between\\nthe two samples.\\n1. The measurement scale is at least interval.\\n\\n\\nThese [lecture notes][1] should get you started.\\n\\n\\n  [1]: http://www.stat.wvu.edu/~xiawang/courses/stat551/hwk/hw14.pdf,added 4 characters in body,
1833,5,845,ae39831d-4c4a-4ec8-b121-424e0b79e00f,2010-07-28 00:08:20.0,8.0,"If you want to go down the non-parametric route you could always try the squared ranks test. \\n\\nFor the unpaired case, the assumptions for this test (taken from [here][1]) are:\\n\\n1. Both samples are random samples from their respective populations.\\n1. In addition to independence within each sample there is mutual independence between\\nthe two samples.\\n1. The measurement scale is at least interval.\\n\\nThese [lecture notes][2] describe the unpaired case in detail.\\n\\nFor the paired case you will have to change this procedure slightly. Midway down [this page][3] should give you an idea of where to start.\\n\\n\\n  [1]: http://www.stat.wvu.edu/~xiawang/courses/stat551/hwk/hw14.pdf\\n  [2]: http://www.stat.wvu.edu/~xiawang/courses/stat551/hwk/hw14.pdf\\n  [3]: http://www.itl.nist.gov/div898/software/dataplot/refman1/auxillar/signrank.htm",Adding paired case,
1834,2,862,cfea4436-7eaf-4882-96b8-c04c816496f4,2010-07-28 00:23:22.0,159.0,"The Freedman-Diaconis rule is very robust and works well in practice. The bin-width is set to h=2*IQR*n^(-1/3). So the number of bins is (max-min)/h.\\n\\nIn R, you can use `hist(x,breaks=""FD"")`\\n\\n",,
1835,2,863,2c1ce6c8-9246-4abc-8e96-16901f31860a,2010-07-28 00:29:41.0,196.0,"Mike,\\n\\nI agree that an ANOVA based on total time probably isn't the correct approach here.  Further, I'm not convinced that Chi Sqaure solves your problem.  Chi square will respect the idea that you can't be in two locations at the same time, but it doesn't address the problem that there are likely dependencies between time N and time N+1.  In regards to this second issue, I see some analogies between your situation and what people run into with eye and mouse tracking data. A multinomial model of some sort may serve your purposes well.  Unfortunately, the details of that type of model are beyond my expertise.  I'm sure some statistics book somewhere has a nice little primer on that topic, but off the top of my head I'd point you towards:\\n\\n - Barr D.J. (2008) Analyzing ‘visual world’ eyetracking data using multilevel logistic regression. Journal of Memory and Language, Special Issue: Emerging Data Analysis (59) pp 457-474\\n - https://r-forge.r-project.org/projects/gmpm/ is a non-parametric approach to the same issue being developed by Dr. Barr\\n\\nIf anything, both of those sources should be more than complete because they get into how to analyze the time course of the position.",,
1836,2,864,d0527c31-0a23-4768-b157-413418589969,2010-07-28 01:00:22.0,,"I am going to suggest an answer that is very different from that of a traditional ANOVA. Let T be the total time that is available for an animal to spend in all the zones. You could define T as the total amount of waking time or some such. Suppose that you have J zones. Then by definition you have:\\n\\nSum T_j = T\\n\\nYou could normalize the above by dividing the lhs and the rhs by T and you get\\n\\nSum P_j = 1\\n\\nwhere P_j is the proportion of time that an animal spends in zone j.\\n\\nNow the question you have is if P_j is significantly different from 1 / J for all j.\\n\\nYou could assume that P_j follows a [dirichlet distribution][1] and estimate two models.\\n\\n**Null Model** \\n\\nSet the parameters of the distribution such that P_j = 1 / J. (Setting the parameters of the distribution to 1 will do.)\\n\\n**Alternative Model**\\n\\nSet the parameters of the distribution to be a function of zone specific covariates. You could then estimate the model parameters.\\n\\nYou would choose the alternative model if it outperforms the null model on some critera (e.g., likelihood ratio).\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Dirichlet_distribution",,user28
1837,2,865,4783cda0-15c5-4dd5-9f75-1361a0314fbd,2010-07-28 01:06:56.0,401.0,"Just to expand on Rob's answer a bit, suppose that we want to know the cumulative distribution function (CDF) of the highest value of N independent draws from a standard normal distribution, X1, ..., XN. Call this highest value Y1, the first order statistic. Then the CDF is:<br>\\nPr(Y1 < x) = Pr(max(X1, ..., XN) < x) = Pr(X1 < x, ..., XN < X)<br>\\n= Pr(X1 < x) * ... Pr(XN < x) = Pr(X < x)^100, <br>\\nwhere the second line follows by independence of the draws.\\n\\nRob uses the standard notation that Phi(x) is defined as Pr(X < x) for a standard normal---<i>i.e.,</i> Phi(x) is the standard normal CDF.\\n\\nThe probability density function (PDF) of the first order statistic is just the derivative of the CDF with respect to X:<br>\\nPr(X1 = x) = 100*Pr(X < x)^99 Pr(X = x),<br>\\nthe CDF at x raised to 99 (N-1) times the PDF at x times 100.\\n\\nHope that helps,<br>\\nCharlie",,
1838,2,866,218fd1a6-e45b-4aef-be5c-bed08ed8fd23,2010-07-28 01:10:18.0,455.0,"Say I want to estimate a large number of parameters, and I want to penalize some of them because I believe they should have little effect compared to the others. How do I decide what penalization scheme to use? When is ridge regression more appropriate? When should I use lasso?",,
1839,1,866,218fd1a6-e45b-4aef-be5c-bed08ed8fd23,2010-07-28 01:10:18.0,455.0,When should I use lasso vs ridge?,,
1840,3,866,218fd1a6-e45b-4aef-be5c-bed08ed8fd23,2010-07-28 01:10:18.0,455.0,<regression>,,
1841,2,867,e15f16bf-06fb-45eb-96fb-dcad8ec3bb53,2010-07-28 01:17:57.0,364.0,"(Caveat Emptor: I'm not an expert in this area)\\n\\nIf you just want to talk about differences in time spent per location, then submitting the ""time-per-location"" data as counts in a multinomial mixed model (see the MCMCglmm package for R), using subject as a random effect, should do the trick. \\n\\nIf you want to talk about differences in location preference *through* time, then maybe bin time to reasonable intervals (maybe to the resolution of your timing device?), classify each interval according to the mouse's location at that time (eg. if 3 locations, each interval gets labelled either 1, 2, or 3), and again use a multinomial mixed effects model with subject as a random effect but this time add interval as a fixed effect (though possibly only after factorizing interval, which drops power but should help capture non-linearities through time).",,
1842,5,680,ce307f4a-3b5f-4cfc-9ea4-b968e118f19a,2010-07-28 02:37:46.0,74.0,"If you have enormous data sets - ones that make Excel or Notepad load slowly, then a database is a good way to go. MS SQL Server Express is free, and it's easy to connect with JMP and other programs. You may want to sample in this case. You don't have to normalize the data in the database. Otherwise, CSV is sharing-friendly. \\n\\nIn terms of analysis, here are some starting points:\\n\\n- Describe one variable:\\n\\nHistogram  \\nSummary statistics (mean etc)\\n\\n- Describe relationship between variables:\\n\\nScatter Plot  \\nCorrelation  \\nMosaic plot for categorical  \\nContingency table for categorical  \\nAssociation Analysis (for huge binary datasets)  \\n\\n- Predict a real number: regression (OLS regression or machine learning regression techniques)\\n\\n- Predict a whole number: classification (logistic regression or machine learning techniques)\\n\\n- Put observations into ""natural"" groups: clustering\\n\\n- Put attributes into ""natural"" groups: factoring\\n\\n- Quantifying Risk = Standard Deviation, or proportion of times that ""bad things"" happen\\n\\n- Likelihood of a successfully completed iteration given x number of story points = Logistic Regression\\n\\nGood luck!",deleted 12 characters in body,
1843,5,677,30ed10d6-1261-4d59-b2ee-c77e7587be70,2010-07-28 02:40:52.0,74.0,"Let's say you have some data like this:\\n\\n    Return Symbol News Text\\n    -4%	DELL   Centegra and Dell Services recognized with Outsourcing Center's...\\n    7%	 MSFT	Rising Service Revenues Benefit VMWare\\n    1%	 CSCO	Cisco Systems (CSCO) Receives 5 Star Strong Buy Rating From S&P\\n    4%	 GOOG	Summary Box: Google eyes more government deals\\n    7%	 AAPL	Sohu says 2nd-quarter net income rises 10 percent on higher...\\n\\nYou want to predict the return based on the text, or the source of the text.\\n\\nThis is called Text Mining. \\n\\nWhat you do ultimately is create an enormous matrix like this:\\n\\n    Return Centegra Rising Services Recognized...\\n    -4%    0.23     0      0.11     0.34\\n    7%     0        0.1    0.23     0\\n    ...\\n\\nThat has one column for every word, and one row for each return, and a weighted score for each word. The score is generated via TFIDF or SVD method. Then you run a regression and see if you can predict which words predict the return.\\n\\nBook: Fundamentals of Predictive Text Mining, Weiss\\n\\nSoftware: GATE or RapidMiner with Text Plugin\\n\\nYou should also do a search on Google Scholar and read up on the ins and outs. ",edited body; added 48 characters in body,
1845,2,868,0e0d9369-6577-46c6-b9de-e4b9ca27ea78,2010-07-28 03:15:34.0,9426.0,"I created a quick fun Excel Spreadsheet tonight to try and predict which video games I'll enjoy if I buy them. I'm wondering if this quick example makes sense from a Logistic Regression perspective and if I am computing all of the values correctly.\\n\\nUnfortunately, if I did everything correctly I doubt I have much to look forward to on my XBOX or PS3 ;)\\n\\nI laid out a few categories and weighted them like so (Real spreadsheet lists twice as many or so):\\n\\n<code>\\n4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1\\n\\nVisually Stunning &nbsp; Exhilirating&nbsp; Artistic&nbsp; Sporty\\n</code>\\n\\nThen I went through some games I have and rated them in each category (ratings of 0-4). I then set a separate cell to be the value of Beta_0 and tuned that until the resulting percentages all looked about right.\\n\\nNext I entered in my expected ratings for the new games I **was** looking forward to and got percentages for those.\\n\\nExample:\\nBeta_0 := -35\\n\\n<code>\\n4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1\\n\\nVisually Stunning &nbsp; Exhilirating&nbsp; Artistic&nbsp; Sporty\\n\\n4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1\\n</code>\\n\\nWould be calculated as\\n\\nP = 1 / [1 + e^(-35 + (4*4 + 4*4 + 3*0 + 1*1)]\\nP = 88.1%\\n\\nIf I were to automate the regression am I correct in thinking I'd be tuning Beta_0 to make it so the positive training examples come out high and the negative training examples come out low?\\n\\nI'm completely new to this (just started today thanks to this site actually!) so please have no concern about bruising my ego, I'm eager to learn more.\\n\\nThanks!",,
1846,1,868,0e0d9369-6577-46c6-b9de-e4b9ca27ea78,2010-07-28 03:15:34.0,9426.0,Training a Logistic Regression Model,,
1847,3,868,0e0d9369-6577-46c6-b9de-e4b9ca27ea78,2010-07-28 03:15:34.0,9426.0,<logit><logistic>,,
1848,2,869,72168708-aada-4a6d-814e-1a89e0ee491e,2010-07-28 03:49:27.0,196.0,"Usually in logistic regression you'd want ""successes"" to be 1 and ""failures"" to be 0, but so long as you are consistent in how you enter your data and interpret it, the coefficients don't really care.",,
1849,2,870,4d05cfa5-a97f-4794-965d-3b6d10cd774a,2010-07-28 03:54:56.0,520.0,"Given a list of p-values generated from independent tests, sorted in ascending order, one can use the [Benjamini-Hochberg procedure][1] for [multiple testing correction][2]. For each p-value, the Benjamini-Hochberg procedure allows you to calculate the False Discovery Rate (FDR) for each of the p-values. That is, at each ""position"" in the sorted list of p-values, it will tell you what proportion of those are likely to be false rejections of the null hypothesis.\\n\\nMy question is, are these FDR values to be referred to as ""**[q-values][3]**"", or as ""**corrected p-values**""?\\n\\nNote that this question may have a larger implication for when to use q-value for other multiple testing correction methods, but the BH procedure is the only one I have a good familiarity with.\\n\\n\\n  [1]: http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf\\n  [2]: http://en.wikipedia.org/wiki/False_discovery_rate#Independent_tests\\n  [3]: http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBIQFjAA&url=http%3A%2F%2Fwww.genomine.org%2Fpapers%2Fdirectfdr.pdf&ei=T6ZPTMCPFYGC8gbGi63bDQ&usg=AFQjCNFpHYKmL1nRZilkhVr84cHr4V9uIg",,
1850,1,870,4d05cfa5-a97f-4794-965d-3b6d10cd774a,2010-07-28 03:54:56.0,520.0,Multiple hypothesis testing correction with Benjamini-Hochberg gives p-values or q-values?,,
1851,3,870,4d05cfa5-a97f-4794-965d-3b6d10cd774a,2010-07-28 03:54:56.0,520.0,<hypothesis-testing>,,
1852,2,871,048ff5f7-ae3f-42da-85eb-471ed3e954f6,2010-07-28 04:08:23.0,520.0,"I realize this is pedantic and trite, but as a researcher in a field outside of statistics, with limited formal education in statistics, I always wonder if I'm writing ""p-value"" correctly. Specifically:\\n\\n 1. Is the ""p"" supposed to be capitalized?\\n 1. Is the ""p"" supposed to be italicized? (Or in mathematical font, in TeX?)\\n 1. Is there supposed to be a hyphen between ""p"" and ""value""?\\n 1. Alternatively, is there no ""proper"" way of writing ""p-value"" at all, and any dolt will understand what I mean if I just place ""p"" next to ""value"" in some permutation of these options?",,
1853,1,871,048ff5f7-ae3f-42da-85eb-471ed3e954f6,2010-07-28 04:08:23.0,520.0,"Correct formatting (capitalization, italicization, hyphenation) of ""p-value""?",,
1854,3,871,048ff5f7-ae3f-42da-85eb-471ed3e954f6,2010-07-28 04:08:23.0,520.0,<hypothesis-testing>,,
1855,2,872,aa4164dd-b5e8-4e3c-8c3d-298a445b9b31,2010-07-28 04:21:06.0,159.0,"This seems to be a style issue with different journals and publishers adopting different conventions (or allowing a mixed muddle of styles depending on authors' preferences). My own preference, for what it's worth, is p-value, hyphenated with no italics and no capitalization.",,
1856,2,873,7c189ff5-e5f1-4354-80da-28d97de89133,2010-07-28 04:24:05.0,251.0,The [ASA House Style][1] seems to recommend italicizing the p with hyphen: *p*-value.  A google scholar search shows [varied spellings][2].\\n\\n\\n  [1]: http://www.amstat.org/publications/chance/assets/style.pdf\\n  [2]: http://scholar.google.com/scholar?q=p+value&hl=en&btnG=Search&as_sdt=80001&as_sdtp=on,,
1857,12,873,5739c035-1532-4af7-b19d-eae26cfbb0ee,2010-07-28 04:24:13.0,251.0,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""}]}",,
1858,13,873,9350ca71-d419-426a-a042-505937eab350,2010-07-28 04:25:02.0,251.0,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""}]}",,
1859,2,874,1d428f4e-528c-4d5b-b43e-2355d1e6c8b5,2010-07-28 04:26:17.0,260.0,"Ridge or lasso are forms of regularized linear regressions.  The regularization can also be interpreted as prior in a maximum a posteriori estimation method.  Under this interpretation, the ridge and the lasso make different assumptions on the class of linear transformation they infer to relate input and output data.   In the ridge, the coefficients of the linear transformation are normal distributed and in the lasso they are  Laplace distributed.  In the lasso, this makes it easier for the coefficients to be zero and therefore easier to eliminate some of your input variable as not contributing to the output.\\n\\nThere are also some practical considerations.  The ridge is a bit easier to implement and faster to compute, which may matter depending on the type of data you have. \\n\\nIf you have both implemented, use subsets of your data to find the ridge and the lasso and compare how well they work on the left out data.  The errors should give you an idea of which to use.   ",,
1860,2,875,b0b23925-8245-4b78-85fe-edcc98b6da55,2010-07-28 04:29:37.0,163.0,"There do not appear to be ""standards"".  For example:\\n\\n - The [Nature style guide][1] refers to ""P value""\\n - This [APA style guide][2] refers to ""*p* value""\\n - The [Blood style guide][3] says:\\n    - Capitalize and italicize the *P* that introduces a *P* value\\n    - Italicize the *p* that represents the Spearman rank correlation test\\n - [Wikipedia][4] uses ""p-value"", but italicizes the *p* if used by itself\\n\\nMy brief, unscientific survey suggests that the most common combination is lower-case, italicized *p* without a hyphen.\\n\\n\\n  [1]: http://www.nature.com/nature/authors/gta/#a5.6\\n  [2]: http://my.ilstu.edu/~jhkahn/apastats.html\\n  [3]: http://bloodjournal.hematologylibrary.org/authors/stylecheckforfigs.dtl\\n  [4]: http://en.wikipedia.org/wiki/P-value",,
1861,5,837,a46e14e7-d7f5-405e-9ec1-3b8ffd90f231,2010-07-28 05:20:42.0,501.0,"I am creating multiple logistic regression models using lrm from Harrell's Design package in R.  One model I would like to make is the model with no predictors.  eg I want to predict a constant c such that \\n\\nlogit(Y) ~ c\\n\\nI know I how to compute c (divide the number of ""1""s by the total), what I would like is to use lrm so I can manipulate it as a model in a consistent way with the other models I am making.  Is this possible, and if so how?  \\n\\nI have tried so far\\n\\n    library(Design)\\n    data(mtcars)\\n    lrm(am ~ 1, data=mtcars)\\n\\nwhich gives the error:\\n\\n    Error in dimnames(stats) <- list(names(cof), c(""Coef"", ""S.E."", ""Wald Z"",  :\\n        length of 'dimnames' [1] not equal to array extent\\n\\nand I have tried\\n\\n    lrm(am ~ ., data=mtcars)\\n\\nBut this uses all the predictors, rather then none of the predictors.\\n\\nThanks in advance\\n    \\n",deleted 38 characters in body,
1863,2,876,b713a2c0-a978-40be-a738-3bdbc1db1f85,2010-07-28 05:55:31.0,530.0,"Keep in mind that ridge regression can't zero out coefficients; thus, you either end up including all the coefficients in the model, or none of them. In contrast, the LASSO does both parameter shrinkage and variable selection automatically. If some of your covariates are highly correlated, you may want to look at the Elastic Net [3] instead of the LASSO.\\n\\nI'd personally recommend using the Non-negative Garotte (NNG) [1] as its consistent in terms of estimation and variable selection [2]. Unlike LASSO and ridge regression, NNG requires an initial estimate that is then shrunk towards the origin. In the original paper, Breiman recommends the least squares solution for the initial estimate (you may however want to start the search from a ridge regression solution and use something like GCV to select the penalty parameter).\\n\\nIn terms of available software, I've implemented the original NNG in MATLAB (based on Breiman's original FORTRAN code). You can download it from: \\n\\nhttp://www.emakalic.org/blog/wp-content/uploads/2010/04/nngarotte.zip\\n\\nBTW, if you prefer a Bayesian solution, check out [4,5].\\n\\nReferences:\\n\\n[1] Breiman, L. Better Subset Regression Using the Nonnegative Garrote Technometrics, 1995, 37, 373-384\\n\\n[2] Yuan, M. & Lin, Y. On the non-negative garrotte estimator Journal of the Royal Statistical Society (Series B), 2007, 69, 143-161\\n\\n[3] Zou, H. & Hastie, T. Regularization and variable selection via the elastic net Journal of the Royal Statistical Society (Series B), 2005, 67, 301-320\\n\\n[4] Park, T. & Casella, G. The Bayesian Lasso Journal of the American Statistical Association, 2008, 103, 681-686\\n\\n[5] Kyung, M.; Gill, J.; Ghosh, M. & Casella, G. Penalized Regression, Standard Errors, and Bayesian Lassos Bayesian Analysis, 2010, 5, 369-412\\n",,
1864,5,812,94bfb078-71cf-488a-baf8-a02e5905b818,2010-07-28 06:19:09.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT/UPDATE with the help of Simon Byrne's answer:\\n - sparsity (S-sparse assumption, l^p ball and weak l^p ball for p<1) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used as a structural assumption in functional data analysis. \\n\\nOn the other hand, inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity by wavelet and fourier transform. This make the critical difference mentionned by Simon not so critical?",deleted 21 characters in body,
1865,2,877,aff0d606-fa25-4972-b59b-d5899b93bcb2,2010-07-28 06:31:36.0,531.0,"Has anyone gone through some papers using Vector Error Correction Models in causality applications with more than one cointegration vectors, say two. I guess there will be more than one ECM terms. How to assess the endogeneity of the  left hand variables if t-stats on different ECM coefficients yield different (conflicting) results. For left hand side variables to be endogenous should we have both ECM coefficients negative and significant?\\n\\nJaved Iqbal",,
1866,1,877,aff0d606-fa25-4972-b59b-d5899b93bcb2,2010-07-28 06:31:36.0,531.0,Time Series Econometrics: VECM with multiple cointegration vectors,,
1867,3,877,aff0d606-fa25-4972-b59b-d5899b93bcb2,2010-07-28 06:31:36.0,531.0,<econometrics>,,
1868,2,878,b0ecebd7-dc5c-4e8d-855e-a531565ad7cf,2010-07-28 06:49:12.0,,Absence of Evidence is not Evidence of Absence\\nCarl Sagen,,Tzippy
1869,16,878,b0ecebd7-dc5c-4e8d-855e-a531565ad7cf,2010-07-28 06:49:12.0,-1.0,,,
1870,5,730,a9823498-0c2e-4ef9-90bd-29434ca3b841,2010-07-28 06:55:15.0,159.0,">All models are wrong, but some are useful. (George E. P. Box)",added 1 characters in body,
1871,5,744,26c22161-b09f-491d-9f61-184a0bfe1215,2010-07-28 06:55:37.0,159.0,">""An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem."" -- John Tukey",added 1 characters in body,
1872,5,738,a9d76cc7-3375-4029-8b1f-a6c4123ed239,2010-07-28 06:56:01.0,159.0,"> Say you were standing with one foot in the oven and one foot in an ice bucket.  According to the percentage people, you should be perfectly comfortable.  \\n-Bobby Bragan, 1963",edited body,
1873,5,739,26fef92a-5058-4ab0-8f6b-92d71cbea2ae,2010-07-28 06:56:38.0,159.0,">""To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.""<br>\\n-- Ronald Fisher (1938)",added 6 characters in body,
1874,5,812,93be8783-6094-4602-bae5-d5d95d63f26f,2010-07-28 07:14:14.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT/UPDATE with the help of Simon Byrne's answer:\\n\\n - sparsity (S-sparse assumption, l^p ball and weak l^p ball for p<1) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used as a structural assumption in functional data analysis. \\n\\nOn the other hand, inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity by wavelet and fourier transform. This make the critical difference mentionned by Simon not so critical?",added 2 characters in body,
1875,2,879,950f1075-cfb7-4987-a450-c7a5f30a1a53,2010-07-28 07:23:22.0,251.0,"Maybe the paper ""[Variations on the histogram][1]"" by Denby and Mallows will be of interest:\\n\\n> This new display which we term ""dhist"" (for diagonally-cut histogram) preserves the desirable features of both the equal-width hist and the equal-area hist. It will show tall narrow bins like the e-a hist when there are spikes in the data and will show isolated outliers just like the usual histogram.\\n\\nThey also mention that code in R is available on request.\\n\\n\\n  [1]: http://pubs.research.avayalabs.com/pdfs/ALR-2007-003-paper.pdf",,
1876,5,11,31022a9e-2b47-4a08-87dd-24386203118c,2010-07-28 07:58:52.0,34.0,"Is there a good, modern treatment covering the various methods of multivariate interpolation, including which methodologies are typically best for particular types of problems? I'm interested in a solid statistical treatment including error estimates under various model assumptions.\\n\\nAn example:\\n\\n<a href=""http://en.wikipedia.org/wiki/Inverse_distance_weighting"">Shepard's method</a>\\n\\nSay we're sampling from a multivariate normal distribution with unknown parameters. What can we say about the standard error of the interpolated estimates?\\n\\nI was hoping for a pointer to a general survey addressing similar questions for the various types of multivariate interpolations in common use. ","clarification, example",
1877,4,11,31022a9e-2b47-4a08-87dd-24386203118c,2010-07-28 07:58:52.0,34.0,Multivariate Interpolation Approaches,"clarification, example",
1878,6,11,31022a9e-2b47-4a08-87dd-24386203118c,2010-07-28 07:58:52.0,34.0,<multivariable><interpolation>,"clarification, example",
1879,5,490,4a999c15-c5c9-4c1d-b624-8f9f9d937f63,2010-07-28 08:07:36.0,223.0,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n\\n\\nEDIT: Proposed procedures \\n\\n - Greedy forward selection http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/497#497\\n - Backward elimination http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/501#501\\n - Metropolis scanning / MCMC http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/505#505\\n - penalized logistic regression http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/606#606\\n\\nAs this is community wiki there can be more discussion and update\\n\\nI have one important remark: in a certain sence, you all give a procedure that permit ordering of variables but not variable selection (you are quite evasive on how to select the number of features, I guess you all use cross validation ? ) Can you improve the answers in this direction ? (as this is community wiki you don't need to be the answer writter to add an information about how to select the number of variables?)\\n",added 1080 characters in body,
1880,2,880,40e8f76d-5fd3-40e4-b28e-f896ce4c05cf,2010-07-28 08:15:40.0,223.0,"My question is about binary classification in very high dimension (more features than observation).\\n\\n **Assume** that for each variable i=1,..,p you have a measure of importance T[i] than exactly measure the interest of feature i for the classification problem. \\n\\n**Question:** What is the most efficient way to run cross validation in this case? \\n\\n\\n**Notations** n is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example p=10000 and n=100). ",,
1881,1,880,40e8f76d-5fd3-40e4-b28e-f896ce4c05cf,2010-07-28 08:15:40.0,223.0,Cross validation to select the number of used variables in very high dimensional classification,,
1882,3,880,40e8f76d-5fd3-40e4-b28e-f896ce4c05cf,2010-07-28 08:15:40.0,223.0,<machine-learning><classification>,,
1883,2,881,a2ad7ff7-2bc9-42b7-928d-86e18828d08a,2010-07-28 08:15:51.0,34.0,"Here's something I've wondered about for a while, but haven't been able to discover the correct terminology. Say you have a relatively complicated density function that you suspect might have a close approximation as a sum of (properly weighted) simpler density functions. Have such things been studied? I'm particularly interested in reading about any applications.\\n\\nHere's one example I've found:\\n\\n<a href=""http://www.soa.org/library/research/transactions-of-society-of-actuaries/1966/january/tsa66v18pt1n5211.pdf"">EXPANSION OF PROBABILITY DENSITY FUNCTIONS AS A SUM OF GAMMA DENSITIES WITH APPLICATIONS IN RISK THEORY</a>\\n",,
1884,1,881,a2ad7ff7-2bc9-42b7-928d-86e18828d08a,2010-07-28 08:15:51.0,34.0,Series Expansion of a Density Function,,
1885,3,881,a2ad7ff7-2bc9-42b7-928d-86e18828d08a,2010-07-28 08:15:51.0,34.0,<probability>,,
1886,5,490,0c8728a5-9118-4582-9cc4-38bd5eacd9ea,2010-07-28 08:16:44.0,223.0,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n\\n\\nEDIT: Proposed procedures \\n\\n - Greedy forward selection http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/497#497\\n - Backward elimination http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/501#501\\n - Metropolis scanning / MCMC http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/505#505\\n - penalized logistic regression http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/606#606\\n\\nAs this is community wiki there can be more discussion and update\\n\\nI have one important remark: in a certain sence, you all give a procedure that permit ordering of variables but not variable selection (you are quite evasive on how to select the number of features, I guess you all use cross validation ? ) Can you improve the answers in this direction ? (as this is community wiki you don't need to be the answer writter to add an information about how to select the number of variables? I have openned a question in this direction here http://stats.stackexchange.com/questions/880/cross-validation-to-select-the-number-of-used-variables-in-very-high-dimensional)\\n",added 175 characters in body,
1887,2,882,a5681b44-cddd-49e3-8f23-a17feda0d528,2010-07-28 08:21:47.0,223.0,"**Histogram density estimation** is estimating the density with a sum of **piecewise functions** (density of a uniform).\\n\\n**KDE** is using a sum of **smooth function** (gaussian  is an example). \\n\\nThere are also techniques using fourier, wavelet, splines. ",,
1888,12,873,0c04d776-c444-47a5-b8ee-85312b069ff5,2010-07-28 08:25:00.0,251.0,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""}]}",,
1889,5,882,91120f56-63e7-427f-be88-730be57599b7,2010-07-28 08:28:36.0,223.0,**Histogram density estimation** is estimating the density with a sum of **piecewise functions** (density of a uniform).\\n\\n**KDE** is using a sum of **smooth function** (gaussian  is an example). \\n\\n,deleted 59 characters in body,
1890,5,882,d5c39dcb-6c1c-4139-aecc-b4e3c7272461,2010-07-28 08:34:05.0,223.0,**Histogram density estimation** is estimating the density with a sum of **piecewise functions** (density of a uniform).\\n\\n**KDE** is using a sum of **smooth function** (gaussian  is an example) (as long as they are positive they can be transformed into a density by normalization) ,added 82 characters in body,
1891,5,755,fc93e194-8959-4fc6-8e9c-27ccdca93737,2010-07-28 09:01:45.0,183.0,"I don't know about famous, but the following is one of my favourites:\\n\\n> Conducting data analysis is like\\n> drinking a fine wine. It is important\\n> to swirl and sniff the wine, to unpack\\n> the complex bouquet and to appreciate\\n> the experience. Gulping the wine\\n> doesn’t work.\\n\\n-Daniel B. Wright (2003), see [PDF of Article][1].\\n\\n\\n  [1]: http://www2.fiu.edu/~dwright/pdf/makefriends.pdf",made block quote,
1892,2,883,808c7a44-4de8-4bf3-bbb9-92f4c76bc14e,2010-07-28 09:09:51.0,159.0,"You can do this with **mixture modeling**. There are a number of R packages on CRAN for doing this. Search for ""mixture"" at http://cran.r-project.org/web/packages/",,
1893,5,882,298bf516-29fd-428e-a41e-0d3a0b7e5de4,2010-07-28 09:11:34.0,223.0,"**Histogram density estimation** is estimating the density with a sum of **piecewise functions** (density of a uniform).\\n\\n**KDE** is using a sum of **smooth function** (gaussian  is an example) (as long as they are positive they can be transformed into a density by normalization) \\n\\nThe use of ""**mixture**"" in statistic is about convex combination of densities... (note that it is related to the use of a bayes principle) ",added 144 characters in body,
1894,5,490,3f09519b-24c8-4303-9045-49ecb0a24f2f,2010-07-28 09:12:04.0,88.0,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n\\n\\nEDIT: Proposed procedures \\n\\n - Greedy forward selection http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/497#497\\n - Backward elimination http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/501#501\\n - Metropolis scanning / MCMC http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/505#505\\n - penalized logistic regression http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/606#606\\n\\nAs this is community wiki there can be more discussion and update\\n\\nI have one important remark: in a certain sence, you all give a procedure that permit ordering of variables but not variable selection (you are quite evasive on how to select the number of features, I guess you all use cross validation?) Can you improve the answers in this direction? (as this is community wiki you don't need to be the answer writter to add an information about how to select the number of variables? I have openned a question in this direction here http://stats.stackexchange.com/questions/880/cross-validation-to-select-the-number-of-used-variables-in-very-high-dimensional)\\n\\nEDIT by mbq:  \\nIn principle there are TWO feature selection problems -- first, and more popular is minimal optimal, which states that we want to find minimal (in size) subset of features that gives the best accuracy of some classifier. The second is all relevant, which is a selection of all features that are relevant (nonlinearly correlated) with the decision; this in theory is not classifier dependent, and is useful for model interpretation. It is also a method to avoid feature selection overfitting. \\n",Added info about two fs problems.,
1895,5,490,ddf50868-2d37-448c-a654-88d553b17e53,2010-07-28 09:22:22.0,223.0,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? The aim here is to discuss what is the feature selection procedure that reduces the best the classification error. \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n\\n\\nEDIT: Proposed procedures \\n\\n - Greedy forward selection http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/497#497\\n - Backward elimination http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/501#501\\n - Metropolis scanning / MCMC http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/505#505\\n - penalized logistic regression http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/606#606\\n\\nAs this is community wiki there can be more discussion and update\\n\\nI have one important remark: in a certain sence, you all give a procedure that permit ordering of variables but not variable selection (you are quite evasive on how to select the number of features, I guess you all use cross validation?) Can you improve the answers in this direction? (as this is community wiki you don't need to be the answer writter to add an information about how to select the number of variables? I have openned a question in this direction here http://stats.stackexchange.com/questions/880/cross-validation-to-select-the-number-of-used-variables-in-very-high-dimensional)\\n\\n\\n","To avoid the distiction proposed by mbq I have added a sentence in the first paragraph. This will avoid confusion. I think that if mbq wants to discuss other problem he should open another question, otherwise the answer here won't be clear enough.",
1896,5,880,a41a2426-cab9-4676-ada3-eface46387e6,2010-07-28 09:24:41.0,223.0,"My question is about binary classification in very high dimension (more features than observation).\\n\\n **Assume** that for each variable i=1,..,p you have a measure of importance T[i] than exactly measure the interest of feature i for the classification problem. \\n\\n**Question:** What is the most efficient way to run cross validation in this case? My question is not about how to write the code but on the version of cross validation to use.\\n\\n\\n**Notations** n is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example p=10000 and n=100). ",added 93 characters in body,
1897,16,497,00d5d563-77db-4df2-9a1a-0bf89316f6d9,2010-07-28 09:35:43.0,190.0,,,
1898,2,884,013d6053-4126-4345-99dc-1675359bbe43,2010-07-28 09:54:14.0,541.0,I was at a talk a few months back where the speaker used the term 'degrees of freedom'. She briefly said something along the lines of it meaning the number of values used to form a statistic that are free to vary.\\n\\nWhat does this mean? I'm specifically looking for an intuitive explanation. ,,
1899,1,884,013d6053-4126-4345-99dc-1675359bbe43,2010-07-28 09:54:14.0,541.0,"What are ""degees of freedom""?",,
1900,3,884,013d6053-4126-4345-99dc-1675359bbe43,2010-07-28 09:54:14.0,541.0,<intuition>,,
1901,5,252,66c5273c-264e-42a4-9335-f5cce3bc6892,2010-07-28 10:02:05.0,81.0,"For me personally, I use the following three packages the most, all available from the awesome [Omega Project for Statistical Computing][1] (I do not claim to be an expert, but for my purposes they are very easy to use):\\n\\n - **RCurl**: It has lots of options which allows access to websites that the default functions in base R would have difficulty with I think it's fair to say. It is an R-interface to the libcurl library, which has the *added* benefit of a whole community outside of R developing it. Also available on [CRAN][2].\\n\\n - **XML**: It is very forgiving of parsing malformed XML/HTML. It is an R-interface to the libxml2 library and again has the *added* benefit of a whole community outside of R developing it Also available on [CRAN][3].\\n - **RJSONIO**: It allows one to parse the text returned from a json call and organise it into a list structure for further analysis.The competitor to this package is rjson but this one has the advantage of being vectorised, readily extensible through S3/S4, fast and scalable to large data.  \\n\\n\\n  [1]: http://www.omegahat.org\\n  [2]: http://cran.r-project.org/web/packages/RCurl/index.html\\n  [3]: http://cran.r-project.org/web/packages/XML/index.html",added links to CRAN based on Dirks comment.,
1903,5,880,63aae317-e2e6-43df-8230-6639495d24cc,2010-07-28 11:25:43.0,223.0,"My question is about binary classification in very high dimension (more features than observation).\\n\\n **Problem:** Assume that for each variable i=1,..,p you have a measure of importance T[i] than exactly measure the interest of feature i for the classification problem. The problem of selecting a subset of feature to reduce optimally the classification error is then reduced to that of finding the number of features. \\n\\n**Question:** What is the most efficient way to run cross validation in this case? My question is not about how to write the code but on the version of cross validation to use when trying to find the number of selected feature (to minimize the classification error).\\n\\n\\n**Notations** n is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example p=10000 and n=100). ",added 248 characters in body,
1904,2,886,4324ea70-8eac-4581-94f3-f9d9576fe769,2010-07-28 11:31:59.0,,"The 'fundamental' idea of statistics for estimating parameters is [maximum likelihood][1]. I am wondering what is the corresponding idea in machine learning.\\n\\nQn 1. Would it be fair to say that the 'fundamental' idea in machine learning for estimating parameters is: 'Loss Functions'\\n\\n[Note: It is my impression that machine learning algorithms often optimize a loss function and hence the above question.]\\n\\nQn 2: Is there any literature that attempts to bridge the gap between statistics and machine learning?\\n\\n[Note: Perhaps, by way of relating loss functions to maximum likelihood. (e.g., OLS is equivalent to maximum likelihood for normally distributed errors etc)]\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Maximum_likelihood",,user28
1905,1,886,4324ea70-8eac-4581-94f3-f9d9576fe769,2010-07-28 11:31:59.0,,What is the 'fundamental' idea of machine learning for estimating parameters?,,user28
1906,3,886,4324ea70-8eac-4581-94f3-f9d9576fe769,2010-07-28 11:31:59.0,,<machine-learning><maximum-likelihood><loss-functions>,,user28
1907,5,833,a2281c9c-f244-4b6d-94da-00c6768a0f82,2010-07-28 11:37:58.0,495.0,"Yes and no. At the theoretical level, both cases can use similar techniques and frameworks (an excellent example being Gaussian process regression).\\n\\nThe critical difference is the assumptions used to prevent overfitting (regularization):\\n\\n - In the functional case, there is usually some assumption of smoothness, in other words, values occurring close to each other should be similar in some systematic way. This leads to the use of techniques such as splines, loess, Gaussian processes, etc.\\n\\n - In the high-dimensional case, there is usually an assumption of sparsity: that is, only a subset of the dimensions will have any signal. This leads to techniques aiming at identifying those dimensions (Lasso, LARS, slab-and-spike priors, etc.)\\n\\nUPDATE:\\n\\nI didn't really think about wavelet/Fourier methods, but yes, the thresholding techniques used for such methods are aiming for sparsity in the projected space. Conversely, some high-dimensional techniques assume a projection on to a lower-dimensional manifold (e.g. principal component analysis), which is a type of smoothness assumption.",added 353 characters in body,
1908,2,887,f39eaefc-a32d-4d73-bb69-580081d134f6,2010-07-28 12:02:21.0,213.0,"Suppose there is a very big (infinite?) population of normally distributed values with unknown mean and variance.\\n\\nSuppose also that we have a sample, *S*, of *n* values from the entire population. We can calculate mean and standard deviation for this sample (we use *n-1* for stdev calculation).\\n\\nNow, we have an additional number, *x*, and we would like to test whether it is an vis-a-vis the general population. My intuitive approach is to calculate Z as follows:\\n\\nZ = (x - mean(S))/stdev(S)\\n\\nand then test it against standard distribution if *n>30* or against t-distribution if *n<30*.\\n\\nHowever, this approach doesn't account for *n*, the size of the sample. What is the right way to solve this question provided there is only single sample *S*?",,
1909,1,887,f39eaefc-a32d-4d73-bb69-580081d134f6,2010-07-28 12:02:21.0,213.0,Basic question regarding variance and stdev of a sample,,
1910,3,887,f39eaefc-a32d-4d73-bb69-580081d134f6,2010-07-28 12:02:21.0,213.0,<standard-deviation><normality><variance><outliers><sample>,,
1911,5,887,6f2db050-4f9d-4fd3-8923-30ec2d5f2c30,2010-07-28 12:07:49.0,213.0,"Suppose there is a very big (infinite?) population of normally distributed values with unknown mean and variance.\\n\\nSuppose also that we have a sample, *S*, of *n* values from the entire population. We can calculate mean and standard deviation for this sample (we use *n-1* for stdev calculation).\\n\\nThe first and most important question is how is stdev(S) related to the standard deviation of the entire population?\\n\\nAn illustration for this issue is the second question:\\n\\nSuppose we have an additional number, *x*, and we would like to test whether it is an vis-a-vis the general population. My intuitive approach is to calculate Z as follows:\\n\\nZ = (x - mean(S))/stdev(S)\\n\\nand then test it against standard distribution if *n>30* or against t-distribution if *n<30*.\\n\\nHowever, this approach doesn't account for *n*, the size of the sample. What is the right way to solve this question provided there is only single sample *S*?",emphaszie the basics,
1913,2,889,1344b099-db2e-4bf9-9d39-8769dd6be768,2010-07-28 12:12:22.0,,"The sample size is accounted for when you calculate the standard deviation (you should divide by n instead of n-1 for the purpose of hypothesis testing). You should also note that the z-test is for testing whether the true mean of the population is some particular value. Thus, it takes the form:\\n\\nz = (mean(S) - mu) / stdev(S)\\n\\nIt does not make sense to substitute x instead of mu in the above statistic.",,user28
1914,2,890,da52cbc5-7a01-4c9d-a977-e734e5d598db,2010-07-28 12:13:03.0,,"I am struggling a little bit at the moment with a question related to logistic regression. I have a model that predicts the occurrence of animal based on land cover with reference to forest. I am not grasping the concept of a reference class and struggle to extrapolate the model onto a new area. Any explanations or guidance towards papers, lecture notes etc would be highly appreciated. \\n\\nMany thanks in advance \\nMike",,Mike
1915,1,890,da52cbc5-7a01-4c9d-a977-e734e5d598db,2010-07-28 12:13:03.0,,Logistic Regression: Reference Category and Prediction,,Mike
1916,3,890,da52cbc5-7a01-4c9d-a977-e734e5d598db,2010-07-28 12:13:03.0,,<logistic>,,Mike
1917,12,889,d6d681a1-a5e5-4ee4-a052-be195e996ee8,2010-07-28 12:16:11.0,,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""}]}",,user28
1918,5,889,18f373c3-b3f7-4b73-8c78-d172c9c7dc01,2010-07-28 12:20:09.0,,My first answer was full of errors. Here is a corrected version:\\n\\nThe correct way to test is as follows:\\n\\nz = (mean(S) - mu) / (stdev(S) / sqrt(n) )\\n\\nSee: [Student's t-test][1]\\n\\nNote the following:\\n\\n1. The sample size is accounted for when you divide the standard deviation by the square root of the sample size.\\n\\n2. You should also note that the z-test is for testing whether the true mean of the population is some particular value. It does not make sense to substitute x instead of mu in the above statistic.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Student's_t-test#Independent_one-sample_t-test,added 168 characters in body; added 34 characters in body,user28
1919,13,889,96f5e0b1-53da-48c4-9552-13d3129f9822,2010-07-28 12:22:08.0,,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""}]}",,user28
1920,6,884,2683015d-ed08-46cb-bc7e-60d43c2f1725,2010-07-28 12:33:17.0,88.0,<beginner>,edited tags,
1921,6,887,a8191b27-4afe-4708-b50c-7573ecc23058,2010-07-28 12:44:30.0,8.0,<standard-deviation><variance><normality><sample>,edited tags,
1926,2,893,0dd75a0e-9237-43cd-9bad-9dc9491fcd20,2010-07-28 12:48:14.0,236.0,"I really like first sentence from \\n[The Little Handbook of Statistical Practice. Degrees of Freedom Chapter][1]\\n\\n\\n> One of the questions an instrutor\\n> dreads most from a mathematically\\n> unsophisticated audience is, ""What\\n> exactly is degrees of freedom?""\\n\\nI think you can get really good understanding about degrees of freedom from reading this chapter.\\n\\n\\n  [1]: http://www.jerrydallal.com/LHSP/dof.htm",,
1928,2,894,366d9be9-772b-4e4e-8d2f-90033e07b54c,2010-07-28 12:49:31.0,1356.0,"Or simply: the number of elements in a numerical array that you're allowed to change so that the value of the statistic remains unchanged.\\n\\n    # for instance if:\\n    x + y + z = 10\\n\\nyou can change, for instance, *x* and *y* at random, but you cannot change *z*, 'cause you'll change the value of the statistic (&Sigma; = 10). So, in this case df = 2.",,
1929,2,895,454cbfed-aa85-4dab-b9bd-931be0a21621,2010-07-28 12:50:26.0,8.0,"I'm finding it rather tricky to see what you are asking:\\n\\n1. If you want to know whether the Var(S) is different from the population variance, then see this previous [answer][1].\\n2. If you want to determine whether the mean(S) and the mean(X) are the same, then look at [Independent two-sample t-tests][2].\\n3. If you want to test whether mean(S) is equal to the population mean, then see @Srikant answer above, i.e. a [one-sample t-test][3].\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/841/comparing-the-variance-of-paired-observations/845#845\\n  [2]: http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test\\n  [3]: http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_one-sample_t-test",,
1934,2,897,1c841043-eefd-49d9-a498-dc630f198d5c,2010-07-28 13:32:32.0,284.0,What is the difference between offline and [online learning][1]?  Is it just a matter of learning over the entire dataset (offline) vs. learning incrementally (one instance at a time)?  What are examples of algorithms used in both?\\n\\n  [1]: http://en.wikipedia.org/wiki/Online_machine_learning,,
1935,1,897,1c841043-eefd-49d9-a498-dc630f198d5c,2010-07-28 13:32:32.0,284.0,Online vs offline learning?,,
1936,3,897,1c841043-eefd-49d9-a498-dc630f198d5c,2010-07-28 13:32:32.0,284.0,<beginner><machine-learning>,,
1938,5,611,003f8052-2f07-450a-a54e-d5f43eafeffc,2010-07-28 13:43:17.0,223.0,"The concept of density is much wider than you may think. A density of a probability measure $P$ can be defined with respect to a measure $\\lambda$ that dominates $P$ by the Radon Nikodym Theorem (see http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem). Here density should be understood as a density with respect to the counting measure defined on the mentionned countable set. I agree that it is not extremely rigorous not to mention the reference when talking about a density (but who mention density  wrt lesbesgue measure?), but it pose no problem while reading the paper in question so .... \\n\\n\\n----------\\n\\n\\nAdditional Annex Notes\\n I have seen a certain number of machine learning notes (I won't do delation) where  the reference measure is not the counting measure and we see things such as $P(X=x|Y=y)$ with X being a continuous variable (with a density wrt Lebesgues) (to apply Bayes principle and derive the Bayes rule). I guess people want to be pedagogic and do not want to bother students with technical details :) ",added 1 characters in body,
1939,2,898,1bdab76b-7a10-47d1-a107-201aeaf9f7d7,2010-07-28 13:49:07.0,5.0,"I am interested in tools/techniques that can be used for analysis of [streaming data in ""real-time""][1]*, where latency is an issue.  The most common example of this is probably price data from a financial market, although it also occurs in other fields (e.g. finding trends on Twitter or in Google searches).\\n\\nIn my experience, the most common software category for this is [""**complex event processing**""][2].  This includes commercial software such as [Streambase][3] and [Aleri][4] or open-source ones such as [Esper][5] or [Telegraph][6] (which was the basis for [Truviso][7]).  \\n\\nMany existing models are not suited to this kind of analysis because they're too computationally expensive.  Are any models** specifically designed to deal with real-time data?  What tools can be used for this?\\n\\n<i>\\n* By ""real-time"", I mean ""analysis on data *as it is created*"".  So I do not mean ""data that has a time-based relevance"" (as in [this talk by Hilary Mason][8]).\\n\\n** By ""model"", I mean a mathematical abstraction that describe the behavior of an object of study (e.g. in terms of random variables and their associated probability distributions), either for description or forecasting.  This could be a machine learning or statistical model.\\n</i>\\n\\n  [1]: http://en.wikipedia.org/wiki/Real-time_data\\n  [2]: http://en.wikipedia.org/wiki/Complex_event_processing\\n  [3]: http://www.streambase.com/index.htm\\n  [4]: http://www.sybase.com/products/financialservicessolutions/aleristreamingplatform\\n  [5]: http://esper.codehaus.org/\\n  [6]: http://telegraph.cs.berkeley.edu/\\n  [7]: http://www.truviso.com/\\n  [8]: http://www.hilarymason.com/blog/conference-web2-expo-sf/",,
1940,1,898,1bdab76b-7a10-47d1-a107-201aeaf9f7d7,2010-07-28 13:49:07.0,5.0,Modeling of real-time streaming data?,,
1941,3,898,1bdab76b-7a10-47d1-a107-201aeaf9f7d7,2010-07-28 13:49:07.0,5.0,<modeling><software><real-time>,,
1942,2,899,ecd7259f-1ade-44cc-9147-853dbabe0afa,2010-07-28 13:53:18.0,219.0,"I'm trying to separate two groups of values from a single dataset. I can assume that one of the population is normally distributed and is at least half the size of the sample. The values of the second one are either lower or higher than the values from the first one (distribution is unknown). What I'm trying to do is to find the upper and lower limits that would enclose the normally-distributed population from the other.\\n\\nMy assumption provide me with starting point:\\n\\n- all point within the interquartile range of the sample are from the normally-distributed population.\\n\\nI'm trying to test for outliers taking them from the rest of the sample until they don't fit into the 3 st.dev of the normally-distributed population. Which is not ideal, but seem to produce reasonable enough result.\\n\\nIs my assumption statistically sound? What would be a better way to go about this?\\n\\np.s. please fix the tags someone.",,
1943,1,899,ecd7259f-1ade-44cc-9147-853dbabe0afa,2010-07-28 13:53:18.0,219.0,Separating two populations from the sample,,
1944,3,899,ecd7259f-1ade-44cc-9147-853dbabe0afa,2010-07-28 13:53:18.0,219.0,<outliers><dataset>,,
1945,2,900,595902ed-36aa-42c6-a9c3-39f258b789d2,2010-07-28 13:57:09.0,,I am not sure how far this would be relevant to what you want to do but see the paper on adaptive question design called [FASTPACE][1]. The goal of the algorithm is to ask the next question from a survey respondent based on his/her previous questions and answers.\\n\\nThe data does not arrive as fast as stock prices but nevertheless latency is an issue as most survey respondents expect the next question to appear within a few seconds.\\n\\n\\n  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.8664&rep=rep1&type=pdf,,user28
1946,2,901,43523951-a932-4fbb-b118-072dd268f050,2010-07-28 13:59:49.0,11.0,"**RODBC** for accessing data from databases, **sqldf** for performing simple SQL queries on dataframes (although I am forcing myself to use native R commands), and **ggplot2** and **plyr**",,
1947,16,901,43523951-a932-4fbb-b118-072dd268f050,2010-07-28 13:59:49.0,-1.0,,,
1948,2,902,d7970647-b68c-42ba-8d49-e20109b35b1e,2010-07-28 14:01:46.0,247.0,"It is going to depend a lot on what exactly you are looking for, but start at [this][1].\\n\\n\\n  [1]: http://www.cs.rutgers.edu/~muthu/stream-1-1.ps",,
1949,2,903,6afd7bea-7b6d-4f1e-8af9-81ce7484bf8c,2010-07-28 14:06:35.0,11.0,There is a series of Google Tech Talk videos called [Stats 202 - Statistical Aspects of Data Mining][1]\\n\\n\\n  [1]: http://video.google.com/videosearch?q=mease+stats+202&sitesearch=#,,
1950,16,903,6afd7bea-7b6d-4f1e-8af9-81ce7484bf8c,2010-07-28 14:06:35.0,-1.0,,,
1951,2,904,a721092c-3de5-495a-91c7-b15d0a18aa46,2010-07-28 14:16:38.0,549.0,"Functional Data often involves different question.  I've been reading Functional Data Analysis, Ramsey and Silverman, and they spend a lot of times discussing curve registration, warping functions, and estimating derivatives of curves.  These tend to be very different questions than those asked by people interested in studying high-dimensional data.",,
1952,2,905,5e9e6f74-0d3b-4824-9512-21d7d5ae7f4b,2010-07-28 14:37:17.0,549.0,"Online learning means that you are doing it as the data comes in...offline means that you have a static dataset.\\n\\nSo for online learning, you (typically) have more data, but you have time constraints.  Another wrinkle that can affect online learning is that your concepts might change through time.\\n\\nLet's say you want to build a classifier to recognize spam.  You can acquire a large corpus of e-mail, label it, and train a classifier on it.  This would be offline learning.  Or, you can take all the e-mail coming into your system, and continuously update your classifier (labels may be a bit tricky...).  This would be online learning.",,
1955,5,85,f6730a3b-9cc9-41e8-82c1-1f476565398e,2010-07-28 14:54:34.0,87.0,"A random variable is a variable whose value depends on unknown events.  We can summarize the unknown events as ""state"", and then the random variable is a function of the state.\\n\\nExample:  Suppose we have three dice rolls (D1,D2,D3).  Then the state S=(D1,D2,D3).  One random variable X is the number of 5s. X=(D1==5?)+(D2==5?)+(D3==5?).   Another random variable Y is the sum of the dice rolls Y=D1+D2+D3.  ",added 2 characters in body,
1956,2,908,843cdeaa-6402-4ee1-a2a1-fe38f9d98208,2010-07-28 14:58:10.0,88.0,"You miss one important issue -- there is almost never such thing as T[i]. Think of a simple problem in which the sum of two attributes (of a similar amplitude) is important; if you'd remove one of them the importance of the other will suddenly drop. Also, big amount of irrelevant attributes is the accuracy of most classifiers, so along their ability to assess importance. Last but not least, stochastic algorithms will return stochastic results, and so even the T[i] ranking can be unstable. So in principle you should at least recalculate T[i] after each (or at least after each non trivially redundant) attribute is removed.\\n\\nGoing back to the topic, the question which CV to choose is mostly problem dependent; with very small number of cases LOO may be the best choice because all other start to reduce to it; still small is rather n=10 not n=100. So I would just recommend random subsampling (which I use most) or K-fold (then with recreating splits on each step). Still, you should also collect not only mean but also the standard deviation of error estimates; this can be used to (approximately) judge which changes of mean are significant ans so help you decide when to cease the process.",,
1957,2,909,cb11d61c-ab44-4c05-b7d6-e99362ac49f7,2010-07-28 15:24:38.0,364.0,"This assumes that you don't even know if the second distribution is normal or not; I basically handle this uncertainty by focusing only on the normal distribution. This may or may not be the best approach.\\n\\nIf you can assume that the two populations are completely separated (i.e., all values from distribution A are less than all values from distribution B), then one approach is to use the optimize() function in R to search for the break-point that yields estimates of the mean and sd of the normal distribution that make the data most likely:\\n\\n    #generate completely separated data\\n    a = rnorm(100)\\n    b = rnorm(100,10)\\n    while(!all(a<b)){\\n    	a = rnorm(100)\\n    	b = rnorm(100,10)\\n    }\\n    \\n    #create a mix\\n    mix = c(a,b)\\n    \\n    #""forget"" the original distributions\\n    rm(a)\\n    rm(b)\\n    \\n    #try to find the break point between the distributions\\n    break_point = optimize(\\n    	f = function(x){\\n    		data_from_a = mix[mix<x]\\n    		likelihood = dnorm(data_from_a,mean(data_from_a),sd(data_from_a))\\n    		SLL = sum(log(likelihood))\\n    		return(SLL)\\n    	}\\n    	, interval = c(sort(mix)[2],max(mix))\\n    	, maximum = TRUE\\n    )$maximum\\n    \\n    #label the data\\n    labelled_mix = data.frame(\\n    	x = mix\\n    	, source = ifelse(mix<break_point,'A','B')\\n    )\\n    print(labelled_mix)\\n\\nIf you can't assume complete separation, then I think you'll have to assume some distribution for the second distribution and then use mixture modelling. Note that mixture modelling won't actually label the individual data points, but will give you the mixture proportion and estimates of the parameters of each distribution (eg. mean, sd, etc.).\\n",,
1958,2,910,ad24668a-14d4-4144-9733-a221079b280a,2010-07-28 15:29:33.0,88.0,"There is a trivial answer -- there is no parameter estimation in machine learning! We don't assume that our models are equivalent to some hidden background models; we treat both reality and the model as black boxes and we try to shake the model box (train in official terminology) so that its output will be similar to that of the reality box.\\n\\nThe concept of not only likelihood but the whole model selection based on the training data is replaced by optimizing the accuracy (whatever defined; in principle the goodness in desired use) on the unseen data; this allows to optimize both precision and recall in a coupled manner. This leads to the concept of an ability to generalize, which is achieved in different ways depending on the learner type.\\n\\nThe answer to the question two depends highly on definitions; still I think that the nonparametric statistics is something that connects the two. ",,
1959,2,911,b803685e-ee61-4487-85b5-1198b5271307,2010-07-28 15:32:38.0,549.0,"Interesting question.  What do you mean by weight?  \\n\\nI would be inclined to do a bootstrap...pick your favorite statistic (i.e. Fisher's Exact), and compute it on your data.  Then assign new cells to each instance according to your null hypothesis, and repeat the process 999 times.  This should give a pretty good empirical distribution for your test statistic under the null hypothesis, and allow easy computation of your p-value!",,
1960,2,912,e38d90b7-cd8f-43e9-96a2-e0b0d4050f9d,2010-07-28 15:37:01.0,549.0,"The other issue is that you put in your data, and the algorithm learns the weights and the Beta_0 for you...I don't know if Excel can do logistic regression...if it doesn't, I'd be inclined to use R to learn your model (and predict future cases for you!).",,
1961,5,899,c4707c9f-fd31-48bb-abf2-54d5bf20fbe3,2010-07-28 16:07:02.0,219.0,"I'm trying to separate two groups of values from a single dataset. I can assume that one of the population is normally distributed and is at least half the size of the sample. The values of the second one are both lower or higher than the values from the first one (distribution is unknown). What I'm trying to do is to find the upper and lower limits that would enclose the normally-distributed population from the other.\\n\\nMy assumption provide me with starting point:\\n\\n- all point within the interquartile range of the sample are from the normally-distributed population.\\n\\nI'm trying to test for outliers taking them from the rest of the sample until they don't fit into the 3 st.dev of the normally-distributed population. Which is not ideal, but seem to produce reasonable enough result.\\n\\nIs my assumption statistically sound? What would be a better way to go about this?\\n\\np.s. please fix the tags someone.",deleted 2 characters in body,
1962,2,913,d571fb0a-d454-4cea-8abd-a9033faf6a39,2010-07-28 16:15:07.0,59.0,"Comparing two variables, I came up with the following chart. the x, y pairs represent independent observations of data on the field.  I've doen [Pearson correlation][1] on it and have found one of 0.6. \\n\\nMy end goal is to establish a relationship between y and x such that y = f(x).\\n\\nWhat analaysis would you recommend to obtain some form ofa relationship between the two variables?\\n\\n![Graph][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Correlation_and_dependence\\n  [2]: http://koopics.com/ask_math_chart.jpg",,
1963,1,913,d571fb0a-d454-4cea-8abd-a9033faf6a39,2010-07-28 16:15:07.0,59.0,Relationships between two variables,,
1964,3,913,d571fb0a-d454-4cea-8abd-a9033faf6a39,2010-07-28 16:15:07.0,59.0,<statistical-analysis>,,
1965,2,914,e8057afa-d23f-40c5-b765-83e87de390eb,2010-07-28 16:21:41.0,88.0,"What you are looking for is called regression; there are a lot of methods you can do it, both statistical and machine learning ones. If you want to find f, you must use statistics; in that case you must first assume that f is of some form, like f:y=a*x+b and then use some regression method to fit the parameters.",,
1966,2,915,d4e5a519-e449-4fe7-9952-cac2569c62a3,2010-07-28 16:24:38.0,247.0,"And just eyeballing the data, you are probably going to want to transform the data, as (at least to me) it looks skewed. Looking at the histograms of the two variables should suggest which transforms may be beneficial.",,
1967,5,914,f2b50ee6-f84a-4bf9-ac69-9c86f78ba302,2010-07-28 16:31:04.0,88.0,"What you are looking for is called regression; there are a lot of methods you can do it, both statistical and machine learning ones. If you want to find f, you must use statistics; in that case you must first assume that f is of some form, like f:y=a*x+b and then use some regression method to fit the parameters.  \\nThe plot suggests there are a lot of outliers (elements that does not follow f(x)); you may need robust regression to get rid of them.",added 138 characters in body,
1968,2,916,ee93e2f8-f862-4fcd-8bd9-b4e09a0c7cab,2010-07-28 16:31:53.0,287.0,"Normality seems to be strongly violated at least by your y variable. I would log transform y to see if that cleans things up a bit. Then, fit a regression to log(y) ~ x.  The formula the regression will return will be of the form log(y) = \\alpha +  \\beta*x which you can transform back to the original scale by y = exp(\\alpha +  \\beta*x)",,
1969,5,915,e248ee10-3e00-49f9-b133-5e2b99cc3f0a,2010-07-28 16:48:05.0,247.0,"And just eyeballing the data, you are probably going to want to transform the data, as (at least to me) it looks skewed. Looking at the histograms of the two variables should suggest which transforms may be beneficial.\\n\\nAs suggested by mbq, more text [here][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Data_transformation_(statistics)",added 119 characters in body,
1970,5,902,675f8e22-0c81-4454-b280-d9c4a3c3f556,2010-07-28 16:49:31.0,247.0,"It is going to depend a lot on what exactly you are looking for, but start at [Data Streams: Algorithms and Application by Muthukrishnan ][1].\\n\\nThere are many others that can be found by googling ""data stream algorithms"", or following the references in the paper.\\n\\n  [1]: http://www.cs.rutgers.edu/~muthu/stream-1-1.ps\\n\\n",added 58 characters in body; added 121 characters in body,
1971,2,917,f0163954-53fb-4af1-902d-cf0195e8db5d,2010-07-28 17:01:12.0,287.0,"Like drknexus said, for a logistic regression, your outcome measure needs to be 0 and 1. I'd go back and recode your outcome as 0 (didn't like it), or 1 (did like it). Then, abandon excel and load the data into R (it's really not as intimidating as it looks).  Your regression will look something like this:\\n\\n    glm(Liked ~ Visually.Stunning + Exhilarating + Artistic + Sporty, family = binomial, data = data)\\n\\nThe regression will return betas for each feature in terms of log-odds. So, for every 1 point increase in `Artistic`, for instance, you'll have a value for how much that increases or decreases the log-odds of your enjoyment. Most of the betas will be positive, unless you _dislike_ sporty games or something.\\n\\nNow, you'll have to ask yourself some interesting questions. The assumption of the model is that the values on each of these scores affect your enjoyment _independently_, which probably isn't true! A game that is very Visually.Stunning and Exhilarating is probably way better than you would expect given those component parts. And it's probably the case that if a game gets scores of 1 on all features except Sporty, which gets a 4, that high Sporty score is worth less than if the other scores were higher.\\n\\nThat is, many or all of your features probably _interact_. To fit an accurate model, then, you'll want to add in these interactions. That formula would look like this:\\n\\n    glm(Liked ~ Visually.Stunning * Exhilarating * Artistic * Sporty, family = binomial, data = data)\\n\\nNow, there are two points of difficulty here. First, you need to have more data to fit a good model with this many interactions than the pure independence model. Second, you risk overfitting, which means that the model will very accurately describe the original data, but will be less good at making accurate predictions for future data.\\n\\nNeedless to say, some people spend all day fitting and refitting models like this one. ",,
1972,2,918,16635ba2-5b61-4ee2-8189-5cb9ae731696,2010-07-28 17:04:38.0,251.0,"This area roughly falls into two categories.  The first concerns stream processing and querying issues and associated models and algorithms.  The second is efficient algorithms and models for learning from data streams (or data stream mining).\\n\\nIt's my impression that the CSP industry is connected to the first area.  For example, StreamBase originated from the [Aurora][1] project at Brown/Brandeis/MIT.  A similar project was Widom's [STREAM][2] at Stanford.  Reviewing the publications at either of those projects' sites should help exploring the area.  \\n\\nA nice paper summarizing the research issues (in 2002) from the first area is *[Models and issues in data stream systems][3]* by Babcock et al.  In stream mining, I'd recommend starting with *[Mining Data Streams: A Review][4]* by Gaber et al.\\n\\n\\n\\n  [1]: http://www.cs.brown.edu/research/aurora/\\n  [2]: http://infolab.stanford.edu/stream/\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.9846\\n  [4]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.80.798\\n",,
1973,2,919,89cd3ef2-aa39-461e-b6eb-aa309e405f43,2010-07-28 17:14:08.0,287.0,"I'm not sure I exactly understand your question, but I'm assuming your confusion involves a categorical predictor in your model. When it comes to continuous variables in a regression, the coefficients for each predictor are weights for the value of the predictor to produce the predicted y value:e.g.  y = 2*x   \\n\\nHowever, with a categorical variable, weights are meaningless. What does 2*Male mean, or in your case, 2*forest.\\n\\nSo, the coefficients returned for levels of categorical variables represent how different they are from some reference level. In experimental settings, your reference level would be the control group, and then you would get a coefficient for every treatment group, indicating what the size of the effect of each treatment was.\\n\\nIn my own research, and I'm guessing in yours too, there isn't always a meaningful control category for the reference level. So, what I'd do is set the reference level to whatever category would make exposition of the comparisons easiest. Or, you could use different contrasts, like sum contrasts, but those have their own difficulties especially if you have sparse data for one or more categories.",,
1974,2,920,51ceafb5-0c3c-49f5-b167-9719bc1f62c2,2010-07-28 17:15:09.0,559.0,"I completed a Monte Carlo simulation that consisted of 1,000,000 individual simulations. The simulation returns a variable, p, that can be either 1 or 0. I then weight the simulations based on predefined criteria and calculate the probability of p. I also calculate a risk ratio using p:\\n> Risk ratio = P(p|test case) /\\n> P(p|control case)\\n\\nI had eight Monte Carlo runs, which consist of one control case and seven test cases. I need to know if the probabilities of p are statistical different compared to the other cases. I know I can use a multiple comparison test or nonparametric ANOVA to test individual variables, but how do I do this for probabilities? \\n\\n\\n----------\\n\\n\\nFor example are these two probabilities statistically different?:\\n\\n*Probabilities*:\\n\\nP(p|test #3) = 4.08 X 10^-5\\n\\nP(p|test #4) = 6.10 X 10^-5\\n\\n*Risk Ratios*:\\n\\nRisk Ratio(test #3) = 0.089\\n\\nRisk Ratio (test #4) = 0.119 \\n\\n",,
1975,1,920,51ceafb5-0c3c-49f5-b167-9719bc1f62c2,2010-07-28 17:15:09.0,559.0,Test if probabilities are statistically different?,,
1976,3,920,51ceafb5-0c3c-49f5-b167-9719bc1f62c2,2010-07-28 17:15:09.0,559.0,<statistical-analysis><probability><statistical-significance>,,
1979,2,922,08082ebb-a7f3-42b9-8aaf-eed2bd350ced,2010-07-28 17:28:47.0,251.0,"I don't think there is a fundamental idea around parameter estimation in Machine Learning.  The ML crowd will happily maximize the likelihood or the posterior, as long as the algorithms are efficient and predict ""accurately"".  The focus is on computation, and results from statistics are widely used.\\n\\nIf you're looking for fundamental ideas in general, then in computational learning theory, [PAC][1] is central; in statistical learning theory, [structural risk miniminization][2]; and there are other areas (for example, see the *[Prediction Science][3]* post by John Langford).\\n\\nOn bridging statistics/ML, the divide seems exagerrated.  I liked gappy's [answer][4] to the ""Two Cultures"" question.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Probably_approximately_correct_learning\\n  [2]: http://en.wikipedia.org/wiki/Structural_risk_minimization\\n  [3]: http://hunch.net/?p=612\\n  [4]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning/607#607\\n\\n\\n",,
1981,5,894,0d854fe9-65e6-493a-89ab-b990b56a0333,2010-07-28 17:34:31.0,1356.0,"Or simply: the number of elements in a numerical array that you're allowed to change so that the value of the statistic remains unchanged.\\n\\n    # for instance if:\\n    x + y + z = 10\\n\\nyou can change, for instance, *x* and *y* at random, but you cannot change *z* (you can, but not at random, therefore you're not *free* to change it - see Harvey's comment), 'cause you'll change the value of the statistic (&Sigma; = 10). So, in this case df = 2.\\n\\n",corrected semantical error,
1982,2,924,50b89b6a-14f8-4e94-a9bf-4534c3ba9f84,2010-07-28 17:36:24.0,559.0,"For 1,000,000 observations, I observed a discrete event, X, 3 times for the control group and 10 times for the test group. How do I determine for a large number of observations (1,000,000), if three is statistically different than ten?",,
1983,1,924,50b89b6a-14f8-4e94-a9bf-4534c3ba9f84,2010-07-28 17:36:24.0,559.0,"Determine if three is statistically different than ten for a very large number of observations (1,000,000)",,
1984,3,924,50b89b6a-14f8-4e94-a9bf-4534c3ba9f84,2010-07-28 17:36:24.0,559.0,<statistical-analysis><analysis><statistical-significance>,,
1985,2,925,1a25014c-ac71-428c-b1a4-fe6e7106f4d3,2010-07-28 17:40:56.0,,I would be really surprised if you find the difference statistically significant. Having said that you may want to use a test for a difference of proportions (3 out of 1M vs 10 out of 1M).,,user28
1986,6,924,1cbbea1b-ceba-4bf1-b942-e25fd139c417,2010-07-28 17:41:36.0,,<hypothesis-testing>,edited tags,user28
1987,2,926,3b90ed2a-7c2a-4b1f-87fe-b2a5b025a904,2010-07-28 17:43:48.0,287.0,"I think a simple chi-squared test will do the trick. Do you have 1,000,000 observations for both control and test? If so, your table of observations will be (in R code)\\n\\nEdit: Woops! Left of a zero!\\n\\n    m <- rbind(c(3, 1000000-3), c(10, 1000000-10))\\n    #      [,1]   [,2] \\n    # [1,]    3 999997\\n    # [2,]   10 999990\\n\\nAnd chi-squared test will be\\n\\n    chisq.test(m)\\n\\nWhich returns chi-squared = 2.7692, df = 1, p-value = 0.0961, which is not statistically significant at the p < 0.05 level. I'd be surprised if these could be clinically significant anyway.",,
1988,2,927,6480c709-9809-490d-a57b-f0e6dbf4b0b7,2010-07-28 17:43:49.0,319.0,"What are some podcasts related to statistical analysis?  I've found some audio recordings of college lectures on ITunes U, but I'm not aware of any statistical podcasts.  The closest thing I'm aware of is an operations research podcast [The Science of Better][1].  It touches on statistical issues, but it's not specifically a statistical show.\\n\\n\\n  [1]: http://www.scienceofbetter.org/podcast/",,
1989,1,927,6480c709-9809-490d-a57b-f0e6dbf4b0b7,2010-07-28 17:43:49.0,319.0,Statistical podcasts,,
1990,3,927,6480c709-9809-490d-a57b-f0e6dbf4b0b7,2010-07-28 17:43:49.0,319.0,<untagged>,,
1991,16,927,6480c709-9809-490d-a57b-f0e6dbf4b0b7,2010-07-28 17:43:49.0,319.0,,,
1992,5,926,82e1180f-0255-48e6-9464-08bf699921f1,2010-07-28 17:51:05.0,287.0,"I think a simple chi-squared test will do the trick. Do you have 1,000,000 observations for both control and test? If so, your table of observations will be (in R code)\\n\\nEdit: Woops! Left off a zero!\\n\\n    m <- rbind(c(3, 1000000-3), c(10, 1000000-10))\\n    #      [,1]   [,2] \\n    # [1,]    3 999997\\n    # [2,]   10 999990\\n\\nAnd chi-squared test will be\\n\\n    chisq.test(m)\\n\\nWhich returns chi-squared = 2.7692, df = 1, p-value = 0.0961, which is not statistically significant at the p < 0.05 level. I'd be surprised if these could be clinically significant anyway.",spelling,
1993,2,928,d0353465-a817-4b58-bcd7-b855b3a8b238,2010-07-28 17:57:51.0,1356.0,"This one is bothering me for a while, and a great dispute was held around it. In psychology (as well as in other social sciences), we deal with different ways of dealing with numbers :-) i.e. **the levels of measurement**. It's also common practice in psychology to standardize some questionnaire, hence transform the data into percentile scores (in order to assess a respondent's position within the representative sample).\\n\\nLong story short, if you have a variable that holds the data expressed in percentile scores, how should you treat it? As an ordinal, interval, or even ratio variable?!\\n\\nIt's not ratio, cause there no real 0 (0<sup>th</sup> percentile doesn't imply absence of measured property, but the variable's smallest value). I advocate the view that percentile scores are ordinal, since P<sub>70</sub> - P<sub>50</sub> is not equal to P<sub>50</sub> - P<sub>30</sub>, while the other side says it's interval. \\n\\nPlease gentlemen, cut the cord. Ordinal or interval?",,
1994,1,928,d0353465-a817-4b58-bcd7-b855b3a8b238,2010-07-28 17:57:51.0,1356.0,Measurement level of percentile scores,,
1995,3,928,d0353465-a817-4b58-bcd7-b855b3a8b238,2010-07-28 17:57:51.0,1356.0,<beginner><measurement>,,
1996,2,929,fc82cd50-ff20-4550-a39a-a6529aa64784,2010-07-28 17:59:42.0,560.0,"How comprehensive is the following book - What interpretations are missing?\\n\\nInterpretations of Probability, Andrei Khrennikov, 2009, de Gruyter, ISBN 978-3-11-020748-4 \\n\\nhttp://www.degruyter.com/cont/fb/ma/detailEn.cfm?isbn=9783110207484&sel=pi\\n\\nContents:http://www.degruyter.com/files/pdf/9783110207484Contents.pdf",,
1997,1,929,fc82cd50-ff20-4550-a39a-a6529aa64784,2010-07-28 17:59:42.0,560.0,Probability  Interpretations,,
1998,3,929,fc82cd50-ff20-4550-a39a-a6529aa64784,2010-07-28 17:59:42.0,560.0,<probability>,,
1999,5,918,f59e426b-3f13-4ea2-ae7d-2038242bc8d0,2010-07-28 18:04:28.0,251.0,"This area roughly falls into two categories.  The first concerns stream processing and querying issues and associated models and algorithms.  The second is efficient algorithms and models for learning from data streams (or data stream mining).\\n\\nIt's my impression that the CSP industry is connected to the first area.  For example, StreamBase originated from the [Aurora][1] project at Brown/Brandeis/MIT.  A similar project was Widom's [STREAM][2] at Stanford.  Reviewing the publications at either of those projects' sites should help exploring the area.  \\n\\nA nice paper summarizing the research issues (in 2002) from the first area is *[Models and issues in data stream systems][3]* by Babcock et al.  In stream mining, I'd recommend starting with *[Mining Data Streams: A Review][4]* by Gaber et al.\\n\\nBTW, I'm not sure exactly what you're interested in as far as specific models.  If it's stream mining and classification in particular, the [VFDT][5] is a popular choice.  The two review papers (linked above) point to many other models and it's very contextual.\\n\\n\\n  [1]: http://www.cs.brown.edu/research/aurora/\\n  [2]: http://infolab.stanford.edu/stream/\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.9846\\n  [4]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.80.798\\n  [5]: http://en.wikipedia.org/wiki/Incremental_decision_tree#VFDT",added 329 characters in body,
2000,2,930,6e0ea5b4-f8d2-43b5-8743-4edb617501e5,2010-07-28 18:05:11.0,88.0,Continuous (interval); this is a method how to convert ordinal data to something that may have some distribution that makes sense.,,
2001,12,930,47c53e2c-3cd7-4789-8fc7-495b3a2406c0,2010-07-28 18:06:59.0,88.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""}]}",,
2002,13,930,69277a60-d35e-44d7-ba92-0713a960fd8b,2010-07-28 18:08:36.0,88.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""}]}",,
2003,2,931,1ff79f08-fe25-443c-bf58-3d697c9209f3,2010-07-28 18:17:46.0,561.0,You may be interested in the following link: http://www.ats.ucla.edu/stat/seminars/ where the UCLA Statistical Computing unit of the UCLA has *very nice* screen-casts available. I have found them very useful in the past. They function essentially as lectures. Top-quality teaching.,,
2004,16,931,1ff79f08-fe25-443c-bf58-3d697c9209f3,2010-07-28 18:17:46.0,-1.0,,,
2005,2,932,f1e8b0bb-d4fb-4f1f-925c-89cca7431e57,2010-07-28 18:18:26.0,,"**Background to understand my answer**\\n\\nThe critical property that distinguishes between ordinal and interval scale is whether we can take *ratio of differences*. While you cannot take ratio of direct measures for either scale the ratio of differences is meaningful for interval but not ordinal (See: http://en.wikipedia.org/wiki/Level_of_measurement#Interval_scale).\\n\\nTemperature is the classic example for an interval scale. Consider the following:\\n\\n80 f = 26.67 c\\n\\n40 f = 4.44 c and\\n\\n20 f = -6.67 c\\n\\nDifferences between the first and the second is:\\n\\n40 f and 22.23 c\\n\\nDifference between the second and the third is:\\n\\n20 f and 11.11 c\\n\\nNotice that the ratio is the same irrespective of the scale on which we measure temperature. \\n\\nA classic example of ordinal data is ranks. If three teams are ranked 1, 2 and 4 then a statement like so does not make sense: ""Team 1's difference in strength vis-a-vis team 2 is half of team 2's difference in strength relative to team 4."" (Abuse of notation: I am using the same number as rank and team index).\\n\\n**Answer to your question**\\n\\nIs ratio of differences in percentiles meaningful? In other words, is the ratio of difference in percentiles invariant to the underlying scale? Consider, for example: (P<sub>70</sub>-P<sub>50</sub>) / (P<sub>50</sub>-P<sub>30</sub>)?\\n\\nSuppose that these percentiles are based on an underlying score between 0-100 and we compute the above ratio. Clearly, we would obtain the same ratio of percentile differences under arbitrary linear transformation of the score (e.g., multiply all scores by 10 so that the range is between 0-1000 and compute the percentiles).\\n\\nThus, my answer: **Interval**",,user28
2006,2,933,225824cd-240b-4c11-937a-e4d69c984214,2010-07-28 18:21:10.0,561.0,"I have done very well with reading the official documentation. It is well-written, sometimes injected with humour (!) and, if you're willing to spend the time to learn Stata properly, is an absolute goldmine.",,
2007,2,934,2aa6f016-173c-4d60-be8f-3d1c5fbf42f1,2010-07-28 18:25:00.0,511.0,"\\nIf statistics is all about maximizing likelihood, then machine learning is all about minimizing loss. Since you don't know the loss you will incur on future data, you minimize an approximation, ie empirical loss.\\n\\nFor instance, if you have a prediction task and are evaluated by the number of misclassifications, you could train parameters so that resulting model produces the smallest number of misclassifications on the training data. ""Number of misclassifications"" (ie, 0-1 loss) is a hard loss function to work with because it's not differentiable, so you approximate it with a smooth ""surrogate"". For instance, log loss is an upper bound on 0-1 loss, so you could minimize that instead, and this will turn out to be the same as maximizing conditional likelihood of the data. With parametric model this approach becomes equivalent to logistic regression.\\n\\nIn a structured modeling task, and log-loss approximation of 0-1 loss, you get something different from maximum conditional likelihood, you will instead maximize <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.8122"">product</a> of (conditional) marginal likelihoods.\\n\\nTo get better approximation of loss, people noticed that training model to minimize loss and using that loss as an estimate of future loss is an overly optimistic estimate. So for more accurate (true future loss) minimization they add a bias correction term to empirical loss and minimize that, this is known as structured risk minimization.\\n\\nIn practice, figuring out the right bias correction term may be too hard, so you add an expression ""in the spirit"" of the bias correction term, for instance, sum of squares of parameters. In the end, almost all parametric machine learning supervised classification approaches end up training the model to minimize the following\\n\\nSum_i L(m(x_i,w),y_i) + P(w)\\n\\nwhere m is your model parametrized by vector w, i is taken over all datapoints {x_i,y_i}, L is some computationally nice approximation of your true loss and P(w) is some bias-correction/regularization term\\n\\nFor instance if you x \\in {-1,1}^d, y \\in {-1,1}, a typical approach would be to let m(x)=w.x, L(m(x),y)=-log(m(x)y), P(w)=q w.w, and choose q by cross validation",,
2008,5,934,af6fb205-be11-40b9-acc6-857ee1e96974,2010-07-28 18:35:23.0,511.0,"If statistics is all about maximizing likelihood, then machine learning is all about minimizing loss. Since you don't know the loss you will incur on future data, you minimize an approximation, ie empirical loss.\\n\\nFor instance, if you have a prediction task and are evaluated by the number of misclassifications, you could train parameters so that resulting model produces the smallest number of misclassifications on the training data. ""Number of misclassifications"" (ie, 0-1 loss) is a hard loss function to work with because it's not differentiable, so you approximate it with a smooth ""surrogate"". For instance, log loss is an upper bound on 0-1 loss, so you could minimize that instead, and this will turn out to be the same as maximizing conditional likelihood of the data. With parametric model this approach becomes equivalent to logistic regression.\\n\\nIn a structured modeling task, and log-loss approximation of 0-1 loss, you get something different from maximum conditional likelihood, you will instead maximize <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.8122"">product</a> of (conditional) marginal likelihoods.\\n\\nTo get better approximation of loss, people noticed that training model to minimize loss and using that loss as an estimate of future loss is an overly optimistic estimate. So for more accurate (true future loss) minimization they add a bias correction term to empirical loss and minimize that, this is known as structured risk minimization.\\n\\nIn practice, figuring out the right bias correction term may be too hard, so you add an expression ""in the spirit"" of the bias correction term, for instance, sum of squares of parameters. In the end, almost all parametric machine learning supervised classification approaches end up training the model to minimize the following\\n\\nSum_i L(m(x_i,w),y_i) + P(w)\\n\\nwhere m is your model parametrized by vector w, i is taken over all datapoints {x_i,y_i}, L is some computationally nice approximation of your true loss and P(w) is some bias-correction/regularization term\\n\\nFor instance if you x \\in {-1,1}^d, y \\in {-1,1}, a typical approach would be to let m(x)=sign(w.x), L(m(x),y)=-log(y x.w), P(w)=q w.w, and choose q by cross validation",added 4 characters in body,
2009,2,935,4c7c0ae1-9ba2-4eb3-af31-3cb98d504d7b,2010-07-28 18:36:09.0,39.0,"Though quantum probability and negative probability models are quite interesting, this is hardly exhaustive of nonstandard models of probability.  There are for instance, [imprecise probability models][1], and models that violate Kolmogorov's countable additivity axiom, and more.\\n\\nAs an aside, the book may be more properly called Models of Probability.  Interpretations of probability, generally involve competing understandings of probability as limiting frequency, propensities, subjective beliefs, etc.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Imprecise_probability",,
2011,5,935,2c7c00b1-b667-4048-972f-99451a1b4065,2010-07-28 19:00:51.0,39.0,"Though quantum probability and negative probability models are quite interesting, this is hardly exhaustive of nonstandard models of probability.  There are for instance, [imprecise probability models][1], and models that violate Kolmogorov's countable additivity axiom, and more.\\n\\nAs an aside, the book may be more properly called 'Models of Probability'.  [Interpretations of probability][2], generally involve characterizing the competing understandings of probability as logically proscribed values, limiting frequencies, propensities, subjective beliefs, etc.   Models, or axiomatizations can certainly be motivated by these understandings, but the problem of creating a variant system is different than arguing for a particular interpretation.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Imprecise_probability\\n  [2]: http://plato.stanford.edu/entries/probability-interpret/",added 307 characters in body,
2012,5,935,4b34d03c-8c3a-4870-8699-e7de793fdcdf,2010-07-28 19:14:49.0,39.0,"Though quantum probability and negative probability models are quite interesting, this is hardly exhaustive of nonstandard models of probability.  There are for instance, [imprecise probability models][1], and models that violate Kolmogorov's countable additivity axiom, and more.\\n\\nAs an aside, the book may be more properly called 'Models of Probability'.  [Interpretations of probability][2], generally involve characterizing the competing understandings of probability as logically prescribed values, limiting frequencies, propensities, subjective beliefs, etc.   Models, or axiomatizations can certainly be motivated by these understandings, but the problem of creating a variant system is different than arguing for a particular interpretation.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Imprecise_probability\\n  [2]: http://plato.stanford.edu/entries/probability-interpret/",edited body,
2013,2,936,f9d5c544-084b-4517-8bc4-f2639b4f3e59,2010-07-28 19:22:44.0,253.0,"There is [econtalk][1], it is mostly about economics, but delves very often to issues of research, science, and statistics.\\n\\n\\n  [1]: http://www.econtalk.org/",,
2014,16,936,f9d5c544-084b-4517-8bc4-f2639b4f3e59,2010-07-28 19:22:44.0,-1.0,,,
2015,5,860,167cc0ff-5b7b-43d3-aa8b-a831069d5296,2010-07-28 19:33:16.0,511.0,"Let f be your true distribution, and g the family from which you are trying to fit your data. Then theta, maximum likelihood estimator of parameters of g, is a random variable. You could formulate model selection as finding distribution family g that minimizes expected KL divergence between f and g(\\theta), which can be written as\\n\\nEntropy(f)-E_x E_y[log(g(x|theta(y)))]\\n\\nSince you are minimizing over g's, Entropy(f) term doesn't matter and you look for g that maximizes E_x E_y[log(g(x|theta(y)))]\\n\\nLet L(\\theta(y)|y) be the likelihood of data y according to g(\\theta). You could estimate E_x E_y[log(g(x|theta(y)))] as log(L(\\theta(y)|y)) but that estimator is biased\\n\\nAkaike's showed that when f belongs to family g with dimension k, the following estimator asymptotically unbiased\\n\\nlog(L(theta(y)|y))-k\\n\\nBurnham has more details in this <a href=""http://faculty.washington.edu/skalski/classes/QERM597/papers_xtra/Burnham%20and%20Anderson.pdf"">paper</a>, also blog <a href=""http://www.emakalic.org/blog/?p=26"">post</a> by Enes Makalic has further explanation and references","correction, AIC is only asymptotically unbiased, AICc is unbiased for finite sample sizes; added 117 characters in body; added 3 characters in body",
2016,2,937,cfbe2f0d-5da5-4cf5-b695-f9eb5cf363e3,2010-07-28 19:34:00.0,251.0,"BBC's [More or Less][1] is often concerned with numeracy and statistical literacy issues.  But it's not specifically about statistics.  Their [About][2] page has some background.\\n\\n> More or Less is devoted to the powerful, sometimes beautiful, often abused but ever ubiquitous world of numbers.\\n  The programme was an idea born of the sense that numbers were the principal language of public argument.\\n  [...]\\n\\n  [1]: http://news.bbc.co.uk/2/hi/programmes/more_or_less/default.stm\\n  [2]: http://news.bbc.co.uk/2/hi/programmes/more_or_less/1628489.stm\\n\\n\\n",,
2017,16,937,cfbe2f0d-5da5-4cf5-b695-f9eb5cf363e3,2010-07-28 19:34:00.0,-1.0,,,
2018,5,918,264d60f8-0318-4a81-b1ac-cc3dc835d524,2010-07-28 20:07:08.0,251.0,"This area roughly falls into two categories.  The first concerns stream processing and querying issues and associated models and algorithms.  The second is efficient algorithms and models for learning from data streams (or data stream mining).\\n\\nIt's my impression that the CEP industry is connected to the first area.  For example, StreamBase originated from the [Aurora][1] project at Brown/Brandeis/MIT.  A similar project was Widom's [STREAM][2] at Stanford.  Reviewing the publications at either of those projects' sites should help exploring the area.  \\n\\nA nice paper summarizing the research issues (in 2002) from the first area is *[Models and issues in data stream systems][3]* by Babcock et al.  In stream mining, I'd recommend starting with *[Mining Data Streams: A Review][4]* by Gaber et al.\\n\\nBTW, I'm not sure exactly what you're interested in as far as specific models.  If it's stream mining and classification in particular, the [VFDT][5] is a popular choice.  The two review papers (linked above) point to many other models and it's very contextual.\\n\\n\\n  [1]: http://www.cs.brown.edu/research/aurora/\\n  [2]: http://infolab.stanford.edu/stream/\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.9846\\n  [4]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.80.798\\n  [5]: http://en.wikipedia.org/wiki/Incremental_decision_tree#VFDT",CSP -> CEP,
2019,2,938,61f937a1-3df9-47b6-9ba5-6d1de1786bd4,2010-07-28 20:43:32.0,,"Gelman et al. is well-regarded but explictly intended for a graduate course. If you don't have substantial prior coursework in statistics, it is largely a waste.",,Paul Riedesel
2020,16,938,61f937a1-3df9-47b6-9ba5-6d1de1786bd4,2010-07-28 20:43:32.0,-1.0,,,
2021,2,939,a255a004-eb9e-4029-bfee-27ff3d81ab2d,2010-07-28 20:43:33.0,559.0,Is the Yates' correction for continuity used only for 2X2 matrices? ,,
2022,1,939,a255a004-eb9e-4029-bfee-27ff3d81ab2d,2010-07-28 20:43:33.0,559.0,Yates' correction for continuity only for 2X2?,,
2023,3,939,a255a004-eb9e-4029-bfee-27ff3d81ab2d,2010-07-28 20:43:33.0,559.0,<statistics><contingency-tables>,,
2024,2,940,a061fe4f-4b53-413e-942d-acd9c78a3307,2010-07-28 20:50:25.0,87.0,"If you have 1,000,000 independent ""coin flips"" that can produce 1 with probabilty (prob) and 0 with probability (1-prob), then the number of 1's observed will follow a [Binomial distribution][1].  \\n\\nTests of statistical significance are rejection tests, i.e. reject the hypothesis that the two parameters are equal if the probability that param2 is observed in test2 when the true value is param1 is less than a certain number, like 5%, 1%, or 0.1%.  These tests are typically constructed from the cumulative distribution function.  \\n\\nThe cumulative distribution function for a binomial is ugly, but can be found in R and probably some other statistics packages as well.  \\n\\nBut the good news is that with 1,000,000 cases you don't need to do that.... you would if you had a relatively small number of cases.  \\n\\nBecause you have 1,000,000 independent flips, the CDF of a normal distribution is a good approximation (the Law of Large Numbers plays a role here).  The mean and variance you need to use are the obvious ones, and are in the [Binomial Wikipedia][2] article... You are then comparing two normally distributed variables and can use all the standard tests you would use with normally distributed variables. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Binomial_distribution\\n  [2]: http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation",,
2025,2,941,788b26f3-02dd-41f7-87ea-6a291ae6a7a1,2010-07-28 21:12:03.0,251.0,"It's derived for binomial/hypergeometric distributions, so it's applicable to 2x2 or 2x1 cases.",,
2026,2,942,d11bed05-53b7-476a-b814-8a8de7aa85a8,2010-07-28 21:13:23.0,75.0,"I've been beginning to work my way through [Statistical Data Mining Tutorials by Andrew Moore][1] (highly recommended for anyone else first venturing into this field).  I started by reading this [extremely interesting PDF entitled ""Introductory overview of time-series-based anomaly detection algorithms""][2] in which Moore traces through many of the techniques used in the creation of an algorithm to detect disease outbreaks.  Halfway through the slides, on page 27, he lists a number of other ""state of the art methods"" used to detect outbreaks.  The first one listed is **wavelets**.  Wikipeida describes a wavelet as\\n\\n> a wave-like oscillation with an\\n> amplitude that starts out at zero,\\n> increases, and then decreases back to\\n> zero. It can typically be visualized\\n> as a ""brief oscillation""\\n\\nbut does not describe their application to statistics and my Google searches yield highly academic papers that assume a knowledge of how wavelets relate to statistics or full books on the subject.\\n\\nI would like a basic understanding of how wavelets are applied to time-series anomaly detection, much in the way Moore illustrates the other techniques in his tutorial.  Can someone provide an explanation of how detection methods using wavelets work or a link to an understandable article on the matter?\\n\\n\\n  [1]: http://www.autonlab.org/tutorials/\\n  [2]: http://www.autonlab.org/tutorials/biosurv01.pdf",,
2027,1,942,d11bed05-53b7-476a-b814-8a8de7aa85a8,2010-07-28 21:13:23.0,75.0,Application of wavelets to time-series-based anomaly detection algorithms,,
2028,3,942,d11bed05-53b7-476a-b814-8a8de7aa85a8,2010-07-28 21:13:23.0,75.0,<beginner><statistical-analysis><time-series>,,
2029,5,940,df7789f0-9c07-488d-8ba4-dc4a34c27958,2010-07-28 21:19:40.0,87.0,"If you have 1,000,000 independent ""coin flips"" that can produce 1 with probabilty (prob) and 0 with probability (1-prob), then the number of 1's observed will follow a [Binomial distribution][1].  \\n\\nTests of statistical significance are rejection tests, i.e. reject the hypothesis that the two parameters are equal if the probability that param2 is observed in test2 when the true value is param1 is less than a certain number, like 5%, 1%, or 0.1%.  These tests are typically constructed from the cumulative distribution function.  \\n\\nThe cumulative distribution function for a binomial is ugly, but can be found in R and probably some other statistics packages as well.  \\n\\nBut the good news is that with 1,000,000 cases you don't need to do that.... you would if you had a relatively small number of cases.  \\n\\nBecause you have 1,000,000 independent flips, the CDF of a normal distribution is a good approximation (the Law of Large Numbers plays a role here).  The mean and variance you need to use are the obvious ones, and are in the [Binomial Wikipedia][2] article... You are then comparing two normally distributed variables and can use all the standard tests you would use with normally distributed variables. \\n\\nFor instance, if the true probability were 40*10^-6 then in 1,000,000 tests you would expect to see 40 +/- 6  positive cases. If the acceptance interval for a test is, for instance, 5 standard deviations wide on each side, then this would be compatible with both observations.  If it were just 3 std dev wide on each side, one case would fit and the other would be statistically different.  \\n\\n  [1]: http://en.wikipedia.org/wiki/Binomial_distribution\\n  [2]: http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation",added 237 characters in body; added 13 characters in body; added 30 characters in body; added 113 characters in body,
2030,2,943,0720c7bc-075c-4aad-ab28-feb5889f95fb,2010-07-28 21:31:14.0,279.0,"If you can assume bivariate normality, then you can develop a likelihood-ratio test comparing the two possible covariance matrix structures. The unconstrained (H_a) maximum likelihood estimates are well known - just the sample covariance matrix, the constrained ones (H_0) can be derived by writing out the likelihood (and will probably be some sort of ""pooled"" estimate). \\n\\nIf you don't want to derive the formulas, you can use SAS or R to fit a repeated measures model with unstructured and compound symmetry covariance structures and compare the likelihoods.",,
2031,2,944,575f2db2-1149-48b9-bc52-657f9e0e5059,2010-07-28 21:45:46.0,,"When I type a left paren or any quote in the R console, it automatically creates a matching one to the right of my cursor. I guess the idea is that I can just type the expression I want inside without having to worry about matching, but I find it annoying, and would rather just type it myself. How can I disable this feature?\\n\\nI am using R 2.8.0 on OSX 10.5.8.",,anonymous
2032,1,944,575f2db2-1149-48b9-bc52-657f9e0e5059,2010-07-28 21:45:46.0,,How can I get R to stop autocompleting my quotes/parens?,,anonymous
2033,3,944,575f2db2-1149-48b9-bc52-657f9e0e5059,2010-07-28 21:45:46.0,,<r>,,anonymous
2034,2,945,55ff6fb1-3812-40d5-bb83-6965927cc3bd,2010-07-28 21:49:13.0,334.0,"Well either use a different IDE -- this is entirely a feature of the OS X app -- or try to configure the feature in question. \\n\\nAs for IDEs / R environments, I'm rather happy with [ESS](http://ess.r-project.org)  which works on every platform R works on. ",,
2035,2,946,6d9e2746-1727-4d1f-bad0-620e12d767e2,2010-07-28 21:50:29.0,569.0,"New to the site.  I am just getting started with R, and want to replicate a feature that is available in SPSS.  \\n\\nSimply, I build a ""Custom Table"" in SPSS with a single categorical variable in the column and many continous/scale variables in the rows (no interactions, just stacked on top of each other).  \\n\\nThe table reports the means and valid N's for each column (summary statistics are in the rows), and select the option to generate significance tests for column means (each column against the others) using alpha .05 and adjust for unequal variances.\\n\\nHere is my question.\\n\\nHow can I replicate this in R?  What is my best option to build this table  and what tests are available that will get me to the same spot?  Since I am getting used to R, I am still trying to navigate around what is available.\\n\\nMany thanks in advance!",,
2036,1,946,6d9e2746-1727-4d1f-bad0-620e12d767e2,2010-07-28 21:50:29.0,569.0,Column Means Significance Tests in R,,
2037,3,946,6d9e2746-1727-4d1f-bad0-620e12d767e2,2010-07-28 21:50:29.0,569.0,<r><mean><statistical-significance><spss>,,
2038,2,947,61b74dbd-74f2-45e7-8968-b982d2335e38,2010-07-28 21:51:15.0,287.0,"On OSX, go to `R > Preferences > Editor` and deselect `Match braces/quotes`",,
2039,2,948,c8c1e5c2-5b35-4ef4-ad0c-2c29f9a3d0c6,2010-07-28 21:54:16.0,287.0,"    summary(df)\\n\\nWill give you 5 number summaries and counts of `NA` for continuous variables, and counts for categorical variables.\\n\\nAs for the significance tests, you'll have to do that by hand with `t.test()` or `wilcox.test()`.",,
2040,2,949,c68652f2-7cb2-4ae2-bcd2-c37dd2c51e11,2010-07-28 21:59:02.0,511.0,"Suppose x is in {0,1}^d and y is in {0,1} and we model the task of predicting y given x using logistic regression. When can logistic regression coefficients be written in closed form?\\n\\nOne example is when we use a saturated model.\\n\\nIE, define P(y|x)\\propto exp(sum_i w_i f_i({x}_i)), where i indexes sets in the power-set of {x1,...,x_d), and f_i returns 1 if all variables in i'th set are 1, 0 otherwise. Then you can express each w_i in a trained logistic regression model as a logarithm of a rational function of statistics of the data. \\n\\nAre there other interesting examples when closed form exists?",,
2041,1,949,c68652f2-7cb2-4ae2-bcd2-c37dd2c51e11,2010-07-28 21:59:02.0,511.0,When is logistic regression solved in closed form?,,
2042,3,949,c68652f2-7cb2-4ae2-bcd2-c37dd2c51e11,2010-07-28 21:59:02.0,511.0,<logistic>,,
2043,5,949,f74abf09-ae4c-4200-99a9-1d941b10c317,2010-07-28 22:27:36.0,511.0,"Suppose x is in {0,1}^d and y is in {0,1} and we model the task of predicting y given x using logistic regression. When can logistic regression coefficients be written in closed form?\\n\\nOne example is when we use a saturated model.\\n\\nIE, define P(y|x)\\propto exp(sum_i w_i f_i({x}_i)), where i indexes sets in the power-set of {x1,...,x_d), and f_i returns 1 if all variables in i'th set are 1, 0 otherwise. Then you can express each w_i in this logistic regression model as a logarithm of a rational function of statistics of the data. \\n\\nAre there other interesting examples when closed form exists?",deleted 5 characters in body,
2044,2,950,5da589e4-8fde-4bbc-9172-e2b4ee32fb3f,2010-07-28 23:28:44.0,99.0,"One does not need to know about conditional probability or Bayes Theorem to figure out that it is best to switch your answer.\\n\\nSuppose you initially pick Door 1.   Then the probability of Door 1 being a winner is 1/3 and the probability of Doors 2 or 3 being a winner is 2/3.   If Door 2 is shown to be a loser by the host's choice then the probabilty that 2 or 3 is a winner is still 2/3.   But since Door 2 is a loser, Door 3 must have a 2/3 probability of being a winner.     \\n\\n",,
2045,2,951,d084755c-d2c3-47b9-9a35-7286427c44d2,2010-07-29 00:48:11.0,172.0,What is the relationship between a Nonhomogeneous Poisson process and a process that has heavy tail distribution for its inter arrival times?\\n\\nAny pointer to a resource that can shed some light on this question would be hugely appreciated\\n\\n,,
2046,1,951,d084755c-d2c3-47b9-9a35-7286427c44d2,2010-07-29 00:48:11.0,172.0,Nonhomogeneous Poisson and Heavy tail inter arrival time distribution,,
2047,3,951,d084755c-d2c3-47b9-9a35-7286427c44d2,2010-07-29 00:48:11.0,172.0,<distributions><poisson>,,
2048,2,952,f4e05bbc-08ec-4ec5-ba42-2eb710c2cb29,2010-07-29 01:45:55.0,513.0,"I have a survey of 16 questions, each with four possible responses. The purpose of the survey is to measure the respondent's propensity towards four categories (which we will denote A, B, C, D). Each of the four responses per question are representative of an aspect of the four categories, A, B, C, D.\\n\\nThe respondent rank orders each of the four responses (we will denote the first response by ""4"", the second by ""3"", etc).\\n\\nTo score the categories, we add the responses up based on the coding above. There are 16 x (4 + 3 + 2 + 1) = 160 total points. The sums for each category are computed, and the maximum score is deemed the respondent's dominant category.\\n\\nTherefore each survey looks like the following (in CSV format)\\n\\nquestion_num, A, B, C, D<br />\\n1, 4, 3, 1, 2<br />\\n2, 3, 4, 1, 2<br />\\n3, 3, 4, 2, 1<br />\\n4, 4, 3, 1, 2<br />\\n5, 4, 3, 1, 2<br />\\n6, 4, 3, 2, 1<br />\\n7, 4, 3, 1, 2<br />\\n.<br />\\n.<br />\\n.<br />\\n16, 3, 4, 1, 2<br />\\nsums, 64, 48, 24, 24\\n\\nI have about 325 surveys completed.\\n\\nMy question is that I want to look for possible redundancy in the survey so I can eliminate some questions easing the burden on the future respondents.\\n\\nMy first pass was to do a multi-logistic regression with the response as the dominant category (described above). Would this\\n\\nI'm not sure PCA will reveal anything useful.\\n\\nAny thoughts?\\n\\nThanks,<br />\\nAdam",,
2049,1,952,f4e05bbc-08ec-4ec5-ba42-2eb710c2cb29,2010-07-29 01:45:55.0,513.0,find survey redundancy,,
2050,3,952,f4e05bbc-08ec-4ec5-ba42-2eb710c2cb29,2010-07-29 01:45:55.0,513.0,<logistic><surveys>,,
2051,2,953,07e5546f-45f3-4b84-9ca6-d2514be7a00b,2010-07-29 02:00:32.0,183.0,"I agree with the suggestions about running a regression possibly with log(y) as the outcome variable or some other suitable transformation. I just wanted to add one comment, if you are reporting the bivariate association, you might prefer:\\n(a) to correlate log(x) and log(y),\\n(b) Spearman's rho, which correlates the ranks of the two variables.\\n",,
2052,2,954,343c7b02-11bd-47c1-a7ba-429a71afc995,2010-07-29 02:11:34.0,183.0,"To follow on from Dirk's comment, if you don't like your current IDE, check out some of the existing discussion on R IDEs:\\nhttp://stackoverflow.com/questions/1439059/best-ide-texteditor-for-r\\n",,
2053,5,439,9518ce33-7510-417a-a14b-9920ec48803a,2010-07-29 02:46:37.0,30.0,"As you said, it's not necessarily the case that a mathematician may want a rigorous book. Maybe the goal is to get some intuition of the concepts quickly, and then fill in the details. I recommend two books from CMU professors, both published by Springer: ""All of Statistics"" by Larry Wasserman is quick and informal. ""Theory of Statistics"" by Mark Schervish is rigorous and relatively complete. It has decision theory, finite sample, some asymptotics and sequential analysis.\\n\\nAdded 7/28/10: There is one additional reference that is orthogonal to the other two: very rigorous, focused on learning theory, and short. It's by Smale (Steven Smale!) and Cucker, ""[On the Mathematical Foundations of Learning][1]"". Not easy read, but the best crash course on the theory.\\n\\n\\n  [1]: http://www.ams.org/journals/bull/2002-39-01/S0273-0979-01-00923-5/home.html",Added reference.,
2054,2,955,62fdad1b-e369-483c-ad37-095ea51d6991,2010-07-29 03:25:03.0,175.0,"Just wonder, is there any data analysis/ statistic/ data mining work that are available on freelance basis?\\n\\nThis could be subjective and argumentative, which is why I put it as CW. ",,
2055,1,955,62fdad1b-e369-483c-ad37-095ea51d6991,2010-07-29 03:25:03.0,175.0,Data Analysis Work-- Is there Any Freelance Opportunity?,,
2056,3,955,62fdad1b-e369-483c-ad37-095ea51d6991,2010-07-29 03:25:03.0,175.0,<statistical-analysis>,,
2057,2,956,b63372b5-15b5-4d04-8731-45fe324c21e3,2010-07-29 04:06:29.0,25.0,"As Robin said, you've got the Benjamini-Hochberg method backwards. With that method, you set a value for Q (upper case Q; the maximum desired FDR) and it then sorts your comparisons into two piles. The goal is that no more than Q% of the comparisons in the ""discovery"" pile are false (and thus at least 100%-Q%) are true. \\n\\nIf you computed a new value for each comparison, which is the value of Q at which that comparisons would just barely be considered a discovery, then those new values are q-values (lower case q; see the link to a paper by John Storey in the original question).\\n",,
2059,2,957,5788912c-75be-48c2-b5b8-30be26b36c27,2010-07-29 04:15:44.0,25.0,"I think you need to nail down the question you are asking, before you can compute an answer. I think this question is way too vague to answer: ""test whether it is an vis-a-vis the general population"". \\n\\nThe only question I think you can answer is this one: If the new value came from the same population as the others, what is the chance that it will be so far (or further) from the sample mean? That is the question that your equation will begin to answer, although it is not quite right. Here is a corrected equation that includes n.\\n\\nt = (x - mean(S))/(stdev(S)/sqrt(n))\\n\\nCompute the corresponding P value (with n-1 degrees of freedom) and you've answered the question.",,
2060,2,958,4255b9e5-1d9b-43e2-a041-61084df2622c,2010-07-29 05:55:55.0,183.0,"The short answer is ""yes"". Such work does exist.\\nFor example, there is sometimes consulting work in and around universities.\\nAlso, some companies wish to outsource data analysis and statistical activities.\\n\\nIn general, I found that word of mouth was a powerful tool. Once you build up a good reputation in a given community, additional requests for work will follow.\\n\\nsee: http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=statistics+jobs\\n",,
2061,2,959,c1e443ae-648c-43dd-8103-47b3117cdb5a,2010-07-29 06:01:01.0,183.0,"I've never had to perform such analyses, but there is an academic literature on the factor analysis of ipsative tests that would be relevant:\\ne.g.,\\n\\n- http://deepblue.lib.umich.edu/bitstream/2027.42/68736/2/10.1177_004912418000900206.pdf\\n- http://www.informaworld.com/smpp/content~db=all~content=a785042624\\n",,
2062,2,960,e81545be-cf1e-49c9-9e91-a6aa67c999f6,2010-07-29 07:05:38.0,88.0,"So restructure your data merging all user responses, so in such form:\\n\\n    Q1 Q2 Q3 ...\\n    user1 rank for option1 for Q1, user1 rank for option1 for Q2, ...\\n    user1 rank for option2, ...\\n    ...\\n    user2 rank for option1, ...\\n    ...\\n    user325 rank for option4, ...\\n\\nAnd then cluster the questions. I recommend agglomerative clustering, there it is easy to see what questions can be removed.",,
2063,2,961,6371cd5b-d606-4159-b1e2-4a9b764f890b,2010-07-29 07:07:36.0,582.0,"I have a dataset made up of elements from three groups, let's call them G1, G2, and G3.\\nI analysed certain characteristics of these elements and divided them into 3 types of ""behaviour"" T1, T2, and T3 (I used cluster analysis to do that).\\n\\nSo, now I have a 3 x 3 contingency table like this with the counts of elements in the three groups divided by type:\\n\\n          |    T1   |    T2   |    T3   |\\n    ------+---------+---------+---------+---\\n      G1  |   18    |   15    |   65    | \\n    ------+---------+---------+---------+---\\n      G2  |   20    |   10    |   70    |\\n    ------+---------+---------+---------+---\\n      G3  |   15    |   55    |   30    |\\n\\nNow, I can run a Fisher test on these data in R\\n\\n    data <- matrix(c(18, 20, 15, 15, 10, 55, 65, 70, 30), nrow=3)\\n    fisher.test(data)\\n\\nand I get\\n\\n       Fisher's Exact Test for Count Data\\n    \\n    data:  data \\n    p-value = 9.028e-13\\n    alternative hypothesis: two.sided     \\n\\nSo my questions are:\\n\\n* is it correct to use Fisher test this way?\\n\\n* how do I know who is different from who? Is there a post-hoc test I can use? Looking at the data I would say the 3<sup>rd</sup> group has a different behaviour from the first two, how do I show that statistically? \\n\\n* someone pointed me to logit models: are they a viable option for this type of analysis?\\n\\n* any other option to analyse this type of data?\\n\\n\\nThank you a lot\\n\\nnico",,
2064,1,961,6371cd5b-d606-4159-b1e2-4a9b764f890b,2010-07-29 07:07:36.0,582.0,Statistical test for n x m contingency tables,,
2065,3,961,6371cd5b-d606-4159-b1e2-4a9b764f890b,2010-07-29 07:07:36.0,582.0,<statistical-analysis><logit><contingency-tables>,,
2066,2,962,8ff10d76-ce13-47cb-8747-4db2205f71f7,2010-07-29 07:10:39.0,223.0,"I don't know much about anomaly detection but I know wavelet is usefull to detect singularities in a signal (see for example the paper http://www.math.univ-toulouse.fr/~bigot/Site/Publications_files/Spectrometry.pdf (see figure 3 for an illustration) and the references mentionned in the paper.  I guess singularities are sometime anomaly? \\n\\nThe idea here is that the Continuous wavelet transform has maxima lines that propagates along frequencies, the longuer the line is the higher is the singularity. See Figure 3 in the paper to see what I mean ! note that there is a free matlab code related to that paper... \\n\\n\\n----------\\n\\n Additionally, I can give you some heuristic about why the DISRCETE (preceding example is about the continuous one) wavelet transform is interesting for a statistician (excuse non exhaustivity).   \\n\\n-	There is a wide class of (realistic) signal that are transformed into a sparse sequence by the wavelet transform. (compression property)\\n-	A wide class of (quasi stationary) process that are transform into a sequence with almost uncorrelated features (decorrelation property)\\n-	 Wavelet coefficients contain are containing an information that is localized in time and in frequency (at different scales). (multiscale property)\\n-	Wavelet coefficients of a signal concentrate on its singularities. \\n\\n\\n",,
2067,5,739,2b3f57b1-0ee2-4e3c-b4a4-0a3aec7b845d,2010-07-29 07:26:45.0,,">""To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.""<br>\\n\\n-- Ronald Fisher (1938)",added 2 characters in body,sjcockell
2068,2,963,7ed559b7-5515-489e-9a9f-daca3966b6bc,2010-07-29 08:15:59.0,339.0,"As I read in help for the `t.test`, it is only applicable for the 2-sample tests. If you want to perform it to every combination of the columns of a matrix A, taken 2 at a time, you could do something like this (for the moment, I can't recall a better way)\\n\\n    apply(combn(1:dim(A)[2],2),2,function(x) t.test(A[,x[1]],A[,x[2]])) \\n\\n",,
2069,5,963,63a08c81-5c77-4656-9c88-a431ee2880ae,2010-07-29 08:22:43.0,339.0,"As I read in help for the `t.test`, it is only applicable for the 2-sample tests. If you want to perform it to every combination of the columns of a matrix A, taken 2 at a time, you could do something like this (for the moment, I can't recall a better way)\\n\\n    apply(combn(1:dim(A)[2],2),2,function(x) t.test(A[,x[1]],A[,x[2]])) \\n\\nor, if you just want the p.values\\n\\n    t(apply(combn(1:dim(A)[2],2),2,function(x) c(x[1],x[2],(t.test(A[,x[1]],A[,x[2]]))$p.value)))",added 134 characters in body,
2070,2,964,6710406f-dc3f-4525-863f-7847c781ff2d,2010-07-29 08:28:36.0,438.0,"So in R, for instance, this would be:\\n\\n    my_ts_logged_diffed = diff(log(some_ts_object))\\n    plot(my_ts_logged_diffed)\\n\\nThis seems to be part of every experienced analyst/forecaster analytical workflow--in particular, a *visual examination of the plotted data*. What are they looking for--i.e., what useful information does this transformation help reveal?\\n\\nSimilarly, I have a pretty good selection of time series textbooks, tutorials, and the like; nearly all of them mention this analytical step, but none of them say why it's done (i am sure there's a good reason, and one that's apparently too obvious to even mention).\\n\\n(i do indeed routinely rely on this transformation but only for the limited purpose of testing for a normal distribution (i think the test is called *Shapiro-Wilk*). The application of the test just involves (assuming i am applying it correctly) comparing a couple of parameters (a 'W' parameter and the p-value) against a baseline--the Test doesn't appear to require plotting the data).\\n",,
2071,1,964,6710406f-dc3f-4525-863f-7847c781ff2d,2010-07-29 08:28:36.0,438.0,"What are analytics looking for when they  Plot  a differenced, logged time series? ",,
2072,3,964,6710406f-dc3f-4525-863f-7847c781ff2d,2010-07-29 08:28:36.0,438.0,<time-series><data-transformation>,,
2073,2,965,2ab95b1f-f9da-47c1-b997-032fc5c2fa08,2010-07-29 08:46:34.0,56.0,"Most growth/decay processes will at most change the moving quantity at an exponential rate. The differences of the logs of the quantity relate to the local slope &lambda;, so for a underlying exponential growth or decay process it would be flat in t. Any deviation from flat gives you hints if and where there are switchovers between different purly exponential pieces; also for chaotic behavior you would expect more than linear growth or decay, i.e. a ""local &lambda;"" curve that is not flat for small t-ranges.",,
2074,2,966,14f33892-fac5-4319-bb0e-4d781da839f8,2010-07-29 10:22:37.0,334.0,"This is often used for a price to return transformation based on assuming continuously compounded returns. The Campbell, Lo, and MacKinlay book (Econometrics of Financial Markets, 1997) lays it out quite nicely:\\n \\nDefine r_t as the log of gross returns 1 + R_t:\\n\\n    r_t := log(1 + R_t) \\n\\nwhich is the same as the log of the previous prices\\n\\n    log(P_t / P_{t-1}) \\n\\nwhich is the same as \\n\\n    p_t - p_{t-1}\\n\\nwhich is what you asked about. ",,
2075,4,964,5c4a61ee-5893-4d95-a7b5-54b9bb35f85f,2010-07-29 10:33:44.0,438.0,"What are analysts looking for when they  Plot  a differenced, logged time series? ",word in title spelled incorrectly,
2076,2,967,4907c90b-941f-420d-9c6e-98e3e41032c9,2010-07-29 10:50:53.0,215.0,"*""I keep saying that the sexy job in the next 10 years will be statisticians. And I'm not kidding.""*\\n\\nVal Harian",,
2077,16,967,4907c90b-941f-420d-9c6e-98e3e41032c9,2010-07-29 10:50:53.0,-1.0,,,
2078,2,968,420cd14b-4e64-4a7a-8198-2a2a91c4366b,2010-07-29 10:57:18.0,1356.0,"> All generalizations are false,\\n> including this one.\\n\\n*Mark Twain*\\n",,
2079,16,968,420cd14b-4e64-4a7a-8198-2a2a91c4366b,2010-07-29 10:57:18.0,-1.0,,,
2080,2,969,318541cb-b9b1-4d18-a730-87b73c434ef2,2010-07-29 11:25:13.0,144.0,"""An ecologist is a statistician who likes to be outside"". -- apparently a good friend of <a href=""http://r.789695.n4.nabble.com/Why-software-fails-in-scientific-research-td1573062.html#a2275423"">Murray Cooper</a>.",,
2081,16,969,318541cb-b9b1-4d18-a730-87b73c434ef2,2010-07-29 11:25:13.0,-1.0,,,
2082,2,970,595ac023-3759-4fd3-b1d7-ff5c4c1dbd64,2010-07-29 11:41:41.0,247.0,"In general there is none. A Poisson process has inter-arrival times that are exponentially distributed, which does not have heavy tails. ",,
2083,2,971,7c6f1cac-1b0c-4d2b-b35b-fc987150617f,2010-07-29 11:41:49.0,339.0,"At first I think that the Fisher test is used correctly.\\n\\nCount data are better handled using log-linear models (not logit, to ensure that the fitted values are bounded below). In R you can specify `family=poisson` (which sets errors = Poisson and link = log). The log link ensures that all the fitted values are positive, while the Poisson errors take account of the fact that the data are integer and have variances that are equal to their means.\\ne.g. `glm(y~x,poisson)` and the model is fitted with a log link and Poisson errors (to account for the non-normality). \\n\\nIn cases where there is overdispersion (the residual deviance should be equal to the residual degrees of freedom, if the Poisson errors assumption is appropriate), instead of using `quasipoisson` as the error family, you could fit a negative binomial model.\\n(This involves the function `glm.nb` from package `MASS`)\\n\\nIn your case you could fit and compare models using using commands like the following:\\n\\n    observed <- as.vector(data)\\n    Ts<-factor(rep(c(""T1"",""T2"",""T3""),each=3))\\n    Gs<-factor(rep(c(""G1"",""G2"",""G3""),3))\\n\\n    model1<-glm(observed~Ts*Gs,poisson)\\n\\n    #or and a model without the interaction terms\\n    model2<-glm(observed~Ts+Gs,poisson)\\n\\n \\n    #you can compare the two models using anova with a chi-squared test\\n    anova(model1,model2,test=""Chi"")\\n    summary(model1)\\n\\n\\nAlways make sure that your minimal model contains all the nuisance variables.\\n\\nAs for how do we know who is different from who, there are some plots that may help you. R function `assocplot` produces an association plot indicating deviations from independence of rows and columns in a two dimensional contingency table.\\n\\n\\nHere are the same data plotted as a mosaic plot\\n\\n    mosaicplot(data, shade = TRUE)",,
2084,2,972,2ad0de0f-90c0-46aa-ae8d-2ae558f62bf4,2010-07-29 11:43:54.0,566.0,"There are no general heuristics, you should make a grid search, especially since the value of nu must be between 0-1.",,
2085,2,973,e5ab8de4-715d-4dfe-b2dc-b30bc38bf047,2010-07-29 12:02:28.0,223.0,"\\n\\nWhat are the freely available data set for classification with more than 1000 features (or sample points if is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a more focused list that can be used more conveniently, also I propose the following rules:\\n\\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n\\n----------\\n\\n\\nRemark: I'm not sure if this type of question is too much connected to \\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\nso don't hesitate to comment about that (not in answers please)  it's easy too remove the post :) ",,
2086,1,973,e5ab8de4-715d-4dfe-b2dc-b30bc38bf047,2010-07-29 12:02:28.0,223.0,free data set for very high dimensional classification,,
2087,3,973,e5ab8de4-715d-4dfe-b2dc-b30bc38bf047,2010-07-29 12:02:28.0,223.0,<dataset>,,
2088,16,973,e5ab8de4-715d-4dfe-b2dc-b30bc38bf047,2010-07-29 12:02:28.0,223.0,,,
2092,10,955,da32e366-94f1-400c-83ce-5c01a522783b,2010-07-29 12:38:50.0,-1.0,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":88,""DisplayName"":""mbq""},{""Id"":5,""DisplayName"":""Shane""},{""Id"":8,""DisplayName"":""csgillespie""}]}",2,
2093,10,944,c28d5fc6-cf76-43d5-803a-0e6daa07f2a9,2010-07-29 12:40:55.0,-1.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":159,""DisplayName"":""Rob Hyndman""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":8,""DisplayName"":""csgillespie""}]}",2,
2094,2,975,cb92faa4-b935-411f-ac15-29d1d3ee7c81,2010-07-29 12:44:50.0,8.0,"Another good podcast is [In our time][1] by the BBC. It's a weekly podcast (off air for the summer) that deals with topics in History, Religion and Science. I would say that about 1 in 12 podcasts deal with Mathematics and Statistics. Take a look at the podcast archive for [Science subjects][2].\\n\\n\\n  [1]: http://www.bbc.co.uk/radio4/features/in-our-time/\\n  [2]: http://www.bbc.co.uk/radio4/features/in-our-time/archive/science",,
2095,16,975,cb92faa4-b935-411f-ac15-29d1d3ee7c81,2010-07-29 12:44:50.0,-1.0,,,
2098,5,973,3b4305e4-e0f0-46bb-a7bb-88094b6021f1,2010-07-29 13:41:02.0,223.0,"\\nWhat are the freely available data set for classification with more than 1000 features (or sample points if is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a more focused list that can be used more conveniently, also I propose the following rules:\\n\\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n\\n----------\\n\\n\\nRemark: I'm not sure if this type of question is too much connected to \\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\nso don't hesitate to comment about that (not in answers please)  it's easy to remove the post :) ",deleted 3 characters in body,
2099,2,977,b5617840-56b3-4b5a-b9e3-10f52181c589,2010-07-29 13:43:53.0,210.0,"When we are monitoring movements of structures we normally install monitoring points onto the structure before we do any work which might cause movement. This gives us chance to take a few readings before we start doing the work to 'baseline' the readings. \\n\\nQuite often the data is quite variable (the variations in the reading can easily be between 10 and 20% of the fianl movement). The measurements are also often affected by the environment in which they are taken so one set of measurements taken on one project may not have the same accuracy as measurements on another project. \\n\\nIs there any statisitcal method, or rule of thumb that can be applied to say how many baseline readings need to be taken to give a certain accuracy before the first reading is taken? Are there any rules of humb that can be applied to this situation? ",,
2100,1,977,b5617840-56b3-4b5a-b9e3-10f52181c589,2010-07-29 13:43:53.0,210.0,How many measurements are needed to 'baseline' a measurement?,,
2101,3,977,b5617840-56b3-4b5a-b9e3-10f52181c589,2010-07-29 13:43:53.0,210.0,<variance><measurement>,,
2102,2,978,7ba03638-7bef-4b56-ba07-2c5154878426,2010-07-29 13:51:07.0,8.0,"I think you should look at [power calculations][1]. These are often used to decide the sample size of survey or clinical trial. Taken from wikipedia:\\n\\n> A priori power analysis is conducted\\n> prior to the research study, and is\\n> typically used to determine an\\n> appropriate sample size to achieve\\n> adequate power.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Statistical_power",,
2106,2,980,5c675b64-01f9-46bc-ad63-dad284d9641b,2010-07-29 14:04:20.0,,"I haven't studied statistics for over 10 years (and then just a basic course), so maybe my question is a bit hard to understand.\\n\\nAnyway, what I want to do is reduce the number of data points in a serie. The x-axis is number of milliseconds since start of measurement and the x-axis is the reading for that point. \\n\\nOften there is thousands of data points, but I might only need a few hundreds. So my question is: How do I accurately reduce the number of data points?\\n\\nWhat is the process called? (So I can google it)\\nAre there any prefered algorithms (I will implement it in C#)\\n\\nHope you got some clues. Sorry for my lack of proper terminology.",,Pete
2107,1,980,5c675b64-01f9-46bc-ad63-dad284d9641b,2010-07-29 14:04:20.0,,How do I reduce the number of data points in a serie?,,Pete
2108,3,980,5c675b64-01f9-46bc-ad63-dad284d9641b,2010-07-29 14:04:20.0,,<data-visualization>,,Pete
2109,5,980,5e5c830b-f3ca-4a37-ac27-152c197fc021,2010-07-29 14:10:30.0,,"I haven't studied statistics for over 10 years (and then just a basic course), so maybe my question is a bit hard to understand.\\n\\nAnyway, what I want to do is reduce the number of data points in a serie. The x-axis is number of milliseconds since start of measurement and the y-axis is the reading for that point. \\n\\nOften there is thousands of data points, but I might only need a few hundreds. So my question is: How do I accurately reduce the number of data points?\\n\\nWhat is the process called? (So I can google it)\\nAre there any prefered algorithms (I will implement it in C#)\\n\\nHope you got some clues. Sorry for my lack of proper terminology.","Changed ""x-axis"" to ""y-axis"" (which is the correct one)",Pete
2110,2,981,14f69ee1-02a4-4167-8af3-36eb8f8f27aa,2010-07-29 14:15:09.0,8.0,"You could always use [simple random sampling][1]. \\n\\nSuppose you have *N* points and you only want *n* of them. Then generate *n* random numbers from a discrete uniform *U(0, N-1)* distribution. These would be the points you use.\\n\\nIf you want to do this sequentially, i.e. at each point you decide to use it or not, then just accept a point with probability *p*. So if you set *p=0.01* you would accept (on average) 1 point in a hundred.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Simple_random_sample",,
2111,2,982,6f963cdd-59b7-49f1-bfbf-f58397acf474,2010-07-29 14:18:02.0,6.0,"Well, I think the word you're looking for is ""sampling,"" but I'm not sure why you want to do it. Thousands of data points isn't very many. Or are you looking just to plot a smaller number of equally-spaced points? That's usually called ""binning."" \\n\\nIs your goal to generate a visualization? In that case, you might want to keep the raw data, plot it as a scattergraph, then overlay some sort of central tendency (regression line, spline, whatever) to communicate whatever the takehome message ought to be.\\n\\nOr is your goal to numerically summarize the results in some way? In that case, you might want to explain your problem in more detail!",,
2112,2,983,bf2c8a6f-360b-451c-910a-91c4130872cb,2010-07-29 14:26:53.0,6.0,"It really depends on the amount of variance relative to the size of a measurement that you care about. If you need to be able to tell the difference between a mean of 2 and a mean of 0, and your data look like this:\\n\\n    -4.4 3.8 -2.0 -5.1 0.2 7.1 0.9 -5.4 2.8 0.5\\n\\nThen you're going to need a lot more data! But if you only care if the average is less than or more than 10, then that much data is adequate. \\n\\n@cgillespie gives the technically correct response. You need to have some idea what size of effect you care about, as well as some idea how much variance your measurements have. If the equations of power analysis are more than you can deal with, you can always use random numbers in an Excel spreadsheet! Generate columns of random numbers with a normal distribution and various means and variances, then figure out whether the confidence intervals (2 * standard deviation / sqrt(N)) around the mean of each set of numbers include differences that you might are about. Do that a bunch of times. That'll give you a good idea of how many measurements you need, and it's not too hard to explain to others.\\n\\n",,
2113,2,984,9a506efd-f837-4296-bee8-3be02884916e,2010-07-29 14:29:16.0,601.0,"You're not providing enough information.  Why do you want to reduce the data points.  A few thousand is nothing these days.\\n\\nGiven that you want the same result each time you view the same data perhaps you want to simply bin averages.  You have variable spacing on your x-axis.  Maybe you're trying to make that consistent?  In that case you would set a bin width of perhaps 50 msec, or 100, and then average all the points in there.  Make the bin width as large as you need to reduce the data points to the size of the set you want.\\n\\nIt's really a hard question to answer without a reason for why you're getting rid of data.",,
2116,5,980,d59b9a4e-a841-4e8a-838d-e41f212ede6d,2010-07-29 15:10:57.0,,"I haven't studied statistics for over 10 years (and then just a basic course), so maybe my question is a bit hard to understand.\\n\\nAnyway, what I want to do is reduce the number of data points in a serie. The x-axis is number of milliseconds since start of measurement and the y-axis is the reading for that point. \\n\\nOften there is thousands of data points, but I might only need a few hundreds. So my question is: How do I accurately reduce the number of data points?\\n\\nWhat is the process called? (So I can google it)\\nAre there any prefered algorithms (I will implement it in C#)\\n\\nHope you got some clues. Sorry for my lack of proper terminology.\\n\\n---\\n\\nEdit: More details comes here:\\n\\nThe raw data I got is heart rate data, and in the form of number of milliseconds since last beat. Before plotting the data I calculate number of milliseconds from first sample, and the bpm (beats per minute) at each data point (60000/timesincelastbeat). \\n\\nI want to visualize the data, i.e. plot it in a line graph. I want to reduce the number of points in the graph from thousands to some hundreds. \\n\\nOne option would be to calculate the average bpm for every second in the series, or maybe every 5 seconds or so. That would have been quite easy if I knew I would have at least one sample for each of those periods (seconds of 5-seconds-intervals).",Provided more info.,Pete
2117,2,986,8e43696d-5b1e-4f76-b5de-9b502060f5e0,2010-07-29 15:20:56.0,419.0,You can use *multinom* from nnet package for multinomial regression. \\nPost-hoc tests you can use *linearHypothesis* from car package.\\nYou can conduct test of independence using *linearHypothesis* (Wald test) or *anova* (LR test).\\n\\n\\n,,
2118,5,856,398bf8c8-7ee1-48b5-9a68-16f3f93245b7,2010-07-29 15:21:16.0,506.0,"Does anyone know of a variation of Fisher's Exact Test which takes weights into account? For instance [sampling weights][1].  \\nSo instead of the usual 2x2 cross table, every data point has a ""mass"" or ""size"" value weighing the point.\\n\\nExample data:\\n\\n    A B weight\\n    N N 1\\n    N N 3\\n    Y N 1\\n    Y N 2\\n    N Y 6\\n    N Y 7\\n    Y Y 1\\n    Y Y 2\\n    Y Y 3\\n    Y Y 4\\n\\nFisher's Exact Test then uses this 2x2 cross table:\\n\\n    A\\B  N  Y All\\n     N   2  2   4\\n     Y   2  4   6\\n    All  4  6  10\\n\\nIf we would take the weight as an 'actual' number of data points, this would result in:\\n\\n    A\\B  N  Y All\\n     N   4 13  17\\n     Y   3 10  13\\n    All  7 23  30\\n\\nBut that would result in much too high a confidence. One data point changing from N/Y to N/N would make a very large difference in the statistic.  \\nPlus, it wouldn't work if any weight contained fractions.\\n\\n\\n  [1]: http://www.measuredhs.com/help/Datasets/sampling_weights.htm","clarified ""weights""",
2123,2,990,fc7952af-a25c-455a-ac24-adfc8b188e3e,2010-07-29 15:43:35.0,3807.0,> You may be too vague to be wrong and\\n> that's really bad cause that's just\\n> obscuring the issue.\\n\\nBruce Sterling ,,
2124,16,990,fc7952af-a25c-455a-ac24-adfc8b188e3e,2010-07-29 15:43:35.0,-1.0,,,
2125,2,991,7414966b-43e5-4eb3-8c8c-ae787612d2f9,2010-07-29 15:45:39.0,603.0,"usually, you plot such a series to check the extend to which it exhibits heteroskedasticity.\\nDepending on the answer, you may have to model the residuals, even if you are only interested in the mean. \\n\\nOn the top of my head, this article is a practical example:\\n\\nhttp://www.google.com/url?sa=t&source=web&cd=1&ved=0CBIQFjAA&url=http%3A%2F%2Fdss.ucsd.edu%2F~jhamilto%2FJHamilton_Engle.pdf&ei=CKJRTN-uO4XeOO_LhL4E&usg=AFQjCNEP4dL3_uRf28371SnBS4lhhnYsSw&sig2=tJRjwUetH8XTHuijcXYmbg ",,
2126,2,992,5742e6aa-9f10-42fe-b433-58750d2ec055,2010-07-29 15:50:24.0,603.0,"Try a bivariate robust regression \\n(see http://cran.r-project.org/web/packages/rrcov/vignettes/rrcov.pdf for an intro).\\n\\nIf your data points are all positive, you might want to try to regress log(y) on log(x).\\nNote that log() is *not* a substitute for a robust regression, but it sometimes makes the results more interpretable.\\n\\n\\n",,
2127,5,981,5dda807d-4422-4b06-85fe-9fee03e4e829,2010-07-29 15:51:03.0,8.0,"You could always use [simple random sampling][1]. \\n\\nSuppose you have *N* points and you only want *n* of them. Then generate *n* random numbers from a discrete uniform *U(0, N-1)* distribution. These would be the points you use.\\n\\nIf you want to do this sequentially, i.e. at each point you decide to use it or not, then just accept a point with probability *p*. So if you set *p=0.01* you would accept (on average) 1 point in a hundred.\\n\\n**Edit following question edit**\\n\\nFollowing your question update I would recommend smoothing. \\n\\nPossibly something like a simple moving average type scheme. For example, you could work out the averages per second to get:\\n\\n    s_0, s_1, s_2, s_3, s_4, ....\\n\\nThe you could combine the s_i's with a [weighted moving average][2], i.e. values in the past provide less information.\\n\\nYou could go for something more advanced, like a [kernel smoother][3]. You will need to be careful that you don't smooth too much, since I assume that a sudden drop should be picked up very quickly in your scenario. \\n\\nThere should be *C#* libraries available for this sort of stuff.\\n\\nIf *N* is too large to handle/cope with, then I would thin it down in a sequential manner (as described above) and then smooth.\\n\\n\\n\\nAfter your update, I would probably go with smoothing. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Simple_random_sample\\n  [2]: http://en.wikipedia.org/wiki/Moving_average#Weighted_moving_average\\n  [3]: http://en.wikipedia.org/wiki/Kernel_smoother",Updated answer following question edit,
2129,2,993,eb306ff3-5c01-4300-94ee-12633cdec242,2010-07-29 15:57:21.0,506.0,"A one-sided Fisher's Exact test gives p-value = **0.092284**.\\n\\n    function p = fexact(k, x, m, n)\\n    %FEXACT Fisher's Exact test.\\n    %   Y = FEXACT(K, X, M, N) calculates the P-value for Fisher's\\n    %   Exact Test.\\n    %   K, X, M and N must be nonnegative integer vectors of the same\\n    %   length.  The following must also hold:\\n    %   X <= N <= M, X <= K <= M and K + N - M <= X.  Here:\\n    %   K is the number of items in the group,\\n    %   X is the number of items in the group with the feature,\\n    %   M is the total number of items,\\n    %   N is the total number of items with the feature,\\n    \\n    if nargin < 4\\n       help(mfilename);\\n       return;\\n    end\\n    nr = length(k);\\n    if nr ~= length(x) | nr ~= length(m) | nr ~= length(n)\\n       help(mfilename);\\n       return;\\n    end\\n    \\n    na = nan;\\n    v = na(ones(nr, 1));\\n    mi = max(0, k + n - m);\\n    ma = min(k, n);\\n    \\n    d = hygepdf(x, m, k, n) * (1 + 5.8e-11);\\n    for i = 1:nr\\n      y = hygepdf(mi(i):ma(i), m(i), k(i), n(i));\\n      v(i) = sum(y(y <= d(i)));\\n    end\\n    p = max(min(v, 1), 0);\\n    p(isnan(v)) = nan;\\n\\nFor your example, try `fexact(1e6, 3, 2e6, 13)`.",,
2130,2,994,c989e280-3438-4c9c-ada9-4772b6c6f107,2010-07-29 16:00:17.0,,"[The Drunkard's Walk: How Randomness Rules Our Lives][1] by Leonard Mlodinow is an excellent book for laypeople. Enjoyable and educational.\\n\\nIt might not be a textbook, but it makes you think about the world in the right way.\\n\\n  [1]: http://www.amazon.co.uk/Drunkards-Walk-Randomness-Rules-Lives/dp/0713999225",,sjcockell
2131,16,994,c989e280-3438-4c9c-ada9-4772b6c6f107,2010-07-29 16:00:17.0,-1.0,,,
2132,2,995,d9911d4a-0b77-42a8-b9e8-1627611c71ef,2010-07-29 16:02:18.0,,*Collaborative Statistics* is CC BY: http://cnx.org/content/col10522/latest/,,Nicole
2133,16,995,d9911d4a-0b77-42a8-b9e8-1627611c71ef,2010-07-29 16:02:18.0,-1.0,,,
2134,2,996,fc8d6fd3-1fed-49fb-8e1b-53baa12d75a9,2010-07-29 16:13:41.0,603.0,Multivariate robust estimators were designed for this!\\n\\nmultivariate S-estimator:\\n\\nhttp://cran.r-project.org/web/packages/rrcov/vignettes/rrcov.pdf,,
2135,5,993,64bb025d-67bc-40e9-a00f-fea2c8f00442,2010-07-29 16:28:09.0,506.0,"A (two-sided) Fisher's Exact test gives p-value = **0.092284**.\\n\\n    function p = fexact(k, x, m, n)\\n    %FEXACT Fisher's Exact test.\\n    %   Y = FEXACT(K, X, M, N) calculates the P-value for Fisher's\\n    %   Exact Test.\\n    %   K, X, M and N must be nonnegative integer vectors of the same\\n    %   length.  The following must also hold:\\n    %   X <= N <= M, X <= K <= M and K + N - M <= X.  Here:\\n    %   K is the number of items in the group,\\n    %   X is the number of items in the group with the feature,\\n    %   M is the total number of items,\\n    %   N is the total number of items with the feature,\\n    \\n    if nargin < 4\\n       help(mfilename);\\n       return;\\n    end\\n    nr = length(k);\\n    if nr ~= length(x) | nr ~= length(m) | nr ~= length(n)\\n       help(mfilename);\\n       return;\\n    end\\n    \\n    na = nan;\\n    v = na(ones(nr, 1));\\n    mi = max(0, k + n - m);\\n    ma = min(k, n);\\n    \\n    d = hygepdf(x, m, k, n) * (1 + 5.8e-11);\\n    for i = 1:nr\\n      y = hygepdf(mi(i):ma(i), m(i), k(i), n(i));\\n      v(i) = sum(y(y <= d(i)));\\n    end\\n    p = max(min(v, 1), 0);\\n    p(isnan(v)) = nan;\\n\\nFor your example, try `fexact(1e6, 3, 2e6, 13)`.",fix,
2136,2,997,c960e544-f535-4e85-bb0e-cabef999f312,2010-07-29 16:50:11.0,25.0,"The huge denominators through off one's intuition. Since the sample sizes are identical, and the proportions low, the problem can be recast: 13 events occurred, and were expected (by null hypothesis) to occur equally in both groups. In fact the split was 3 in one group and 10 in the other. How rare is that? The binomial test answers.\\n\\nEnter this line into R:\\nbinom.test(7,100,0.20,alternative=""two.sided"")\\n\\nThe two-tail P value is 0.09229, identical to four digits, to the results of Fisher's test. \\n\\nLooked at that way, the results are not surprising. The problem is equivalent to this one: If you flipped a coin 13 times, how surprising would it be to see three or fewer, or ten or more, heads. One of those outcomes would occur 9.23% of the time. \\n\\n",,
2137,2,998,44fb5c84-5fcc-4885-addc-48a208eafd3b,2010-07-29 18:06:31.0,3807.0,"In addition to the other answers:\\nIf you have 1,000,000 observations and when your event comes up only a few times, you are likely to want to look at a lot of different events.\\nIf you look at 100 different events you will run into problems if you work with p<0.05 as criteria for significance. ",,
2138,2,999,f43d62ca-7406-4c5e-974e-e10842a05ed6,2010-07-29 18:33:43.0,,"If you are coming from a SAS or SPSS background, check out:\\n\\nhttp://sites.google.com/site/r4statistics/\\n\\nThis is the companion site to the book, R for SAS and SPSS Users by Robert Muenchen and a free version of the book can be found here.\\n\\n",,alex
2139,6,156,72c43686-f7a8-4006-ae59-a1b5253ef01f,2010-07-29 18:34:37.0,8.0,<regression>,edited tags,
2140,2,1000,87087ca3-2164-49df-a5a7-1df348b283c5,2010-07-29 18:39:45.0,549.0,"Well, if you have a point process that you try modeling as a Poisson process, and find it has heavy tails, there are several possibilities.  What are the key assumptions for a Poisson Process:\\n\\n-There is a constant rate function\\n-Events are memoryless, that is P(E in (t,t+d)) is independent of t and when other events are.\\n-The waiting time until the next event is exponentially distributed (kinda what the previous two are saying)\\n\\nSo, how can you violate these assumptions to get heavy tails?\\n\\n-Non-constant rate function.  If the rate function switches between, say, two values, you'll have too many short wait-times, and too many long wait-times, given the overall rate function.  This can show itself as having heavy tails.\\n-The waiting time is not exponentially distributed.  In which case, you don't have a Poisson process.  You have some other sort of point process.\\n\\nNote that in the extreme case, any point process can be modeled by a NHPP - put a delta function at each event, and set the rate to 0 elsewhere.  I think we can all agree that this is a poor model, having little predictive power.   So if you are interested in a NHPP, you'll want to think a bit about whether that is the right model, or whether you are overly-adjusting a model to fit your data.",,
2141,2,1001,11647961-1fa4-4bb1-8a47-974bf3b71d57,2010-07-29 18:46:47.0,608.0,"I have distributions from two different data sets and I would like to\\nmeasure how similar their distributions (in terms of their bin\\nfrequencies) are. In other words, I am not interested in the correlation of \\ndata point sequences but rather in the their distributional properties with respect to similarity. Currently I can only observe a similarity in eye-balling which is not enough. I don't want to assume causality and I don't want to predict at this point. So, I assume that correlation is the way to go. \\n\\nSpearman's Correlation Coefficient is used to compare non-normal data and since I don't know anything about the real underlying distribution in my data, I think it would be a save bet. I wonder if this measure can also be used to\\ncompare distributional data rather than the data poitns that are\\nsummarized in a distribution. Here the example code in R that exemplifies\\nwhat I would like to check:\\n\\naNorm <- rnorm(1000000)<br>\\nbNorm <- rnorm(1000000)<br>\\ncUni <- runif(1000000)<br>\\nha <- hist(aNorm)<br>\\nhb <- hist(bNorm)<br>\\nhc <- hist(cUni)<br>\\nprint(ha$counts)<br>\\nprint(hb$counts)<br>\\nprint(hc$counts)<br>\\n# relatively similar<br>\\nn <- min(c(NROW(ha$counts),NROW(hb$counts)))<br>\\ncor.test(ha$counts[1:n], hb$counts[1:n], method=""spearman"")<br>\\n# quite different<br>\\nn <- min(c(NROW(ha$counts),NROW(hc$counts)))<br>\\ncor.test(ha$counts[1:n], hc$counts[1:n], method=""spearman"")<br>\\n\\nDoes this make sense or am I violating some assumptions of the coefficient?\\n\\nThanks,\\nR.",,
2142,1,1001,11647961-1fa4-4bb1-8a47-974bf3b71d57,2010-07-29 18:46:47.0,608.0,Spearman's Correlation Coefficient to compare distributions?,,
2143,3,1001,11647961-1fa4-4bb1-8a47-974bf3b71d57,2010-07-29 18:46:47.0,608.0,<distributions>,,
2145,2,1003,b421d662-d76d-4875-99bb-cb6c12f11cac,2010-07-29 18:59:01.0,36.0,"Try IPSUR, Introduction to Probability and Statistics Using R. It's free in the GNU sense of the word. \\n\\nhttp://ipsur.r-forge.r-project.org/book/index.php\\n\\nIt's definitely open source - on the download page you can download the LaTeX source or the lyx source used to generate this. ",,
2146,16,1003,b421d662-d76d-4875-99bb-cb6c12f11cac,2010-07-29 18:59:01.0,-1.0,,,
2147,2,1004,fed730b4-e43c-4791-9457-3201428eba40,2010-07-29 18:59:32.0,88.0,"Rather use [Kolmogorov–Smirnov test][1], which is exactly what you need. R function `ks.test` implements it.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test",,
2148,2,1005,ee38d5f4-c9b6-452d-abd4-e4ea4683887b,2010-07-29 19:01:51.0,36.0,"Try IPSUR, Introduction to Probability and Statistics Using R. It's a free book, free in the GNU sense of the word. \\n\\nhttp://ipsur.r-forge.r-project.org/book/index.php\\n\\nIt's definitely open source - on the download page you can download the LaTeX source or the lyx source used to generate this. ",,
2149,5,973,0cdee456-32e5-4445-b33b-efbb4e346539,2010-07-29 19:07:04.0,223.0,"What are the freely available data set for classification with more than 1000 features (or sample points if is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a more focused list that can be used more conveniently, also I propose the following rules:\\n\\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n\\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n\\n----------\\n\\n\\nRemark: I'm not sure if this type of question is too much connected to \\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\nso don't hesitate to comment about that (not in answers please)  it's easy to remove the post :) ",added 2 characters in body,
2150,5,1004,4afec22d-3d83-48e6-b393-b026162ae9ea,2010-07-29 19:14:52.0,88.0,"Rather use [Kolmogorov–Smirnov test][1], which is exactly what you need. R function `ks.test` implements it.\\n\\nAlso check [this question][2].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\\n  [2]: http://stats.stackexchange.com/questions/411/motivation-for-kolmogorov-distance-between-distributions",added 144 characters in body,
2151,2,1006,5173105a-4d90-4952-953f-f4923210a05e,2010-07-29 19:22:17.0,419.0,"In this case Poisson is good approximation for distribution for number of cases.\\nThere is simple formula to approximate variance of log RR (delta method) .\\n\\nlog RR = 10/3 = 1.2, \\nse log RR = sqrt(1/3+1/10) = 0.66, so 95%CI = (-0.09; 2.5)\\n\\nIt is not significant difference at 0.05 level using two-sided test.\\nLR based Chi-square test for Poisson model gives p=0.046 and Wald test p=0.067.\\nThis results are similar to Pearson Chi-square test without continuity correction (Chi2 with correction p=0.096).\\nAnother possibility is chisq.test with option simulate.p.value=T, in this case p=0.092 (for 100 000 simulations).\\n\\nIn this case test statistics is rather discrete, so Fisher test can be conservative.\\nThere is some evidence that difference can be significant. Before final conclusion data collecting process should be taken into account.\\n \\n",,
2154,2,1008,cce653dd-c329-4a84-ac13-c70e5a7e186c,2010-07-29 19:39:02.0,419.0,"P value from theoretical point of view is some realization of random variable.\\nThere is some standard (in probability) to use upper case letters for random variables and lower case for realizations.\\nIn table headers we should use P (maybe *italicize*), in text together with its value p=0.0012 and in text describing for example methodology p-value .",,
2155,5,981,c78ec3d2-94f3-4c87-ac28-3e3ec8e9dd76,2010-07-29 20:06:56.0,8.0,"You have two problems: too many points and how to smooth over the remaining points.\\n\\n**Thinning your sample**\\n\\nIf you have too many observations arriving in real time, you could always use [simple random sampling][1] to thin your sample. Note, for this too be true, the number of points would have to be very large.\\n\\nSuppose you have *N* points and you only want *n* of them. Then generate *n* random numbers from a discrete uniform *U(0, N-1)* distribution. These would be the points you use.\\n\\nIf you want to do this sequentially, i.e. at each point you decide to use it or not, then just accept a point with probability *p*. So if you set *p=0.01* you would accept (on average) 1 point in a hundred. \\n\\n\\n\\n**Smoothing**\\n\\nPossibly something like a simple moving average type scheme. Or you could go for something more advanced like a [kernel smoother][2] (as others suggested). You will need to be careful that you don't smooth too much, since I assume that a sudden drop should be picked up very quickly in your scenario. \\n\\nThere should be *C#* libraries available for this sort of stuff.\\n\\n\\n\\n**Conclusion**\\n\\nThin if necessary, then smooth.\\n\\n  [1]: http://en.wikipedia.org/wiki/Simple_random_sample\\n  [2]: http://en.wikipedia.org/wiki/Kernel_smoother",Redid the  answer to make it clearer.,
2156,2,1009,ea567fb3-c9c0-405a-8083-47ab483cd875,2010-07-29 20:09:37.0,419.0, 1. For data in IQR range you should use\\n    truncated normal distribution (for\\n    example R package gamlss.tr) to\\n    estimate parameters of this\\n    distribution.  \\n 2. Another approach is using mixture models with 2 or 3 components (distributions). You can fit   such models using gamlss.mx package (distributions from package gamlss.dist can be specified for\\n    each component of mixture).\\n,,
2158,2,1010,14b86db5-b807-4b70-bbf0-10d8adcd59cd,2010-07-29 21:20:25.0,8.0,"If I understand correctly, then you can just fit a mixture of two Normals to the data. There are lots of R packages that are available to do this. This example uses the [mixtools][1] package:\\n\\n    #Taken from the documentation\\n    library(mixtools)\\n    data(faithful)\\n    attach(faithful)\\n    \\n    #Fit two Normals\\n    wait1 = normalmixEM(waiting, lambda = 0.5)\\n    plot(wait1, density=TRUE, loglik=FALSE)\\n\\nThis gives:\\n\\n![Mixture of two Normals][2]\\n\\nThe package also contains more sophisticated methods - check the documentation.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/mixtools/index.html\\n  [2]: http://img294.imageshack.us/img294/4213/kernal.jpg\\n",,
2159,2,1011,18ab08ad-319a-4d28-be89-95605955ae9a,2010-07-29 21:21:02.0,378.0,"Essentially they're looking for the log of the fold-change from one time point to the next because intuitively, it's easier to think about log-fold changes visually than actual fold-changes.\\n\\nLog-fold changes make decreases and increases simply a difference in sign, so that a log-2-fold increase is the same distance as a log-2-fold decrease (i.e. |log 2| = |log 0.5|). In addition, many systems exhibit multiplicative effects for independent events, e.g. biological systems and economic systems, where two independent x-fold increases results in an x^2 fold increase, whereas on a log scale, this becomes an additive (and thus easier to see) 2*log(x) increase.\\n\\nAlso, `diff(log(x))` is prettier than `x[-1]/x[-length(x)]`.",,
2160,2,1012,ea87d49e-b556-470b-a8cc-c7b39b9020bd,2010-07-29 21:29:57.0,196.0,"I understand that the standard distribution of a uniform distribution is not really a meaningful quantity.  However a colleague wants to compare models that use either a Gaussian distribution or a uniform distribution and for other reasons needs the standard distributions of these two distributions to be equal.  In R I can do a simulation...\\n\\n    sd(runif(100000000))\\n    sd(runif(100000000,min=0,max=2))\\n\\nand see that the calculated standard deviation is likely to be ~.2887 * the range of the uniform distribution.  However, I was wondering if there was an equation that could yield the exact value, and if so, what that formula was.\\n",,
2161,1,1012,ea87d49e-b556-470b-a8cc-c7b39b9020bd,2010-07-29 21:29:57.0,196.0,What would the calculated value of the standard distribution of a uniform distribution be?,,
2162,3,1012,ea87d49e-b556-470b-a8cc-c7b39b9020bd,2010-07-29 21:29:57.0,196.0,<distributions><uniform><normal-distribution>,,
2163,5,1012,04084a3a-df6d-418b-94db-1ecdcae56991,2010-07-29 21:36:26.0,196.0,"I understand that the standard deviation of a uniform distribution is not really a meaningful quantity.  However a colleague wants to compare models that use either a Gaussian distribution or a uniform distribution and for other reasons needs the standard devation of these two distributions to be equal.  In R I can do a simulation...\\n\\n    sd(runif(100000000))\\n    sd(runif(100000000,min=0,max=2))\\n\\nand see that the calculated standard deviation is likely to be ~.2887 * the range of the uniform distribution.  However, I was wondering if there was an equation that could yield the exact value, and if so, what that formula was.\\n",wrong word,
2164,4,1012,04084a3a-df6d-418b-94db-1ecdcae56991,2010-07-29 21:36:26.0,196.0,What would the calculated value of the standard deviation of a uniform distribution be?,wrong word,
2165,2,1013,8acb26b9-3d09-49fd-9d9d-848843368837,2010-07-29 21:47:00.0,56.0,"The variance of the continous uniform distribution on the interval [0,1] is 12<sup>-1/2</sup>&asymp;0.288675. The [Wikipedia article][1] lists of it's more properties.\\n\\n[1]: http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29",,
2166,2,1014,0e32619e-ce90-4095-be6d-e2d45205779d,2010-07-29 21:54:24.0,614.0,"In general, the standard deviation of a continous uniform distribution is (max - min) / sqrt(12).",,
2167,2,1015,3aeffa9b-decd-4116-920c-7a474d661e78,2010-07-29 22:03:55.0,108.0,"I am trying to calculate the reliability in an elicitation exercise by analysing some test-retest questions given to the experts. The experts elicited a series of probability distributions which were then compared with the true value (found at a later date). The standardized quadratic scores were then computed, which are the values that I am using to calculate the reliability. \\n\\nWhich reliability method would be appropriate here?  I was looking mostly at Pearson's correlation and Chronbach's alpha (and got some negative values using both methods) but I am not sure this is the right approach.",,
2168,1,1015,3aeffa9b-decd-4116-920c-7a474d661e78,2010-07-29 22:03:55.0,108.0,Reliability in Elicitation Exercise,,
2169,3,1015,3aeffa9b-decd-4116-920c-7a474d661e78,2010-07-29 22:03:55.0,108.0,<elicitation>,,
2170,2,1016,73eb3f80-a120-45ef-96b8-8f332fe9f5db,2010-07-29 22:04:32.0,614.0,I got a linear regression model with the sample and variable observations and I want to know:\\n\\n1. Whether a specific variable is significant enough to remain included in the model.\\n\\n2. Whether another variable (with observations) ought to be included in the model.\\n\\nWhich statistics can help me out? How can get them most efficiently?,,
2171,1,1016,73eb3f80-a120-45ef-96b8-8f332fe9f5db,2010-07-29 22:04:32.0,614.0,Is a variable significant in a linear regression model?,,
2172,3,1016,73eb3f80-a120-45ef-96b8-8f332fe9f5db,2010-07-29 22:04:32.0,614.0,<untagged>,,
2175,2,1018,0aa614db-ffa8-4f04-bae9-4138a05404f9,2010-07-29 22:30:13.0,88.0,[**Arcene**][1]  \\nn=900  \\np=10000 (3k is artificially added noise)  \\nk=2   \\nFrom NIPS2003.\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Arcene,,
2176,16,1018,0aa614db-ffa8-4f04-bae9-4138a05404f9,2010-07-29 22:30:13.0,-1.0,,,
2177,2,1019,912809aa-48eb-47e7-8c70-3d2180206d93,2010-07-29 22:32:44.0,88.0,[**Dexter**][1]  \\nn=2600  \\np=20000 (10k+53 is artificial noise)  \\nk=2 (balanced)  \\nFrom NIPS2003.\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Dexter,,
2178,16,1019,912809aa-48eb-47e7-8c70-3d2180206d93,2010-07-29 22:32:44.0,-1.0,,,
2179,2,1020,6184f3a5-429b-47b4-8ade-4a161ef66e76,2010-07-29 22:35:28.0,88.0,"[**Dorothea**][1]  \\nn=1950   \\np=100000 (0.1M, half is artificially added noise)  \\nk=2 (~10x unbalanced)   \\nFrom NIPS2003.\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Dorothea",,
2180,16,1020,6184f3a5-429b-47b4-8ade-4a161ef66e76,2010-07-29 22:35:28.0,-1.0,,,
2181,5,1018,0d2b895c-02ff-4bff-a5b6-03b0fa6289f7,2010-07-29 22:36:30.0,88.0,[**Arcene**][1]  \\nn=900  \\np=10000 (3k is artificially added noise)  \\nk=2 (~balanced)  \\nFrom NIPS2003.\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Arcene,added 11 characters in body,
2182,2,1021,f7fc8ea2-cd75-45bd-bfe7-ae6cb52bb977,2010-07-29 22:38:21.0,88.0,[**Gisette**][1]  \\nn=13500  \\np=5000 (half is artificially added noise)  \\nk=2 (balanced)  \\nFrom [NIPS2003][2].\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Gisette\\n  [2]: http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf,,
2183,16,1021,f7fc8ea2-cd75-45bd-bfe7-ae6cb52bb977,2010-07-29 22:38:21.0,-1.0,,,
2186,5,1020,1a7d4fd5-3b68-4405-b123-7bb835b45e20,2010-07-29 22:41:33.0,88.0,"[**Dorothea**][1]  \\nn=1950   \\np=100000 (0.1M, half is artificially added noise)  \\nk=2 (~10x unbalanced)   \\nFrom [NIPS2003][2].\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Dorothea\\n  [2]: http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf",added 77 characters in body,
2187,5,1019,02552cfd-6ae5-4f4c-9389-31b960a303fd,2010-07-29 22:41:53.0,88.0,[**Dexter**][1]  \\nn=2600  \\np=20000 (10k+53 is artificial noise)  \\nk=2 (balanced)  \\nFrom [NIPS2003][2].\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Dexter\\n  [2]: http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf,added 79 characters in body,
2188,5,1018,b3928f26-fa38-44c8-acc8-269f81b9d905,2010-07-29 22:42:33.0,88.0,[**Arcene**][1]  \\nn=900  \\np=10000 (3k is artificially added noise)  \\nk=2 (~balanced)  \\nFrom [NIPS2003][2].\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Arcene\\n  [2]: http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf,added 77 characters in body,
2192,2,1023,581e19a8-16d2-467c-8788-9819e90cef39,2010-07-29 23:04:08.0,74.0,"Introductory, advanced, and even obscure, please.\\n\\nMostly to test myself. I like to make sure I know what the heck I'm talking about :)\\n\\nThanks",,
2193,1,1023,581e19a8-16d2-467c-8788-9819e90cef39,2010-07-29 23:04:08.0,74.0,Where can I find good statistics quizzes?,,
2194,3,1023,581e19a8-16d2-467c-8788-9819e90cef39,2010-07-29 23:04:08.0,74.0,<teaching>,,
2195,2,1024,3033c894-0021-4e85-9002-3c5f915b95c3,2010-07-29 23:25:12.0,378.0,"For part 1, you're looking for the [F-test][1]. Calculate your residual sum of squares from each model fit and calculate an F-statistic, which you can use to find p-values from either an F-distribution or some other null distribution that you generate yourself.\\n\\n  [1]: http://en.wikipedia.org/wiki/F-test#Regression_problems",,
2196,10,929,3e19ae33-5699-408e-b68e-9cb948537efc,2010-07-29 23:43:25.0,-1.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":8,""DisplayName"":""csgillespie""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":159,""DisplayName"":""Rob Hyndman""}]}",4,
2197,10,685,1b570e71-777a-4727-bd52-dd0e44766168,2010-07-29 23:45:11.0,-1.0,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":88,""DisplayName"":""mbq""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":8,""DisplayName"":""csgillespie""},{""Id"":159,""DisplayName"":""Rob Hyndman""}]}",3,
2198,2,1025,bc83042d-4f10-43eb-ba57-14b49539acae,2010-07-29 23:45:46.0,3807.0,Calculating averages leads to a different dataset than simply reducing the number of data points.\\nIf one heartbeat per minute is much faster than the other heart beats you will lose the signal through your smoothing process.\\n\\nIf you summary 125-125-0-125-125 as 100 than the story that the data tells is different through your smoothing.\\n\\nSometimes the heart even skips beats and I believe that's an event that interesting for however wants to look at plotted heart rate data.\\nI would really hate the guy who programmed my Polar watch that logs my heart rate and gives me data points every 5 seconds when he had implemented a smoothing algorithm.,,
2200,2,1026,a522f7e2-9c69-4e77-9052-ee72331b4244,2010-07-30 00:00:15.0,159.0,"Statistical significance is not usually a good basis for determining whether a variable should be included in a model. Statistical tests were designed to test hypotheses, not select variables. I know a lot of textbooks discuss variable selection using statistical tests, but this is generally a bad approach. See Harrell's book *[Regression Modelling Strategies][1]* for some of the reasons why. These days, variable selection based on the AIC (or something similar) is usually preferred.\\n\\n\\n  [1]: http://www.amazon.com/Regression-Modeling-Strategies-Frank-Harrell/dp/0387952322",,
2201,2,1027,7a5c999a-0f13-488c-8232-28ce8ee94378,2010-07-30 00:44:09.0,,"You would use Cronbach alpha if you do not know the true value but if you do know the true value then it seems a bit pointless to use Cronbach alpha. The use of Pearson correlation also seems a bit odd as you do not actually have a paired set of values. I would suggest using something like the [Mean Squared Error (MSE)][1]. Suppose that you have N experts and that the  expected estimate for the expert i is given by $\\hat{\\theta_i}$ and your true value is $\\theta$. Then,\\n\\n$MSE = \\frac{\\sum_i (\\hat{\\theta_i} - \\theta)^2}{N}$\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Mean_squared_error",,user28
2202,2,1028,0a8307e0-f172-47cd-bb6f-9f2ae1ef3369,2010-07-30 00:55:20.0,608.0,"I am comparing two distributions with KL divergence which returns me a non-standardized number that, according to what I read about this measure, is the amount of information that is required to transform one hypothesis into the other. I have two questions:\\n\\na) Is there a way to quantify a KL divergence so that it has a more meaningful interpretation, e.g. like an effect size or a R^2? Any form of standardization?\\n\\nb) In R, when using KLdiv (flexmix package) one can set the 'esp' value (standard esp=1e-4) that sets all points smaller than esp to some standard in order to provide numerical stability. I have been playing with different esp values and, for my data set, I am getting an increasingly larger KL divergence the smaller a number I pick. What is going on? I would expect that the smaller the esp, the more reliable the results should be since they let more 'real values' become part of the statistic. No? I have to change the esp since it otherwise does not calculate the statistic but simply shows up as NA in the result table...\\n\\nThanks in advance,\\nAmpleforth ",,
2203,1,1028,0a8307e0-f172-47cd-bb6f-9f2ae1ef3369,2010-07-30 00:55:20.0,608.0,Questions about KL divergence?,,
2204,3,1028,0a8307e0-f172-47cd-bb6f-9f2ae1ef3369,2010-07-30 00:55:20.0,608.0,<distributions>,,
2205,16,1023,f2b58d98-721e-4ee0-be4e-aa9f70a74289,2010-07-30 01:01:24.0,74.0,,,
2206,2,1029,fe85fe2d-b471-4b42-a7e3-1050a5f419e3,2010-07-30 02:42:38.0,183.0,I wrote a post compiling links of  Practice Questions for Statistics in Psychology (Undergraduate Level).\\nhttp://jeromyanglim.blogspot.com/2009/12/practice-questions-for-statistics-in.html\\n\\nThe questions would fall into the introductory category.,,
2207,16,1029,fe85fe2d-b471-4b42-a7e3-1050a5f419e3,2010-07-30 02:42:38.0,-1.0,,,
2208,5,997,b901a22b-9099-47a2-8089-305b5c2f867e,2010-07-30 03:46:01.0,25.0,"The huge denominators through off one's intuition. Since the sample sizes are identical, and the proportions low, the problem can be recast: 13 events occurred, and were expected (by null hypothesis) to occur equally in both groups. In fact the split was 3 in one group and 10 in the other. How rare is that? The binomial test answers.\\n\\nEnter this line into R:\\nbinom.test(3,13,0.5,alternative=""two.sided"")\\n\\nThe two-tail P value is 0.09229, identical to four digits, to the results of Fisher's test. \\n\\nLooked at that way, the results are not surprising. The problem is equivalent to this one: If you flipped a coin 13 times, how surprising would it be to see three or fewer, or ten or more, heads. One of those outcomes would occur 9.23% of the time. \\n\\n","corrected the numbers in the binom.test function, which were completely wrong in every way.",
2209,2,1030,0c4e1734-e695-4470-9c8a-62090aa48c12,2010-07-30 03:53:08.0,530.0,"The KL(p,q) divergence between distributions p(.) and q(.) has an intuitive information theoretic interpretation which you may find useful. \\n\\nSuppose we observe data x generated by some probability distribution p(.). A lower bound on the average codelength in bits required to state the data generated by p(.) is given by the entropy of p(.). \\n\\nNow, since we don't know p(.) we choose another distribution, say, q(.) to encode (or describe, state) the data. The average codelength of data generated by p(.) and encoded using q(.) will necessarily be longer than if the true distribution p(.) was used for the coding. The KL divergence tells us about the inefficiencies of this alternative code. In other words, the KL divergence between p(.) and q(.) is the average number of **extra** bits required to encode data generated by p(.) using coding distribution q(.). The KL divergence is non-negative and equal to zero iff the actual data generating distribution is used to encode the data.\\n\\n",,
2210,2,1031,a24bb28d-10e9-4294-a77d-63e0e71d5e88,2010-07-30 05:29:11.0,223.0,"KL has a deep meaning in the interpretation of a set of dentities as a manifold. \\n\\nConsider a parametrized family of probability distributions D=(f(x,theta)) (given by densities in R^n), where x is a random variable and theta is a parameter in R^p. You may all knnow that the fisher information matrix F=(F_ij) is \\n\\nF_ij=E[d(logf(x,theta))/dtheta_i d(logf(x,theta))/dtheta_j]\\n\\nWith this notation D is a riemannian manifold and F(theta) is a Riemannian metric tensor. \\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if p=1 you can really imagine your parametrized density as a curve (instead of a subset of infinite dimension) and F_11 is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to your question :** the squared distance ds^2 between two distributions p(x,theta) and p(x,theta+dtheta) on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\nds^2=sum F_ij dtheta^i dtheta^j\\nand it is known to be twice the Kullback Leibler Divergence:\\nds^2=2KL(p(x,theta),p(x,theta+d theta))\\n\\n",,
2211,5,1015,5ef49ca0-b77b-4cdc-ae81-23064a3f036a,2010-07-30 06:41:59.0,108.0,I am trying to calculate the reliability in an elicitation exercise by analysing some test-retest questions given to the experts. The experts elicited a series of probability distributions which were then compared with the true value (found at a later date) by computing the standardized quadratic scores. These scores are the values that I am using to calculate the reliability between the test-retest results. \\n\\nWhich reliability method would be appropriate here?  I was looking mostly at Pearson's correlation and Chronbach's alpha (and got some negative values using both methods) but I am not sure this is the right approach.,added 32 characters in body,
2213,2,1032,dc4041ab-114a-426d-81f6-e7a87e8b47fb,2010-07-30 08:04:50.0,531.0,You will find many applications of Mathematical Statistics in 'Mathematical Statistics and Data Analysis' by John A. Rice. The 'Application Index' lists all applications discussed in the text. \\n\\nJaved,,
2214,16,1032,dc4041ab-114a-426d-81f6-e7a87e8b47fb,2010-07-30 08:04:50.0,-1.0,,,
2215,2,1033,2372d73a-a967-400d-b8e3-c2886a135894,2010-07-30 09:49:02.0,,"""how is stdev(S) related to the standard deviation of the entire population?""\\n\\nI don't know if the ""Confidence Interval"" concept might be what you are looking for? \\n\\nStdev(S) is an Estimate of the standard deviation of the entire population. To see how good an estimate, confidence intervals could be computed, and these would be dependent on the sample size.\\n\\nSee for e.g., Simulation and the Monte Carlo Method, Rubinstein & Kroese.\\n",,Anubala Varikat
2219,2,1035,5b5f2d0a-016a-446b-95c4-c57bb5ac5ae9,2010-07-30 09:54:10.0,419.0,"Another solution to your problem (without transforming variables) is regression with error distribution other then **Gaussian** for example **Gamma** or **skewed t-Student**.\\nGamma is in GLM family, so there is a lot of software to fit model with this error distribution.\\n\\n",,
2220,2,1036,38ca2a15-7e13-40d1-8a55-bf3f16b4f234,2010-07-30 10:03:37.0,557.0,"Sivia and Skilling, Data analysis: a Bayesian tutorial (2ed) 2006 246p 0198568320\\n[books.goo](http://books.google.com/books?id=zN-yliq6eZ4C&dq=isbn:0198568320&source=gbs_navlinks_s):\\n\\n> Statistics lectures have been a source\\n> of much bewilderment and frustration\\n> for generations of students. This book\\n> attempts to remedy the situation by\\n> expounding a logical and unified\\n> approach to the whole subject of data\\n> analysis. This text is intended as a\\n> tutorial guide for senior\\n> undergraduates and research students\\n> in science and engineering ...\\n\\nI don't know the other recommendations though.\\n\\n",,
2221,16,1036,38ca2a15-7e13-40d1-8a55-bf3f16b4f234,2010-07-30 10:03:37.0,-1.0,,,
2224,5,981,278e4bf9-8c82-422c-9297-921a3d179550,2010-07-30 10:44:57.0,8.0,"You have two problems: too many points and how to smooth over the remaining points.\\n\\n**Thinning your sample**\\n\\nIf you have too many observations arriving in real time, you could always use [simple random sampling][1] to thin your sample. Note, for this too be true, the number of points would have to be very large.\\n\\nSuppose you have *N* points and you only want *n* of them. Then generate *n* random numbers from a discrete uniform *U(0, N-1)* distribution. These would be the points you use.\\n\\nIf you want to do this sequentially, i.e. at each point you decide to use it or not, then just accept a point with probability *p*. So if you set *p=0.01* you would accept (on average) 1 point in a hundred. \\n\\nIf your data is unevenly spread and you only want to thin dense regions of points, then just make your thinning function a bit more sophisticated. For example, instead of *p*, what about:\\n\\n![alt text][2]\\n\\nwhere ![alt text][3] is a positive number and *t* is the time since the last observation. If the time between two points is large, i.e. large *t*, the probability of accepting a point will be one. Conversely, if two points are close together, the probability of accepting a point will be *1-p*.\\n\\nYou will need to experiment with values of ![alt text][3] and *p*.\\n\\n\\n\\n**Smoothing**\\n\\nPossibly something like a simple moving average type scheme. Or you could go for something more advanced like a [kernel smoother][4] (as others suggested). You will need to be careful that you don't smooth too much, since I assume that a sudden drop should be picked up very quickly in your scenario. \\n\\nThere should be *C#* libraries available for this sort of stuff.\\n\\n\\n\\n**Conclusion**\\n\\nThin if necessary, then smooth.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Simple_random_sample\\n  [2]: http://mathurl.com/2vlkhjs.png\\n  [3]: http://mathurl.com/343luwd.png\\n  [4]: http://en.wikipedia.org/wiki/Kernel_smoother",Adding a more sophasticated thinning function.,
2225,5,981,cc5d14a5-7e96-45d4-b644-7915f9133213,2010-07-30 11:00:12.0,8.0,"You have two problems: too many points and how to smooth over the remaining points.\\n\\n**Thinning your sample**\\n\\nIf you have too many observations arriving in real time, you could always use [simple random sampling][1] to thin your sample. Note, for this too be true, the number of points would have to be very large.\\n\\nSuppose you have *N* points and you only want *n* of them. Then generate *n* random numbers from a discrete uniform *U(0, N-1)* distribution. These would be the points you use.\\n\\nIf you want to do this sequentially, i.e. at each point you decide to use it or not, then just accept a point with probability *p*. So if you set *p=0.01* you would accept (on average) 1 point in a hundred. \\n\\nIf your data is unevenly spread and you only want to thin dense regions of points, then just make your thinning function a bit more sophisticated. For example, instead of *p*, what about:\\n\\n![alt text][2]\\n\\nwhere ![alt text][3] is a positive number and *t* is the time since the last observation. If the time between two points is large, i.e. large *t*, the probability of accepting a point will be one. Conversely, if two points are close together, the probability of accepting a point will be *1-p*.\\n\\nYou will need to experiment with values of ![alt text][3] and *p*.\\n\\n\\n\\n**Smoothing**\\n\\nPossibly something like a simple moving average type scheme. Or you could go for something more advanced like a [kernel smoother][4] (as others suggested). You will need to be careful that you don't smooth too much, since I assume that a sudden drop should be picked up very quickly in your scenario. \\n\\nThere should be *C#* libraries available for this sort of stuff.\\n\\n\\n\\n**Conclusion**\\n\\nThin if necessary, then smooth.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Simple_random_sample\\n  [2]: http://mathurl.com/2vlkhjs.png\\n  [3]: http://mathurl.com/343luwd.png\\n  [4]: http://en.wikipedia.org/wiki/Kernel_smoother",Messing about with latex math; edited body,
2226,6,290,7b68a486-59bc-43b5-8866-db69e4d28f31,2010-07-30 11:17:40.0,8.0,<books>,edited tags,
2227,6,652,89d5fc87-3956-4868-96a4-3e8ef85228a5,2010-07-30 11:18:37.0,8.0,<beginner><machine-learning><bayesian><books>,edited tags,
2228,6,287,af86b824-f600-42de-9a2c-73885fb9796e,2010-07-30 11:38:17.0,159.0,<estimation><gmm><method-of-moments>,edited tags,
2231,6,1015,2a99e013-148d-4704-b545-66ab8ca5a38b,2010-07-30 11:46:33.0,66.0,<elicitation><reliability>,edited tags,
2232,2,1038,fc4a2721-406a-419c-9b0a-eba84d0e4b82,2010-07-30 12:11:18.0,1356.0," 1. ***Wilcox, Rand R.*** - *BASIC STATISTICS - Understanding\\n    Conventional Methods and Modern\\n    Insights*, Oxford University Press,\\n    2009\\n 2. ***Hoff, Peter D.*** - *A First Course in\\n    Bayesian Statistical Methods*,\\n    Springer, 2009\\n\\n 3. ***Dalgaard, Peter*** - *Introductory\\n    Statistics with R, Second Edition*, Springer, 2008\\n\\nalso take a glance at [this link][1], though it's R-specific, there are plenty of books that can guide you through basic statistical techniques.\\n\\n\\n  [1]: http://stackoverflow.com/questions/192369/books-for-learning-the-r-language/2270793#2270793",,
2233,16,1038,fc4a2721-406a-419c-9b0a-eba84d0e4b82,2010-07-30 12:11:18.0,-1.0,,,
2234,2,1039,befcc520-dd50-4cc8-846f-41e9b23ad856,2010-07-30 12:15:45.0,319.0,"If you're looking for an elementary text, i.e. one that doesn't have a calculus prerequisite, there's Don Berry's [Statistics: A Bayesian Perspective][1].\\n\\n\\n  [1]: http://www.amazon.com/gp/product/0534234720?ie=UTF8&tag=theende-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0534234720",,
2235,16,1039,befcc520-dd50-4cc8-846f-41e9b23ad856,2010-07-30 12:15:45.0,-1.0,,,
2236,5,1031,4b5c9969-7b97-4149-9859-63e9cd062d81,2010-07-30 13:08:39.0,223.0,"KL has a deep meaning in the interpretation of a set of dentities as a manifold. \\n\\nConsider a parametrized family of probability distributions D=(f(x,theta)) (given by densities in R^n), where x is a random variable and theta is a parameter in R^p. You may all knnow that the fisher information matrix F=(F_ij) is \\n\\nF_ij=E[d(logf(x,theta))/dtheta_i d(logf(x,theta))/dtheta_j]\\n\\nWith this notation D is a riemannian manifold and F(theta) is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if p=1 you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and F_11 is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance ds^2 between two distributions p(x,theta) and p(x,theta+dtheta) on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\nds^2=sum F_ij dtheta^i dtheta^j\\nand it is known to be twice the Kullback Leibler Divergence:\\nds^2=2KL(p(x,theta),p(x,theta+d theta))\\n\\n",added 91 characters in body,
2237,5,997,381b8811-ff37-4797-bcdf-ae80cbfb1c8a,2010-07-30 14:04:33.0,25.0,"The huge denominators throw off one's intuition. Since the sample sizes are identical, and the proportions low, the problem can be recast: 13 events occurred, and were expected (by null hypothesis) to occur equally in both groups. In fact the split was 3 in one group and 10 in the other. How rare is that? The binomial test answers.\\n\\nEnter this line into R:\\nbinom.test(3,13,0.5,alternative=""two.sided"")\\n\\nThe two-tail P value is 0.09229, identical to four digits, to the results of Fisher's test. \\n\\nLooked at that way, the results are not surprising. The problem is equivalent to this one: If you flipped a coin 13 times, how surprising would it be to see three or fewer, or ten or more, heads. One of those outcomes would occur 9.23% of the time. \\n\\n",Typo,
2238,2,1040,1bd8b3fd-bfad-4cb1-9518-204256f7ef31,2010-07-30 14:24:17.0,168.0,"Consider the following model \\n\\nY_i = f(X_i) + e_i \\n\\nfrom which we observe n iid data points {X_i, Y_i}. Suppose that X_i is a d dimensional feature vector. And suppose that a ordinary least squares estimate is fit to data, that is,\\n\\n\\hat \\beta = \\argmin \\sum_i (Y_i - \\sum_j X_ij \\beta_j)^2\\n\\nSince a wrong model is estimated, what is the interpretation for the confidence interval around estimated coefficients? \\n\\nMore generally, does it make sense to estimate confidence intervals around parameters in a misspecified model? And what does the confidence interval tell us in such a case?\\n\\n\\n",,
2239,1,1040,1bd8b3fd-bfad-4cb1-9518-204256f7ef31,2010-07-30 14:24:17.0,168.0,What is the interpretation/meaning of confidence intervals in misspecified models?,,
2240,3,1040,1bd8b3fd-bfad-4cb1-9518-204256f7ef31,2010-07-30 14:24:17.0,168.0,<modeling><estimation><model-selection>,,
2241,5,1040,9fd1ed44-476f-474e-a9ce-29388bc456bf,2010-07-30 14:33:42.0,5.0,"Consider the following model \\n\\n$Y_i = f(X_i) + e_i$\\n\\nfrom which we observe n iid data points {X_i, Y_i}. Suppose that X_i is a d dimensional feature vector. And suppose that a ordinary least squares estimate is fit to data, that is,\\n\\n$\\hat \\beta = \\argmin \\sum_i (Y_i - \\sum_j X_ij \\beta_j)^2$\\n\\nSince a wrong model is estimated, what is the interpretation for the confidence interval around estimated coefficients? \\n\\nMore generally, does it make sense to estimate confidence intervals around parameters in a misspecified model? And what does the confidence interval tell us in such a case?\\n\\n\\n",added 1 characters in body; added 2 characters in body,
2242,2,1041,aa9ae261-956a-4c7a-9eee-b66fe9e7c1f9,2010-07-30 14:38:39.0,,"The confidence interval that you obtain is conditional on the model being correct and the interpretation is also conditional on the model being the correct one. If you **know** that the model is incorrect then obviously you would not use it to compute the confidence interval. \\n\\nIn reality, you do not know the **true** model and so you have no way to tell if you have a misspecified model (although you do have ways to assess misspecification, e.g., examine if residuals are normally distributed, diagnostic plots of fitted vs observed values etc). So, to my mind, the real question is if the model is misspecified, to what extent can you rely on confidence intervals as a way to assess where the true parameter is. I suspect that the answer is specific to the degree of misspecification that is coming from f(x) i.e., the degree to which f(x) departs from the assumptions of OLS.",,user28
2243,6,1040,cd2b48ba-8bcd-45d2-9eea-b8cb6fe717c8,2010-07-30 14:45:32.0,,<modeling><estimation><model-selection><confidence-interval>,edited tags,user28
2244,5,1040,5091fd7a-836b-4239-a182-35b562d9abd5,2010-07-30 15:15:21.0,168.0,"Consider the following model \\n\\n$Y_i = f(X_i) + e_i$\\n\\nfrom which we observe n iid data points $\\left( X_i, Y_i \\right)_{i=1}^n$. Suppose that $X_i \\in \\mathbb{R}^d$ is a $d$ dimensional feature vector. And suppose that a ordinary least squares estimate is fit to data, that is,\\n\\n$\\hat \\beta = {\\rm arg} \\min_{\\beta \\in \\mathbb{R}^d} \\sum_i (Y_i - \\sum_j X_{ij} \\beta_j)^2$\\n\\nSince a wrong model is estimated, what is the interpretation for the confidence interval around estimated coefficients? \\n\\nMore generally, does it make sense to estimate confidence intervals around parameters in a misspecified model? And what does the confidence interval tell us in such a case?\\n\\n\\n",updated latex equations; added 11 characters in body,
2248,2,1043,28a47815-8dd7-4ee1-b396-d087bdca74e2,2010-07-30 15:37:06.0,601.0,"OK, so your data is very expensive to get.  If you have some indication of the shape of the data then perhaps a bootstrap / bayesian / optimization (sticking in keywords :)) approach would work best.  See the optim command in R as an example.  You would need to know some things though.  For example, could we assume something like normality?  If so then fitting a small number of data points to the normal distribution will likely give you a much better estimate of your parameters than simple mean and sdev values.",,
2249,2,1044,5dac27a6-71a4-42d4-97cd-3d174a150440,2010-07-30 15:43:28.0,319.0,"**Sweave** lets you embed R code in a LaTeX document.  The results of executing the code, and optionally the source code, become part of the final document.\\n\\nSo instead of, for example, pasting an image produced by R into a LaTeX file, you can paste the R *code* into the file and keep everything in one place.",,
2250,16,1044,5dac27a6-71a4-42d4-97cd-3d174a150440,2010-07-30 15:43:28.0,-1.0,,,
2251,5,729,5a268a54-d6f1-4ab3-accb-8496af7ed61a,2010-07-30 16:05:38.0,25.0,I\\n\\n> n God we trust. All others must bring\\n> data. \\n\\n(W. Edwards Deming),added 13 characters in body,
2252,2,1045,56965ede-d481-4c72-87ae-24868b89c352,2010-07-30 16:06:54.0,,"The wiki article on [credible intervals][1] has the following statement:\\n\\n> credible intervals and confidence intervals treat [nuisance parameters][2] in radically different ways.\\n\\nWhat is the **radical** difference that the wiki talks about?\\n\\nCredible intervals are based on the posterior distribution of the parameter and confidence interval is based on the maximum likelihood associated with the data generating process. It seems to me that how credible and confidence intervals are computed is **not dependent** on whether the parameters are nuisance or not. So, I am a bit puzzled by this statement.\\n\\nPS: I am aware of alternative approaches to dealing with nuisance parameters under frequentist inference but I think they are less common than standard maximum likelihood. (See this question on the difference between [partial, profile and marginal likelihoods][3].)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Credible_interval\\n  [2]: http://en.wikipedia.org/wiki/Nuisance_parameter\\n  [3]: http://stats.stackexchange.com/questions/622/what-is-the-difference-between-a-partial-likelihood-profile-likelihood-and-margi",,user28
2253,1,1045,56965ede-d481-4c72-87ae-24868b89c352,2010-07-30 16:06:54.0,,Is there a radical difference in how bayesian and frequentist approaches treat nuisance parameters?,,user28
2254,3,1045,56965ede-d481-4c72-87ae-24868b89c352,2010-07-30 16:06:54.0,,<confidence-interval><credible-interval>,,user28
2255,5,1031,8cca0004-f66b-4dcc-8305-cf12e26fa2ce,2010-07-30 16:52:59.0,223.0,"KL has a deep meaning in the interpretation of a set of dentities as a manifold. \\n\\nConsider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_ij)$ is \\n \\n$F_ij=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$\\n\\nWith this notation D is a riemannian manifold and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if p=1 you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and F_11 is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance ds^2 between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\n$ds^2= \\sum F_ij d \\theta^i d \\theta^j$\\n\\nand it is known to be twice the Kullback Leibler Divergence:\\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\n",added 48 characters in body; added 13 characters in body; added 9 characters in body; deleted 4 characters in body,
2256,2,1046,23199013-3c0d-4503-83be-ca9b08ec49da,2010-07-30 16:58:42.0,253.0,"If you are using GNU/Linux previous answers by Shane and Dirk are great.\\n\\nIf you need a solution for windows, there is one in this post:\\n\\n[Parallel Multicore Processing with R (on Windows)][1]\\n\\nAlthough the package is not yet on CRAN. it can be downloaded from that link.\\n\\n  [1]: http://www.r-statistics.com/2010/04/parallel-multicore-processing-with-r-on-windows/",,
2257,5,1031,834f96b8-9b9b-42c5-8a8e-80dbb8af9c34,2010-07-30 17:00:55.0,223.0,"KL has a deep meaning in the interpretation of a set of dentities as a manifold. \\n\\nConsider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_{ij})$ is \\n \\n$F_{ij}=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$\\n\\nWith this notation D is a riemannian manifold and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if $p=1$ you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and F_11 is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance $ds^2$ between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\n$ds^2= \\sum F_{ij} d \\theta^i d \\theta^j$\\n\\nand it is known to be twice the Kullback Leibler Divergence:\\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\n",added 10 characters in body,
2258,2,1047,86f11811-9d95-4b34-a6f6-d1f220a7c283,2010-07-30 17:00:57.0,614.0,"I'm comparing a sample and checking whether it distributes as some, discrete, distribution.  However, I'm not enterily sure that Kolmogorov-Smirnov applies. [Wikipedia](http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) seems to imply it does not. If it does not, how can I test the sample's distribution?",,
2259,1,1047,86f11811-9d95-4b34-a6f6-d1f220a7c283,2010-07-30 17:00:57.0,614.0,Is Kolmogorov-Smirnov test valid with discrete distributions?,,
2260,3,1047,86f11811-9d95-4b34-a6f6-d1f220a7c283,2010-07-30 17:00:57.0,614.0,<hypothesis-testing>,,
2261,5,1018,627e310a-c5b3-476f-9575-921e47a798cd,2010-07-30 17:06:20.0,223.0,"\\n - [**Arcene**][1]   n=900  p=10000 (3k is artificially added noise)  k=2 (~balanced)  From [NIPS2003][2].\\n - [**Dexter**][3]  n=2600  p=20000 (10k+53 is artificial noise)  k=2 (balanced)  From [NIPS2003][2].\\n - [**Gisette**][1]  n=13500  p=5000 (half is artificially added noise)  k=2 (balanced)  From [NIPS2003][2].\\n - [**Dorothea**][1]  n=1950   p=100000 (0.1M, half is artificially added noise)  k=2 (~10x unbalanced)   From [NIPS2003][2].\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Arcene\\n  [2]: http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf\\n  [3]: http://archive.ics.uci.edu/ml/datasets/Dexter\\n  [4]: http://archive.ics.uci.edu/ml/datasets/Gisette\\n  [5]: http://archive.ics.uci.edu/ml/datasets/Dorothea",grouping mbq's posts; added 166 characters in body,
2262,2,1048,096e8687-89f3-4173-9c8f-f81d6cb1ea39,2010-07-30 17:10:09.0,247.0,It does not apply to discrete distributions. See http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm for example.\\n\\nIs there any reason you can't use a chi-square goodness of fit test?\\nsee http://www.itl.nist.gov/div898/handbook/eda/section3/eda35f.htm for more info.,,
2263,2,1049,8d04b551-fa65-4834-a5e3-f203e2f679b2,2010-07-30 17:10:24.0,,One resource is 'Some hints for the R beginner' at\\nhttp://www.burns-stat.com/pages/Tutor/hints_R_begin.html\\n,,Patrick Burns
2264,5,1015,c14d5a74-3d5c-4f20-aebd-7ccfcd479b17,2010-07-30 17:11:39.0,108.0,"I am trying to calculate the reliability in an elicitation exercise by analysing some test-retest questions given to the experts. The experts elicited a series of probability distributions which were then compared with the true value (found at a later date) by computing the standardized quadratic scores. These scores are the values that I am using to calculate the reliability between the test-retest results. \\n\\nWhich reliability method would be appropriate here?  I was looking mostly at Pearson's correlation and Chronbach's alpha (and got some negative values using both methods) but I am not sure this is the right approach.\\n\\n---\\n\\n**UPDATE:**\\nBackground information\\n\\n\\nThe data were collected from a number of students who were asked to predict their own actual exam mark in four chosen modules by giving a probability distribution of the marks. One module was then repeated at a later date (hence the test-retest exercise).\\n\\nOnce the exam was taken, and the real results were available, the standardized quadratic scores were computed. These scores are proper scoring rules used to compare assessed probability distributions with the observed data which might be known at a later stage.\\n\\nThe probability score *Q* is defined as:\\n\\n![Quadratic score][1]\\n\\nwhere *k* is the total number of elicited probabilities and *j* is the true outcome.\\n\\nMy question is which reliability method would be more appropriate when it comes to assessing the reliability between the scores of the repeated modules? I calculated Pearson's correlation and Chronbach's alpha (and got some negative values using both methods) but there might be a better approach.\\n\\n\\n  [1]: http://img717.imageshack.us/img717/9424/chart2j.png",Added some background information; deleted 2 characters in body,
2265,5,973,1f0ac0c0-9bd4-40b5-8ca3-3874b1c1054e,2010-07-30 17:12:56.0,223.0,"What are the freely available data set for classification with more than 1000 features (or sample points if is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a more focused list that can be used more conveniently, also I propose the following rules:\\n\\n - One post **per Personn** (otherwise it will be difficult for me to sort everything which is what I will try to do at the end)  \\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n\\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n\\n----------\\n\\n\\nRemark: I'm not sure if this type of question is too much connected to \\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\nso don't hesitate to comment about that (not in answers please)  it's easy to remove the post :) ",added 132 characters in body,
2266,8,1018,fc7c9dec-1aae-44b7-939e-fd83b4504c12,2010-07-30 18:00:06.0,190.0,[**Arcene**][1]  \\nn=900  \\np=10000 (3k is artificially added noise)  \\nk=2 (~balanced)  \\nFrom [NIPS2003][2].\\n\\n\\n  [1]: http://archive.ics.uci.edu/ml/datasets/Arcene\\n  [2]: http://www.nipsfsc.ecs.soton.ac.uk/papers/NIPS2003-Datasets.pdf,Rollback to [b3928f26-fa38-44c8-acc8-269f81b9d905],
2267,5,949,241e48fd-5b03-4b14-8516-8643df63dc72,2010-07-30 18:39:17.0,511.0,"Take $x \\in {0,1}^d$ and $y \\in \\{0,1\\}$ and suppose we model the task of predicting y given x using logistic regression. When can logistic regression coefficients be written in closed form?\\n\\nOne example is when we use a saturated model.\\n\\nIE, define $P(y|x) \\propto \\exp(\\sum_i w_i f_i(\\{x\\}_i))$, where i indexes sets in the power-set of $\\{x1,\\ldots,x_d\\}$, and $f_i$ returns 1 if all variables in i'th set are 1, 0 otherwise. Then you can express each $w_i$ in this logistic regression model as a logarithm of a rational function of statistics of the data. \\n\\nAre there other interesting examples when closed form exists?",added 20 characters in body; added 5 characters in body,
2268,5,567,1a4ba118-5751-4332-8ae1-d4e6ccff209f,2010-07-30 19:07:26.0,92.0,"  I've often found the Engineering Statistics Handbook useful. It can be found [here][1].  \\n\\n \\n  Although I've never read it myself, I hear [Introduction to Probability and Statistics Using R][2] is very good. It's a full ~400 page ebook (also available as an actual book). As a bonus, it also teaches you R, which of course you want to learn anyways.\\n\\n\\n  [1]: http://www.itl.nist.gov/div898/handbook/\\n  [2]: http://www.lulu.com/items/volume_68/8123000/8123594/3/print/IPSUR.pdf",added 337 characters in body; added 6 characters in body; edited body,
2269,2,1050,bc6822ae-f824-4a0b-aa82-1a0a0ab77dc6,2010-07-30 20:00:45.0,1356.0,"Here's a fresh one: [Introduction to Probability and Statistics Using R ][1]. It's R-specific, though, but it's a great one. I haven't read it yet, but it seems fine so far...\\n\\n\\n  [1]: http://www.lulu.com/product/file-download/introduction-to-probability-and-statistics-using-r/12037733",,
2270,16,1050,bc6822ae-f824-4a0b-aa82-1a0a0ab77dc6,2010-07-30 20:00:45.0,-1.0,,,
2272,2,1051,3a7dc286-ca3d-45fd-8d3a-235b86d7e733,2010-07-30 21:15:43.0,251.0,"The fundamental difference is that in maximum likelihood based methods we can't integrate the nuisance parameters out (because the likelihood function is not a PDF and doesn't obey probability laws).\\n\\nIn maximum likelihood methods, the marginal/conditional likelihoods are defined differently from the question you linked.  There is a notion of an *integrated* (marginal/conditional) likelihood function, but this is not strictly the marginal likelihood function.  \\n\\nSay you have a parameter of interest, $\\theta$, a nuisance parameter, $\\lambda$.  Suppose a transformation of your data $X$ to $(Y, Z)$ exists such that either $Y$ or $Y|Z$ depends only on $\\theta$.  If $Y$ depends on $\\theta$, then the joint density can be written \\n\\n$f(\\theta, \\lambda) = f_Y(Y; \\theta) f_{Z|Y}(Z|Y; \\theta, \\lambda)$.  \\n\\nIn the latter case, we have \\n\\n$f(\\theta, \\lambda) = f_{Y|Z}(Y|Z; \\theta) f_Z(Z; \\theta, \\lambda)$.  \\n\\nIn either case, the factor depending on $\\theta$ alone is of interest.  In the former, it is the definition of the marginal likelihood and in the latter, the conditional likelihood.  Cox used the same framework to define partial likelihood, with marginal/conditional likelihoods as special cases.\\n\\nIn difficult cases, we start work with a profile likelihood.  To eliminate bias in the MLE, we try to obtain approximations for marginal or conditional likelihoods, usually through a ""modified profile likelihood"" function (yet another likelihood function!).  There are many details, but the short story is that the likelihood methods treat nuisance parameters quite differently than Bayesian methods.\\n\\nThere are arguments in favor of an integrated likelihood function and lead to something resembling the Bayesian framework.  If you're interested, I can dig up some references.\\n\\n",,
2273,5,1051,cd72dc0d-ad23-468a-bcb7-8b926ccc636a,2010-07-30 21:23:38.0,251.0,"The fundamental difference is that in maximum likelihood based methods we can't integrate the nuisance parameters out (because the likelihood function is not a PDF and doesn't obey probability laws).\\n\\nIn maximum likelihood methods, the marginal/conditional likelihoods are defined differently from the question you linked.  There is a notion of an *integrated* (marginal/conditional) likelihood function, but this is not strictly the marginal likelihood function.  \\n\\nSay you have a parameter of interest, $\\theta$, a nuisance parameter, $\\lambda$.  Suppose a transformation of your data $X$ to $(Y, Z)$ exists such that either $Y$ or $Y|Z$ depends only on $\\theta$.  If $Y$ depends on $\\theta$, then the joint density can be written \\n\\n$f(Y, Z; \\theta, \\lambda) = f_Y(Y; \\theta) f_{Z|Y}(Z|Y; \\theta, \\lambda)$.  \\n\\nIn the latter case, we have \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y|Z}(Y|Z; \\theta) f_Z(Z; \\theta, \\lambda)$.  \\n\\nIn either case, the factor depending on $\\theta$ alone is of interest.  In the former, it is the definition of the marginal likelihood and in the latter, the conditional likelihood.  Cox used the same framework to define partial likelihood, with marginal/conditional likelihoods as special cases.\\n\\nIn difficult cases, we start work with a profile likelihood.  To eliminate bias in the MLE, we try to obtain approximations for marginal or conditional likelihoods, usually through a ""modified profile likelihood"" function (yet another likelihood function!).  There are many details, but the short story is that the likelihood methods treat nuisance parameters quite differently than Bayesian methods.  In particular, the estimated likelihoods don't account for uncertainty in the nuisance.\\n\\nThere are arguments in favor of an integrated likelihood function and lead to something resembling the Bayesian framework.  If you're interested, I can dig up some references.\\n\\n",added 89 characters in body; added 12 characters in body,
2274,5,1051,c9758464-a6f6-47d6-80ac-857174df80be,2010-07-30 21:31:13.0,251.0,"The fundamental difference is that in maximum likelihood based methods we can't integrate the nuisance parameters out (because the likelihood function is not a PDF and doesn't obey probability laws).\\n\\nIn maximum likelihood methods, the marginal/conditional likelihoods are defined differently from the question you linked.  There is a notion of an *integrated* (marginal/conditional) likelihood function, but this is not strictly the marginal likelihood function.  \\n\\nSay you have a parameter of interest, $\\theta$, a nuisance parameter, $\\lambda$.  Suppose a transformation of your data $X$ to $(Y, Z)$ exists such that either $Y$ or $Y|Z$ depends only on $\\theta$.  If $Y$ depends on $\\theta$, then the joint density can be written \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y}(Y; \\theta) f_{Z|Y}(Z|Y; \\theta, \\lambda)$.  \\n\\nIn the latter case, we have \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y|Z}(Y|Z; \\theta) f_{Z}(Z; \\theta, \\lambda)$.  \\n\\nIn either case, the factor depending on $\\theta$ alone is of interest.  In the former, it's the basis for the definition of the marginal likelihood and in the latter, the conditional likelihood.  Cox used the same framework to define partial likelihood, with marginal/conditional likelihoods as special cases.\\n\\nIn difficult cases, we start work with a profile likelihood.  To eliminate bias in the MLE, we try to obtain approximations for marginal or conditional likelihoods, usually through a ""modified profile likelihood"" function (yet another likelihood function!).  There are many details, but the short story is that the likelihood methods treat nuisance parameters quite differently than Bayesian methods.  In particular, the estimated likelihoods don't account for uncertainty in the nuisance.\\n\\nThere are arguments in favor of an integrated likelihood function and lead to something resembling the Bayesian framework.  If you're interested, I can dig up some references.\\n\\n",latex fix; added 13 characters in body,
2275,5,949,fb67109b-4e09-49bd-8267-7266721b2179,2010-07-30 21:57:09.0,511.0,"Take $x \\in {0,1}^d$ and $y \\in \\{0,1\\}$ and suppose we model the task of predicting y given x using logistic regression. When can logistic regression coefficients be written in closed form?\\n\\nOne example is when we use a saturated model.\\n\\nIE, define $P(y|x) \\propto \\exp(\\sum_i w_i f_i(\\mbox{{}x\\\\mbox{}}_i))$, where i indexes sets in the power-set of $\\{x_1,\\ldots,x_d\\}$, and $f_i$ returns 1 if all variables in i'th set are 1, 0 otherwise. Then you can express each $w_i$ in this logistic regression model as a logarithm of a rational function of statistics of the data. \\n\\nAre there other interesting examples when closed form exists?",added 14 characters in body,
2276,2,1052,943d7f2d-9133-4e99-981b-1d651d9460d1,2010-07-30 22:38:06.0,,my question particularly applies to network reconstruction,,puzzled
2277,1,1052,943d7f2d-9133-4e99-981b-1d651d9460d1,2010-07-30 22:38:06.0,,What is the major difference between correlation and mutual information?,,puzzled
2278,3,1052,943d7f2d-9133-4e99-981b-1d651d9460d1,2010-07-30 22:38:06.0,,<correlation>,,puzzled
2279,5,1051,d859f7d1-4e05-400c-a64f-916c24ef09bf,2010-07-30 23:37:35.0,251.0,"The fundamental difference is that in maximum likelihood based methods we can't integrate the nuisance parameters out (because the likelihood function is not a PDF and doesn't obey probability laws).\\n\\nIn maximum likelihood methods, the marginal/conditional likelihoods are defined differently from the question you linked.  There is a notion of an *integrated* (marginal/conditional) likelihood function, but this is not strictly the marginal likelihood function.  \\n\\nSay you have a parameter of interest, $\\theta$, a nuisance parameter, $\\lambda$.  Suppose a transformation of your data $X$ to $(Y, Z)$ exists such that either $Y$ or $Y|Z$ depends only on $\\theta$.  If $Y$ depends on $\\theta$, then the joint density can be written \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y}(Y; \\theta) f_{Z|Y}(Z|Y; \\theta, \\lambda)$.  \\n\\nIn the latter case, we have \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y|Z}(Y|Z; \\theta) f_{Z}(Z; \\theta, \\lambda)$.  \\n\\nIn either case, the factor depending on $\\theta$ alone is of interest.  In the former, it's the basis for the definition of the marginal likelihood and in the latter, the conditional likelihood.  Cox used the same framework to define partial likelihood, with marginal/conditional likelihoods as special cases.\\n\\nIf we can't find such a transformation, we look at other likelihood functions to eliminate the nuisance.  We usually start with a profile likelihood.  To eliminate bias in the MLE, we try to obtain approximations for marginal or conditional likelihoods, usually through a ""modified profile likelihood"" function (yet another likelihood function!).  \\n\\nThere are many details, but the short story is that the likelihood methods treat nuisance parameters quite differently than Bayesian methods.  In particular, the estimated likelihoods don't account for uncertainty in the nuisance.  Bayesian methods do account for it through the specification of a prior.\\n\\nThere are arguments in favor of an integrated likelihood function and lead to something resembling the Bayesian framework.  If you're interested, I can dig up some references.",added 163 characters in body,
2280,5,1051,7fc0ff24-dc57-4cb8-9913-8fa94191ed1b,2010-07-30 23:53:35.0,251.0,"The fundamental difference is that in maximum likelihood based methods we can't integrate the nuisance parameters out (because the likelihood function is not a PDF and doesn't obey probability laws).\\n\\nIn maximum likelihood methods, the ideal way to deal with nuisance parameters is through marginal/conditional likelihoods, but these are defined differently from the question you linked.  (There is a notion of an *integrated* (marginal/conditional) likelihood function as in the linked question, but this is not strictly the marginal likelihood function.)  \\n\\nSay you have a parameter of interest, $\\theta$, a nuisance parameter, $\\lambda$.  Suppose a transformation of your data $X$ to $(Y, Z)$ exists such that either $Y$ or $Y|Z$ depends only on $\\theta$.  If $Y$ depends on $\\theta$, then the joint density can be written \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y}(Y; \\theta) f_{Z|Y}(Z|Y; \\theta, \\lambda)$.  \\n\\nIn the latter case, we have \\n\\n$f(Y, Z; \\theta, \\lambda) = f_{Y|Z}(Y|Z; \\theta) f_{Z}(Z; \\theta, \\lambda)$.  \\n\\nIn either case, the factor depending on $\\theta$ alone is of interest.  In the former, it's the basis for the definition of the marginal likelihood and in the latter, the conditional likelihood.  The important point here is to isolate a component that depends on $\\theta$ alone.\\n\\nIf we can't find such a transformation, we look at other likelihood functions to eliminate the nuisance.  We usually start with a profile likelihood.  To eliminate bias in the MLE, we try to obtain approximations for marginal or conditional likelihoods, usually through a ""modified profile likelihood"" function (yet another likelihood function!).  \\n\\nThere are many details, but the short story is that the likelihood methods treat nuisance parameters quite differently than Bayesian methods.  In particular, the estimated likelihoods don't account for uncertainty in the nuisance.  Bayesian methods do account for it through the specification of a prior.\\n\\nThere are arguments in favor of an integrated likelihood function and lead to something resembling the Bayesian framework.  If you're interested, I can dig up some references.",added 93 characters in body; deleted 31 characters in body,
2281,5,729,ddd3839b-925d-459a-97d5-a8876c333028,2010-07-31 00:19:53.0,461.0,> In God we trust. All others must bring\\n> data. \\n\\n(W. Edwards Deming),deleted 4 characters in body,
2282,5,628,e66a5329-539a-4cd6-9892-4335cd344d7d,2010-07-31 00:34:24.0,,"The likelihood function usually depends on many parameters. Depending on the application, we are usually interested in only a subset of these parameters. For example, in linear regression, interest typically lies in the slope coefficients and not on the error variance. \\n\\nDenote the parameters we are interested in as &beta; and the parameters that are not of primary interest as &theta;. The standard way to approach the estimation problem is to maximize the likelihood function so that we obtain estimates of &beta; and &theta;. However, since the primary interest lies in &beta; partial, profile and marginal likelihood offer alternative ways to estimate &beta; without estimating &theta;\\n\\nIn order to see the difference denote the standard likelihood by L(&beta;, &theta;|data). \\n\\n**Maximum Likelihood**\\n\\nFind &beta; and &theta; that maximizes L(&beta;, &theta;|data).\\n\\n**Partial Likelihood**\\n\\nIf we can write the likelihood function as:\\n\\nL(&beta;, &theta;|data) = L1(&beta;|data) L2(&theta;|data)\\n\\nThen we simply maximize L1(&beta;|data).\\n\\n**Profile Likelihood**\\n\\nIf we can express &theta; as a function of &beta; then we replace &theta; with the corresponding function. \\n\\nSay, &theta; = g(&beta;). Then, we maximize:\\n\\nL(&beta;, g(&beta;)|data)\\n\\n**Marginal Likelihood**\\n\\nWe integrate out &theta; from the likelihood equation by exploiting the fact that we can identify the probability distribution of &theta; conditional on &beta;.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Sufficient_statistic",fixed typo in partial likelihood,user28
2283,4,884,7a60512b-ee7f-4fc5-bddc-e570bd3340f5,2010-07-31 00:46:05.0,159.0,"What are ""degrees of freedom""?",edited title,
2284,4,964,6264f36e-a975-4d1d-8cc8-ff4a3722861d,2010-07-31 00:48:08.0,159.0,"What are analysts looking for when they  plot  a differenced, logged time series? ",edited title,
2285,5,980,168215fb-b65a-4e9a-b21e-a6be541d0264,2010-07-31 00:49:57.0,159.0,"I haven't studied statistics for over 10 years (and then just a basic course), so maybe my question is a bit hard to understand.\\n\\nAnyway, what I want to do is reduce the number of data points in a series. The x-axis is number of milliseconds since start of measurement and the y-axis is the reading for that point. \\n\\nOften there is thousands of data points, but I might only need a few hundreds. So my question is: How do I accurately reduce the number of data points?\\n\\nWhat is the process called? (So I can google it)\\nAre there any prefered algorithms (I will implement it in C#)\\n\\nHope you got some clues. Sorry for my lack of proper terminology.\\n\\n---\\n\\nEdit: More details comes here:\\n\\nThe raw data I got is heart rate data, and in the form of number of milliseconds since last beat. Before plotting the data I calculate number of milliseconds from first sample, and the bpm (beats per minute) at each data point (60000/timesincelastbeat). \\n\\nI want to visualize the data, i.e. plot it in a line graph. I want to reduce the number of points in the graph from thousands to some hundreds. \\n\\nOne option would be to calculate the average bpm for every second in the series, or maybe every 5 seconds or so. That would have been quite easy if I knew I would have at least one sample for each of those periods (seconds of 5-seconds-intervals).",spelling fixes,
2286,4,980,168215fb-b65a-4e9a-b21e-a6be541d0264,2010-07-31 00:49:57.0,159.0,How do I reduce the number of data points in a series?,spelling fixes,
2287,2,1053,bf1aad10-ed66-4bd1-a4cd-fe670c244fc4,2010-07-31 00:51:52.0,172.0,I am looking for a good book/tutorial to learn the survival analysis. I am also interested in references on doing survival analysis in R,,
2288,1,1053,bf1aad10-ed66-4bd1-a4cd-fe670c244fc4,2010-07-31 00:51:52.0,172.0,References for Survival Analysis,,
2289,3,1053,bf1aad10-ed66-4bd1-a4cd-fe670c244fc4,2010-07-31 00:51:52.0,172.0,<survival-analysis><books>,,
2290,2,1054,c4e2fb83-beda-4043-a273-a622a880f22f,2010-07-31 00:59:11.0,172.0,What is the equivalent command in R for the stcox command in STATA? ,,
2291,1,1054,c4e2fb83-beda-4043-a273-a622a880f22f,2010-07-31 00:59:11.0,172.0,R command for stcox in STATA,,
2292,3,1054,c4e2fb83-beda-4043-a273-a622a880f22f,2010-07-31 00:59:11.0,172.0,<r><survival-analysis>,,
2293,2,1055,0264995e-039a-499e-800e-8dbf0a2749bc,2010-07-31 01:08:08.0,159.0,"Correlation measures the *linear* relationship between two variables, X and Y. Mutual information is more general and measures the reduction of uncertainty in Y after observing X. It is the KL distance between the joint density and the product of the individual densities.\\nSo MI can measure non-linear relationships and other more complicated relationships.\\n",,
2294,2,1056,75e13a28-29d1-4c6f-ab9f-0e9546cfe652,2010-07-31 01:41:53.0,251.0,"I like:\\n\\n- [Survival Analysis: Techniques for Censored and Truncated Data][1] (Klein & Moeschberger)\\n- [Modeling Survival Data: Extending the Cox Model][2] (Therneau)\\n\\nThe first does a good job of straddling theory and model building issues.  It's mostly focused on semi-parametric techniques, but there is reasonable coverage of parametric methods.  It doesn't really provide any R or other code examples, if that's what you're after.\\n\\nThe second is heavy with modeling on the Cox PH side (as the title might indicate).  It's by the author of the [survival][3] package in R and there are plenty of R examples and mini-case studies.  I think both books complement each other, but I'd recommend the first for getting started.  \\n\\nA quick way to get started in R is David Diez's [guide][4].\\n\\n\\n  [1]: http://www.powells.com/biblio/72-9780387239187-0\\n  [2]: http://www.powells.com/biblio/72-9780387239187-0\\n  [3]: http://cran.r-project.org/web/views/Survival.html\\n  [4]: http://www.statgrad.com/teac/surv/R_survival.pdf",,
2295,6,1052,f0c3a154-a90f-4f3a-8fca-d3fba44df80f,2010-07-31 02:05:21.0,,<correlation><mutual-information>,edited tags,user28
2296,2,1057,dd11a8cd-c866-44a8-8ca1-91a1b1e6fe7c,2010-07-31 02:05:44.0,251.0,"To add to Rob's answer ... with respect to reverse engineering a network, MI may be preferred over correlation when you want to extract causal rather than associative links in your network.  Correlation networks are purely associative.  But for MI, you need more data and computing power.",,
2297,2,1058,a4d788d4-7954-4f54-9ee1-d40c5eb3bea9,2010-07-31 02:08:06.0,251.0,"In package [survival][1], it's `coxph`.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/survival/index.html",,
2298,5,1025,83724521-13df-4931-91e4-9ebc5e554d3a,2010-07-31 02:09:30.0,3807.0,Calculating averages leads to a different dataset than simply reducing the number of data points.\\nIf one heartbeat per minute is much faster than the other heart beats you will lose the signal through your smoothing process.\\n\\nIf you summary 125-125-0-125-125 as 100 than the story that the data tells is different through your smoothing.\\n\\nSometimes the heart even skips beats and I believe that's an event that interesting for however wants to look at plotted heart rate data.\\n\\nI would therefore propose that you calculate the distance between two points with a formula like `d=sqrt((time1-time2)^2 + (bpm1-bpm2))`.\\n\\nYou set a minimum distance in your program.\\nThen you iterate through your data and after every point you delete all following points for which d is smaller than your minimum distance.\\n\\nAs the unit of time and bpm isn't the same you might want to think about how you can find a way to scale the units meaningfully. To do this task right you should speak to the doctors who in the end have to interpret your graphs and ask them what information they consider to be essential.,added 450 characters in body,
2299,5,1058,287879b5-7f79-4f41-b5ae-491731733062,2010-07-31 02:23:51.0,251.0,"In package [survival][1], it's `coxph`.  John Fox has a nice introduction to using coxph in R:\\n\\n- [Cox Proportional-Hazards Regression for Survival Data][2]\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/survival/index.html\\n  [2]: http://cran.r-project.org/doc/contrib/Fox.../appendix-cox-regression.pdf\\n",added 202 characters in body,
2300,5,1055,52375e95-3513-4007-a5f6-bfeb67411cbf,2010-07-31 03:04:03.0,159.0,"Correlation measures the *linear* relationship (Pearson's correlation) or *monotonic* relationship (Spearman's correlation) between two variables, X and Y. \\n\\nMutual information is more general and measures the reduction of uncertainty in Y after observing X. It is the KL distance between the joint density and the product of the individual densities. So MI can measure non-monotonic relationships and other more complicated relationships.\\n",added 83 characters in body,
2301,6,1054,573530d8-b756-4e5e-a217-ddfedf460be5,2010-07-31 04:29:25.0,,<r><survival-analysis><rpkg-survival>,edited tags,user28
2302,6,884,877fa082-809f-494d-b3df-185d1664e01c,2010-07-31 04:30:07.0,,<beginner><degrees-of-freedom>,edited tags,user28
2303,2,1059,5ca70f85-f367-4d72-86d2-2dfb3111ac96,2010-07-31 05:49:03.0,183.0,"> ""Million to one chances crop up nine times out of ten.""\\n\\n-Terry Pratchett",,
2304,16,1059,5ca70f85-f367-4d72-86d2-2dfb3111ac96,2010-07-31 05:49:03.0,-1.0,,,
2305,5,973,3da5fdbe-69ea-4f7c-b662-09a61d6ac2ca,2010-07-31 06:13:32.0,223.0,"What are the freely available data set for classification with more than 1000 features (or sample points if is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a more focused list that can be used more conveniently, also I propose the following rules:\\n\\n - One post **per dataset** \\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n\\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n\\n----------\\n\\n\\nRemark: I'm not sure if this type of question is too much connected to \\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\nso don't hesitate to comment about that (not in answers please)  it's easy to remove the post :) ",deleted 102 characters in body,
2306,2,1060,1c31a7ed-444b-4c02-bd89-f925ffd6af81,2010-07-31 09:39:47.0,339.0,"I'll use an example so that you can reproduce the results \\n\\n    # mortality \\n    mort = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/cmort.dat""),start=1970, frequency=52)\\n\\n    # temperature\\n    temp = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/temp.dat""), start=1970, frequency=52)\\n\\n    #pollutant particulates\\n    part = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/part.dat""), start=1970, frequency=52)\\n\\n    temp = temp-mean(temp)\\n    temp2 = temp^2\\n    trend = time(mort)\\n\\nNow, fit a model for mortality data\\n\\n    fit = lm(mort ~ trend + temp + temp2 + part, na.action=NULL)\\n\\nWhat I want now is to reproduce the result of the AIC command \\n\\n    AIC(fit)\\n    [1] 3332.282\\n\\nAccording to R's help file for AIC, AIC = -2 * log.likelihood + 2 * npar.\\nIf I'm correct I think that log.likelihood is given using the following formula:\\n\\n    n = length(mort)\\n    RSS = anova(fit)[length(anova(fit)[,2]),2] # there must be better ways to get this, anyway\\n    (log.likelihood <- -n/2*(log(2*pi)+log(RSS/n)+1)\\n\\n     [1] -1660.135\\nThis is approximately equal to\\n\\n    logLik(fit)\\n    'log Lik.' -1660.141 (df=6)\\n\\nAs far as I can tell, the number of parameters in the model are 5 (how can I get this number programmatically ??). So AIC should be given by:\\n\\n    -2 * log.likelihood + 2 * 5\\n    [1] 3330.271\\n\\nOoops, it seems like I should have used 6 instead of 5 as the number of parameters. What is wrong with those calculations? \\n\\n",,
2307,1,1060,1c31a7ed-444b-4c02-bd89-f925ffd6af81,2010-07-31 09:39:47.0,339.0,R's AIC formula question,,
2308,3,1060,1c31a7ed-444b-4c02-bd89-f925ffd6af81,2010-07-31 09:39:47.0,339.0,<r><modeling><time-series><aic>,,
2309,2,1061,69451cd3-86f6-48a6-ac0e-94d2ddc6b292,2010-07-31 10:04:04.0,603.0,> -2*logLik(fit)+2*(length(fit$coef)+1)\\n[1] 3332.282\\n\\n(you forgot; you have 6 parameter because $\\sigma_{\\epsilon}$ also has to be estimated!,,
2310,2,1062,b22db330-d253-4ca7-a290-9c547e96916e,2010-07-31 14:19:13.0,368.0,"In general inference, why orthogonal parameters are useful, and why is it worth trying to find a new parametrization that makes the parameters orthogonal ? \\n\\nI have seen some textbook examples, not so many, and would be interested in more concrete examples and/or motivation.",,
2311,1,1062,b22db330-d253-4ca7-a290-9c547e96916e,2010-07-31 14:19:13.0,368.0,orthogonal parametrization,,
2312,3,1062,b22db330-d253-4ca7-a290-9c547e96916e,2010-07-31 14:19:13.0,368.0,<beginner><multivariable>,,
2313,2,1063,e8c86deb-ebab-4325-ac90-acd52dfe27e7,2010-07-31 14:29:15.0,353.0,My stats has been self taught but a lot of material I read point to a dataset having mean 0 and standard deviation of 1\\n\\nIf that is the case then:\\n\\n1) Why is mean 0 and SD 1 a nice property to have\\n\\n2) Why does a random variable drawn from this sample equal 0.5?  The chance of drawing 0.001 is the same as 0.5 so this should be flat distribution...\\n\\n3) When people talk about Z Scores what do they actually mean here   ,,
2314,1,1063,e8c86deb-ebab-4325-ac90-acd52dfe27e7,2010-07-31 14:29:15.0,353.0,Why Are Mean 0 And Standard Deviation 1 Distributions Always Used,,
2315,3,1063,e8c86deb-ebab-4325-ac90-acd52dfe27e7,2010-07-31 14:29:15.0,353.0,<beginner><probability>,,
2316,16,1063,e8c86deb-ebab-4325-ac90-acd52dfe27e7,2010-07-31 14:29:15.0,353.0,,,
2317,2,1064,ea059939-4372-4c52-a253-6f1304a71ae8,2010-07-31 15:36:12.0,253.0,A nice one I came about:\\n\\n> I think it is much more interesting to\\n> live with uncertunty then to live with\\n> answers that might be wrong.\\n\\nBy Richard Feynman ([link][1])\\n\\n\\n  [1]: http://www.youtube.com/watch?v=zeCHiUe1et0&feature=player_embedded#!,,
2318,16,1064,ea059939-4372-4c52-a253-6f1304a71ae8,2010-07-31 15:36:12.0,-1.0,,,
2319,2,1065,6bd89de8-15be-48d4-99aa-286d1b557399,2010-07-31 15:46:05.0,601.0,"1)  At the beginning the most useful answer is probably that mean of 0 and sd of 1 are mathematically convenient.  If you can work out the probabilities for a distribution with a mean of 0 and standard deviation of 1 you can work them out for any similar distribution of scores with a very simple equation.\\n\\n2)  I'm not following this question.  The mean of 0 and standard deviation of 1 usually applies to the standard normal distribution, often called the bell curve.  The most likely value is the mean and it falls off as you get farther away.  If you have a truly flat distribution then there is no value more likely than another.  Your question here is poorly formed.  Were you looking at questions about coin flips perhaps?  Look up binomial distribution and central limit theorem.\\n\\n3)  ""mean here""?  Where?  The simple answer for z-scores is that they are your scores scaled as if your mean were 0 and standard deviation were 1.  Another way of thinking about it is that it takes an individual score as the number of standard deviations that score is from the mean.  The equation is calculating the (score - mean) / standard deviation.  The reasons you'd do that are quite varied but one is that in intro statistics courses you have tables of probabilities for different z-scores (see answer 1).\\n\\nIf you looked up z-score first, even in wikipedia, you would have gotten pretty good answers.",,
2320,16,1065,6bd89de8-15be-48d4-99aa-286d1b557399,2010-07-31 15:46:05.0,-1.0,,,
2321,5,1012,85aa3c51-7ebc-482b-9032-ea4e7b8b935f,2010-07-31 17:48:57.0,196.0,"I understand that the standard deviation of a uniform distribution is not a meaningful quantity (correction: not a meaningful quantity other than being a _very_ round about way of expressing the range).  However a colleague wants to compare models that use either a Gaussian distribution or a uniform distribution and for other reasons needs the standard devation of these two distributions to be equal.  In R I can do a simulation...\\n\\n    sd(runif(100000000))\\n    sd(runif(100000000,min=0,max=2))\\n\\nand see that the calculated standard deviation is likely to be ~.2887 * the range of the uniform distribution.  However, I was wondering if there was an equation that could yield the exact value, and if so, what that formula was.\\n",you'd like that wouldn't you?,
2322,2,1066,f0cb7084-7aaf-4b58-9e52-3ac80acb28a2,2010-07-31 18:20:26.0,634.0,"My question is actually quite short, but I'll have to start by describing the context since I am not sure how to directly ask it.\\n\\nConsider the following ""game"":\\n\\nWe have a segment of length n (""large segment"") and m integers (""lengths""), all considerably smaller than n. For each of the m lengths we draw a random  sub-segment of its length on the large segment. For example, if the large segment is of size 1000 (i.e. 1..1000) and we are given lengths 20, 10, 50, than a possible solution would be: 31..50, 35..44, 921..970 (sub-segments of lengths 20, 10 and 50 respectively).\\n\\nNotes:\\n1. This is just a toy example. We usually have many more lengths so there are many overlaps and each position in the large segment is covered by multiple sub-segments. \\n2. Remember that the lengths are given; only their mapping to the large segment is random.\\n3. Drawing a sub-segment of length k is done bu simply drawing a number from a uniform distribution over 1..n-k (a sub-segment of size k can start at position 1, 2, ... n-k).\\n\\nNow, we conduct many simulations of the process an d record the data. We finally examine for each position the distribution of number of sub-segments covering this position. If we look at positions that are relatively far from the edges of the large segment, the distribution in each such position is normal, and all the distributions look the same.\\nThe ""problem"" is that the positions at the ends do not look normal at all. This is not surprising, since, for example, if we are now drawing a sub-segment of length 10, the only way the very first position in the large segment will be covered is if we draw 1, whereas, for example, the 10th position will be covered if we draw 1,2,3,..10.\\n\\nWhat I am trying to figure out is what is the kind of distribution we see in the ""edge"" positions (it's not normal, but I think it usually looks like a normal distribution with its tail cut in one direction), and also how can I approximate this distribution density function from my simulations. For the ""center"" positions, I just estimate the mean and standard deviation and since I beleive the distributions there are normal - I can use the normal density function. This alos makes me think if I really need to treat the positions in a categorical way - ""near the edges"" and ""not near the edges"", or whether there are actually the same in some sort (some generalization of the normal distribution?).\\n\\nThank you, and sorry again for the length of the post.\\nDave",,
2323,1,1066,f0cb7084-7aaf-4b58-9e52-3ac80acb28a2,2010-07-31 18:20:26.0,634.0,approximating density function for a non-normal distribution,,
2324,3,1066,f0cb7084-7aaf-4b58-9e52-3ac80acb28a2,2010-07-31 18:20:26.0,634.0,<distributions><normality>,,
2329,2,1068,f79928f8-3dca-4865-8d81-82bdadad2739,2010-07-31 18:50:56.0,493.0,"I'm not sure I understand your question exactly, but I assume you are looking for the probability mass at each point, where an event is defined as a subsegment covering a particular position.  If this is true, I believe you should be able to work out the exact probability mass function.\\n\\nFor each subsegment of length K the distribution at a particular location between K and N-K+1 is uniform and proportional to K.  The distribution at the tails is stepwise increasing from 1 to K.  This can be seen by just working out one example.\\n\\nGiven multiple subsegments of different sizes, simply add these functions up.  You can then normalize everything so the weights sum to one if you want a proper distribution function.  Given a length and a list of subsegment lengths, this probability mass function should be easy to code up in the language of your choice.\\n\\n ",,
2330,5,1068,3d0b4edc-f769-4f17-aa0c-1843397f8e91,2010-07-31 19:06:43.0,493.0,"I'm not sure I understand your question exactly, but I assume you are looking for the probability mass at each point, where an event is defined as a subsegment covering a particular position.  If this is true, I believe you should be able to work out the exact probability mass function.\\n\\nFor each subsegment of length K the distribution at a particular location between K and N-K+1 is uniform and proportional to K.  The distribution at the tails is stepwise increasing from 1 to K.  This can be seen by just working out one example.\\n\\nGiven multiple subsegments of different sizes, simply add these functions up.  You can then normalize everything so the weights sum to one if you want a proper distribution function.  Given a length and a list of subsegment lengths, this probability mass function should be easy to code up in the language of your choice.\\n\\nIf you are interested in the *number of segments* covering any point, this is simply the sum of Bernoulli random variables with different p, defined at each point using the same pmf.  The simplest approach would be to enumerate the possibilities from 1 to K.",added 257 characters in body,
2331,2,1069,d7bc92d3-41c8-404c-910c-5493ca2e3fbf,2010-07-31 19:47:20.0,39.0,"This is a good, if underspecified question.  Simply put, obtaining an [orthogonal][1] parametrization allows for parameters of interest to be conveniently related to other parameters, particularly in establishing needed minimizations. \\n\\nIn the case of statistical inference, orthogonal parametrization can yield useful statistics by way of minimization (or its dual) on orthogonal parameters.  For instance, [Cox and Reid][3] use the orthogonality of nuisance parameters (and their appropriately applied maximum likelihood estimates) to construct a generalization of a liklihood ratio statistic for a parameter of interest. \\n\\nTo see how orthogonality allows for this requires an understanding of the properties of commonly used mathematical spaces and the construction of estimators, which is essentially an issue of [information geometry][2].  See [Information Geometry, Bayesian Inference, Ideal Estimates and Error Decomposition][4] for a lucid, but technical description of orthogonality and its role in statistical inference.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Orthogonal_coordinates\\n  [2]: http://en.wikipedia.org/wiki/Information_geometry\\n  [3]: http://www.jstor.org/pss/2345476\\n  [4]: http://omega.albany.edu:8008/ignorance/zhu98.pdf",,
2332,5,1069,88f2b0b1-ba37-4858-bdc3-1d2d8ab0deda,2010-07-31 20:01:29.0,39.0,"This is a good, if underspecified question.  \\n\\nSimply put, obtaining an [orthogonal][1] parametrization allows for parameters of interest to be conveniently related to other parameters, particularly in establishing needed minimizations. Where or not this is useful depends on what you are trying to do (in the case of some physics problems, for instance, orthogonal parametrization may obscure the symmetries of interest).\\n\\nIn the case of statistical inference, orthogonal parametrization can allow the use of  statistics by way of minimization (or its dual) on orthogonal parameters.  For instance, [Cox and Reid][3] use the orthogonality of nuisance parameters (and their appropriately applied maximum likelihood estimates) to construct a generalization of a liklihood ratio statistic for a parameter of interest. \\n\\nTo see how orthogonality allows for this requires an understanding of the properties of commonly used mathematical spaces and the construction of estimators, which is essentially an issue of [information geometry][2].  See [Information Geometry, Bayesian Inference, Ideal Estimates and Error Decomposition][4] for a lucid, but technical description of orthogonality and its role in statistical inference.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Orthogonal_coordinates\\n  [2]: http://en.wikipedia.org/wiki/Information_geometry\\n  [3]: http://www.jstor.org/pss/2345476\\n  [4]: http://omega.albany.edu:8008/ignorance/zhu98.pdf",added 194 characters in body,
2333,2,1070,d7c5048f-2317-4570-9a07-91243df7e49d,2010-07-31 20:17:03.0,251.0,"In Maximum Likelihood, the term orthogonal parameters is used when you can achieve a clean factorization of a joint likelihood function.  Say your data have two parameters $\\theta$ and $\\lambda$.  If you can rewrite the joint likelihood:\\n\\n$L(\\theta, \\lambda) = L_{1}(\\theta) L_{2}(\\lambda)$\\n\\nthen we call $\\theta$ and $\\lambda$ *orthogonal parameters*.  The obvious case is when you have independence, but this is not necessary for the definition as long as factorization can be achieved.  Orthogonal parameters are desirable because, if $\\theta$ is of interest, then you can perform inference using $L_{1}$. \\n\\nWhen we don't have orthogonal parameters, we try to find factorizations like\\n\\n$L(\\theta, \\lambda) = L_{1}(\\theta) L(\\theta, \\lambda)$\\n\\nand perform inference using $L_1$.  In this case, we must argue that the information loss due to excluding $L(\\theta, \\lambda)$ is low.  This leads to the concept of [marginal likelihood][1].\\n\\n  [1]: http://stats.stackexchange.com/questions/1045/is-there-a-radical-difference-in-how-bayesian-and-frequentist-approaches-treat-nu/1051#1051\\n  \\n",,
2337,2,1072,0aa89a73-6a5d-42bc-bb45-4e0e4bcf8ec6,2010-07-31 21:22:55.0,603.0,"I second Rob's comment. An increasingly prefered alternative is to include all your variables and shrink them towards 0. See Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.\\n\\nhttp://www-stat.stanford.edu/~tibs/lasso/lasso.pdf\\n\\n",,
2338,2,1073,6bf58437-c135-4d4a-ae59-18b7335cf911,2010-07-31 21:33:59.0,638.0,"Our aim is to eliminate **both** goats.  We do this by marking one goat ourselves.  The quizmaster is then forced to choose between revealing the car or the other goat. Revealing the car is out of the question, so the quizmaster will reveal and eliminate the one goat we did not know about.  We then switch to the remaining door, thereby eliminating the goat we marked with our first choice, and get the car.\\n\\nThis strategy only fails if we do not mark a goat, but the car instead.  But that is unlikely: there are two goats and only one car.\\n\\nSo we have a chance of 2 in 3 to win the car.",,
2339,2,1074,ef74d70c-0556-4049-a40f-ff82235803b1,2010-07-31 22:01:41.0,638.0,"The lesson?  Reformulate the question, and search for a strategy instead of looking at the situation.  Turn the thing on its head, work backwards...\\n\\nPeople are generally bad at working with chance.  Animals typically fare better, once they discover that either A or B gives a higher payout _on average_; they stick to the choice with the better average.  (don't have a reference ready - sorry.)\\n\\nThe first thing people are tempted to do when seeing a 80/20 distribution, is to spread their choices to match the pay-out: 80% on the better choice, and 20% on the other.\\nThis will result in a pay-out of 68%.\\n\\nAgain, there is a valid scenario for people to choose such a strategy: If the odds shift over time, there's a good reason for sending out a probe and try the choice with the lower chance of success.\\n\\nAn important part of mathematical statistics actually studies the behaviour of processes to determine whether they **are** random or not. \\n\\n",,
2347,5,1070,2779a68c-1452-4a1d-8c1b-9aea8ac8ecc3,2010-08-01 04:31:00.0,251.0,"In Maximum Likelihood, the term orthogonal parameters is used when you can achieve a clean factorization of a multi-parameter likelihood function.  Say your data have two parameters $\\theta$ and $\\lambda$.  If you can rewrite the joint likelihood:\\n\\n$L(\\theta, \\lambda) = L_{1}(\\theta) L_{2}(\\lambda)$\\n\\nthen we call $\\theta$ and $\\lambda$ *orthogonal parameters*.  The obvious case is when you have independence, but this is not necessary for the definition as long as factorization can be achieved.  Orthogonal parameters are desirable because, if $\\theta$ is of interest, then you can perform inference using $L_{1}$. \\n\\nWhen we don't have orthogonal parameters, we try to find factorizations like\\n\\n$L(\\theta, \\lambda) = L_{1}(\\theta) L_{2}(\\theta, \\lambda)$\\n\\nand perform inference using $L_1$.  In this case, we must argue that the information loss due to excluding $L_{2}$ is low.  This leads to the concept of [marginal likelihood][1].\\n\\n  [1]: http://stats.stackexchange.com/questions/1045/is-there-a-radical-difference-in-how-bayesian-and-frequentist-approaches-treat-nu/1051#1051\\n  \\n",added 10 characters in body; deleted 9 characters in body,
2348,2,1079,917a618a-e897-49fe-9eb2-94feac70fcc6,2010-08-01 06:26:34.0,87.0,"Careful... just because the PCs are constructed to be orthogonal to each other does not mean that there is not a pattern or that one PC can not appear to ""explain"" something about the other PCs.\\n\\nConsider 3D data (X,Y,Z) describing a large number of points on the surface of an American football (it is an ellipsoid -- not a sphere --  for those who have never watched American football).   Imagine that the football is in an arbitrary configuration so that neither X nor Y nor Z is along the long axis of the football.\\n\\nPrincipal components will place PC1 along the long axis of the football, the axis that describes the most variance in the data. \\n\\nFor any point in the PC1 dimension along the long axis of the football, the planar slice represented by PC2 and PC3 should describe a circle and the radius of this circular slice depends on the PC1 dimension.  It is true that regressions of PC2 or PC3 on PC1 should give a zero coefficient globally, but not over smaller sections of the football.... and it is clear that a 2D graph of PC1 and PC2 would suggest some kind of interesting two-valued nonlinear symmetric pattern.\\n\\n\\n\\n",,
2349,5,1079,806c7bf3-da17-4f19-96c2-1c45db04225b,2010-08-01 06:31:51.0,87.0,"Careful... just because the PCs are constructed to be orthogonal to each other does not mean that there is not a pattern or that one PC can not appear to ""explain"" something about the other PCs.\\n\\nConsider 3D data (X,Y,Z) describing a large number of points distributed evenly on the surface of an American football (it is an ellipsoid -- not a sphere --  for those who have never watched American football).   Imagine that the football is in an arbitrary configuration so that neither X nor Y nor Z is along the long axis of the football.\\n\\nPrincipal components will place PC1 along the long axis of the football, the axis that describes the most variance in the data. \\n\\nFor any point in the PC1 dimension along the long axis of the football, the planar slice represented by PC2 and PC3 should describe a circle and the radius of this circular slice depends on the PC1 dimension.  It is true that regressions of PC2 or PC3 on PC1 should give a zero coefficient globally, but not over smaller sections of the football.... and it is clear that a 2D graph of PC1 and PC2 would suggest some kind of interesting two-valued nonlinear symmetric pattern.\\n\\n\\n\\n",added 19 characters in body,
2350,5,1079,6467749b-8a0c-45e7-b57a-8ef1516965fe,2010-08-01 06:51:22.0,87.0,"Careful... just because the PCs are constructed to be orthogonal to each other does not mean that there is not a pattern or that one PC can not appear to ""explain"" something about the other PCs.\\n\\nConsider 3D data (X,Y,Z) describing a large number of points distributed evenly on the surface of an American football (it is an ellipsoid -- not a sphere --  for those who have never watched American football).   Imagine that the football is in an arbitrary configuration so that neither X nor Y nor Z is along the long axis of the football.\\n\\nPrincipal components will place PC1 along the long axis of the football, the axis that describes the most variance in the data. \\n\\nFor any point in the PC1 dimension along the long axis of the football, the planar slice represented by PC2 and PC3 should describe a circle and the radius of this circular slice depends on the PC1 dimension.  It is true that regressions of PC2 or PC3 on PC1 should give a zero coefficient globally, but not over smaller sections of the football.... and it is clear that a 2D graph of PC1 and PC2 would show an ""interesting"" limiting boundary that is two-valued, nonlinear, and symmetric.\\n\\n\\n\\n",added 13 characters in body,
2351,2,1080,adb4fd25-57ad-4809-a98a-34d248b7c141,2010-08-01 06:53:28.0,572.0,"I think there are several things going on.\\n\\nFor one, the setup implies more information then the solution takes into account. That it is a game show, and the host is asking us if we want to switch.\\n\\nIf you assume the host does not want the show to spend extra money (which is reasonable), then you would assume he would try to convince you to change if you had the right door. \\n\\nThis is a common sense way of looking at the problem that can confuse people, however I do think the main issue is not understanding how the new choice is different then the first (which is more clear in the 100 door case).\\n\\n",,
2352,5,1079,71bb23eb-8725-4265-863d-2f2437e574b0,2010-08-01 06:56:52.0,87.0,"Careful... just because the PCs are constructed to be orthogonal to each other does not mean that there is not a pattern or that one PC can not appear to ""explain"" something about the other PCs.\\n\\nConsider 3D data (X,Y,Z) describing a large number of points distributed evenly on the surface of an American football (it is an ellipsoid -- not a sphere --  for those who have never watched American football).   Imagine that the football is in an arbitrary configuration so that neither X nor Y nor Z is along the long axis of the football.\\n\\nPrincipal components will place PC1 along the long axis of the football, the axis that describes the most variance in the data. \\n\\nFor any point in the PC1 dimension along the long axis of the football, the planar slice represented by PC2 and PC3 should describe a disk (a circle filled in) and the radius of this circular slice depends on the PC1 dimension.  It is true that regressions of PC2 or PC3 on PC1 should give a zero coefficient globally, but not over smaller sections of the football.... and it is clear that a 2D graph of PC1 and PC2 would show an ""interesting"" limiting boundary that is two-valued, nonlinear, and symmetric.\\n\\n\\n\\n",added 19 characters in body,
2353,5,1079,28c10e81-af0b-4690-936d-2a863267929b,2010-08-01 07:02:54.0,87.0,"Careful... just because the PCs are constructed to be orthogonal to each other does not mean that there is not a pattern or that one PC can not appear to ""explain"" something about the other PCs.\\n\\nConsider 3D data (X,Y,Z) describing a large number of points distributed evenly on the surface of an American football (it is an ellipsoid -- not a sphere --  for those who have never watched American football).   Imagine that the football is in an arbitrary configuration so that neither X nor Y nor Z is along the long axis of the football.\\n\\nPrincipal components will place PC1 along the long axis of the football, the axis that describes the most variance in the data. \\n\\nFor any point in the PC1 dimension along the long axis of the football, the planar slice represented by PC2 and PC3 should describe a circle and the radius of this circular slice depends on the PC1 dimension.  It is true that regressions of PC2 or PC3 on PC1 should give a zero coefficient globally, but not over smaller sections of the football.... and it is clear that a 2D graph of PC1 and PC2 would show an ""interesting"" limiting boundary that is two-valued, nonlinear, and symmetric.\\n\\n\\n\\n",deleted 19 characters in body,
2354,2,1081,1fb0f76b-19bf-4edb-85e3-8917ba86cc2e,2010-08-01 07:16:03.0,144.0,"I have been reading Zuur, Ieno and Smith (2007) Analyzing ecological data, and on page 262, they try to explain how nMDS algorithm works. As my background is in biology and not math or statistics per se, I'm having hard time understanding a few points and would ask you if you could elaborate on them. I'm reproducing the entire algorithm list for clarity, and I hope I'm not breaking any laws by doing so.\\n\\n1. Choose a measure of association and calculate the distance matrix D.\\n2. Specify m, the number of axes.\\n3. Construct a starting configuration E. This can be done using PCoA.\\n4. Regress the configuration on D: D_ij = (alpha) + (beta)E_ij + (epsilon)_ij.\\n5. Measure the relationship between the m dimensional configuration and the real dinstances by fitting a non-parametric (monotonic) regression curve in the Shepard diagram. A monotonic regression is constrained to increase. If a parametric regression line is used, we obtain PCoA.\\n6. The discrepancy from the fitted curve is called STRESS.\\n7. Using non-linear optimization routines, obtain a new estimation of E and go from step 4 until convergence.\\n\\nQuestions:\\nIn 4., we regress the configuration to D. Where do we use the estimated parameters (alpha), (beta) and (epsilon)? Are these used to measure distance from the regression (Shepard diagram) in this new configuration\\n\\nIn regard to number 7, can you talk a little about non-linear optimisation routines? My internet search came up pretty much empty in terms of a layman's explanation. I'm interested in knowing what this routine tries to achieve (in nMDS). And I guess the next question depends on knowing these routines: what represents convergence? What converges to where?\\n\\nCan someone add ""nmds"" tag? I can't create new tags yet...",,
2355,1,1081,1fb0f76b-19bf-4edb-85e3-8917ba86cc2e,2010-08-01 07:16:03.0,144.0,help me understand nMDS algorithm,,
2356,3,1081,1fb0f76b-19bf-4edb-85e3-8917ba86cc2e,2010-08-01 07:16:03.0,144.0,<nonparametric>,,
2357,5,282,53bc8a7a-ac69-4669-8103-ed6693134468,2010-08-01 09:06:59.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 12 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. \\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of who did best overall across all subjects.\\n\\nEDIT: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n",improved based on comment.,
2358,5,282,3a1761d1-40a2-4014-9176-13f884c2426c,2010-08-01 09:21:12.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 12 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could look at the types of subjects each student is maybe more suited to.\\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of the type of subjects each student is more suited to.\\n\\nEDIT 1: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n\\nEDIT 2: full credit to @drpaulbrewer for his comment in improving this answer.\\n",added 149 characters in body; added 47 characters in body; added 4 characters in body,
2359,5,282,80d6c3e3-fb80-4842-8238-41843fa74fca,2010-08-01 09:30:24.0,81.0,"**First, lets define a score:**\\n\\nJohn, Mike and Kate get the following percentages for exams in Maths, Science, English and Music as follows:\\n\\n          Maths    Science    English    Music    \\n    John  80        85          60       55  \\n    Mike  90        85          70       45\\n    Kate  95        80          40       50\\n\\n\\nIn this case there are 12 scores in total. Each **score** represents the exam results for each person in a particular subject. So a score in this case is simply a representation of where a row and column intersect.\\n\\n\\n**Now lets informally define a Principal Component:**\\n\\nIn the table above, can you easily plot the data in a 2D graph? No, because there are four subjects (which means four variables), i.e.:\\n\\n - You could plot two subjects in the exact same way you would with x & y co-ordinates in a 2D graph. \\n - You could even plot three subjects in the same way you would plot x, y & z in a 3D graph (though this is generally bad practice). \\n\\nBut how would you plot 4 subjects?\\n\\nAt the moment we have four variables which each represent just one subject. So a method around this might be to somehow combine the subjects into maybe just two new variables which we can then plot. This is known as *Multidimensional scaling*.\\n\\nPrincipal Component analysis is a form of multidimensional scaling. It is a linear transformation of the variables into a lower dimensional space which retain maximal amount of information about the variables. For example, this would mean we could look at the types of subjects each student is maybe more suited to.\\n\\nA principle Component is therefore a combination of the original variables after a linear transformation. In **R**, this is:\\n\\n    DF<-data.frame(Maths=c(80, 90, 95), Science=c(85, 85, 80), English=c(60, 70, 40), Music=c(55, 45, 50))\\n    prcomp(DF, scale = FALSE)\\n\\nWhich will give you something like this (first two Principle Components only for sake of simplicity):\\n\\n                    PC1         PC2\\n    Maths    0.27795606  0.76772853 \\n    Science -0.17428077 -0.08162874 \\n    English -0.94200929  0.19632732 \\n    Music    0.07060547 -0.60447104 \\n\\n**So what is a Principal Component Score?**\\n\\nIt's a score from the table at the end of this post.\\n\\nThe output from **R** means we can now plot each persons score across all subjects in a 2D graph as follows:\\n\\n          x                                       y\\n    John 0.28*80 + -0.17*90 + -0.94*60 + 0.07*55  0.77*80 + -0.08*85 + 0.19*60 + -0.60*55 \\n    Mike 0.28*90 + -0.17*85 + -0.94*70 + 0.07*45  0.77*90 + -0.08*85 + 0.19*70 + -0.60*45\\n    Kate 0.28*95 + -0.17*80 + -0.94*40 + 0.07*50  0.77*95 + -0.08*80 + 0.19*40 + -0.60*50\\n\\n\\nWhich simplifies to:\\n\\n          x       y\\n    John  -45.45  33.2\\n    Mike  -51.9   48.8\\n    Kate  -21.1   44.35\\n\\nThere are ***six principle component scores*** in the table above. You can now plot the scores in a 2D graph to get a sense of the type of subjects each student is perhaps more suited to.\\n\\nEDIT 1: Hmm, I probably could have thought up a better example, and there is more to it than what I've put here, but I hope you get the idea.\\n\\nEDIT 2: full credit to @drpaulbrewer for his comment in improving this answer.\\n",added 8 characters in body,
2360,6,1081,6a46c042-25e4-4ae3-9d8f-251012b6f096,2010-08-01 09:32:26.0,88.0,<nonparametric><multidimensional-scaling>,edited tags,
2361,2,1082,6d6f4c7d-2363-40ab-9362-bf7a5912ac20,2010-08-01 11:48:48.0,,"I am trying to assess the significance of the obtained MI matrix. The initial input was a array of 3000 genes by 45 timepoints. MI was computed resulting in a array of 3600 by 3600.  I am thus comparing my results to a shuffled matrix with the same dimensions. I permutate the columns 100 times, thus have 100 results for each element in the matrix. At this stage shall I take the mean for each value in the cell and then overall mean of the matrix MI values to estimate the threshold cutoff? Is taking the mean plus 3SD sensible? Ideally comparison of probability density function between my model and the random should show large discrepancy.",,CLOCK
2362,1,1082,6d6f4c7d-2363-40ab-9362-bf7a5912ac20,2010-08-01 11:48:48.0,,How to define the significance threshold for mutual information in terms of probability of that value occurring in surrogate set?,,CLOCK
2363,3,1082,6d6f4c7d-2363-40ab-9362-bf7a5912ac20,2010-08-01 11:48:48.0,,<correlation>,,CLOCK
2364,2,1083,989e076f-fa73-485d-a229-d4c7ea523a63,2010-08-01 11:56:51.0,603.0,"Generally, by allowing assymetry, you expect the effect of shocks to last longers: i.e. the half-life increases (the half life is the number of units of time, after a 1 S.D. shock to $\\epsilon_{t-1}$ for $\\hat{\\sigma}_t|I_{t-1}$ to come back to the its unconditional value.) \\n\\nHere is a code snipped that downloads stock data, fits (e)Garch and computes half lifes, in R:\\n\\n    install.packages(""rgarch"",repos=""http://R-Forge.R-project.org"")\\n    install.packages(""fGarch"")\\n    install.packages(""fImport"")\\n    library(rgarch)\\n    library(rgarch)\\n    library(fImport)\\n    library(fGarch)\\n    d1<-yahooSeries(symbols=""ibm"",nDaysBack=1000,frequency=c(""daily""))[,4]\\n    dprice1<-diff(log(as.numeric(d1[length(d1):1])))\\n    spec1<-ugarchspec(variance.model=list(model=""eGARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    spec2<-ugarchspec(variance.model=list(model=""fGARCH"",submodel=""GARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    fit1<-ugarchfit(data=dprice1,spec=spec1)\\n    fit2<-ugarchfit(data=dprice1,spec=spec2)\\n    halflife(fit1)\\n    halflife(fit2)\\n\\n\\n",,
2365,5,1083,43141a57-608e-4966-b59f-29d1864063a7,2010-08-01 12:02:06.0,603.0,"Generally, by not allowing for assymetry, you expect the effect of shocks to last longers: i.e. the half-life increases (the half life is the number of units of time, after a 1 S.D. shock to $\\epsilon_{t-1}$ for $\\hat{\\sigma}_t|I_{t-1}$ to come back to the its unconditional value.) \\n\\nHere is a code snipped that downloads stock data, fits (e)Garch and computes half lifes, in R:\\n\\n    install.packages(""rgarch"",repos=""http://R-Forge.R-project.org"")\\n    install.packages(""fGarch"")\\n    install.packages(""fImport"")\\n    library(rgarch)\\n    library(rgarch)\\n    library(fImport)\\n    library(fGarch)\\n    d1<-yahooSeries(symbols=""ibm"",nDaysBack=1000,frequency=c(""daily""))[,4]\\n    dprice1<-diff(log(as.numeric(d1[length(d1):1])))\\n    spec1<-ugarchspec(variance.model=list(model=""eGARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    spec2<-ugarchspec(variance.model=list(model=""fGARCH"",submodel=""GARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    fit1<-ugarchfit(data=dprice1,spec=spec1)\\n    fit2<-ugarchfit(data=dprice1,spec=spec2)\\n    halflife(fit1)\\n    halflife(fit2)\\n\\n\\nThe reason for this is that generally speaking, negative spells tend to be more persistent.\\nIf you don't control for this, you will generally bias the $\\beta$ (i.e. persistance parameters) upwards.",added 8 characters in body; added 198 characters in body; edited body,
2366,5,1083,b824ef74-73e0-4ba2-b80d-70fad70dac18,2010-08-01 12:07:13.0,603.0,"Generally, by not allowing for assymetry, you expect the effect of shocks to last longers: i.e. the half-life increases (the half life is the number of units of time, after a 1 S.D. shock to $\\epsilon_{t-1}$ for $\\hat{\\sigma}_t|I_{t-1}$ to come back to the its unconditional value.) \\n\\nHere is a code snipped that downloads stock data, fits (e)Garch and computes half lifes, in R:\\n\\n    install.packages(""rgarch"",repos=""http://R-Forge.R-project.org"")\\n    install.packages(""fGarch"")\\n    install.packages(""fImport"")\\n    library(rgarch)\\n    library(fImport)\\n    library(fGarch)\\n    d1<-yahooSeries(symbols=""ibm"",nDaysBack=1000,frequency=c(""daily""))[,4]\\n    dprice1<-diff(log(as.numeric(d1[length(d1):1])))\\n    spec1<-ugarchspec(variance.model=list(model=""eGARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    spec2<-ugarchspec(variance.model=list(model=""fGARCH"",submodel=""GARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    fit1<-ugarchfit(data=dprice1,spec=spec1)\\n    fit2<-ugarchfit(data=dprice1,spec=spec2)\\n    halflife(fit1)\\n    halflife(fit2)\\n\\n\\nThe reason for this is that generally speaking, negative spells tend to be more persistent.\\nIf you don't control for this, you will generally bias the $\\beta$ (i.e. persistance parameters) upwards.\\n\\n",added 4 characters in body; deleted 21 characters in body,
2367,2,1084,7c35786c-37f0-4e67-8244-9089252ee899,2010-08-01 12:08:02.0,642.0,"I'm trying to visualize a set of data that represents human body mass over time, taken from (usually) daily weighings.\\n\\nBecause body mass tends to fluctuate +/- 3 pounds based on hydration I would like to draw a strongly smoothed line graph to minimize the fluctuation.\\n\\nAny help on what the equation would look like is much appreciated, or even just some names/links to send me in the right direction.",,
2368,1,1084,7c35786c-37f0-4e67-8244-9089252ee899,2010-08-01 12:08:02.0,642.0,Equation to calculate a smooth line given an irregular time series?,,
2369,3,1084,7c35786c-37f0-4e67-8244-9089252ee899,2010-08-01 12:08:02.0,642.0,<data-visualization>,,
2371,2,1086,ef77b088-4888-4c80-8b79-304c6ab26339,2010-08-01 12:29:28.0,159.0,There are several methods of estimating a smooth trend line. Some of the most popular are:\\n\\n - Local linear regression (loess being a popular robust implementation)\\n - Smoothing splines \\n - Regression splines (of various flavors).\\n\\nMy preference is to use **penalized regression splines** which you can fit using the [mgcv][1] package in R.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/mgcv/,,
2372,5,1084,e5f8de9e-ff3c-467e-8fd9-19823162b8ff,2010-08-01 12:34:39.0,642.0,"I'm trying to visualize a set of data that represents human body mass over time, taken from (usually) daily weighings.\\n\\nBecause body mass tends to fluctuate +/- 3 pounds based on hydration I would like to draw a strongly smoothed line graph to minimize the fluctuation.\\n\\nAny help on what the equation would look like is much appreciated, or even just some names/links to send me in the right direction.\\n\\n**EDIT:**\\nI need to code the visualization in Javascript, so I need understanding of the math involved, rather than a library that will do it for me.",added 154 characters in body,
2373,5,1086,6c53594f-cc41-4ea4-8580-9228264096bd,2010-08-01 12:48:13.0,159.0,"There are several methods of estimating a smooth trend line. Some of the most popular are:\\n\\n - Moving average smoother\\n - Local linear regression (loess being a popular robust implementation)\\n - Smoothing splines \\n - Regression splines (of various flavors).\\n\\nMy preference is to use **penalized regression splines** which you can fit using the [mgcv][1] package in R.\\n\\n**Update**: since you want to code it yourself, you might find it simplest to start with a moving average smoother (for each day, just take an average of observations from the k days on either side with k to be selected so as to give the appropriate level of smoothness).\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/mgcv/",added 275 characters in body; added 28 characters in body,
2374,5,1083,3bf3c858-6dff-4ebd-a391-3102f146d181,2010-08-01 13:38:50.0,603.0,"Generally, by not allowing for assymetry, you expect the effect of shocks to last longers: i.e. the half-life increases (the half life is the number of units of time, after a 1 S.D. shock to $\\epsilon_{t-1}$ for $\\hat{\\sigma}_t|I_{t-1}$ to come back to the its unconditional value.) \\n\\nHere is a code snipped that downloads stock data, fits (e)Garch and computes half lifes, in R:\\n\\n    install.packages(""rgarch"",repos=""http://R-Forge.R-project.org"")\\n    install.packages(""fGarch"")\\n    install.packages(""fImport"")\\n    library(rgarch)\\n    library(fImport)\\n    library(fGarch)\\n    d1<-yahooSeries(symbols=""ibm"",nDaysBack=1000,frequency=c(""daily""))[,4]\\n    dprice1<-diff(log(as.numeric(d1[length(d1):1])))\\n    spec1<-ugarchspec(variance.model=list(model=""eGARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    spec2<-ugarchspec(variance.model=list(model=""fGARCH"",submodel=""GARCH"",garchOrder=c(1,1)),mean.model=list(armaOrder=c(0,0),include.mean=T))\\n    fit1<-ugarchfit(data=dprice1,spec=spec1)\\n    fit2<-ugarchfit(data=dprice1,spec=spec2)\\n    halflife(fit1)\\n    halflife(fit2)\\n\\n\\nThe reason for this is that generally speaking, negative spells tend to be more persistent.\\nIf you don't control for this, you will generally bias the $\\beta$ (i.e. persistance parameters) downwards.\\n\\n",added 2 characters in body,
2375,2,1087,34c30444-aaf9-443e-b001-902b4450b720,2010-08-01 13:51:19.0,643.0,"There is a nice and reasonably recent discussion of this problem here:\\n\\nwww.icms.org.uk/downloads/mixtures/Robert.pdf\\n\\nEssentially, there are several standard strategies, and each has pros and cons. The most obvious thing to do is to formulate the prior in such a way as to ensure there is only one posterior mode (eg. order the means of the mixuture components), but this turns out to have a strange effect on the posterior, and therefore isn't generally used. Next is to ignore the problem during sampling, and then post-process the output to re-label the components to keep the labels consistent. This is easy to implement and seems to work OK. The more sophisticated approaches re-label on-line, either by keeping a single mode, or deliberately randomly permuting the labels to ensure mixing over multiple modes. I quite like the latter approach, but this still leaves the problem of how to summarise the output meaningfully. However, I see that as a separate problem.\\n",,
2376,2,1088,5cdf219d-6a17-4330-859b-4ff389e2fc71,2010-08-01 14:16:22.0,643.0,"In addition to those already mentioned, I like Rob Hyndman's blog:\\n\\nhttp://robjhyndman.com/researchtips/\\n\\nI guess he's too modest to mention it himself! ;-)\\n\\n",,
2377,16,1088,5cdf219d-6a17-4330-859b-4ff389e2fc71,2010-08-01 14:16:22.0,-1.0,,,
2378,2,1089,45cdfe19-dbad-4303-8a6e-a23c4ccad386,2010-08-01 14:21:18.0,643.0,"""The true logic of this world is in the calculus of probabilities"" - James Clerk Maxwell\\n",,
2379,16,1089,45cdfe19-dbad-4303-8a6e-a23c4ccad386,2010-08-01 14:21:18.0,-1.0,,,
2380,5,1031,154cbe66-94d5-43b9-b332-e53246e928a4,2010-08-01 14:25:21.0,223.0,"KL has a deep meaning in the interpretation of a set of dentities as a manifold. \\n\\nConsider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_{ij})$ is \\n \\n$F_{ij}=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$\\n\\nWith this notation $D$ is a riemannian manifold and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if $p=1$ you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and $F_{11}$ is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance $ds^2$ between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\n$ds^2= \\sum F_{ij} d \\theta^i d \\theta^j$\\n\\nand it is known to be twice the Kullback Leibler Divergence:\\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\n",added 6 characters in body,
2381,2,1090,ff9be6b8-fbb1-4d67-b289-f916d3004750,2010-08-01 14:25:28.0,643.0,"A bit obscure this one, but a great quote about subjective probability:\\n\\n""... There is no way, however, in which the individual can avoid the burden of responsibility for his own evaluations. The key cannot be found that will unlock the enchanted garden wherein, among the fairy-rings and the shrubs of magic wands, beneath the trees laden with monads and noumena, blossom forth the flowers of probabilitas realis. With these fabulous blooms safely in our button-holes we would be spared the necessity of forming opinions, and the heavy loads we bear upon our necks would be rendered superflous once and for all."" - Bruno de Finetti, Theory of Probability, Vol 2\\n",,
2382,16,1090,ff9be6b8-fbb1-4d67-b289-f916d3004750,2010-08-01 14:25:28.0,-1.0,,,
2383,5,632,4cc05c92-308c-493c-9b7d-e9e39ff6a92b,2010-08-01 14:27:15.0,223.0,"Assume you observe $X1,\\dots,Xn$ iid from a normal with mean zero and variance $\\sigma^2$. The (empirical) standard deviation is the square root of the estimation $\\hat{\\sigma}^2$ of $\\sigma^2$ (unbiased or not that is not the question). As an estimator (obtained with $X1,\\dots,Xn$), $\\hat{sigma}$ has a variance that can be calculated theoretically. Maybe what you call the standard deviation of standard deviation is actually the square root of the variance of the  standard deviation, i.e. $\\sqrt{E[(\\sigma-\\hat{\\sigma})^2]}$?  It is not an estimator, it is a theoretical quantity (something like $\\sigma/\\sqrt{n}$ to be confirmed) that can be calculated explicitely !",added 8 characters in body,
2384,5,973,a6b44110-15e7-4638-962a-df43b388909a,2010-08-01 14:28:53.0,223.0,"What are the freely available data set for classification with more than 1000 features (or sample points if it is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a more focused list that can be used more conveniently, also I propose the following rules:\\n\\n - One post **per dataset** \\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n\\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n\\n----------\\n\\n\\nRemark: I'm not sure if this type of question is too much connected to \\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\nso don't hesitate to comment about that (not in answers please)  it's easy to remove the post :) ",added 3 characters in body,
2385,5,882,ae7bc736-c5d1-4d25-af5e-9a025ae9621c,2010-08-01 14:29:37.0,223.0,"**Histogram density estimator** is estimating the density with a sum of **piecewise functions** (density of a uniform).\\n\\n**KDE** is using a sum of **smooth function** (gaussian  is an example) (as long as they are positive they can be transformed into a density by normalization) \\n\\nThe use of ""**mixture**"" in statistic is about convex combination of densities... (note that it is related to the use of a bayes principle) ",deleted 1 characters in body,
2386,6,1062,1ed60f38-ea96-4f91-b96c-46d5796999e3,2010-08-01 16:45:02.0,39.0,<beginner><multivariable>,edited tags,
2387,2,1091,fc1bbfdc-e01e-4174-9848-940e61f71a56,2010-08-01 18:14:31.0,,"There are TWO options for the inferential F-tests In SPSS. \\nMultivariate does NOT assume sphericity, adn so makes use of a different pairwise correlation for each pair of variables. \\nThe ""tests of within subjects effects"", including any post hoc tests, assumes sphericity and makes some corrections for using a common correlation across all tests. These procedures are a legacy of the days when computation was expensive, and are a waste of time with modern computing facilities.\\n\\nMy recommendation is to take the omnibus MULTIVARIATE F for any repeated measures. Then follow up with post hoc pairwise t-test, or ANOVA with only 2 levels in each repeated measure comparison if there are also between subject factors. I would make the simple bon ferroni correction of dividing the alpha level by the number of tests.\\n\\nAlso be sure to look at the effect size [available in the option dialogue]. Large effect sizes that are 'close' to significant may be more worthy of attention [and future experiments] than small, but significant effects.\\n\\nA more sophisticated approach is available in SPSS procedure MIXED, and also in less user friendly [but free] packages such as R.\\n\\nSummary, in SPSSS, multivariate F followed by pairwise post hocs eith Bon Ferroniwith Bonferroni should be sufficient for most needs.\\n",,Diana Kornbrot
2388,2,1092,74bc4db9-085f-4440-a0a3-1681c3260f34,2010-08-01 18:41:27.0,251.0,"Dawy et al. outline an algorithm in *[Gene mapping and marker clustering using Shannon's mutual information][1]* (2006).  If you're using R, you may prefer the [BUS][2] package in Bioconductor.  \\n\\n\\n  [1]: http://www.ece.iit.edu/~biitcomm/research/references/Zaher%20Dawy/Zaher%20Dawy%202006/Gene%20Mapping%20and%20Marker%20Clustering%20Using%20Shannon%92s%20Mutual%20Information.pdf\\n  [2]: http://www.bioconductor.org/packages/2.6/bioc/vignettes/BUS/inst/doc/bus.pdf\\n",,
2389,16,139,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2390,16,140,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2391,16,142,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2392,16,144,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2393,16,202,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2394,16,211,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2395,16,253,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2396,16,280,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2397,16,657,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2398,16,668,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2399,16,671,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2400,16,999,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2401,16,1005,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2402,16,1049,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2403,16,138,00000000-0000-0000-0000-000000000000,2010-08-01 18:56:25.0,123.0,,,
2404,2,1093,e3e646d3-9044-4cfc-93da-241f50fc7e38,2010-08-01 22:31:15.0,647.0,"I'm looking for a distribution to model a vector of $k$ binary random variables, $X_1, \\ldots, X_k$.  Suppose I have observed that $\\sum_i X_i = n$.  In this case I do not want to treat them as independent Bernoulli random variables.  Instead, I would like something like the multinomial:\\n\\n$P(X_1=x_1, \\ldots, X_k=x_k) = f(x_1, \\ldots, x_k; n, p_1, \\ldots, p_k) = \\frac{n!}{x_1! \\cdots x_k!} \\prod_{i=1}^k p_i^{x_i}$\\n\\nbut instead of the $x_i$ being nonnegative integers, I want them restricted to be either 0 or 1.  I have been trying to see if the [multivariate hypergeometric][1] is appropriate, but I'm not sure.\\n\\nThanks in advance for any advice.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Hypergeometric_distribution",,
2405,1,1093,e3e646d3-9044-4cfc-93da-241f50fc7e38,2010-08-01 22:31:15.0,647.0,Density function for a multivariate Bernoulli-like distribution,,
2406,3,1093,e3e646d3-9044-4cfc-93da-241f50fc7e38,2010-08-01 22:31:15.0,647.0,<distributions>,,
2407,2,1094,d493f54f-83b2-461c-826e-b734c549176c,2010-08-01 22:38:49.0,,If you can normalize by n and and assuming that treating them as probabilities/proportions makes sense in your context you can use the [dirichlet distribution][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Dirichlet_distribution,,user28
2408,2,1095,9f94df1b-90c8-45d7-92ca-d83e061f6a62,2010-08-01 22:52:13.0,282.0,"This question concerns  an implementation of the topmoumoute natural gradient (tonga) algorithm as described in page 5 in the paper Le Roux et al 2007 http://research.microsoft.com/pubs/64644/tonga.pdf.\\n\\nI understand that the basic idea is to augment stochastic gradient ascent with the covariance of the stochastic gradient estimates. Basically, the natural gradient approach multiplies a stochastic gradient with the inverse of the covariance of the gradient estimates in order to weight each component of the gradient by the variance of this component. We prefer moving into directions that show less variance during the stochastic gradient estimates: \\n\\n$ng \\propto C^{-1} g$\\n\\nSince updating and inverting the covariance in an online optimisation setting is costly, the authors describe a ``cheap'' approximate update algorithm as described on page 5 as:\\n\\n$C_t = \\gamma \\hat{C}_{t-1} + g_tg_t^T$ where  $C_{t-1}$ is the low rank approximation at time step t-1. Writing  $\\hat{C}_{t} = X_tX_t^T$ with $X_t =\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$ they use an iterative update rule for the  gram matrix $X_t^T X_T = G_t = \\left(\\begin{array}{cc}\\n                                                                                                                                                                                                                                          \\gamma G_{t-1} & \\sqrt{\\gamma} X^T_{t-1}g_t\\\\\\n\\gamma g^T_t X_{t-1} & \\sqrt{\\gamma} g_t^T g_t\\\\                                                                                                                                                                                                                                         \\end{array}\\n\\right)$\\n\\nThey then state ``To keep a low-rank estimate of  $\\hat{C}_{t} = X_tX_t^T$, we can compute its eigendecomposition and keep only the first k eigenvectors. This can be made low cost using its relation to that of the Gram matrix $G_t= X_t^T X_T$:\\n$G_t = VDV^T$\\n$C_t = (X_tVD^{-\\frac12})D(X_tVD^{-\\frac12})^T$''\\n\\nBecause it's cheaper than updating and decomposing G at every step, they then suggest that you should update X for several steps using \\n$C_{t+b} = X_{t+b}X^T_{t+b}  = X_{t+b}X_{t+b}^T$ with $X_{t+b} = \\left[\\gamma U_t, \\ \\gamma^{\\frac{b-1}{2}g_{t+1}},...\\ \\  \\gamma^{-\\frac12}g_{t+b-1}, \\ \\gamma^{\\frac{t+b}{2}g_{t+b}}\\right]$\\n\\nI can see why you can get $C_t$ from $G_t$ using the eigendecomposition. But I'm unsure about their update rule for X. The authors don't explain where U is coming from. I assume (by notation) this is the first k eigenvectors of $C_t$, correct? But if so, why would the formula for $X_t$ be a good approximation for $X_t$? When I implement this update rule, $X_t$ does not seem to be a good approximation of the ''real'' $X_t$ (that you would get from $X_t = [\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$) at all. So why should I then be able to get a good approximation of $C^{-1}$ from this (I don't)? The authors are also not quite clear about how they keep $G_t$ from growing (The size oft he matrix $G_t$ increases at each iterative update). I assume they replace $G_t$ by $\\hat V\\hat D\\hat V^T$ with $\\hat V$ and $\\hat D$ being the first k components of the eigendecomposition?\\n\\nSo in summary: \\n- I tried implementing this update rule, but I'm not getting good results and am unsure my implementation is correct\\n- why should the update rule for $X_t$ be reasonable? Is $U_t$ really the first k eigenvectors of $C_t$? (Clearly I cannot let $X_t$ and $G_t$ grow for each observed gradient $g_t$)\\n- This is far fetched, but has anyone implemented this low rank approximate update of the covariance before and has some code to share so I can compare it to my implementation?\\n\\nAs a simple example if I simulate having 15 gradients in matlab like: \\n\\n    X = rand(15, 5);\\n    c = cov(X);\\n    e = eig(c);\\n    %if the eigenvectos of C would give a good approximation of the original , this should give an appoximation of c, no?:\\n    c_r = e'*e; %no resemblence to c\\n\\nSo I'm quite certainly doing it wrong, I guess U might actually not be the eigenvectors of C, but then what is U?\\nAny suggestions or references would be  most welcome!\\n\\n(I hope the latex notation works, doesn't work in the preview)",,
2409,1,1095,9f94df1b-90c8-45d7-92ca-d83e061f6a62,2010-08-01 22:52:13.0,282.0,"tonga: low rank approximation of the natural gradient, question regarding Le Roux et al. 2007",,
2410,3,1095,9f94df1b-90c8-45d7-92ca-d83e061f6a62,2010-08-01 22:52:13.0,282.0,<algorithms>,,
2411,5,1095,cbd0b50d-23df-4eb5-8e36-21ac05c6f25b,2010-08-01 22:58:23.0,282.0,"This question concerns  an implementation of the topmoumoute natural gradient (tonga) algorithm as described in page 5 in the paper Le Roux et al 2007 http://research.microsoft.com/pubs/64644/tonga.pdf.\\n\\nI understand that the basic idea is to augment stochastic gradient ascent with the covariance of the stochastic gradient estimates. Basically, the natural gradient approach multiplies a stochastic gradient with the inverse of the covariance of the gradient estimates in order to weight each component of the gradient by the variance of this component. We prefer moving into directions that show less variance during the stochastic gradient estimates: \\n\\n$ng \\propto C^{-1} g$\\n\\nSince updating and inverting the covariance in an online optimisation setting is costly, the authors describe a ``cheap'' approximate update algorithm as described on page 5 as:\\n\\n$C_t = \\gamma \\hat{C}_{t-1} + g_tg_t^T$ where  $C_{t-1}$ is the low rank approximation at time step t-1. Writing  $\\hat{C}_{t} = X_tX_t^T$ with $X_t =\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$ they use an iterative update rule for the  gram matrix $X_t^T X_T = G_t = $\\n\\n\\n|$\\gamma G_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\sqrt{\\gamma} X^T_{t-1}g_t$\\n\\n$\\gamma g^T_t X_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\sqrt{\\gamma} g_t^Tg_t$                                                                                  \\n\\nThey then state ``To keep a low-rank estimate of  $\\hat{C}_{t} = X_tX_t^T$, we can compute its eigendecomposition and keep only the first k eigenvectors. This can be made low cost using its relation to that of the Gram matrix\\n\\n$G_t= X_t^T X_T$:\\n\\n$G_t = VDV^T$\\n\\n$C_t = (X_tVD^{-\\frac12})D(X_tVD^{-\\frac12})^T$''\\n\\nBecause it's cheaper than updating and decomposing G at every step, they then suggest that you should update X for several steps using \\n$C_{t+b} = X_{t+b}X^T_{t+b}  = X_{t+b}X_{t+b}^T$ with $X_{t+b} = \\left[\\gamma U_t, \\ \\gamma^{\\frac{b-1}{2}g_{t+1}},...\\ \\  \\gamma^{-\\frac12}g_{t+b-1}, \\ \\gamma^{\\frac{t+b}{2}g_{t+b}}\\right]$\\n\\nI can see why you can get $C_t$ from $G_t$ using the eigendecomposition. But I'm unsure about their update rule for X. The authors don't explain where U is coming from. I assume (by notation) this is the first k eigenvectors of $C_t$, correct? But if so, why would the formula for $X_t$ be a good approximation for $X_t$? When I implement this update rule, $X_t$ does not seem to be a good approximation of the ''real'' $X_t$ (that you would get from $X_t = [\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$) at all. So why should I then be able to get a good approximation of $C^{-1}$ from this (I don't)? The authors are also not quite clear about how they keep $G_t$ from growing (The size oft he matrix $G_t$ increases at each iterative update). I assume they replace $G_t$ by $\\hat V\\hat D\\hat V^T$ with $\\hat V$ and $\\hat D$ being the first k components of the eigendecomposition?\\n\\nSo in summary:\\n \\n\\n - I tried implementing this update rule, but I'm not getting good results and am unsure my implementation is correct\\n\\n - why should the update rule for $X_t$ be reasonable? Is $U_t$ really the first k eigenvectors of $C_t$? (Clearly I cannot let $X_t$ and $G_t$ grow for each observed gradient $g_t$)\\n\\n - This is far fetched, but has anyone implemented this low rank approximate update of the covariance before and has some code to share so I can compare it to my implementation?\\n\\nAs a simple example if I simulate having 15 gradients in matlab like: \\n\\n    X = rand(15, 5);\\n    c = cov(X);\\n    e = eig(c);\\n    %if the eigenvectos of C would give a good approximation of the original , this should give an appoximation of c, no?:\\n    c_r = e'*e; %no resemblence to c\\n\\nSo I'm quite certainly doing it wrong, I guess U might actually not be the eigenvectors of C, but then what is U?\\nAny suggestions or references would be  most welcome!\\n\\n(I hope the latex notation works, doesn't work in the preview)",oh dear,
2412,5,1095,e8a18dd5-ed78-4626-a7e1-fd574dfff737,2010-08-01 23:04:20.0,282.0,"This question concerns  an implementation of the topmoumoute natural gradient (tonga) algorithm as described in page 5 in the paper Le Roux et al 2007 http://research.microsoft.com/pubs/64644/tonga.pdf.\\n\\nI understand that the basic idea is to augment stochastic gradient ascent with the covariance of the stochastic gradient estimates. Basically, the natural gradient approach multiplies a stochastic gradient with the inverse of the covariance of the gradient estimates in order to weight each component of the gradient by the variance of this component. We prefer moving into directions that show less variance during the stochastic gradient estimates: \\n\\n$ng \\propto C^{-1} g$\\n\\nSince updating and inverting the covariance in an online optimisation setting is costly, the authors describe a ``cheap'' approximate update algorithm as described on page 5 as:\\n\\n$C_t = \\gamma \\hat{C}_{t-1} + g_tg_t^T$ where  $C_{t-1}$ is the low rank approximation at time step t-1. Writing  $\\hat{C}_{t} = X_tX_t^T$ with $X_t =\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$ they use an iterative update rule for the  gram matrix $X_t^T X_T = G_t = $\\n\\n\\n($\\gamma G_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\sqrt{\\gamma} X^T_{t-1}g_t$\\n\\n$\\gamma g^T_t X_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\sqrt{\\gamma} g_t^Tg_t$)                                                                                \\n\\nThey then state ``To keep a low-rank estimate of  $\\hat{C}_{t} = X_tX_t^T$, we can compute its eigendecomposition and keep only the first k eigenvectors. This can be made low cost using its relation to that of the Gram matrix\\n\\n$G_t= X_t^T X_T$:\\n\\n$G_t = VDV^T$\\n\\n$C_t = (X_tVD^{-\\frac12})D(X_tVD^{-\\frac12})^T$''\\n\\nBecause it's cheaper than updating and decomposing G at every step, they then suggest that you should update X for several steps using \\n$C_{t+b} = X_{t+b}X_{t+b}^T$ with $X_{t+b} = \\left[\\gamma U_t, \\ \\gamma^{\\frac{b-1}{2}g_{t+1}},...\\ \\  \\gamma^{-\\frac12}g_{t+b-1}, \\ \\gamma^{\\frac{t+b}{2}g_{t+b}}\\right]$\\n\\nI can see why you can get $C_t$ from $G_t$ using the eigendecomposition. But I'm unsure about their update rule for X. The authors don't explain where U is coming from. I assume (by notation) this is the first k eigenvectors of $C_t$, correct? But if so, why would the formula for $X_t$ be a good approximation for $X_t$? When I implement this update rule, $X_t$ does not seem to be a good approximation of the ''real'' $X_t$ (that you would get from $X_t = [\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$) at all. So why should I then be able to get a good approximation of $C^{-1}$ from this (I don't)? The authors are also not quite clear about how they keep $G_t$ from growing (The size oft he matrix $G_t$ increases at each iterative update). I assume they replace $G_t$ by $\\hat V\\hat D\\hat V^T$ with $\\hat V$ and $\\hat D$ being the first k components of the eigendecomposition?\\n\\nSo in summary:\\n \\n\\n - I tried implementing this update rule, but I'm not getting good results and am unsure my implementation is correct\\n\\n - why should the update rule for $X_t$ be reasonable? Is $U_t$ really the first k eigenvectors of $C_t$? (Clearly I cannot let $X_t$ and $G_t$ grow for each observed gradient $g_t$)\\n\\n - This is far fetched, but has anyone implemented this low rank approximate update of the covariance before and has some code to share so I can compare it to my implementation?\\n\\nAs a simple example if I simulate having 15 gradients in matlab like: \\n\\n    X = rand(15, 5);\\n    c = cov(X);\\n    e = eig(c);\\n    %if the eigenvectos of C would give a good approximation of the original , this should give an appoximation of c, no?:\\n    c_r = e'*e; %no resemblence to c\\n\\nSo I'm quite certainly doing it wrong, I guess U might actually not be the eigenvectors of C, but then what is U?\\nAny suggestions or references would be  most welcome!\\n\\n(sorry about the terrible layout, looks like only a subset of latex is supported, no arrays for matriced and embedded latex doesn't look particularly good, all the formulas are much more readable on page 5 of the referenced paper :)",deleted 14 characters in body; deleted 6 characters in body; added 169 characters in body,
2413,5,1095,d92e171f-6e4f-42dc-920d-c05c882ed082,2010-08-01 23:10:57.0,282.0,"This question concerns  an implementation of the topmoumoute natural gradient (tonga) algorithm as described in page 5 in the paper Le Roux et al 2007 http://research.microsoft.com/pubs/64644/tonga.pdf.\\n\\nI understand that the basic idea is to augment stochastic gradient ascent with the covariance of the stochastic gradient estimates. Basically, the natural gradient approach multiplies a stochastic gradient with the inverse of the covariance of the gradient estimates in order to weight each component of the gradient by the variance of this component. We prefer moving into directions that show less variance during the stochastic gradient estimates: \\n\\n$ng \\propto C^{-1} g$\\n\\nSince updating and inverting the covariance in an online optimisation setting is costly, the authors describe a ``cheap'' approximate update algorithm as described on page 5 as:\\n\\n$C_t = \\gamma \\hat{C}_{t-1} + g_tg_t^T$ where  $C_{t-1}$ is the low rank approximation at time step t-1. Writing  $\\hat{C}_{t} = X_tX_t^T$ with $X_t =\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$ they use an iterative update rule for the  gram matrix $X_t^T X_T = G_t = $\\n\\n\\n($\\gamma G_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\sqrt{\\gamma} X^T_{t-1}g_t$\\n\\n$\\sqrt{\\gamma} g^T_t X_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $g_t^Tg_t$)                                                                                \\n\\nThey then state ``To keep a low-rank estimate of  $\\hat{C}_{t} = X_tX_t^T$, we can compute its eigendecomposition and keep only the first k eigenvectors. This can be made low cost using its relation to that of the Gram matrix\\n\\n$G_t= X_t^T X_T$:\\n\\n$G_t = VDV^T$\\n\\n$C_t = (X_tVD^{-\\frac12})D(X_tVD^{-\\frac12})^T$''\\n\\nBecause it's cheaper than updating and decomposing G at every step, they then suggest that you should update X for several steps using \\n$C_{t+b} = X_{t+b}X_{t+b}^T$ with $X_{t+b} = \\left[\\gamma U_t, \\ \\gamma^{\\frac{b-1}{2}g_{t+1}},...\\ \\  \\gamma^{-\\frac12}g_{t+b-1}, \\ \\gamma^{\\frac{t+b}{2}g_{t+b}}\\right]$\\n\\nI can see why you can get $C_t$ from $G_t$ using the eigendecomposition. But I'm unsure about their update rule for X. The authors don't explain where U is coming from. I assume (by notation) this is the first k eigenvectors of $C_t$, correct? But if so, why would the formula for $X_t$ be a good approximation for $X_t$? When I implement this update rule, $X_t$ does not seem to be a good approximation of the ''real'' $X_t$ (that you would get from $X_t = [\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$) at all. So why should I then be able to get a good approximation of $C^{-1}$ from this (I don't)? The authors are also not quite clear about how they keep $G_t$ from growing (The size oft he matrix $G_t$ increases at each iterative update). I assume they replace $G_t$ by $\\hat V\\hat D\\hat V^T$ with $\\hat V$ and $\\hat D$ being the first k components of the eigendecomposition?\\n\\nSo in summary:\\n \\n\\n - I tried implementing this update rule, but I'm not getting good results and am unsure my implementation is correct\\n\\n - why should the update rule for $X_t$ be reasonable? Is $U_t$ really the first k eigenvectors of $C_t$? (Clearly I cannot let $X_t$ and $G_t$ grow for each observed gradient $g_t$)\\n\\n - This is far fetched, but has anyone implemented this low rank approximate update of the covariance before and has some code to share so I can compare it to my implementation?\\n\\nAs a simple example if I simulate having 15 gradients in matlab like: \\n\\n    X = rand(15, 5);\\n    c = cov(X);\\n    e = eig(c);\\n    %if the eigenvectos of C would give a good approximation of the original , this should give an appoximation of c, no?:\\n    c_r = e'*e; %no resemblence to c\\n\\nSo I'm quite certainly doing it wrong, I guess U might actually not be the eigenvectors of C, but then what is U?\\nAny suggestions or references would be  most welcome!\\n\\n(sorry about the terrible layout, looks like only a subset of latex is supported, no arrays for matriced and embedded latex doesn't look particularly good, all the formulas are much more readable on page 5 of the referenced paper :)",deleted 7 characters in body,
2414,5,1095,f67af4a3-3345-4282-aea0-f2b4f56baedb,2010-08-01 23:16:38.0,282.0,"This question concerns  an implementation of the topmoumoute natural gradient (tonga) algorithm as described in page 5 in the paper Le Roux et al 2007 http://research.microsoft.com/pubs/64644/tonga.pdf.\\n\\nI understand that the basic idea is to augment stochastic gradient ascent with the covariance of the stochastic gradient estimates. Basically, the natural gradient approach multiplies a stochastic gradient with the inverse of the covariance of the gradient estimates in order to weight each component of the gradient by the variance of this component. We prefer moving into directions that show less variance during the stochastic gradient estimates: \\n\\n$ng \\propto C^{-1} g$\\n\\nSince updating and inverting the covariance in an online optimisation setting is costly, the authors describe a ``cheap'' approximate update algorithm as described on page 5 as:\\n\\n$C_t = \\gamma \\hat{C}_{t-1} + g_tg_t^T$ where  $C_{t-1}$ is the low rank approximation at time step t-1. Writing  $\\hat{C}_{t} = X_tX_t^T$ with $X_t =\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$ they use an iterative update rule for the  gram matrix $X_t^T X_T = G_t = $\\n\\n\\n($\\gamma G_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $\\sqrt{\\gamma} X^T_{t-1}g_t$\\n\\n$\\sqrt{\\gamma} g^T_t X_{t-1}$ &nbsp;&nbsp;&nbsp;&nbsp; $g_t^Tg_t$)                                                                                \\n\\nThey then state ``To keep a low-rank estimate of  $\\hat{C}_{t} = X_tX_t^T$, we can compute its eigendecomposition and keep only the first k eigenvectors. This can be made low cost using its relation to that of the Gram matrix\\n\\n$G_t= X_t^T X_T$:\\n\\n$G_t = VDV^T$\\n\\n$C_t = (X_tVD^{-\\frac12})D(X_tVD^{-\\frac12})^T$''\\n\\nBecause it's cheaper than updating and decomposing G at every step, they then suggest that you should update X for several steps using \\n$C_{t+b} = X_{t+b}X_{t+b}^T$ with $X_{t+b} = \\left[\\gamma U_t, \\ \\gamma^{\\frac{b-1}{2}g_{t+1}},...\\ \\  \\gamma^{-\\frac12}g_{t+b-1}, \\ \\gamma^{\\frac{t+b}{2}g_{t+b}}\\right]$\\n\\nI can see why you can get $C_t$ from $G_t$ using the eigendecomposition. But I'm unsure about their update rule for X. The authors don't explain where U is coming from. I assume (by notation) this is the first k eigenvectors of $C_t$, correct? But if so, why would the formula for $X_t$ be a good approximation for $X_t$? When I implement this update rule, $X_t$ does not seem to be a good approximation of the ''real'' $X_t$ (that you would get from $X_t = [\\sqrt{\\gamma} X_{t-1}\\ \\ g_t]$) at all. So why should I then be able to get a good approximation of $C^{-1}$ from this (I don't)? The authors are also not quite clear about how they keep $G_t$ from growing (The size oft he matrix $G_t$ increases at each iterative update). I assume they replace $G_t$ by $\\hat V\\hat D\\hat V^T$ with $\\hat V$ and $\\hat D$ being the first k components of the eigendecomposition?\\n\\nSo in summary:\\n \\n\\n - I tried implementing this update rule, but I'm not getting good results and am unsure my implementation is correct\\n\\n - why should the update rule for $X_t$ be reasonable? Is $U_t$ really the first k eigenvectors of $C_t$? (Clearly I cannot let $X_t$ and $G_t$ grow for each observed gradient $g_t$)\\n\\n - This is far fetched, but has anyone implemented this low rank approximate update of the covariance before and has some code to share so I can compare it to my implementation?\\n\\nAs a simple example if I simulate having 15 gradients in matlab like: \\n\\n    X = rand(15, 5);\\n    c = cov(X);\\n    e = eig(c);\\n    %if the eigenvectos of C would give a good approximation of the original , this should give an appoximation of c, no?:\\n    c_r = e'*e; %no resemblence to c\\n\\nSo I'm quite certainly doing it wrong, I guess U might actually not be the eigenvectors of C, but then what is U?\\nAny suggestions or references would be  most welcome!\\n\\n(sorry about the terrible layout, looks like only a subset of latex is supported, no arrays for matriced and embedded latex doesn't look particularly good, all the formulas are much more readable on page 5 of the referenced paper :)\\nAlso, is this considered off-topic here? It's really more related to optimisation and machine-learning...)",added 108 characters in body,
2415,5,1079,37a4e1c9-b3dd-4174-b001-c3097c0a1943,2010-08-01 23:22:03.0,87.0,"Careful... just because the PCs are by construction orthogonal to each other does not mean that there is not a pattern or that one PC can not appear to ""explain"" something about the other PCs.\\n\\nConsider 3D data (X,Y,Z) describing a large number of points distributed evenly on the surface of an American football (it is an ellipsoid -- not a sphere --  for those who have never watched American football).   Imagine that the football is in an arbitrary configuration so that neither X nor Y nor Z is along the long axis of the football.\\n\\nPrincipal components will place PC1 along the long axis of the football, the axis that describes the most variance in the data. \\n\\nFor any point in the PC1 dimension along the long axis of the football, the planar slice represented by PC2 and PC3 should describe a circle and the radius of this circular slice depends on the PC1 dimension.  It is true that regressions of PC2 or PC3 on PC1 should give a zero coefficient globally, but not over smaller sections of the football.... and it is clear that a 2D graph of PC1 and PC2 would show an ""interesting"" limiting boundary that is two-valued, nonlinear, and symmetric.\\n\\n\\n\\n",deleted 2 characters in body,
2421,5,1094,e2ee636b-01b5-4a25-9956-8273cece20eb,2010-08-02 02:01:37.0,,"**Update**\\n\\nIn light of your comments, here is an updated answer:\\n\\n**Approach 1: Difficult to implement/analyze**\\n\\nConsider the simple case of $k$ = 3 and $n$ = 2. In other words you toss 3 coins (with probabilities $p_1$, $p_2$ and $p_3$). Then, the required mass function for the above case is:\\n\\n$p_1 p_2 (1-p_3) + p_1 (1-p_2) p_3  + (1-p_1) p_2 p_3$\\n\\nThe above reduces to the [binomial][1] if the probabilities $p_i$ are all identical. \\n\\nIn the general case, you will have ${k \\choose n}$ terms where each term is unique with a structure similar to the one above.\\n\\n**Approach 2: Easier to analyze/implement**\\n\\nInstead of the above, you could model each $X_i$ as a [bernoulli variable][2] with probability $p_i$. You could then assume that $p_i$ follows a [dirichlet distribution][3].\\n\\nYou would then estimate the model parameters by constructing the posterior distribution for $p_i$ conditional on observing $n$ successes.\\n\\n<strike>If you can normalize by n and and assuming that treating them as probabilities/proportions makes sense in your context you can use the [dirichlet distribution][3].</strike>\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Binomial_distribution\\n  [2]: http://en.wikipedia.org/wiki/Bernoulli_trial\\n  [3]: http://en.wikipedia.org/wiki/Dirichlet_distribution",added 1079 characters in body; deleted 3 characters in body,user28
2422,2,1097,f529647e-80f3-4c4b-9530-e081e97da591,2010-08-02 02:20:30.0,438.0,"I realize this question has been answered, but i don't think the extant answer really engages the question beyond pointing to a link generally related to the question's subject matter. In particular, the link describes one technique for programmatic network configuration, but that is not a ""[a] standard and accepted method"" for network configuration.\\n\\nIf you want to set NN architecture programmatically. By following a small set of clear rules, you can get a ""correct"" architecture (layer/node configuration), though of course it won't be optimal. Once this basic network is initialized, you can optimize the configuration during training using a number of ancillary algorithms; one family of these works by ""pruning"" nodes based on (small) values of the weight vector after a certain number of training epochs--in other words, eliminating 'unnecessary' nodes (more on this below).\\n\\nSo every NN (i.e., MLP) has three types of layers: input, hidden, and output.<hr>\\n\\n**The Input Layer**\\n\\nSimple--this layer configuration is completely and uniquely determined once you know the shape of your training data. Specifically, there's only one input layer, and the number of neurons comprising that layer is equal to the number of dimensions (columns) in your data. Some NN configurations add one additional node for a bias term.<hr>\\n\\n\\n**The Output Layer**\\n\\nNN's have only one of them. It's size (number of neurons) is completely determined by the chosen model configuration. Is your NN going running in 'Machine' Mode or 'Regression' Mode (the ML convention of using a term that is also used in statistics but assigning a different meaning to it is very confusing). Machine mode: returns a class label (Group I); Regression Mode returns a value (e.g., price). One node in the output layer for Regression Mode; for \\nMachine Mode, one node for each class label in your model.<hr>  \\n\\n\\n**The Hidden Layers**\\n\\nSo those few rules set the number of layers and size (neurons/layer) for both the input and output layers. That leave the hidden layers. \\n\\nHow many hidden layers? Well if your data is linearly separable (which you often know by the time you begin coding a NN) then you don't need any hidden layers at all. Of course, you don't need an NN to resolve your data either, but it will still do the job. \\n\\nBeyond that, as you probably know, there's a mountain of commentary on the question of hidden layer configuration in NNs (see the famous NN FAQ for an [excellent summary][1] of that commentary). One issue within this subject on which there is a consensus is the performance difference from adding additional hidden layers: the situations in which performance improves with a second (or third, etc.) hidden layer are very small. *One hidden layer is sufficient for the large majority of problems.*\\n\\nSo what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is '*the optimal size of the hidden layer is usually between the size of the input and size of the output layers*'. Jeff Heaton, author of ""Introduction to Neural Networks in Java"" offers a few more, [here][2].\\n\\nIn sum, for most problems, one could probably get decent performance (even without a second optimization step) by setting the hidden layer configuration using just two rules: (i) number of hidden layers equals one; and (ii) the number of neurons in that layer is the mean of the neurons in the input and output layers. <hr>\\n\\n\\n**Optimization of the Network Configuration**\\n\\n'Pruning' describes a set of techniques to trim network size (by nodes not layers) to improve computational performance and sometimes resolution performance. The gist of these techniques is removing nodes from the network during training by identifying those nodes which, if removed from the network, would not noticeably affect network performance (i.e., resolution of the data). (Even without using a formal pruning technique, you can get a rough idea of which nodes are not important by looking at your weight matrix after training; look weights very close to zero--it's the nodes on either end of those weights that are often removed during pruning.) Obviously, if you use a pruning algorithm during training then begin with a network configuration that is more likely to have excess (i.e., 'prunable') nodes--in other words, when deciding on a network architecture, err on the side of more neurons, if you add a pruning step.\\n\\nPut another way, by applying a pruning algorithm to your network during training, you can approach optimal network configuration; whether you can do that in a single ""up-front"" (such as a genetic-algorithm-based algorithm) i don't know, though i do know that for now, this two-setp optimization is more common.\\n",,
2423,5,632,8a7307b8-9e52-446d-97c0-bb1e360bb57c,2010-08-02 03:10:44.0,159.0,"Assume you observe $X1,\\dots,Xn$ iid from a normal with mean zero and variance $\\sigma^2$. The (empirical) standard deviation is the square root of the estimation $\\hat{\\sigma}^2$ of $\\sigma^2$ (unbiased or not that is not the question). As an estimator (obtained with $X1,\\dots,Xn$), $\\hat{\\sigma}$ has a variance that can be calculated theoretically. Maybe what you call the standard deviation of standard deviation is actually the square root of the variance of the  standard deviation, i.e. $\\sqrt{E[(\\sigma-\\hat{\\sigma})^2]}$?  It is not an estimator, it is a theoretical quantity (something like $\\sigma/\\sqrt{n}$ to be confirmed) that can be calculated explicitely !",added 1 characters in body,
2424,2,1098,24b8ad71-f78e-4190-b58e-6c8c2e22f820,2010-08-02 04:04:13.0,647.0,"The appropriate distribution is [Wallenius's noncentral hypergeometric distribution][1].  Using an urn analogy, the problem is equivalent to picking $n$ of $k$ balls without replacement, where each ball is a different color: the parameters $p$ are analogous to the weights of picking a particular color.\\n\\nThe problem: it's not very convenient to work with, though there is an [R package][2].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Noncentral_hypergeometric_distributions\\n  [2]: http://cran.r-project.org/web/packages/BiasedUrn/BiasedUrn.pdf",,
2425,2,1099,87e084fa-523f-4a6d-ad2b-ad61425eb4af,2010-08-02 04:40:22.0,588.0,"I am working on some kind of disease infection data, and I am puzzled on how to handle the data as ""categorical"" or ""continuous"".\\n\\n""Infection Count"" \\n\\n - the number of infection cases found in a specific period of time, the count is generated from categorical data (i.e. no. of patient tagged as ""infected"")\\n\\n\\n\\n""Patient Bed Days"" \\n\\n - sum of total number of day stay in the ward by all patients in that ward, again, the count is generated from categorical data (i.e. no. of patient tagged as ""staying in that particular ward"")\\n\\n""infection per patient bed days""\\n\\n - ""infection count"" / ""patient bed days""\\nboth were originally count data, but now becomes a rate\\n\\nCan I still use Chi-Square here to find if the difference in ""infection per patient bed days"" is statistically significant or not?\\n\\nThanks!",,
2426,1,1099,87e084fa-523f-4a6d-ad2b-ad61425eb4af,2010-08-02 04:40:22.0,588.0,"how to handle count data (categorical data), when it has been converted to a rate?",,
2427,3,1099,87e084fa-523f-4a6d-ad2b-ad61425eb4af,2010-08-02 04:40:22.0,588.0,<categorical-data><count-data>,,
2428,2,1100,739e5305-aa49-4f7c-8f4b-e6d6e84ed4e6,2010-08-02 05:16:40.0,30.0,"I will give an itemized answer. Can provide more citations on demand, although this is not really controversial.\\n\\n - Statistics is **not** all about\\n   maximizing (log)-likelihood. That's\\n   anathema to principled bayesians who\\n   just update their posteriors or\\n   propagate their beliefs through an\\n   appropriate model.\\n - A lot of statistics **is** about loss\\n   minimization. And so is a lot of\\n   Machine Learning. Empirical loss\\n   minimization has a different meaning\\n   in ML. For a clear, narrative view,\\n   check out Vapnik's ""The nature of\\n   statistical learning""\\n - Machine Learning is **not** all about\\n   loss minimization. First, because\\n   there are a lot of bayesians in ML;\\n   second, because a number of\\n   applications in ML have to do with\\n   temporal learning and approximate DP.\\n   Sure, there is an objective function,\\n   but it has a very different meaning\\n   than in ""statistical"" learning.\\n\\nI don't think there is a gap between the fields, just many different approaches, all overlapping to some degree. I don't feel the need to make them into systematic disciplines with well-defined differences and similarities, and given the speed at which they evolve, I think it's a doomed enterprise anyway.\\n",,
2429,2,1101,6281d43e-20ca-49b6-b67b-1d1d9886e66b,2010-08-02 05:49:40.0,485.0,"Survival Analysis: A Self-Learning Text \\nby Kleinbaum and Klein\\n\\nis pretty good.  It depends on what you want.  This is more of a non-technical introduction.  It's focused on practical applications and minimizes the mathematics.  Pedegocially, it's also intended for learning outside of the classroom.",,
2430,5,1081,656f74d3-a3c4-4407-a013-9f7055a674a1,2010-08-02 06:08:51.0,144.0,"I have been reading Zuur, Ieno and Smith (2007) Analyzing ecological data, and on page 262, they try to explain how nMDS (non-metric multidimensional scaling) algorithm works. As my background is in biology and not math or statistics per se, I'm having hard time understanding a few points and would ask you if you could elaborate on them. I'm reproducing the entire algorithm list for clarity, and I hope I'm not breaking any laws by doing so.\\n\\n1. Choose a measure of association and calculate the distance matrix D.\\n2. Specify m, the number of axes.\\n3. Construct a starting configuration E. This can be done using PCoA.\\n4. Regress the configuration on D: D_ij = (alpha) + (beta)E_ij + (epsilon)_ij.\\n5. Measure the relationship between the m dimensional configuration and the real dinstances by fitting a non-parametric (monotonic) regression curve in the Shepard diagram. A monotonic regression is constrained to increase. If a parametric regression line is used, we obtain PCoA.\\n6. The discrepancy from the fitted curve is called STRESS.\\n7. Using non-linear optimization routines, obtain a new estimation of E and go from step 4 until convergence.\\n\\nQuestions:\\nIn 4., we regress the configuration to D. Where do we use the estimated parameters (alpha), (beta) and (epsilon)? Are these used to measure distance from the regression (Shepard diagram) in this new configuration\\n\\nIn regard to number 7, can you talk a little about non-linear optimisation routines? My internet search came up pretty much empty in terms of a layman's explanation. I'm interested in knowing what this routine tries to achieve (in nMDS). And I guess the next question depends on knowing these routines: what represents convergence? What converges to where?\\n\\nCan someone add ""nmds"" tag? I can't create new tags yet...",elaborated what nmds is,
2431,2,1102,c9cfaba7-8c79-4194-8233-c46ef9b5af9b,2010-08-02 08:14:36.0,,"Thannks a lot for this very detailed answer. It does make perfect sense to me. If I have a model know. For example: \\n\\n    y = - 2.8 - 1.2 * urban - 3 * forest\\n\\nwith the reference category grassland and I would like to predict the model for a new environment.\\n\\nIf I have a point with grassland, the probability of y would be like:\\n\\n    y = 1 / (1 + exp(2.8))\\nIs that right?\\n\\nAnd lets say for a point in in a forest\\n\\n    y = 1 / (1+ exp(-(-2.8 - 3*1)))\\n\\nThanks a lot again\\n\\nMike\\n",,Mike
2432,2,1103,dd72c6cc-4399-46a7-9367-8e5b6cdeec13,2010-08-02 08:46:38.0,215.0,"\\nTo start with what we're talking about here is the standard normal distribution, a normal distribution with a mean of 0 and a standard deviation of 1. The short-hand for a variable which is distributed as a standard normal distribution is Z. \\n\\nHere are my answers to your questions.\\n\\n(1) I think there are two key reasons why standard normal distributions are attractive. Firstly, any normally distributed variable can be converted or transformed to a standard normal by subtracting its mean from each observation before dividing each observation by the standard deviation. This is called the Z-transformation or the creation of Z-scores. This is very handy especially in the days before computers. \\n\\nIf you wanted to find out the probability of some event from your variable which is normally distributed with mean 65.6 with a standard deviation of 10.2 wouldn't that be a right pain in the backside without a computer? Let's say that this variable is the heights in inches of American women. And let's say that we're interested in finding out the probability that a woman randomly drawn from the population will be very tall - say over 75 inches tall. Well this is a bit of a pain to find out with a computer as I would have to carry around a table for every possible normal distribution with me. However, if I transform this to a Z-score I can use the one table to find out the probability, thus:\\n\\n(x_i - x-bar)/sd_x = Z\\n\\n(75 - 65.6)/10.2 = 0.9215\\n\\nUsing the Z table I find that the cumulative probability P(z < Z) - 0.8212 and therefore the probability of finding a woman as tall or taller than 75 inches is 17.88%. We can do this with **any** normally distributed variable and so this standard normal distribution is very handy.\\n\\nThe second reason why the standard normal distribution is used frequently is due to the interpretation is provides in terms of Z-scores. Each ""observation"" in a Z-transformed variable is how many standard deviations the original untransformed observation was from the mean. This is particularly handy for standardized tests where the raw or absolute performance is less important than the relative performance.\\n\\n(2) I don't follow you here. I think you may be confused as to what we mean by a cumulative distribution function.\\n\\n(3) Z-scores are the individual ""observations"" or datum in a variable which has been Z-transformed. Return to my example of the variable - height of American women in inches. One particular observation of which may be a tall woman of height 75 inches. The Z-score for this is the result of Z-transforming the variable as we did earlier: \\n\\n(x_i - x-bar)/sd_x = Z\\n\\n(75 - 65.6)/10.2 = 0.9215\\n\\nThe Z-score in this case is 0.9215. The interpretation of the Z-score is that this particular woman is 0.9215 standard deviations taller than the mean height. A person who was 55.4 inches tall have a Z-score of 1 and would be 1 standard deviation below mean height. ",,
2433,16,1103,dd72c6cc-4399-46a7-9367-8e5b6cdeec13,2010-08-02 08:46:38.0,-1.0,,,
2434,2,1104,37f078a0-2f89-41c5-bd00-cf97787750da,2010-08-02 08:58:27.0,17.0,">While the individual man is an insoluble puzzle, in the aggregate he becomes a mathematical certainty. You can, for example, never foretell what any one man will be up to, but you can say with precision what an average number will be up to. Individuals vary, but percentages remain constant. So says the statistician.\\n\\nArthur Conan Doyle",,
2435,16,1104,37f078a0-2f89-41c5-bd00-cf97787750da,2010-08-02 08:58:27.0,-1.0,,,
2437,5,1064,8cdc38cf-599d-403f-badc-7e8509d3796b,2010-08-02 09:54:45.0,159.0,A nice one I came about:\\n\\n> I think it is much more interesting to\\n> live with uncertainty then to live with\\n> answers that might be wrong.\\n\\nBy Richard Feynman ([link][1])\\n\\n\\n  [1]: http://www.youtube.com/watch?v=zeCHiUe1et0&feature=player_embedded#!,added 1 characters in body,
2438,2,1106,bd11c892-ebeb-4d35-8948-4bbc1dbf45a4,2010-08-02 09:58:34.0,88.0,"Few opening remarks. In nMDS you have a matrix of dissimilarities $D_{ij}$ (not distances; for instance this can be a per cent of people that said in some poll that i&j are not similar). What you want to obtain is a set of points ($E=\\{X_i\\}$) representing objects on M-dim space; having it, you have the matrix of distances between objects in this space $d_{ij}$.  \\nnMDS tries to guess such $E$ that $d_{ij}$ has the same rank as $D_{ij}$; it is like connecting each object pair with spring the more strong the less dissimilar the pair is and then releasing the whole configuration -- after relaxation, the objects that has been connected using stronger springs will be nearer.   \\nPoint 4 is something like overfitting regression. You have some approximation of objects position $E^a$, and so also approximated distances $d^a_{ij}$. Now you can do regression $d^a_{ij}$~$f(D_{ij})$ and using it count the distances that should be if the $D$ would be represented perfectly $d^r_{ij}=f(D_{ij})$.   \\nStill, because you cannot directly count $E$ from $d^r$ (this is the problem of nonlinear optimization here), you must somehow mutate $E$ so that the distances will approach $d^r$. The standard method here is to mimic physical analogy with springs and move objects which are connected with most extended springs (having largest $|d^a_{ij}-d^r_{ij}|$) towards themselves so that the potential energy (this STRESS) of the system will be minimized mostly.",,
2439,2,1107,d99aa123-63fc-4c68-8a3c-8b7be037203b,2010-08-02 10:03:30.0,,"From a technical purist point of view, you cannot as your ratio ""infection per patient bed days"" is not a continuous variable. For example, an irrational value will never appear in your dataset. However, you can ignore this technical issue and do whatever tests that may be appropriate for your context. By way of analogy, incomes levels are discrete but almost everyone treats them as continuous.\\n\\nBy the way, it is not entirely clear why you want to do a chi-square but I am assuming there is some background context why that makes sense for you.",,user28
2440,2,1108,15d8a4aa-465b-4951-86cd-569e0516010d,2010-08-02 10:55:30.0,442.0,"For me it does not at all sound appropriate to use a chi-square test here.\\n\\nI guess what you wanna do is the following: You have different wards or treatments or whatever else kind of nominal variable (i.e., groups) that divides your data. For each of these groups you collected the *Infection Count* and the *Patient Bed Days* to calculate the *infection per patient bed days*. Know you wanna check for differences between the groups, right?\\n\\nIf so, an analysis of variance (ANOVA, in case of more than two groups) or a t-test (in case of two groups) is probably appropriate given by the reasons in Srikant Vadali's post (and if the assumptions homogeneity of variances and comparable groups sizes are also met) and the `beginner` tag should be added.",,
2441,5,1106,926c5323-406c-41ce-93e6-96b566c8f577,2010-08-02 11:23:47.0,88.0,"Few opening remarks. In nMDS you have a matrix of dissimilarities $D_{ij}$ (not distances; for instance this can be a per cent of people that said in some poll that i&j are not similar). What you want to obtain is a set of points ($E=[X_i]$) representing objects on M-dim space; having it, you have the matrix of distances between objects in this space $d_{ij}$.  \\nnMDS tries to guess such $E$ that $d_{ij}$ has the same rank as $D_{ij}$; it is like connecting each object pair with spring the more strong the less dissimilar the pair is and then releasing the whole configuration -- after relaxation, the objects that has been connected using stronger springs will be nearer.   \\nPoint 4 is something like overfitting regression. You have some approximation of objects position $E^a$, and so also approximated distances $d^a_{ij}$. Now you can do regression $d^a_{ij}$~$f(D_{ij})$ and using it count the distances that should be if the $D$ would be represented perfectly $d^r_{ij}=f(D_{ij})$.   \\nStill, because you cannot directly count $E$ from $d^r$ (this is the problem of nonlinear optimization here), you must somehow mutate $E$ so that the distances will approach $d^r$. The standard method here is to mimic physical analogy with springs and move objects which are connected with most extended springs (having largest $|d^a_{ij}-d^r_{ij}|$) towards themselves so that the potential energy (this STRESS) of the system will be minimized mostly.",deleted 2 characters in body,
2442,2,1109,aec9e722-c389-4159-88e2-cb142a7ae6ea,2010-08-02 11:24:35.0,495.0,"I'm not quite sure what your data look like, or what your precise problem is, but I assume you have a table with the following headings and type:\\n\\n> ward (categorical), infections (integer), patient-bed-days (integer or continuous).\\n\\nand you want to tell if the infection rate is statistically different for different wards?\\n\\nOne way of doing this is to use a Poisson model:\\n\\n> Infections ~ Poisson (Patient bed days * ward infection rate)\\n\\nThis can be achieved by using a Poisson glm, with log link function and the log of patient-bed-days in the offset. In R, the code would look something like:\\n\\n    glm(infections ~ ward + offset(log(patient-bed-days)), family=poisson())",,
2443,2,1110,1bc0ae44-3a78-463d-9d32-1760c1a87ca7,2010-08-02 11:36:35.0,1356.0,"This is a bit off-topic, but here's some good stuff about PCA (mostly in R):\\n\\n<b>HTML</b>\\n\\nhttp://www.statsoft.com/textbook/principal-components-factor-analysis/?button=1 <br />\\nhttp://ordination.okstate.edu/PCA.htm<br />\\nhttp://astrostatistics.psu.edu/datasets/R/MV.html<br />\\nhttp://www.statmethods.net/advstats/factor.html<br />\\n\\n<hr />\\n\\n<b>PDF</b>\\n\\nhttp://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_principal_components_analysis.pdf<br />\\nhttp://www.uga.edu/strata/software/pdf/pcaTutorial.pdf<br />\\nhttp://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf<br />\\nhttp://www.miislita.com/information-retrieval-tutorial/pca-spca-tutorial.pdf<br />\\nhttp://www.unc.edu/~rcm/book/factor.pdf",,
2445,2,1111,8859bf9a-d5f7-4a1d-bb72-5e49e55badbc,2010-08-02 13:53:47.0,302.0,"For a very clear, succinct and applied approach, I highly recommend [Event History Modeling][1] by Box-Steffenmeier and Jones\\n\\n\\n  [1]: http://www.amazon.com/Event-History-Modeling-Scientists-Analytical/dp/0521546737",,
2446,2,1112,36db3e3c-c2f2-4c2e-b640-4721b9e3e79b,2010-08-02 14:38:55.0,652.0,I want to represent a variable as a number between 0 and 1. The variable is a non-negative integer with no inherent bound. I map 0 to 0 but what can I map to 1 or numbers between 0 and 1?\\n\\nI could use the history of that variable to provide the limits. This would mean I have to restate old statistics if the maximum increases. Do I have to do this or are there other tricks I should know about?\\n\\n\\n\\n,,
2447,1,1112,36db3e3c-c2f2-4c2e-b640-4721b9e3e79b,2010-08-02 14:38:55.0,652.0,How to represent an unbounded variable as number between 0 and 1,,
2448,3,1112,36db3e3c-c2f2-4c2e-b640-4721b9e3e79b,2010-08-02 14:38:55.0,652.0,<statistics>,,
2449,2,1113,dbc7700a-3eaa-4b5e-bc96-79fe6e3de351,2010-08-02 14:56:35.0,442.0,"A common trick to do so (e.g., in connectionist modeling) is to use the [hyperbolic tangent tanh][1].\\nIt automatically fits all numbers in the interval between -1 and 1. WHich in youzr case restricts the range from 0 to 1.\\nIn `r` and `matlab` you get it via `tanh()`. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Tanh",,
2450,5,1113,3b4e0469-c8da-4b42-aa00-db8dd6f6c4f9,2010-08-02 15:09:46.0,442.0,"A very common trick to do so (e.g., in connectionist modeling) is to use the [hyperbolic tangent tanh][1] as the 'squashing function"".\\nIt automatically fits all numbers into the interval between -1 and 1. Which in your case restricts the range from 0 to 1.\\nIn `r` and `matlab` you get it via `tanh()`. \\n\\nAnother squashing function is provided by $ f(x) = 1 / (1 + e ^ - ^x ) $, which restricts the range from 0 to 1 (with 0 mapped to .5). So you would have to take the results times 2 and subtract 1 to fit your data into the interval between 0 and 1.\\n\\n  [1]: http://en.wikipedia.org/wiki/Tanh",tried Tex; added 2 characters in body; added 3 characters in body; edited body,
2451,2,1114,1675f483-71ba-431f-bd74-317af83e8966,2010-08-02 15:20:11.0,495.0,"Any sigmoid function will work:\\n\\n - The top half of the [logistic function][1] (multiply by 2, subtract 1)\\n - The [error function][2]\\n - tanh, as suggested by Henrik.\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Logistic_function\\n  [2]: http://en.wikipedia.org/wiki/Error_function",,
2452,5,1113,ab9713dd-dab0-442b-ac99-d423bb4431af,2010-08-02 15:26:11.0,442.0,"A very common trick to do so (e.g., in connectionist modeling) is to use the [hyperbolic tangent tanh][1] as the 'squashing function"".\\nIt automatically fits all numbers into the interval between -1 and 1. Which in your case restricts the range from 0 to 1.\\nIn `r` and `matlab` you get it via `tanh()`. \\n\\nAnother squashing function is provided by $ f(x) = 1 / (1 + e ^ - ^x ) $, which restricts the range from 0 to 1 (with 0 mapped to .5). So you would have to take the results times 2 and subtract 1 to fit your data into the interval between 0 and 1.\\n\\nHere is some simple R code which plots both functions (tanh in red, exp in blue) so you can see how they squash:\\n\\n    x <- seq(0,20,0.001)\\n    plot(x,tanh(x),pch=""."", col=""red"", ylab=""y"")\\n    points(x,(1 / (1 + exp(-x)))*2-1, pch=""."",col=""blue"")\\n\\n  [1]: http://en.wikipedia.org/wiki/Tanh",added graph; added 25 characters in body,
2453,5,1113,a521ad0b-df42-4e4e-a21f-cb48cfeeba3c,2010-08-02 15:31:35.0,442.0,"A very common trick to do so (e.g., in connectionist modeling) is to use the [hyperbolic tangent tanh][1] as the 'squashing function"".\\nIt automatically fits all numbers into the interval between -1 and 1. Which in your case restricts the range from 0 to 1.\\nIn `r` and `matlab` you get it via `tanh()`. \\n\\nAnother squashing function is the logisitc function (thanks to Simon for the name), provided by $ f(x) = 1 / (1 + e ^ - ^x ) $, which restricts the range from 0 to 1 (with 0 mapped to .5). So you would have to multiply the result by 2 and subtract 1 to fit your data into the interval between 0 and 1.\\n\\nHere is some simple R code which plots both functions (tanh in red, logistic in blue) so you can see how both squash:\\n\\n    x <- seq(0,20,0.001)\\n    plot(x,tanh(x),pch=""."", col=""red"", ylab=""y"")\\n    points(x,(1 / (1 + exp(-x)))*2-1, pch=""."",col=""blue"")\\n\\n  [1]: http://en.wikipedia.org/wiki/Tanh",added logistic function as the name,
2454,2,1115,9780fa9d-443e-48f9-8496-35b25593d28e,2010-08-02 16:28:37.0,474.0,"I have computed percent change from time1 to time2 for several variables, can I regress Pch_earnings=pch_prod, pch_price?\\nIf I run a model with actual data and dummy for time1=1, time2=0, dummy is not stat. sign. but there are large changes. Thanks",,
2455,1,1115,9780fa9d-443e-48f9-8496-35b25593d28e,2010-08-02 16:28:37.0,474.0,Linear Regression,,
2456,3,1115,9780fa9d-443e-48f9-8496-35b25593d28e,2010-08-02 16:28:37.0,474.0,<regression>,,
2457,2,1116,44e8345a-c943-4d0a-83ea-919be695361d,2010-08-02 16:29:37.0,,"""Survival analysis using SAS: a practical guide"" by Paul D. Allison provides a good guide to the connection between the math and SAS code - how to think about your information, how to code, how to interpret results. Even if you are using R, there will be parallels that could prove useful.",,wave97
2458,2,1117,0a18eb2e-b6c7-41d2-bfd5-3901aa683190,2010-08-02 16:43:44.0,419.0,The answer depends on the degree of misspecification and sample size. In small and moderate samples simplified model will fit (in most cases) better to data then the true model.\\nIn moderate and large samples residuals don't have to be normal as due to CLT regression coefficients are normal anyway.\\n\\n\\n,,
2459,2,1118,3494248c-9818-45c8-a8de-997da847c623,2010-08-02 16:43:49.0,,"You're likely to have problems with covariance - your model fails to meet the assumption of linear regression that the observations are independent, because a subject in your study will be correlated with itself between time 1 and time 2 (By the way, what are your observations?  One for each product type?)\\n\\nYou might want to look into ""repeated measures"" methods.",,stoplan
2460,2,1119,704dd814-963c-4f1f-900c-14af7e75b0f3,2010-08-02 16:49:48.0,,"In addition to the good suggestions by Henrik and Simon Byrne, you could use f(x) = x/(x+1).  By way of comparison, the logistic function will exaggerate differences as x grows larger.  That is, the difference between f(x) and f(x+1) will be larger with the logistic function than with f(x) = x/(x+1).  You may or may not want that effect.",,stoplan
2461,2,1120,74b383fa-12a9-4ba4-9bbd-14eee7e1fdec,2010-08-02 16:55:33.0,,There are some very good learning materials here: http://scc.stat.ucla.edu/mini-courses/materials-from-past-mini-courses/spring-2009-mini-course-materials/,,stoplan
2462,16,1120,74b383fa-12a9-4ba4-9bbd-14eee7e1fdec,2010-08-02 16:55:33.0,-1.0,,,
2463,2,1121,0729d707-8283-4f85-ba07-60238497e053,2010-08-02 16:57:33.0,162.0,"There are two ways to implement this that I use commonly. I am always working with realtime data, so this is written from that perspective. Here's some pseudo-code:\\n\\n**Using a trainable minmax:**\\n\\n<code>define function peak:<br />\\n// keeps the highest value it has received<br /></code>\\n\\n<code>define function trough:<br />\\n// keeps the lowest value it has received<br /></code>\\n\\n<code>define function calibrate:<br />\\n// toggles whether peak() and trough() are receiving values or not</code>\\n\\n<code>define function scale:<br />\\n// maps input range [trough.value() to peak.value()] to [0.0 to 1.0]</code>\\n\\nThis function requires that you either perform an initial training phase (by using calibrate()) or that you re-train either at certain intervals or according to certain conditions. For instance, imagine a function like this:\\n\\n<code>define function outBounds (val, thresh):<br />\\nif val > (thresh*peak.value()) || val < (trough.value() / thresh):<br />\\ncalibrate()<br />\\n// peak and trough are normally not receiving values, but if outBounds() receives a value that is more than 1.5 times the current peak or less than the current trough divided by 1.5, then calibrate() is called which allows the function to recalibrate automatically.</code>\\n\\n**Using an historical minmax:**\\n\\n<code>var arrayLength = 1000</code><br />\\n<code>var histArray[arrayLength]</code>\\n\\n<code>define historyArray(f):<br />\\nhistArray.pushFront(f) //adds f to the beginning of the array</code><br/>\\n\\n<code>define max(array):<br />\\n// finds maximum element in histArray[]<br />\\nreturn max</code><br />\\n\\n<code>define min(array):<br />\\n// finds minimum element in histArray[]<br />\\nreturn min</code><br />\\n\\n<code>define function scale:<br />\\n// maps input range [min(histArray) to max(histArray)] to [0.0 to 1.0]</code>\\n\\n<code>main()</code><br />\\n<code>historyArray(histArray)</code><br />\\n<code>scale(min(histArray), max(histArray), histArray[0])<br />\\n// histArray[0] is the current element</code>",,
2464,16,1121,0729d707-8283-4f85-ba07-60238497e053,2010-08-02 16:57:33.0,162.0,,,
2465,2,1122,236f53d7-f285-46b9-a35b-0994cb4f3d1c,2010-08-02 17:05:49.0,419.0,1. Almost always in randomized trials\\n2. Almost always in observational study when someone measure all confouders (almost never)\\n3. Sometimes when someone measure some counfounders (IC* algorithim of DAG discovery in Pearl's book Causality)\\n4. In non gaussian linear models with two or more variables but not using correlation as measure of relationship (http://www.cs.helsinki.fi/group/neuroinf/lingam/)\\n\\nMost of discovery algorithms are implemented in Tetrad IV (http://www.phil.cmu.edu/projects/tetrad/)\\n,,
2466,2,1123,c83e0668-0ead-4e06-9ab0-163b4d2710ef,2010-08-02 17:08:08.0,,"in many papers i see data representing a rate of success  (i.e a number between 0 and 1) modeled as a gaussian. this is clearly a sin (the range of variation of the gaussian is all of R),\\nbut how bad is that sin? under what assumptions would you say it is tolerable?",,ddan
2467,1,1123,c83e0668-0ead-4e06-9ab0-163b4d2710ef,2010-08-02 17:08:08.0,,modeling a success rate as a gaussian,,ddan
2468,3,1123,c83e0668-0ead-4e06-9ab0-163b4d2710ef,2010-08-02 17:08:08.0,,<data><model-selection>,,ddan
2469,2,1124,e8a8a9ed-7f1b-4a32-91a8-32eef5e636a2,2010-08-02 17:15:04.0,334.0,"Could you quote from ""many papers"" so that we would get some context?  Between ""Gaussian"" and ""number between 0 and 1"" I see slight conflict as the draws from a Gaussian are not bounded. Maybe you meant p-values?",,
2470,2,1125,d45512f0-c7a4-4377-9664-525793d1e59b,2010-08-02 17:15:14.0,,"It depends on the data. While the normal distribution does span the real line do note that nearly 99% of the values are contained within 3 standard deviations of the mean. Thus, if the following conditions hold it may be a reasonable assumption:\\n\\n(a) the data range is such that 99% of the data falls between [$\\mu - 3\\sigma,\\mu+3\\sigma]$ \\n\\n(b) the data is unimodal \\n\\n(c) the data 'passes' other relevant tests for [normality][1]\\n\\nHaving said that some decision needs to be taken in the event that a draw from this distribution falls below 0 or above 1. Two ideas in such a situation: \\n\\n(a) If draw < 0 set the draw to 0 and if draw > 1 set the draw to 1 or\\n\\n(b) Model the distribution as truncated normal with the cut off points at 0 and 1.\\n\\n  [1]: http://en.wikipedia.org/wiki/Normality_test",,user28
2471,2,1126,d44e6207-9901-4dc6-a52b-96a84ab63d00,2010-08-02 17:29:12.0,660.0,"Joshua Epstein wrote a paper titled ""Why Model?"" available at http://www.santafe.edu/media/workingpapers/08-09-040.pdf in which gives 16 reasons:\\n\\n 1. Explain (very distinct from predict)\\n 2. Guide data collection\\n 3. Illuminate core dynamics\\n 4. Suggest dynamical analogies\\n 5. Discover new questions\\n 6. Promote a scientific habit of mind\\n 7. Bound (bracket) outcomes to plausible ranges\\n 8. Illuminate core uncertainties.\\n 9. Offer crisis options in near-real time\\n 10. Demonstrate tradeoffs / suggest efficiencies\\n 11. Challenge the robustness of prevailing theory through perturbations\\n 12. Expose prevailing wisdom as incompatible with available data\\n 13. Train practitioners\\n 14. Discipline the policy dialogue\\n 15. Educate the general public\\n 16. Reveal the apparently simple (complex) to be complex (simple)\\n\\n(Epstein elaborates on many of the reasons in more detail in his paper.)\\n\\nI would like to ask the community:\\n\\n * are there are additional reasons that Epstein did not list?\\n * is there a more elegant way to conceptualize (a different grouping perhaps) these reasons?\\n * are any of Epstein's reasons flawed or incomplete?\\n * are their clearer elaborations of these reasons?\\n",,
2472,1,1126,d44e6207-9901-4dc6-a52b-96a84ab63d00,2010-08-02 17:29:12.0,660.0,Reasons besides prediction to build models?,,
2473,3,1126,d44e6207-9901-4dc6-a52b-96a84ab63d00,2010-08-02 17:29:12.0,660.0,<modeling>,,
2474,5,1121,ad870f77-f159-477b-92f7-f47774616166,2010-08-02 17:33:15.0,162.0,"There are two ways to implement this that I use commonly. I am always working with realtime data, so this assumes continuous input. Here's some pseudo-code:\\n\\n**Using a trainable minmax:**\\n\\n<code>define function peak:<br />\\n// keeps the highest value it has received<br /></code>\\n\\n<code>define function trough:<br />\\n// keeps the lowest value it has received<br /></code>\\n\\n<code>define function calibrate:<br />\\n// toggles whether peak() and trough() are receiving values or not</code>\\n\\n<code>define function scale:<br />\\n// maps input range [trough.value() to peak.value()] to [0.0 to 1.0]</code>\\n\\nThis function requires that you either perform an initial training phase (by using calibrate()) or that you re-train either at certain intervals or according to certain conditions. For instance, imagine a function like this:\\n\\n<code>define function outBounds (val, thresh):<br />\\nif val > (thresh*peak.value()) || val < (trough.value() / thresh):<br />\\ncalibrate()<br />\\n// peak and trough are normally not receiving values, but if outBounds() receives a value that is more than 1.5 times the current peak or less than the current trough divided by 1.5, then calibrate() is called which allows the function to recalibrate automatically.</code>\\n\\n**Using an historical minmax:**\\n\\n<code>var arrayLength = 1000</code><br />\\n<code>var histArray[arrayLength]</code>\\n\\n<code>define historyArray(f):<br />\\nhistArray.pushFront(f) //adds f to the beginning of the array</code><br/>\\n\\n<code>define max(array):<br />\\n// finds maximum element in histArray[]<br />\\nreturn max</code><br />\\n\\n<code>define min(array):<br />\\n// finds minimum element in histArray[]<br />\\nreturn min</code><br />\\n\\n<code>define function scale:<br />\\n// maps input range [min(histArray) to max(histArray)] to [0.0 to 1.0]</code>\\n\\n<code>main()</code><br />\\n<code>historyArray(histArray)</code><br />\\n<code>scale(min(histArray), max(histArray), histArray[0])<br />\\n// histArray[0] is the current element</code>",deleted 8 characters in body,
2475,2,1127,8f2365df-32cf-4795-a660-40e627ae968b,2010-08-02 17:55:23.0,187.0,"You can do it, but I think that using percentages in a regression framework is likely to lead to a model that has little value.  I would try to generalize the model so that the percentage change is a special case, but that more complex behaviour is possible.",,
2476,2,1128,2e83c990-04fa-46e0-be86-5c4b45aed53a,2010-08-02 17:57:15.0,334.0,">  Reason 17.  Write a paper.\\n\\nSort-of just kidding but not really.  There seems to be a bit of overlap between some of his points (eg 1, 5, 6, 12, 14).",,
2477,2,1129,efaca9f1-b445-4ebc-927d-2fb72179cc52,2010-08-02 18:00:10.0,187.0,"Another vote for Rob's answer.\\n\\nThere are also some interesting ideas in the ""relative importance"" literature.  This work develops methods that seek to determine how much importance is associated with each of a number of candidate predictors.  There are Bayesian and Frequentist methods.  Check the ""relaimpo"" package in R for citations and code.",,
2478,6,1123,66a21097-f5bd-4b5f-9e84-f328717047ab,2010-08-02 18:04:58.0,,<distributions><normality>,edited tags,user28
2479,2,1130,fa1adf5c-cd72-47cc-9275-29859a7831fb,2010-08-02 18:44:15.0,474.0,"I have data for about 1 year, 100 obs, mult obs per subject, transactions occur on weekly basis but have 6-12 subjects per week, there is no order to this. There is a policy change in latter half of year, I want to model change in dep. var due to policy change as a dummy var: time1=0, time2=1. Is this a case for fxd effects estimation? The number of weeks per subject varies a lot and the number of weeks in time1 is greater than time2.  Computed means for time1, time2 and percent change=large change in dep. var, estimated linear model:\\npay=X1 X2 Time(dummy). Dummy var is not stat signif. Any suggestions as to how to model this? Can I treat it as panel data? Thanks",,
2480,1,1130,fa1adf5c-cd72-47cc-9275-29859a7831fb,2010-08-02 18:44:15.0,474.0,Regression-mult obs per subject,,
2481,3,1130,fa1adf5c-cd72-47cc-9275-29859a7831fb,2010-08-02 18:44:15.0,474.0,<regression>,,
2482,2,1131,dbb715c6-5b83-42f9-9cfb-427ed3324469,2010-08-02 18:52:56.0,8.0,"> Save money\\n\\nI build mathematical/statistical of cellular mechanisms. For example, how a particular protein affects cellular ageing. The role of the model is mainly prediction, but also to save money. It's far cheaper to employ a single modeller than (say) a few wet-lab biologists with the associated equipment costs. Of course modelling doesn't fully replace the experiment, it just aids the process.",,
2483,2,1132,5287a696-4f18-4894-8c3e-97a37c5dbeed,2010-08-02 19:16:14.0,8.0,> For fun!\\n\\nI'm sure most statisticians/modellers do their job because they enjoy it. Getting paid to do something you enjoy is quite nice!,,
2484,2,1133,1dea1389-eb8c-4b46-8cb2-2cc4cb8a6b13,2010-08-02 19:19:42.0,287.0,"I have cross classified data in a 2 x 2 x 6 table. Let's call the dimensions `response`, `A` and `B`. I fit a logistic regression to the data with the model `response ~ A * B`. An analysis of deviance of that model says that both terms and their interaction are significant.\\n\\nHowever, looking at the proportions of the data, it looks like only 2 or so levels of `B` are responsible for these significant effects. I would like to test to see which levels are the culprits. Right now, my approach is to perform 6 chi-squared tests on 2 x 2 tables of `response ~ A`, and then to adjust the p-values from those tests for multiple comparisons (using the Holm adjustment).\\n\\nMy question is whether there is a better approach to this problem. Is there a more principled modeling approach, or multiple chi-squared test comparison approach?",,
2485,1,1133,1dea1389-eb8c-4b46-8cb2-2cc4cb8a6b13,2010-08-02 19:19:42.0,287.0,Multiple Chi-Squared Tests,,
2486,3,1133,1dea1389-eb8c-4b46-8cb2-2cc4cb8a6b13,2010-08-02 19:19:42.0,287.0,<categorical-data><logistic><multiple-comparisons><chi-squared>,,
2487,2,1134,5c1fecc9-b2d1-4743-8085-4e6333151d1f,2010-08-02 19:28:31.0,5.0,"> dimension reduction\\n\\nSometimes there can be too much data, so forming an initial model allows for further analysis. ",,
2488,2,1135,7242c3df-6e9e-4007-af60-71e3adb71ea4,2010-08-02 19:38:11.0,5.0,> regulation\\n\\nGovernment agencies require firms to provide reports using certain models. This provides for a degree of standardization in oversight. An example is the use of Value-at-Risk in the financial sector. ,,
2489,2,1136,1784541b-90ed-475f-8fa2-0eddd2a5deaa,2010-08-02 19:44:20.0,5.0,"This is closely related to some of the others, but:\\n\\n> Eliminate human judgement\\n\\nHuman decision making is subject to many different forces and biases. That means that you not only get different answers to the same question, but you can also end up with really suboptimal outcomes. Examples would be the over-confidence bias or anchoring.  ",,
2490,2,1137,120cf459-593b-4c9f-8d0e-5849645be1c6,2010-08-02 20:17:03.0,,"Regress the X1 value for time1 (and any other covariates you want) on the X1 variable for time 2 (your dependent variable).  Your regression model will look something like this:\\n\\n""x1 time 2"" = ""x1 time1"" + x2 + x3 + x4 etc.\\n\\nYour regression coefficients for x2....xn will be the effect of changes of that variable on ""x1 time2"" controlling for ""x1 time 1"" (and everything else in the model).  Therefore you'll be able to see what is going on at time2 controlling for where things were at time 1.",,
2491,2,1138,7e487c84-17dc-4ca4-8bbc-80f59de7094d,2010-08-02 20:20:52.0,59.0,"> Repetitive problems that involve some form of benefit / cost\\n\\nIn my field, we model the same set of variables in different locations, time frame, and magnitudes\\n\\n",,
2493,2,1140,49086139-a874-4d3e-9d67-e34dd8730ee0,2010-08-02 20:31:22.0,665.0,http://eagereyes.org\\nby Robert Kosara (~5 posts a month). This blog includes tutorials and discussion articles plus it has a great home page with lots of links to useful information.\\n,,
2494,16,1140,49086139-a874-4d3e-9d67-e34dd8730ee0,2010-08-02 20:31:22.0,-1.0,,,
2495,2,1141,1ffa7509-b4d3-49c7-8dfe-b80f6bac889b,2010-08-02 20:35:41.0,665.0,http://datavisualization.ch<br> \\nby Benjamin Wiederkehr and others (~15 links a month). If you want heaps of links you can subscribe to their twitter feed twitter slash datavis (~5 links a day) \\n\\nahhh... i'm a new member and so i can only post one link per post.,,
2496,16,1141,1ffa7509-b4d3-49c7-8dfe-b80f6bac889b,2010-08-02 20:35:41.0,-1.0,,,
2497,2,1142,2c702617-5fc1-4d63-831e-f90f7ff45b12,2010-08-02 20:37:27.0,667.0,"I am working with a large amount of time series. These time series are basically network measurements coming every 10 minutes, and some of them are periodic (i.e. the bandwidth), while some other aren't (i.e. the amount of routing traffic).\\n\\nI would like a simple algorithm for doing an online ""outlier detection"". Basically, I want to keep in memory (or on disk) the whole historical data for each time series, and I want to detect any outlier in a live scenario (each time a new sample is captured). What is the best way to achieve these results?\\n\\nI'm currently using a moving average in order to remove some noise, but then what next? Simple things like standard deviation, mad, ... against the whole data set doesn't work well (I can't assume the time series are stationary), and I would like something more ""accurate"", ideally a black box like:\\n\\ndouble outlier_detection(double* vector, double value);\\n\\nwhere vector is the array of double containing the historical data, and the return value is the anomaly score for the new sample ""value"" .",,
2498,1,1142,2c702617-5fc1-4d63-831e-f90f7ff45b12,2010-08-02 20:37:27.0,667.0,Simple algorithm for online outlier detection of a generic time series,,
2499,3,1142,2c702617-5fc1-4d63-831e-f90f7ff45b12,2010-08-02 20:37:27.0,667.0,<time-series><outliers><mathematical-statistics><real-time>,,
2503,2,1144,e405ad91-c2d5-4e31-baf8-f25e21653e1c,2010-08-02 20:48:01.0,666.0,You could use the standard deviation of the last N measurements (you have to pick a suitable N). A good anomaly score would be how many standard deviations a measurement is from the moving average.,,
2504,2,1145,84b79249-ac30-46dd-9f01-4cef811a24f0,2010-08-02 20:53:20.0,666.0,"It's usually a small sin. In nature, most phenomena can't realistically receive any value in R, but we model them as if they could.\\n\\nThe greater sin is to assume that the rate of success is shaped like a normal distribution if it isn't.",,
2505,2,1146,580cc633-d2be-4158-a2c6-c183446caa06,2010-08-02 20:54:01.0,668.0,"> Control\\n\\nA major aspect of the dynamic modelling literature is associated with control. This kind of work spans a lot of disciplines from politics/economics (see, e.g. Stafford Beer), biology (see e.g. N Weiner's 1948 work on Cybernetics) through to contemporary state space control theory (see for an intro Ljung 1999). \\n\\nControl is kind of related to Epstein's 9 and 10, and Shane's answers about human judgement / regulation, but I thought it made sense to be explicit. Indeed, at the end of my engineering undergraduate career I would have given you a very concise response to the uses of modelling: control, inference and prediction. I guess inference, by which I mean filtering/smoothing/dimension-reduction etc, is maybe similar to Epstein's points 3 and 8. \\n\\nOf course in my later years I wouldn't be so bold as to limit the purposes of modelling to control, inference and prediction. Maybe a fourth, covering many of Epsteins's points, should be ""coercion"" - the only way you should ""educate the public"" is to encourage us to make our own models... ",,
2506,2,1147,c67168e8-25f6-4e5e-aa5a-933850f17bd0,2010-08-02 21:23:37.0,,"I am guessing sophisticated time series model will not work for you because of the time it takes to detect outliers using this methodology. Therefore, here is a workaround: \\n\\n1. First establish a baseline 'normal' traffic patterns for a year based on manual analysis of historical data which accounts for time of the day, weekday vs weekend, month of the year etc. \\n\\n2. Use this baseline along with some simple mechanism (e.g., moving average suggested by Carlos) to detect outliers.\\n\\nYou may also want to review the statistical [process control literature][1] for some ideas.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Statistical_process_control",,user28
2507,2,1148,68802c7b-b7b5-40fc-8b78-8994d785abbe,2010-08-02 21:50:27.0,33.0,"Seasonally adjust the data such that a normal day looks closer to flat. You could take today's 5:00pm sample and subtract or divide out the average of the previous 30 days at 5:00pm. Then look past N standard deviations (measured using pre-adjusted data) for outliers. \\nThis could be done separately for weekly and daily ""seasons.""",,
2508,2,1149,87089aef-798e-4ec6-b5f8-1e0860de785b,2010-08-02 22:42:32.0,,"The wiki discusses the problems that arise when [multicollinearity][1] is an issue in linear regression. The basic problem is multicollinearity results in unstable parameter estimates which makes it very difficult to assess the effect of independent variables on dependent variables.\\n\\nI understand the technical reasons behind the problems (may not be able to invert $X' X$, ill-conditioned $X' X$ etc) but I am searching for a more intuitive (perhaps geometric?) explanation for this issue.\\n\\nIs there a geometric or perhaps some other form of easily understandable explanation as to why multicollinearity is problematic in the context of linear regression?   \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Multicollinearity",,user28
2509,1,1149,87089aef-798e-4ec6-b5f8-1e0860de785b,2010-08-02 22:42:32.0,,Is there an intuitive explanation why multicollinearity is a problem in linear regression?,,user28
2510,3,1149,87089aef-798e-4ec6-b5f8-1e0860de785b,2010-08-02 22:42:32.0,,<regression><intuition><multicollinearity>,,user28
2511,2,1150,e1e355fd-8beb-4c49-b2de-f8de7ac8ec9c,2010-08-02 22:59:09.0,159.0,"Consider the simplest case where $Y$ is regressed against $X$ and $Z$ and where $X$ and $Z$ are highly positively correlated. Then the effect of $X$ on $Y$ is hard to distinguish from the effect of $Z$ on $Y$ because any increase in $X$ tends to be associated with an increase in $Y$. \\n\\nAnother way to look at this is to consider the equation. If we write $Y = b_0 + b_1X + b_2Z + e$, then the coefficient $b_1$ is the increase in $Y$ for every unit increase in $X$ while holding $Z$ constant. But in practice, it is often impossible to hold $Z$ constant and the positive correlation between $X$ and $Z$ mean that a unit increase in $X$ is usually accompanied by some increase in $Z$ at the same time.\\n\\nA similar but more complicated explanation holds for other forms of multicollinearity.",,
2512,2,1151,d982945e-9574-4fcb-be44-6582cb3afc81,2010-08-02 23:26:02.0,251.0,"The geometric approach is to consider the least squares projection of $Y$ onto the subspace spanned by $X$.\\n\\nSay you have a model:\\n\\n$E[Y | X] = \\beta_{1} X_{1} + \\beta_{2} X_{2} + e$\\n\\nOur estimation space is the plane determined by the vectors $X_{1}$ and $X_{2}$ and the problem is to find coordinates corresponding to $(\\beta_{1}, \\beta_{2})$ which will describe the vector $\\hat{Y}$, a least squares projection of $Y$ on to that plane.\\n\\nNow suppose $X_{1} = 2 X_{2}$, i.e. they're colinear.  Then, the subspace determined by $X_{1}$ and $X_{2}$ is just a line and we have only one degree of freedom.  So we can't determine two values $\\beta_{1}$ and $\\beta_{2}$ as we were asked.\\n",,
2513,2,1152,6570f922-1035-4ea7-9a15-9a25767ad9b1,2010-08-02 23:31:46.0,196.0,"If you estimate the policy change as a fixed effects estimation in the context of an OLS regression you'll over-estimate your degrees of freedom because of the repeated measures by subject.  If you do not think there is an overall trend of time (beyond the policy shift) then there is no reason to keep all of the observations, you could simply aggregate by subject for ""before policy change"" and ""after policy change"" and do a paired samples t-test.  Failing that - if you intend to add more predictors, like perhaps a linear effect of time, you might think about liner mixed effects regression, e.g. \\n\\n    lmer(pay ~ time * policy + (1+time|SubjID))\\n\\n... might be a good place to start.",,
2514,2,1153,f7204f7c-38e1-41ce-8ea2-f00a75b8f43e,2010-08-03 00:54:56.0,159.0,"Here is a simple R function that will find time series outliers (and optionally show them in a plot). It will handle seasonal and non-seasonal time series. The basic idea is to find robust estimates of the trend and seasonal components and subtract them. Then find outliers in the residuals. The test for residual outliers is the same as for the standard boxplot -- points greater than 1.5IQR above or below the upper and lower quartiles are assumed outliers. The number of IQRs above/below these thresholds is returned as an outlier ""score"". So the score can be any positive number, and will be zero for non-outliers.\\n\\nI realise you are not implementing this in R, but I often find an R function a good place to start. Then the task is to translate this into whatever language is required.\\n\\n    tsoutliers <- function(x,plot=FALSE)\\n    {\\n    	x <- as.ts(x)\\n    	if(frequency(x)>1)\\n    		resid <- stl(x,s.window=""periodic"")$time.series[,3]\\n    	else\\n    	{\\n    		tt <- 1:length(x)\\n    		resid <- residuals(loess(x ~ tt))\\n    	}\\n    	resid.q <- quantile(resid,prob=c(0.25,0.75))\\n    	iqr <- diff(resid.q)\\n    	limits <- resid.q + 1.5*iqr*c(-1,1)\\n    	score <- abs(pmin((resid-limits[1])/iqr,0) + pmax((resid - limits[2])/iqr,0))\\n    	if(plot)\\n    	{\\n    		plot(x)\\n    		x2 <- ts(rep(NA,length(x)))\\n    		x2[score>0] <- x[score>0]\\n    		tsp(x2) <- tsp(x)\\n    		points(x2,pch=19,col=""red"")\\n    		return(invisible(score))\\n    	}\\n    	else\\n    		return(score)\\n    }",,
2515,2,1154,05118063-f385-403d-9bb3-71fb24fb91d0,2010-08-03 00:56:33.0,251.0,"If you're worried about assumptions with any particular approach, one approach is to train a number of learners on different signals, then use [ensemble methods][1] and aggregate over the ""votes"" from your learners to make the outlier classification.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Ensembles_of_classifiers",,
2516,5,1113,10ae4ae4-fdd9-4bf4-9052-64b6f932a07f,2010-08-03 00:57:51.0,159.0,"A very common trick to do so (e.g., in connectionist modeling) is to use the [hyperbolic tangent tanh][1] as the 'squashing function"".\\nIt automatically fits all numbers into the interval between -1 and 1. Which in your case restricts the range from 0 to 1.\\nIn `r` and `matlab` you get it via `tanh()`. \\n\\nAnother squashing function is the logistic function (thanks to Simon for the name), provided by $ f(x) = 1 / (1 + e ^{-x} ) $, which restricts the range from 0 to 1 (with 0 mapped to .5). So you would have to multiply the result by 2 and subtract 1 to fit your data into the interval between 0 and 1.\\n\\nHere is some simple R code which plots both functions (tanh in red, logistic in blue) so you can see how both squash:\\n\\n    x <- seq(0,20,0.001)\\n    plot(x,tanh(x),pch=""."", col=""red"", ylab=""y"")\\n    points(x,(1 / (1 + exp(-x)))*2-1, pch=""."",col=""blue"")\\n\\n  [1]: http://en.wikipedia.org/wiki/Tanh",deleted 1 characters in body; edited body,
2517,5,1154,28f97e14-dcfd-4b3c-9ba3-a91fccda012d,2010-08-03 01:49:22.0,251.0,"If you're worried about assumptions with any particular approach, one approach is to train a number of learners on different signals, then use [ensemble methods][1] and aggregate over the ""votes"" from your learners to make the outlier classification.\\n\\nBTW, this might be worth reading or skimming since it references a few approaches to the problem.\\n\\n- [Online outlier detection over data streams][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Ensembles_of_classifiers\\n  [2]: http://ir.lib.sfu.ca/retrieve/2372/etd1964.pdf\\n\\n",added 213 characters in body,
2518,2,1155,439b6d33-b4e4-47f7-877c-4db5e0119595,2010-08-03 02:20:32.0,672.0,"If two regressors are perfectly correlated, their coefficients will be impossible to calculate; it's helpful to consider why they would be difficult to interpret *if we could calculate them*.  In fact, this explains why it's difficult to interpret variables that are not perfectly correlated but that are also not truly independent.\\n\\nSuppose that our dependent variable is the daily supply of fish in New York, and our independent variables include one for whether it rains on that day and one for the amount of bait purchased on that day.  What we don't realize when we collect our data is that every time it rains, fishermen purchase no bait, and every time it doesn't, they purchase a constant amount of bait.  So Bait and Rain are perfectly correlated, and when we run our regression, we can't calculate their coefficients.  In reality, Bait and Rain are probably not perfectly correlated, but we wouldn't want to include them both as regressors without somehow cleaning them of their endogeneity.",,
2519,2,1156,fe055cd0-9965-4d23-8aa4-69c13ebf4d71,2010-08-03 02:28:37.0,253.0,"My (very) layman intuition for this is that the OLS model needs a certain level of ""signal"" in the X variable in order to detect it gives a ""good"" predicting for Y.  If the same ""signal"" is spread over many X's (because they are correlated), then none of the correlated X's can give enough of a ""proof"" (statistical significance) that it is a real predictor.\\n\\n\\nThe previous (wonderful) answers do a great work in explaining why that is the case.",,
2520,5,1151,5d422017-da96-46fc-824a-8da075649ce0,2010-08-03 02:34:37.0,251.0,"The geometric approach is to consider the least squares projection of $Y$ onto the subspace spanned by $X$.\\n\\nSay you have a model:\\n\\n$E[Y | X] = \\beta_{1} X_{1} + \\beta_{2} X_{2}$\\n\\nOur estimation space is the plane determined by the vectors $X_{1}$ and $X_{2}$ and the problem is to find coordinates corresponding to $(\\beta_{1}, \\beta_{2})$ which will describe the vector $\\hat{Y}$, a least squares projection of $Y$ on to that plane.\\n\\nNow suppose $X_{1} = 2 X_{2}$, i.e. they're colinear.  Then, the subspace determined by $X_{1}$ and $X_{2}$ is just a line and we have only one degree of freedom.  So we can't determine two values $\\beta_{1}$ and $\\beta_{2}$ as we were asked.\\n",deleted 4 characters in body,
2521,2,1157,0cafddd3-f08f-4480-9d71-2ba915422be4,2010-08-03 02:45:53.0,11.0,http://chartporn.org/\\n\\nI find the blog name pretty humorous. Great dataviz.,,
2522,16,1157,0cafddd3-f08f-4480-9d71-2ba915422be4,2010-08-03 02:45:53.0,-1.0,,,
2523,2,1158,b291ace5-46ad-4b4f-b9b3-3d5e6ae219b2,2010-08-03 02:49:32.0,159.0,"It's not a blog, but Edward Tufte has an [interesting forum][1] on information design including data visualization.\\n\\n\\n  [1]: http://www.edwardtufte.com/bboard/q-and-a?topic_id=1",,
2524,16,1158,b291ace5-46ad-4b4f-b9b3-3d5e6ae219b2,2010-08-03 02:49:32.0,-1.0,,,
2525,2,1159,b573b004-da96-445d-936b-4ff7d4c4d65b,2010-08-03 02:54:30.0,251.0,"The unprincipled approach is to discard the disproportionate data, refit the model and see if logit/conditional odds ratios for response and A are very different (controlling for B).  This might tell you if there's cause for concern.  Pooling the levels of B is another approach.  On more principled lines, If you're worried about relative proportions inducing Simpson's paradox, then you can look into the conditional and marginal odds ratios for response/A and see if they reverse.\\n\\nFor avoiding multiple comparisons in particular, the only thing that occurs to me is to use a hierarchical model which accounts for random effects across levels.\\n\\n",,
2526,2,1160,0e65e727-8d9f-4668-81e1-cf3331870365,2010-08-03 03:02:57.0,253.0,"R allows us to put code to run in the beginning/end of a session.\\n\\nWhat codes would you suggest putting there?\\n\\nI know of three interesting examples (although I don't have ""how to do them"" under my fingers here):\\n\\n 1. Saving the [session history][1] when closing R.\\n 2. Running a fortune() at the beginning of an R session.\\n 3. I was thinking of having an automated saving of the workspace.  But I didn't set on solving the issue of managing space (so there would always be X amount of space used for that backup)\\n\\n\\nAny more ideas? (or how you implement the above ideas)\\n\\n\\np.s: I am not sure if to put this here or on stackoverflow.  But I feel the people here are the right ones to ask.\\n\\n\\n  [1]: http://stat.ethz.ch/R-manual/R-devel/library/utils/html/savehistory.html",,
2527,1,1160,0e65e727-8d9f-4668-81e1-cf3331870365,2010-08-03 03:02:57.0,253.0,What code would you put before/after your R session ?,,
2528,3,1160,0e65e727-8d9f-4668-81e1-cf3331870365,2010-08-03 03:02:57.0,253.0,<r>,,
2529,16,1160,0e65e727-8d9f-4668-81e1-cf3331870365,2010-08-03 03:02:57.0,253.0,,,
2530,2,1161,1d30b582-5472-476a-933d-f16767b0a2f6,2010-08-03 03:49:10.0,159.0,Some information about how to implement this is provided at `help(.First)` and `help(.Last)`.,,
2531,16,1161,1d30b582-5472-476a-933d-f16767b0a2f6,2010-08-03 03:49:10.0,-1.0,,,
2534,2,1162,8862d5de-bb81-48a2-86ae-ef57fca20c80,2010-08-03 06:26:47.0,251.0,"> To take (useful) action.\\n\\nI'm paraphrasing someone else here, but suppose we built a system of public health around the model that infectious diseases are due to malevolent spirits that spread through contact.  The science of microbes may be an infinitely better model, but you could prevent a good number of contagions nonetheless.  (I think this was on reading a history of cybernetics, but I can't remember who made the point.)\\n\\nThe point is that, along the lines of ""all models bad, some useful"", we need to formulate models and refine them in order to undertake any useful actions with lasting consequences.  Otherwise, we might as well flip coins.\\n",,
2537,4,1160,db88a095-7982-4d07-816e-7574bd7f5346,2010-08-03 07:28:24.0,88.0,What code would you put before/after your R session?,Spelling of the title fixed.,
2538,2,1164,b1a4e749-02d8-4d99-a81c-9ca27693ae3d,2010-08-03 07:49:34.0,438.0,"In solving business problems using data, at least one key model assumptions is invalid a large fraction of the time. The rest of the time, no one bothers to check those assumptions so you never actually know. For instance, that so many of the common web metrics data are ""long-tailed"" (relative to the normal distribution) is, by now, so well documented that we take it for granted. Another example, online communities--even in communities with thousands of members, by far the largest share of contribution to/participation in the community is attributable to a minuscule group of 'super-contributors.' (A few months ago, just after the SO API was made available in beta, a StackOverflow member published a brief analysis from data he collected through the API; his conclusion--less than one percent of the SO members account for most of the activity on SO (presumably asking questions, and answering them), another 1-2% account for the rest, and the overwhelming majority of the members do nothing).\\n\\nSo given the abundance of populations like this of interest to analysts, given that classical models perform demonstrably poorly on these data, and given that robust and resistant methods have been around for a while (at least 20 years, i believe)--why are they not used more often? (I am also wondering why I don't use them more often, but that's not really a question for Stats Exhange.)\\n\\nYes i know that there are textbook chapters devoted entirely to robust statistics and i know there are (a few) R Packages ('robustbase' is the one i am familiar with and use), etc.--but given the obvious advantages of these techniques, they are often clearly the better tools for the job--why are they not used much more often? Shouldn't we expect to see robust (and resistant) statistics used far more often (perhaps even presumptively)?\\n\\nThe only substantive (i.e., technical) reason i have heard is that robust techniques (likewise for resistant methods) lack the power/sensitivity of classical techniques. I don't know if this is indeed true in some cases, but i do know it is not true in many cases.\\n\\nA final word of preemption: yes I know this question does not have a single demonstrably correct answer; very few questions on this Site do. Moreover, this question is a genuine inquiry; it's not a pretext to advance a point of view--i don't have a point of view here, just a question, to which i don't even have a hint of an answer at the moment.",,
2539,1,1164,b1a4e749-02d8-4d99-a81c-9ca27693ae3d,2010-08-03 07:49:34.0,438.0,Why Haven't Robust (and Resistant) Statistics Replaced Classical Techniques?,,
2540,3,1164,b1a4e749-02d8-4d99-a81c-9ca27693ae3d,2010-08-03 07:49:34.0,438.0,<outliers><model-selection><nonparametric><obust>,,
2541,10,1115,7a3e6d85-4144-4f64-ae2e-b33c5e2014af,2010-08-03 07:58:26.0,-1.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":159,""DisplayName"":""Rob Hyndman""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":5,""DisplayName"":""Shane""}]}",4,
2542,2,1165,e7b7f99a-ce49-4f36-b3a6-daf9c70705bd,2010-08-03 08:27:13.0,5.0,"On open, I set R options, load environment variables (eg. global variables, API keys) and open database connections, and then close those connections when exiting. With some of these things, I prefer to do them onLoad of my packages rather than per session. \\n\\nRegarding how to save your session, use the save command. ",,
2543,16,1165,e7b7f99a-ce49-4f36-b3a6-daf9c70705bd,2010-08-03 08:27:13.0,-1.0,,,
2544,6,1164,abb40483-9e3e-422a-9074-d061c15587d1,2010-08-03 09:04:50.0,,<outliers><model-selection><nonparametric><robust>,edited tags,user28
2545,2,1166,577428d9-95d1-46c8-8e69-655b909e50a6,2010-08-03 09:05:56.0,223.0,"I Give an answer in two directions: \\n\\nFirst, I think there are a lot of good approaches in statistic (you will find them in R packages not necessarily with robust mentionned somewhere) which are naturally robust and tested on real data and the fact that you don't find algorithm with ""robust"" mentionned somewhere does not mean it is not robust. Anyway if you think being robust means being universal then you'll never find any robust procedure (no free lunch) you need to have some knowledge/expertise on the data you analyse in order to use adapted tool or to create an adapted model.\\n\\nOn the other hand, some approaches in statistic are not robust because they are dedicated to one single type of model. I think it is good sometime to work in a laboratory to try to understand things. It is also good to treat problem separatly to understand to what problem our solution is... this is how mathematician work. The example of Gaussian model elocant: is so much criticised because the gaussian assumption is never fulfilled but has bring 75% of the ideas used practically in statistic today. ",,
2546,5,655,6cca08fa-4e86-44db-980e-705eaf0bf757,2010-08-03 09:09:40.0,315.0,"You might find useful this one: [The Elements of Statistical Learning: Data Mining, Inference, and Prediction][1]\\n\\nUPDATE #1:\\n\\nThis book might be useful as well: [O'Reilly: Statistics in a Nutshel][2]l\\n\\n\\n  [1]: http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=sr_1_1?ie=UTF8&s=books&qid=1280173685&sr=1-1\\n  [2]: http://www.amazon.com/Statistics-Nutshell-Desktop-Reference-OReilly/dp/0596510497/ref=sr_1_1?s=books&ie=UTF8&qid=1280826474&sr=1-1",added new book,
2550,10,795,9ca49efd-57a3-466f-94c7-a9e5e309f958,2010-08-03 09:14:27.0,-1.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":159,""DisplayName"":""Rob Hyndman""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":5,""DisplayName"":""Shane""},{""Id"":8,""DisplayName"":""csgillespie""}]}",4,
2553,5,1164,f6b37fc3-10f2-4845-a5e4-5d62381e4263,2010-08-03 09:36:03.0,438.0,"When solving business problems using data, it's common that at least one key assumption that under-pins classical statistics is invalid. The rest of the time, no one bothers to check those assumptions so you never actually know. For instance, that so many of the common web metrics are ""long-tailed"" (relative to the normal distribution) is, by now, so well documented that we take it for granted. Another example, online communities--even in communities with thousands of members, it's well-documented that by far the largest share of contribution to/participation in many of these community is attributable to a minuscule group of 'super-contributors.' (E.g., a few months ago, just after the SO API was made available in beta, a StackOverflow member published a brief analysis from data he collected through the API; his conclusion--less than one percent of the SO members account for most of the activity on SO (presumably asking questions, and answering them), another 1-2% accounted for the rest, and the overwhelming majority of the members do nothing).\\n\\nSo given the abundance of populations like this of interest to analysts, and given that classical models perform demonstrably poorly on these data, and given that robust and resistant methods have been around for a while (at least 20 years, i believe)--why are they not used more often? (I am also wondering why *I* don't use them more often, but that's not really a question for Stats Exhange.)\\n\\nYes i know that there are textbook chapters devoted entirely to robust statistics and i know there are (a few) R Packages ('robustbase' is the one i am familiar with and use), etc.--but given the obvious advantages of these techniques, they are often clearly the better tools for the job--why are they not used much more often? Shouldn't we expect to see robust (and resistant) statistics used far more often (perhaps even presumptively) compared with the classical analogs?\\n\\nThe only substantive (i.e., technical) explanation i have heard is that robust techniques (likewise for resistant methods) lack the power/sensitivity of classical techniques. I don't know if this is indeed true in some cases, but i do know it is not true in many cases.\\n\\nA final word of preemption: yes I know this question does not have a single demonstrably correct answer; very few questions on this Site do. Moreover, this question is a genuine inquiry; it's not a pretext to advance a point of view--i don't have a point of view here, just a question, to which i don't even have a hint of an answer at the moment.\\n",correct grammar/usage,
2554,5,1031,a5121f80-ffe9-4fa4-88e6-b1d6b1106d85,2010-08-03 09:37:00.0,223.0,"KL has a deep meaning when you visualize a set of dentities as a manifold within the fisher metric tensor. \\n\\nConsider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_{ij})$ is \\n \\n$F_{ij}=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$\\n\\nWith this notation $D$ is a **riemannian manifold** and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao lower bound theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if $p=1$ you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and $F_{11}$ is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance $ds^2$ between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\n$ds^2= \\sum F_{ij} d \\theta^i d \\theta^j$\\n\\nand it is known to be twice the Kullback Leibler Divergence:\\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\n",added 42 characters in body,
2556,10,1160,224cafe5-952a-41c0-976a-f6b0067dbc58,2010-08-03 12:01:14.0,-1.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""},{""Id"":5,""DisplayName"":""Shane""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":8,""DisplayName"":""csgillespie""},{""Id"":190,""DisplayName"":""Peter Smit""}]}",2,
2557,6,134,1d9dd915-3289-489f-8816-9699a9ffd407,2010-08-03 12:14:50.0,8.0,<algorithms><median>,edited tags,
2558,6,31,411d42e2-84c3-4c68-9541-391c0fb8edc6,2010-08-03 12:16:01.0,8.0,<beginner><hypothesis-testing><t-test><p-value>,edited tags,
2559,2,1169,b805148b-792a-4b69-9940-9c64b5c0d97e,2010-08-03 12:20:09.0,364.0,"I'm looking to check my logic here. \\n\\nSay you measure a quantity in group A, and find the mean is 2 and your 95% confidence interval ranges from 1 to 3. Then you measure the same quantity in group B and find a mean of 4 with a 95% confidence interval that ranges from 3.5 to 4.5. Assuming that A & B are independent, what is the 95% confidence interval for the difference between the groups? Presumably you can compute this using standard t-statistics, but I'd like to know if it's also possible to compute an estimate based on the CI's alone.\\n\\nI reason that the lower bound of the CI of the difference should be the minimum credible difference between A & B; that is, the lower bound of the interval for B (3.5) minus upper bound of the interval for A (3), which yields a lower bound for the difference of 0.5. Similarly, the upper bound of the CI of the difference should be the maximum credible difference between A & B; that is, the upper bound of the interval for B (4.5) minus lower bound of the interval for A (1), which yields a lower bound for the difference of 3.5. This reasoning thus yields a confidence interval for the difference that ranges from 0.5 to 3.5.\\n\\nDoes that make sense, or is this a case where logic and statistics diverge?",,
2560,1,1169,b805148b-792a-4b69-9940-9c64b5c0d97e,2010-08-03 12:20:09.0,364.0,CI for a difference based on independent CIs,,
2561,3,1169,b805148b-792a-4b69-9940-9c64b5c0d97e,2010-08-03 12:20:09.0,364.0,<confidence-interval><credible-interval>,,
2562,2,1170,b3a2927f-ca96-4974-9b24-729d0e189bd4,2010-08-03 12:22:58.0,319.0,"Researchers want small p-values, and you can get smaller p-values if you use methods that make stronger distributional assumptions. In other words, non-robust methods let you publish more papers. Of course more of these papers may be false positives, but a publication is a publication.  That's a cynical explanation, but it's sometimes valid.",,
2563,2,1171,6432a1b6-4775-461b-b035-6b2db57cf3e5,2010-08-03 12:30:00.0,449.0,"No, you can't compute a CI for the difference that way I'm afraid, for the same reason you can't use whether the CIs overlap to judge the statistical significance of the difference. See, for example, \\n\\n""On Judging the Significance of Differences by Examining the Overlap Between Confidence Intervals""\\nNathaniel Schenker, Jane F Gentleman. The American Statistician. August 1, 2001, 55(3): 182-186. doi:10.1198/000313001317097960. \\n<http://pubs.amstat.org/doi/abs/10.1198/000313001317097960>\\n\\nor:\\n\\nOverlapping confidence intervals or standard error intervals: What do they mean in terms of statistical significance?\\nMark E. Payton, Matthew H. Greenstone, and Nathaniel Schenker. Journal of Insect Science 2003; 3: 34. <http://www.ncbi.nlm.nih.gov/pmc/articles/PMC524673/>\\n\\nThe correct procedure requires you also know the sample sizes of both groups. You can then back-compute the two standard deviations from the CIs and use those to conduct a standard two-sample t-test, or to calculate a standard error of the difference and hence a CI for the difference.\\n",,
2564,2,1172,b7bc876a-910b-46c2-8c22-b3b49eba180d,2010-08-03 13:12:04.0,449.0,"As a medical statistician with no previous knowledge of econom(etr)ics, I struggled to get to grips with instrumental variables as I often struggled to follow their examples and didn't understand their rather different terminology (e.g. 'endogeneity', 'reduced form', 'structural equation', 'omitted variables'). Here's a few references I found useful (the first should be freely available, but I'm afraid the others probably require a subscription):\\n\\n* Staiger D. Instrumental Variables. AcademyHealth Cyber Seminar in Health\\nServices Research Methods, March 2002.\\nhttp://www.dartmouth.edu/~dstaiger/wpapers-Econ.htm\\n\\n* Newhouse JP, McClellan M. Econometrics in Outcomes Research: The Use\\nof Instrumental Variables. Annual Review of Public Health 1998;19:17-34.\\nhttp://dx.doi.org/10.1146/annurev.publhealth.19.1.17\\n\\n* Greenland S. An introduction to instrumental variables for epidemiologists. International Journal of Epidemiology 2000;29:722-729. http://dx.doi.org/10.1093/ije/29.4.722\\n\\n* Zohoori N, Savitz DA.  Econometric approaches to epidemiologic data: Relating endogeneity and unobserved heterogeneity to confounding.  Annals of Epidemiology 1997;7:251-257. http://dx.doi.org/10.1016/S1047-2797(97)00023-9\\n\\nI'd also recommend chapter 4 of:\\n\\n* Angrist JD, Pischke JS. Mostly harmless econometrics: an empiricist's companion. Princeton, N.J: Princeton University Press, 2009. http://www.mostlyharmlesseconometrics.com/\\n",,
2565,5,1060,492b9a58-053b-4c86-b5c5-4c19de5bb149,2010-08-03 13:17:31.0,339.0,"I'll use an example so that you can reproduce the results \\n\\n    # mortality \\n    mort = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/cmort.dat""),start=1970, frequency=52)\\n\\n    # temperature\\n    temp = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/temp.dat""), start=1970, frequency=52)\\n\\n    #pollutant particulates\\n    part = ts(scan(""http://www.stat.pitt.edu/stoffer/tsa2/data/part.dat""), start=1970, frequency=52)\\n\\n    temp = temp-mean(temp)\\n    temp2 = temp^2\\n    trend = time(mort)\\n\\nNow, fit a model for mortality data\\n\\n    fit = lm(mort ~ trend + temp + temp2 + part, na.action=NULL)\\n\\nWhat I want now is to reproduce the result of the AIC command \\n\\n    AIC(fit)\\n    [1] 3332.282\\n\\nAccording to R's help file for AIC, AIC = -2 * log.likelihood + 2 * npar.\\nIf I'm correct I think that log.likelihood is given using the following formula:\\n\\n    n = length(mort)\\n    RSS = anova(fit)[length(anova(fit)[,2]),2] # there must be better ways to get this, anyway\\n    (log.likelihood <- -n/2*(log(2*pi)+log(RSS/n)+1))\\n\\n     [1] -1660.135\\nThis is approximately equal to\\n\\n    logLik(fit)\\n    'log Lik.' -1660.141 (df=6)\\n\\nAs far as I can tell, the number of parameters in the model are 5 (how can I get this number programmatically ??). So AIC should be given by:\\n\\n    -2 * log.likelihood + 2 * 5\\n    [1] 3330.271\\n\\nOoops, it seems like I should have used 6 instead of 5 as the number of parameters. What is wrong with those calculations? \\n\\n",added 1 characters in body,
2566,2,1173,176d97a5-0c7f-43d6-978d-0a00031d18a3,2010-08-03 13:36:38.0,8.0,"In my area of research, a popular way of displaying data is to use a combination of a bar chart with ""handle-bars"". For example, \\n\\n![Example plot][1]\\n\\nThe ""handle-bars"" alternate between standard errors and standard deviations depending on the author. Typically, the sample sizes for each ""bar"" are fairly small - around six.\\n\\nThese plots seem to be particularly popular in biological sciences - see the first few papers of [BMC Biology, vol 3][2] for examples.\\n\\nPersonally I don't like these plots. I can think of a few other ways of displaying the data, but I can never convince anyone that my way is better. \\n\\nSo how would you present this data?\\n\\n**R script**\\n\\nThis is the R code I used to generate the plot. That way you can (if you want) use the same data.\\n\\n                                            #Generate the data\\n    set.seed(1)\\n    names = c(""A1"", ""A2"", ""A3"", ""B1"", ""B2"", ""B3"", ""C1"", ""C2"", ""C3"")\\n    prevs = c(38, 37, 31, 31, 29, 26, 40, 32, 39)\\n    \\n    n=6; se = numeric(length(prevs))\\n    for(i in 1:length(prevs))\\n      se[i] = sd(rnorm(n, prevs, 15))/n\\n    \\n                                            #Basic plot\\n    par(fin=c(6,6), pin=c(6,6), mai=c(0.8,1.0,0.0,0.125), cex.axis=0.8)\\n    barplot(prevs,space=c(0,0,0,3,0,0, 3,0,0), names.arg=NULL, horiz=FALSE,\\n            axes=FALSE, ylab=""Percent"", col=c(2,3,4), width=5, ylim=range(0,50))\\n    \\n                                            #Add in the CIs\\n    xx = c(2.5, 7.5, 12.5, 32.5, 37.5, 42.5,  62.5, 67.5, 72.5)\\n    for (i in 1:length(prevs)) {\\n      lines(rep(xx[i], 2), c(prevs[i], prevs[i]+se[i]))\\n      lines(c(xx[i]+1/2, xx[i]-1/2), rep(prevs[i]+se[i], 2))\\n    }\\n    \\n                                            #Add the axis\\n    axis(2, tick=TRUE, xaxp=c(0, 50, 5))\\n    axis(1, at=xx+0.1, labels=names, font=1,\\n         tck=0, tcl=0, las=1, padj=0, col=0, cex=0.1)\\n\\n\\n\\n\\n  [1]: http://img291.imageshack.us/img291/3387/tmpc.jpg\\n  [2]: http://www.biomedcentral.com/bmcbiol/3",,
2567,1,1173,176d97a5-0c7f-43d6-978d-0a00031d18a3,2010-08-03 13:36:38.0,8.0,Displaying data,,
2568,3,1173,176d97a5-0c7f-43d6-978d-0a00031d18a3,2010-08-03 13:36:38.0,8.0,<data-visualization>,,
2569,2,1174,b484782d-80cc-4f59-8cf9-0354cfc33e59,2010-08-03 13:54:19.0,634.0,"I now of normality tests, but how do I test for ""Poissoness""?\\n\\nI have ~1000 sample of non-negative integers, which I suspect are taken from a Poisson distribution, and I would like to test that.\\n\\nThanks,\\n\\nDave",,
2570,1,1174,b484782d-80cc-4f59-8cf9-0354cfc33e59,2010-08-03 13:54:19.0,634.0,how do I test if given samples are taken from a Poisson distribution?,,
2571,3,1174,b484782d-80cc-4f59-8cf9-0354cfc33e59,2010-08-03 13:54:19.0,634.0,<distributions><hypothesis-testing><poisson>,,
2572,2,1175,5605a53f-532d-484f-8bb3-16ab778bc519,2010-08-03 13:55:22.0,495.0,"If the data are *rates*: that is number of successes divided by number of trials, then a very elegant method is a funnel plot. For example, see http://qshc.bmj.com/content/11/4/390.2.full (apologies if the link requires a subscription--let me know and I'll find another).\\n\\nIt may be possible to adapt it to other types of data, but I haven't seen any examples.",,
2573,2,1176,b3977fd2-86d5-4886-b232-70971d83d11b,2010-08-03 13:59:03.0,334.0,"Frank Harrell's (most excellent) keynote entitled ""Information Allergy"" at useR! last month showed alternatives to these: rather than hiding the raw data via the aggregation the bars provide, the raw data is also shown as dots (or points).  ""Why hide the data?"" was Frank's comment. \\n\\nGiven alpa blending, that strikes as a most sensible suggestion (and the whole talk most full of good, and important, nuggets).",,
2574,2,1177,be1ceb31-f051-4f10-9f3a-7d6f720c2844,2010-08-03 14:14:54.0,8.0,I suppose the easiest way is just to do a chi-squared [Goodness of fit][1] test.\\n\\nIn fact here's nice [java applet][2] that will do just that!\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Pearson%27s_chi-square_test\\n  [2]: http://home.ubalt.edu/ntsbarsh/Business-stat/otherapplets/PoissonTest.htm,,
2575,2,1178,2b7aad72-1a7d-482e-bc3b-e71174844d2b,2010-08-03 14:21:43.0,378.0,"You can use the dispersion (ratio of variance to the mean) as a test statistic, since the Poisson should give a dispersion of 1. [Here is a link][1] to how to use it as a model test.\\n\\n\\n  [1]: http://www.stats.uwo.ca/faculty/aim/2004/04-259/notes/DispersionTests.pdf",,
2576,2,1179,2380a170-9944-4c1b-bd38-150bc250fb29,2010-08-03 14:26:33.0,199.0,"I'm curious at to why you don't like these plots. I use them all the time. Without wanting to state the blooming obvious, they allow you to compare the means of different groups and see if their 95% CIs overlap (i.e., true mean likely to be different).\\n\\nIt's important to get a balance of simplicity and information for different purposes, I guess. But when I use these plots I am saying- ""these two groups are different from each other in some important way"" [or not].\\n\\nSeems pretty great to me, but I'd be interested to hear counter-examples. I suppose implicit in the use of the plot is that the data do not have a bizzare distribution which renders the mean invalid or misleading.",,
2577,2,1180,18daa98a-c75b-4c22-96c0-2bba2e1965da,2010-08-03 14:39:16.0,319.0,"For a Poisson distribution, the mean equals the variance.  If your sample mean is very different from your sample variance, you probably don't have Poisson data.  The dispersion test also mentioned here is a formalization of that notion.\\n\\nIf your variance is much larger than your mean, as is commonly the case, you might want to try a negative binomial distribution next.  ",,
2578,2,1181,3c92e73c-ebb3-4a87-b742-751bb680a00a,2010-08-03 14:52:44.0,339.0,"Here is a sequence of R commands that may be helpful. Feel free to comment or edit if you spot any mistakes.\\n\\n    set.seed(1)\\n    x.poi<-rpois(n=200,lambda=2.5) # a vector of random variables from the Poisson distr.\\n    \\n    hist(x.poi,main=""Poisson distribution"")\\n\\n    lambda.est <- mean(x.poi) ## estimate of parameter lambda\\n    (tab.os<-table(x.poi)) ## table with empirical frequencies\\n\\n\\n    freq.os<-vector()\\n    for(i in 1: length(tab.os)) freq.os[i]<-tab.os[[i]]  ## vector of emprical frequencies\\n\\n    freq.ex<-(dpois(0:max(x.poi),lambda=lambda.est)*200) ## vector of fitted (expected) frequencies\\n\\n    acc <- mean(abs(freq.os-trunc(freq.ex))) ## absolute goodness of fit index acc\\n    acc/mean(freq.os)*100 ## relative (percent) goodness of fit index\\n\\n    h <- hist(x.poi ,breaks=length(tab.os))\\n    xhist <- c(min(h$breaks),h$breaks)\\n    yhist <- c(0,h$density,0)\\n    xfit <- min(x.poi):max(x.poi)\\n    yfit <- dpois(xfit,lambda=lambda.est)\\n    plot(xhist,yhist,type=""s"",ylim=c(0,max(yhist,yfit)), main=""Poison density and histogram"")\\n    lines(xfit,yfit, col=""red"")\\n\\n    #Perform the chi-square goodness of fit test \\n    #In case of count data we can use goodfit() included in vcd package\\n    library(vcd) ## loading vcd package\\n    gf <- goodfit(x.poi,type= ""poisson"",method= ""MinChisq"")\\n    summary(gf)\\n    plot(gf,main=""Count data vs Poisson distribution"")",,
2579,5,1175,ed0c4393-923e-4638-ba30-956f8e7dd07c,2010-08-03 15:07:17.0,495.0,"If the data are *rates*: that is number of successes divided by number of trials, then a very elegant method is a funnel plot. For example, see http://qshc.bmj.com/content/11/4/390.2.full (apologies if the link requires a subscription--let me know and I'll find another).\\n\\nIt may be possible to adapt it to other types of data, but I haven't seen any examples.\\n\\nUPDATE:\\n\\nHere's a link to an example which doesn't require a subscription (and has a good explanation for how they might be used):\\nhttp://understandinguncertainty.org/fertility\\n\\nThey can be used for non-rate data, by simply plotting mean against standard error, however they may lose some of their simplicity.\\n\\nThe wikipedia article is not great, as it only discusses their use in meta-analyses. I'd argue they could be useful in many other contexts.",added 316 characters in body; added 145 characters in body,
2580,2,1182,03626ae9-1af9-4288-ad12-89a7ec6860b0,2010-08-03 15:08:16.0,364.0,"From a psychological perspective, I advocate plotting the data plus your uncertainty about the data. Thus, in a plot like you show, I would never bother with extending the bars all the way to zero, which only serves to minimize the eye's ability to distinguish differences in the range of the data.\\n\\nAdditionally, I'm frankly anti-bargraph; bar graphs map two variables to the same aesthetic attribute (x-axis location), which can cause confusion. A better approach is to avoid redundant aesthetic mapping by mapping one variable to the x-axis and another variable to another aesthetic attribute (eg. point shape or color or both).\\n\\nFinally, in your plot above, you only include error bars above the value, which hinders one's ability to compare the intervals of uncertainty relative to bars above and below the value.\\n\\nHere's how I would plot the data (via the ggplot2 package). Note that I add lines connecting points in the same series; some argue that this is only appropriate when the series across which the lines are connected are numeric (as seems to be in this case), however as long as there is any reasonable ordinal relationship among the levels of the x-axis variable, I think connecting lines are useful for helping the eye associate points across the x-axis. This can become particularly useful for detecting interactions, which really stand out with lines.\\n\\n    library(ggplot2)\\n    a = data.frame(names,prevs,se)\\n    a$let = substr(a$names,1,1)\\n    a$num = substr(a$names,2,2)\\n    ggplot(data = a)+\\n    layer(\\n        geom = 'point'\\n        , mapping = aes(\\n            x = num\\n            , y = prevs\\n            , colour = let\\n            , shape = let\\n        )\\n    )+\\n    layer(\\n        geom = 'line'\\n        , mapping = aes(\\n            x = num\\n            , y = prevs\\n            , colour = let\\n            , linetype = let\\n            , group = let\\n        )    \\n    )+\\n    layer(\\n        geom = 'errorbar'\\n        , mapping = aes(\\n            x = num\\n            , ymin = prevs-se\\n            , ymax = prevs+se\\n            , colour = let\\n        )\\n        , alpha = .5    \\n    )\\n\\n",,
2581,2,1183,96909bd8-beef-41c2-9fac-581732f90b0c,2010-08-03 15:19:15.0,88.0,"I would use boxplots here; clean, meaningful, nonparametric... Or [vioplot][1] if the distribution is more interesting. \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/vioplot/index.html",,
2582,5,1173,59ed3e18-58ea-410e-af0e-da1c08e378a5,2010-08-03 15:34:17.0,8.0,"In my area of research, a popular way of displaying data is to use a combination of a bar chart with ""handle-bars"". For example, \\n\\n![Example plot][1]\\n\\nThe ""handle-bars"" alternate between standard errors and standard deviations depending on the author. Typically, the sample sizes for each ""bar"" are fairly small - around six.\\n\\nThese plots seem to be particularly popular in biological sciences - see the first few papers of [BMC Biology, vol 3][2] for examples.\\n\\nSo how would you present this data?\\n\\n**Why I dislike these plots**\\n\\nPersonally I don't like these plots. \\n\\n1. When the sample size is  small, why not just display the individual data points. \\n1. Is it the sd or the se that is being displayed? No-one agrees which to use. \\n1. Why use bars at all. The data doesn't (usually) go from 0 but a first pass at the graph suggests it does.\\n1. The graphs don't give an idea about range or sample size of the data.\\n\\n**R script**\\n\\nThis is the R code I used to generate the plot. That way you can (if you want) use the same data.\\n\\n                                            #Generate the data\\n    set.seed(1)\\n    names = c(""A1"", ""A2"", ""A3"", ""B1"", ""B2"", ""B3"", ""C1"", ""C2"", ""C3"")\\n    prevs = c(38, 37, 31, 31, 29, 26, 40, 32, 39)\\n    \\n    n=6; se = numeric(length(prevs))\\n    for(i in 1:length(prevs))\\n      se[i] = sd(rnorm(n, prevs, 15))/n\\n    \\n                                            #Basic plot\\n    par(fin=c(6,6), pin=c(6,6), mai=c(0.8,1.0,0.0,0.125), cex.axis=0.8)\\n    barplot(prevs,space=c(0,0,0,3,0,0, 3,0,0), names.arg=NULL, horiz=FALSE,\\n            axes=FALSE, ylab=""Percent"", col=c(2,3,4), width=5, ylim=range(0,50))\\n    \\n                                            #Add in the CIs\\n    xx = c(2.5, 7.5, 12.5, 32.5, 37.5, 42.5,  62.5, 67.5, 72.5)\\n    for (i in 1:length(prevs)) {\\n      lines(rep(xx[i], 2), c(prevs[i], prevs[i]+se[i]))\\n      lines(c(xx[i]+1/2, xx[i]-1/2), rep(prevs[i]+se[i], 2))\\n    }\\n    \\n                                            #Add the axis\\n    axis(2, tick=TRUE, xaxp=c(0, 50, 5))\\n    axis(1, at=xx+0.1, labels=names, font=1,\\n         tck=0, tcl=0, las=1, padj=0, col=0, cex=0.1)\\n\\n\\n\\n\\n  [1]: http://img291.imageshack.us/img291/3387/tmpc.jpg\\n  [2]: http://www.biomedcentral.com/bmcbiol/3",added 275 characters in body,
2583,5,1182,2dcff5f6-1027-4ba5-b2c3-5297680ff4a9,2010-08-03 15:45:34.0,364.0,"From a psychological perspective, I advocate plotting the data plus your uncertainty about the data. Thus, in a plot like you show, I would never bother with extending the bars all the way to zero, which only serves to minimize the eye's ability to distinguish differences in the range of the data.\\n\\nAdditionally, I'm frankly anti-bargraph; bar graphs map two variables to the same aesthetic attribute (x-axis location), which can cause confusion. A better approach is to avoid redundant aesthetic mapping by mapping one variable to the x-axis and another variable to another aesthetic attribute (eg. point shape or color or both).\\n\\nFinally, in your plot above, you only include error bars above the value, which hinders one's ability to compare the intervals of uncertainty relative to bars above and below the value.\\n\\nHere's how I would plot the data (via the ggplot2 package). Note that I add lines connecting points in the same series; some argue that this is only appropriate when the series across which the lines are connected are numeric (as seems to be in this case), however as long as there is any reasonable ordinal relationship among the levels of the x-axis variable, I think connecting lines are useful for helping the eye associate points across the x-axis. This can become particularly useful for detecting interactions, which really stand out with lines.\\n\\n    library(ggplot2)\\n    a = data.frame(names,prevs,se)\\n    a$let = substr(a$names,1,1)\\n    a$num = substr(a$names,2,2)\\n    ggplot(data = a)+\\n    layer(\\n        geom = 'point'\\n        , mapping = aes(\\n            x = num\\n            , y = prevs\\n            , colour = let\\n            , shape = let\\n        )\\n    )+\\n    layer(\\n        geom = 'line'\\n        , mapping = aes(\\n            x = num\\n            , y = prevs\\n            , colour = let\\n            , linetype = let\\n            , group = let\\n        )    \\n    )+\\n    layer(\\n        geom = 'errorbar'\\n        , mapping = aes(\\n            x = num\\n            , ymin = prevs-se\\n            , ymax = prevs+se\\n            , colour = let\\n        )\\n        , alpha = .5\\n        , width = .5\\n    )\\n\\n![alt text][1]\\n\\n\\n  [1]: http://img818.imageshack.us/img818/3775/plot.jpg","Added plot, changed code slightly to reduce horizontal width of error bars.",
2584,2,1184,78ea7305-83f9-4cef-adb0-e4ea5abd338c,2010-08-03 16:26:48.0,573.0,"Could you recommend an introductory reference to index decomposition analysis, including\\n\\n - different methods (e.g. methods linked to the Laspeyre index and methods linked to the Divisa index)\\n - properties of decomposition methods which can be used to compare the different methods\\n - implementations of methods, e.g. in R\\n\\n? Any hint appreciated.\\n\\n(could not tag as index-decomposition due to missing reputation)",,
2585,1,1184,78ea7305-83f9-4cef-adb0-e4ea5abd338c,2010-08-03 16:26:48.0,573.0,Introduction to index decomposition analysis,,
2586,3,1184,78ea7305-83f9-4cef-adb0-e4ea5abd338c,2010-08-03 16:26:48.0,573.0,<textbooks>,,
2587,6,1184,3303aba5-e5fb-4527-adfa-d842c96ee2a5,2010-08-03 16:45:06.0,88.0,<beginner>,edited tags,
2588,5,1166,58b005e1-5b9c-44fe-bbdf-d2f74bcf363a,2010-08-03 16:51:26.0,223.0,"I Give an answer in two directions: \\n\\n 1. things that are robust are not necessarily labeled robust. If you believe robustness against everything exists then you are naive. \\n 2. Statistical approaches that leave the problem of robustness appart are sometime not adapted to the real world but are often more valuable (as a concept) than an algorithm that looks like kitchening. \\n\\n**developpment**\\n\\nFirst, I think there are a lot of good approaches in statistic (you will find them in R packages not necessarily with robust mentionned somewhere) which are naturally robust and tested on real data and the fact that you don't find algorithm with ""robust"" mentionned somewhere does not mean it is not robust. Anyway if you think being robust means being universal then you'll never find any robust procedure (no free lunch) you need to have some knowledge/expertise on the data you analyse in order to use adapted tool or to create an adapted model.\\n\\nOn the other hand, some approaches in statistic are not robust because they are dedicated to one single type of model. I think it is good sometime to work in a laboratory to try to understand things. It is also good to treat problem separatly to understand to what problem our solution is... this is how mathematician work. The example of Gaussian model elocant: is so much criticised because the gaussian assumption is never fulfilled but has bring 75% of the ideas used practically in statistic today. Do you really think all this is about writting paper to follow the publish or perish rule (which I don't like, I agree) ? ",tried to improve presentation...,
2589,2,1185,3abc1827-eacc-46c6-971a-ef3256998b4a,2010-08-03 17:03:58.0,8.0,"I would suggest that it's a lag in teaching. Most people either learn statistics at collage or University. If statistics is not you first degree and instead did a mathematics or computer science degree then you probably only cover the fundamental statistics modules: \\n\\n1. Probability\\n1. Hypothesis testing\\n1. Regression\\n\\nThis means that when faced with a problem we try and use what we know to solve the problem. \\n\\n * Data isn't Normal - take logs.\\n * Data has annoying outliers - remove them.\\n\\nUnless you stumble across something else, then it's difficult to do something better. It's really hard using Google to find something if you don't know what it's called!\\n\\nI think with all techniques, it will take awhile before the newer techniques filter down. How long did it take standard hypothesis test to be part of a standard statistics curriculum?\\n\\nBTW, with a statistics degree there will still be a lag in teaching - just a shorter one!\\n",,
2590,2,1186,e1cb6488-0233-4758-95fa-fdf07c29aaf5,2010-08-03 17:17:31.0,632.0,"You can draw a single figure in which the observed and expected frequencies are drawn side by side. If the distributions are very different and you also have a variance-mean ratio bigger than one, then a good candidate is the negative binomial. Read the section [Frequency Distributions][1] from `The R Book`. It deals with a very similar problem.\\n\\n\\n  [1]: http://books.google.com/books?id=8D4HVx0apZQC&lpg=PP1&dq=the%20r%20book&hl=el&pg=PA536#v=onepage&q&f=false",,
2591,2,1187,e11f073c-f782-4f98-a3df-e46d2c3f2a92,2010-08-03 17:24:56.0,39.0,"This thesis may be a starting place: [A Comparative Analysis of Index Decomposition Methods][1], by Frédéric Granel.  It should serve as a basic introduction to IDA and the Laspeyre index, but it does not include the Divisa index or any code implementing the methods.  \\n\\nFor more detail, including the Divisa index, you might try [Properties and linkages of some index decomposition analysis methods][2]. As for implementations in R, there seems to be no package for IDA, specifically, but [micEcon][3] and [micEconAids][4] have parts of it, by way of demand analysis.\\n\\nBest of luck.\\n\\n\\n\\n  [1]: https://scholarbank.nus.edu.sg/bitstream/handle/10635/14229/GranelF.pdf?sequence=1\\n  [2]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V2W-4WR2C4X-3&_user=10&_coverDate=11%2F30%2F2009&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1420130047&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=5872e15ee97331e2e332f10a8cefb041\\n  [3]: http://cran.r-project.org/web/packages/micEcon/micEcon.pdf\\n  [4]: http://cran.r-project.org/web/packages/micEconAids/micEconAids.pdf.",,
2592,5,1187,d71bc2e6-d04b-4217-bb5b-da7baa90ec43,2010-08-03 17:30:07.0,39.0,"This thesis may be a starting place: [A Comparative Analysis of Index Decomposition Methods][1], by Frédéric Granel.  It should serve as a basic introduction to IDA and the Laspeyre index, but it does not include the Divisa index or any code implementing the methods.  \\n\\nFor more detail, including the Divisa index, you might try [Properties and linkages of some index decomposition analysis methods][2]. As for implementations in R, there seems to be no package for IDA specifically, but [micEcon][3] and [micEconAids][4] have parts of it, by way of demand analysis.\\n\\nBest of luck.\\n\\n\\n\\n  [1]: https://scholarbank.nus.edu.sg/bitstream/handle/10635/14229/GranelF.pdf?sequence=1\\n  [2]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V2W-4WR2C4X-3&_user=10&_coverDate=11%2F30%2F2009&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1420130047&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=5872e15ee97331e2e332f10a8cefb041\\n  [3]: http://cran.r-project.org/web/packages/micEcon/micEcon.pdf\\n  [4]: http://cran.r-project.org/web/packages/micEconAids/micEconAids.pdf.",deleted 1 characters in body,
2593,4,1174,0e7345c0-b09f-4582-bff9-d663e368b796,2010-08-03 18:31:35.0,88.0,How can I test if given samples are taken from a Poisson distribution?,Formating fix.,
2596,5,1123,6e824e0b-7765-4981-b6dc-8ffa341c8b32,2010-08-03 18:49:43.0,88.0,"In many papers I see data representing a rate of success  (i.e a number between 0 and 1) modeled as a gaussian. This is clearly a sin (the range of variation of the gaussian is all of R),\\nbut how bad is that sin? Under what assumptions would you say it is tolerable?",Fixed spelling.,
2597,4,1123,6e824e0b-7765-4981-b6dc-8ffa341c8b32,2010-08-03 18:49:43.0,88.0,Modeling success rate with gaussian distribution,Fixed spelling.,
2598,5,726,8384eae1-56dd-4291-8701-820623f80668,2010-08-03 19:19:48.0,88.0,"What is your favorite statistician quote? \\nThis is community wiki, so please one quote per answer.  ",deleted 1 characters in body,
2599,5,882,1b8e5c44-2b80-4dc5-a191-10e05ae7acbf,2010-08-03 19:20:24.0,223.0,"**Histogram density estimator** is estimating the density with a sum of **piecewise functions** (density of a uniform).\\n\\n**KDE** is using a sum of **smooth function** (gaussian  is an example) (as long as they are positive they can be transformed into a density by normalization) \\n\\nThe use of ""**mixture**"" in statistic is about convex combination of densities.",deleted 61 characters in body,
2600,2,1188,e5487c9c-f3f0-4a37-b685-282444521151,2010-08-03 19:26:55.0,61.0,"Are you entirely sure that they're using the normal distribution directly?  It's very common to use transformed responses to model success rates, but this involves passing through a link function to move from a Gaussian random variable to a value in [0,1].  A commonly used transform is the probit one, which is just the inverse Gaussian CDF. (So you'd end up with something like $\\Phi^{-1}(p) = X\\beta + \\sigma$, where $\\Phi$ is the Gaussian CDF).\\n\\nIf you're actually using a normal distribution directly to model a result in [0,1], then it strikes me that the variance would have to be so small -- especially for p near 0 or 1 -- that you'd nearly always overfit the model.",,
2601,2,1189,503327dd-3f98-4a7c-96dc-cf5ffb718555,2010-08-03 19:37:30.0,61.0,"You can also use Edgeworth series, if your random variable has a finite mean and variance, which expands the CDF of your random variable in terms of the Gaussian CDF.  At first glance it's not quite as tidy conceptually as using a mixture model, but the derivation is quite pretty and it gives you a closed form with very fast decay in the tail terms.",,
2602,5,611,1c23c241-9f21-4706-9f2a-fbbf19aba079,2010-08-03 19:39:42.0,223.0,"**Heuristic answer:** Without much mathematic you could say that a continuous variable has a density with respect to the Lebesgue measure, and a discrete random variable has a density with respect to the counting measure.\\n\\n**Developped answer:** The concept of density is much wider than you may think. A density of a probability measure $P$ can be defined with respect to a measure $\\lambda$ that dominates $P$ by the Radon Nikodym Theorem (see http://en.wikipedia.org/wiki/Radon%E2%80%93Nikodym_theorem). Here density should be understood as a density with respect to the counting measure defined on the mentionned countable set. I agree that it is not extremely rigorous not to mention the reference when talking about a density (but who mention density  wrt lesbesgue measure?), but it pose no problem while reading the paper in question so .... \\n\\n\\n----------\\n\\n\\nAdditional Annex Notes\\n I have seen a certain number of machine learning notes (I won't do delation) where  the reference measure is not the counting measure and we see things such as $P(X=x|Y=y)$ with X being a continuous variable (with a density wrt Lebesgues) (to apply Bayes principle and derive the Bayes rule). I guess people want to be pedagogic and do not want to bother students with technical details ;) My conclusion would be that even great mathematician can commit abuse of wording or of notation (it is not the case in the paper you mention because the authors of a paper in AoP may have mathematical background), it is not a problem as along as it is understood by everyone...  ",heuristic answer added according to comments...,
2603,5,595,1b43e9f7-195b-4b3e-82c6-1f037c6e756b,2010-08-03 19:40:40.0,223.0,"You could use the (fast :) ) discrete **wavelet transform**. The package wavethresh under R will do all the work. \\nAnyway, I like the solution of @James because it is simple and seems to go straigh to the point.  ",added 4 characters in body,
2604,2,1190,9d520633-939b-4b45-a99f-85b50ab80424,2010-08-03 19:51:33.0,686.0,"Chi-square tests do not seem appropriate.  As others said, provided there are a reasonable number of different rates, you could treat the data as continuous and do regression or ANOVA.  You would then want to look at the distribution of the residuals.  ",,
2605,2,1191,2f2ea788-bad4-4d2d-8a10-2b406981b02e,2010-08-03 19:57:01.0,686.0,"I also like Rob's answer.  And, if you happen to use SAS rather than R, you can use PROC GLMSELECT for models that would be done with PROC GLM, although it works well for some other models, as well.  See\\n\\nFlom and Cassell ""Stopping Stepwise: Why Stepwise Selection Methods are Bad and What you Should Use"" presented at various groups, most recently, NESUG 2009",,
2606,2,1192,84b50f65-5e8d-4449-be38-1becb782990d,2010-08-03 20:00:30.0,686.0,"Yet another way to test this is with a quantile quantile plot.  In R, there is qqplot.",,
2607,2,1193,a95d7ead-a0bf-4d74-bba3-ca059bab7108,2010-08-03 20:07:57.0,686.0,"A variation on the Fisher quotation given above is\\n\\nHiring a physician after the data have been collected is like hiring a physician when your patient is in the morgue.  He may be able to tell you what went wrong, but he is unlikely to be able to fix it.\\n\\nBut I heard this attributed to Box, not Fisher.",,
2608,16,1193,a95d7ead-a0bf-4d74-bba3-ca059bab7108,2010-08-03 20:07:57.0,-1.0,,,
2609,5,1193,2666e823-4b12-43c2-b752-4361401a8af3,2010-08-03 20:10:22.0,455.0,"A variation on the Fisher quotation given [here][1] is\\n\\nHiring a physician after the data have been collected is like hiring a physician when your patient is in the morgue.  He may be able to tell you what went wrong, but he is unlikely to be able to fix it.\\n\\nBut I heard this attributed to Box, not Fisher.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/726/famous-statistician-quotes/739#739",added 96 characters in body,
2610,2,1194,90f4aa65-59d6-47ef-bbbf-0fc3c69b33e3,2010-08-03 20:19:57.0,11.0,"This question has been bugging me for some time, and I was going to write a blog post about it. However, I think it is better left for discussion in this forum. (Should this be community wiki?)\\n\\nBack in April, I attended a talk at the UMD Math Department Statistics group seminar series called ""To Explain or To Predict?"". The talk was given by Prof. Galit Shmueli who teaches at UMD's Smith Business School. Her talk was based on research she did for a paper titled ""Predictive vs. Explanatory Modeling in IS Research"", and a follow up working paper titled [""To Explain or To Predict?""][3]. \\n\\nDr. Shmueli's argument is that the terms predictive and explanatory in a statistical modeling context have become conflated, and that statistical literature lacks a a thorough discussion of the differences. In the paper, she contrasts both and talks about their practical implications. I encourage you to read the papers. \\n\\nI'd like to pose the question to the practitioner community and hear what other people think about this topic, and learn about their particular use of either or both. I know I've fallen into the trap of using an explanatory model for predicting.\\n\\n  [1]: http://www.rhsmith.umd.edu/faculty/gshmueli/web/html/\\n  [2]: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1112893\\n  [3]: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1351252\\n  [4]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning",,
2611,1,1194,90f4aa65-59d6-47ef-bbbf-0fc3c69b33e3,2010-08-03 20:19:57.0,11.0,Practical thoughts on explanatory vs. predictive modeling,,
2612,3,1194,90f4aa65-59d6-47ef-bbbf-0fc3c69b33e3,2010-08-03 20:19:57.0,11.0,<modeling><model-comparison>,,
2613,2,1195,e6800212-75f5-4f69-86c9-d61cf9f49437,2010-08-03 20:23:34.0,687.0,"Many income surveys (especially older ones) truncate key variables, such as household income, at some arbitrary point, to protect confidentiality. This point changes over time. This reduces inequality measures associated with the variable. I am interested in fitting a Pareto tail to the truncated distribution, replacing truncated values with imputed values to mimic the actual distribution. What's the best way to do this?",,
2614,1,1195,e6800212-75f5-4f69-86c9-d61cf9f49437,2010-08-03 20:23:34.0,687.0,How can I apply a Pareto tail to a truncated distribution?,,
2615,3,1195,e6800212-75f5-4f69-86c9-d61cf9f49437,2010-08-03 20:23:34.0,687.0,<distributions><data>,,
2616,2,1196,987b15e6-51b5-4024-86a2-b897e51786cb,2010-08-03 20:26:23.0,666.0,"Statistics is a tool for non-statistical-minded researchers, and they just don't care.\\n\\nI once tried to help with a Medicine article my ex-wife was co-authoring. I wrote several pages describing the data, what it suggested, why certain observations had been excluded from the study... and the lead researcher, a doctor, threw it all away and asked someone to compute a p-value, which is all she (and just about everyone who would read the article) cared about.",,
2617,2,1197,14f99596-9a01-4e3b-a5cf-02ddd2a5ebf0,2010-08-03 20:39:20.0,303.0,"I haven't read her work beyond the abstract of the linked paper, but my sense is that the distinction between ""explanation"" and ""prediction"" should be thrown away and replaced with the distinction between the aims of the practitioner, which are either ""causal"" or ""predictive"". In general, I think ""explanation"" is such a vague word that it means nearly nothing. For example, is Hooke's Law explanatory or predictive? On the other end of the spectrum, are predictively accurate recommendation systems good causal models of explicit item ratings? I think we all share the intuition that the goal of science is explanation, while the goal of technology is prediction; and this intuition somehow gets lost in consideration of the tools we use, like supervised learning algorithms, that can be employed for both causal inference and predictive modeling, but are really purely mathematical devices that are not intrinsically linked to ""prediction"" or ""explanation"".\\n\\nHaving said all of that, maybe the only word that I would apply to a model is interpretable. Regressions are usually interpretable; neural nets with many layers are often not so. I think people sometimes naively assume that a model that is interpretable is providing causal information, while uninterpretable models only provide predictive information. This attitude seems simply confused to me.",,
2618,6,1194,c5924e7c-c4c7-4ae7-a864-895ed8a8d1ac,2010-08-03 20:40:04.0,,<predictive-models>,changed tags to appropriate ones,user28
2619,4,1173,2b8a4a51-9705-427d-9ad4-1a332211b1ac,2010-08-03 20:42:45.0,8.0,"Alternative graphics to ""handle bar"" plots",edited title; edited title,
2620,5,1031,acdad8db-e875-4bc3-8819-637ed17fdf32,2010-08-03 21:02:15.0,223.0,"KL has a deep meaning when you visualize a set of **dentities as a manifold** within the fisher metric tensor, it gives the geodesic distance between two ""close"" distributions. Formally: \\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\nThe following lines are here to explain with details what is meant by this las mathematical formulae. \\n\\n**Definition of the Fisher metric.** \\n\\n\\nConsider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_{ij})$ is \\n \\n$F_{ij}=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$\\n\\nWith this notation $D$ is a riemannian manifold and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao lower bound theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if $p=1$ you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and $F_{11}$ is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance $ds^2$ between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\n$ds^2= \\sum F_{ij} d \\theta^i d \\theta^j$\\n\\nand it is known to be twice the Kullback Leibler Divergence:\\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\n",tried to make a short intuitive answer first and then develop... easier to read ? ,
2621,5,1194,3e1dbbc6-815a-4e0b-8ff0-2c2c8d963b1a,2010-08-03 21:15:08.0,11.0,"This question has been bugging me for some time, and I was going to write a blog post about it. However, I think it is better left for discussion in this forum. \\n\\nBack in April, I attended a talk at the UMD Math Department Statistics group seminar series called ""To Explain or To Predict?"". The talk was given by [Prof. Galit Shmueli][1] who teaches at UMD's Smith Business School. Her talk was based on research she did for a paper titled [""Predictive vs. Explanatory Modeling in IS Research""][2], and a follow up working paper titled [""To Explain or To Predict?""][3]. \\n\\nDr. Shmueli's argument is that the terms predictive and explanatory in a statistical modeling context have become conflated, and that statistical literature lacks a a thorough discussion of the differences. In the paper, she contrasts both and talks about their practical implications. I encourage you to read the papers. \\n\\nThe question I'd like to pose to the practitioner community is: have you ever fallen into the trap of using one when meaning to use the other? I certainly have. How do you know which one to use? How do you define a predictive excercise vs an exlanatory/descriptive one? It would be useful if you could talk about the specific application. \\n\\n  [1]: http://www.rhsmith.umd.edu/faculty/gshmueli/web/html/\\n  [2]: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1112893\\n  [3]: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1351252\\n\\n\\n\\n",Made the question more specific,
2622,2,1198,65c50767-74e5-47d7-9e25-6c6241c4d68e,2010-08-03 21:19:22.0,666.0,"This is out of print but it's very easy to find a copy:\\n\\n[George E. P. Box, Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building][1]\\n\\n\\n  [1]: http://www.amazon.com/Statistics-Experimenters-Introduction-Analysis-Building/dp/0471093157/ref=sr_1_1?s=books&ie=UTF8&qid=1280870116&sr=1-1",,
2623,16,1198,65c50767-74e5-47d7-9e25-6c6241c4d68e,2010-08-03 21:19:22.0,-1.0,,,
2624,2,1199,50475a39-db5c-4fc0-befa-eb863c6abbd4,2010-08-03 21:30:22.0,5.0,"Brad Efron, one of the commentators on [The Two Cultures][2] paper, made the following observation (as discussed [in my earlier question][1]):\\n\\n> Prediction by itself is\\n> only occasionally sufficient. The post\\n> office is happy with any method that\\n> predicts correct addresses from\\n> hand-written scrawls. Peter\\n> Gregory undertook his study for\\n> prediction purposes, but also to\\n> better understand the medical basis of\\n> hepatitis. Most statistical surveys\\n> have the identification of causal\\n> factors as their ultimate goal.\\n\\nCertain fields (eg. Medicine) place a heavy weight on model fitting as explanatory process (the distribution, etc.), as a means to understanding the underlying process that generates the data. Other fields are less concerned with this, and will be happy with a ""black box"" model that has a very high predictive success.  \\n\\nThis can work its way into the model building process as well.  If the only concern is understanding what happened in the available data, one might be less concerned with building a robust model (using a technique like cross validation, for instance).  \\n\\n  [1]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning\\n  [2]: http://www.stat.osu.edu/~bli/dmsl/papers/Breiman.pdf",,
2625,2,1200,7b78438e-8b7c-444e-9510-f0f2cbb3d573,2010-08-03 21:32:40.0,,"I am still a bit unclear as to what the question is. Having said that, to my mind the fundamental difference between predictive and explanatory models is the difference in their focus.\\n\\n**Explanatory Models**\\n\\nBy definition explanatory models have as their primary focus the goal of explaining something in the real world. In most instances, we seek to offer simple and clean explanations. By simple I mean that we prefer parsimony (explain the phenomena with as few parameters as possible) and by clean I mean that we would like to make statements of the following form: ""the effect of changing $x$ by one unit changes $y$ by $\\beta$ holding everything else constant"". Given these goals of simple and clear explanations, explanatory models seek to penalize complex models (by using appropriate criteria such as AIC) and prefer to obtain orthogonal independent variables (either via controlled experiments or via suitable data transformations).\\n\\n**Predictive Models**\\n\\nThe goal of predictive models is to predict something. Thus, they tend to focus less on parsimony or simplicity but more on their ability to predict the dependent variable. \\n\\nHowever, the above is somewhat of an artificial distinction as explanatory models can be used for prediction and sometimes predictive models can explain something.",,user28
2626,5,1198,78dd96f9-e2f9-4575-a7c5-eef0df6f4cf3,2010-08-03 21:39:31.0,666.0,"This book is dynamite:\\n[George E. P. Box, Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building][1]\\n\\nIt starts from zero knowledge of Statistics but it doesn't insult the reader's intelligence. It's incredibly practical but with no loss of rigour; in fact, it underscores the danger of ignoring underlying assumptions (which are often false in real life) of common tests.\\n\\nIt's out of print but it's very easy to find a copy. Follow the link for a few options.\\n\\n\\n  [1]: http://www.amazon.com/Statistics-Experimenters-Introduction-Analysis-Building/dp/0471093157/ref=sr_1_1?s=books&ie=UTF8&qid=1280870116&sr=1-1",added 330 characters in body; edited body,
2627,6,1195,153fd152-0496-448d-8d6e-9f3e62f3f92f,2010-08-03 21:44:30.0,,<pareto-distribution><data-imputation>,fixed tags,user28
2628,5,1199,c35986df-73dd-4b3c-a018-e6f561deb529,2010-08-03 21:46:47.0,5.0,"as others have already said, the distinction is somewhat meaningless, except in so far as the aims of the researcher are concerned. \\n\\nBrad Efron, one of the commentators on [The Two Cultures][2] paper, made the following observation (as discussed [in my earlier question][1]):\\n\\n> Prediction by itself is\\n> only occasionally sufficient. The post\\n> office is happy with any method that\\n> predicts correct addresses from\\n> hand-written scrawls. Peter\\n> Gregory undertook his study for\\n> prediction purposes, but also to\\n> better understand the medical basis of\\n> hepatitis. Most statistical surveys\\n> have the identification of causal\\n> factors as their ultimate goal.\\n\\nCertain fields (eg. Medicine) place a heavy weight on model fitting as explanatory process (the distribution, etc.), as a means to understanding the underlying process that generates the data. Other fields are less concerned with this, and will be happy with a ""black box"" model that has a very high predictive success. This can work its way into the model building process as well.  \\n\\n  [1]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning\\n  [2]: http://www.stat.osu.edu/~bli/dmsl/papers/Breiman.pdf",deleted 58 characters in body,
2629,2,1201,5ea7918a-d785-4d01-b982-6e5f03614704,2010-08-03 21:50:09.0,666.0,"Sorry, no quick answer. There are thick books dedicated to answering this question. Here's a 600-page long example: [Harrell's Regression Modeling Strategies][1]\\n\\n\\n  [1]: http://www.amazon.com/Regression-Modeling-Strategies-Applications-Logistic/dp/1441929185/ref=ntt_at_ep_dpt_1",,
2630,2,1202,e52693e6-1644-416e-bcf1-ae904f9e562d,2010-08-03 22:09:33.0,608.0,"The question in short: What methods can be used to quantify distributional relationships between data when the distribution is un-known?\\n\\nNow the longer story: I have a list of distributions and would like to rank them based on their similarity to a given base-line distribution. Correlation jumps into my mind in such a case and the Spearman correlation coefficient in particular given that it does not make any distributional assumptions. However, I would actually need to create the coefficient based on binned data (as this is done for histograms or densities) rather than the raw data and I don't know if this is actually a valid step or if I am just manufacturing data.\\n\\nIn other words, if I have a 10000 point data set for each distribution, I would first create a binned distribution for each were each bin is of equal width and contains the frequencies of how many points each bin has. Just the way this is done for density plots or histograms. Each bin is on a discrete scale. The data is actually computer screen coordinate data and values are between 1 and 1024. Each pixel position could represent a bin (but larger bins are possible e.g. every 5 pixel being one bin). I would then compare the sequence of bins with each other rather than the raw data. The data set would look like this.\\n\\nbins: 1 2 3 4 .... 1024<br>\\ndist#base:1 2 2 3 ..... 3<br>\\ndist#1: 1 4 5 5 3<br>\\ndist#2: 2 2 3 5 6<br>\\n...<br>\\ndist#1000: 1 2 4 6 6<br>\\n\\nDoes this make sense? Are there better ways of doing that? Are there better statistical methods? The goal of all this is to first) test how close are distributions from measure A to measure B and second) if I can predict one, if the other is missing.\\n\\nBest,\\nAmpleforth",,
2631,1,1202,e52693e6-1644-416e-bcf1-ae904f9e562d,2010-08-03 22:09:33.0,608.0,Ranking distributional data by similarity,,
2632,3,1202,e52693e6-1644-416e-bcf1-ae904f9e562d,2010-08-03 22:09:33.0,608.0,<distributions>,,
2633,5,608,d8191411-6dd1-4622-8b43-06e5e6655a19,2010-08-03 22:18:43.0,196.0,"In a [question][1] elsewhere on this site, several answers mentioned that the AIC is equivalent to  leave-one-out (LOO) cross-validation and that the BIC is equivalent to K-fold cross validation.  Is there a way to empirically demonstrate this in R such that the techniques involved in LOO and K-fold are made clear and demonstrated to be equivalent to the AIC and BIC values?  Well commented code would be helpful in this regard.  In addition, in demonstrating the BIC please use the lme4 package.  See below for a sample dataset...\\n\\n    library(lme4) #for the BIC function\\n\\n    generate.data <- function(seed)\\n    {\\n    	set.seed(seed) #Set a seed so the results are consistent (I hope)\\n    	a <- rnorm(60) #predictor\\n    	b <- rnorm(60) #predictor\\n    	c <- rnorm(60) #predictor\\n    	y <- rnorm(60)*3.5+a+b #the outcome is really a function of predictor a and b but not predictor c\\n    	data <- data.frame(y,a,b,c) \\n    	return(data)	\\n    }\\n   \\n    data <- generate.data(76)\\n    good.model <- lm(y ~ a+b,data=data)\\n    bad.model <- lm(y ~ a+b+c,data=data)\\n    AIC(good.model)\\n    BIC(logLik(good.model))\\n    AIC(bad.model)\\n    BIC(logLik(bad.model))\\n\\nPer earlier comments, below I have provided a list of seeds from 1 to 10000 in which AIC and BIC disagree.  This was done by a simple search through the available seeds, but if someone could provide a way to generate data which would tend to produce divergent answers from these two information criteria it may be particularly informative.\\n\\n    notable.seeds <- read.csv(""http://student.ucr.edu/~rpier001/res.csv"")$seed\\n\\nAs an aside, I thought about ordering these seeds by the extent to which the AIC and BIC disagree which I've tried quantifying as the sum of the absolute differences of the AIC and BIC.  For example, \\n\\n    AICDiff <- AIC(bad.model) - AIC(good.model) \\n    BICDiff <- BIC(logLik(bad.model)) - BIC(logLik(good.model))\\n    disagreement <- sum(abs(c(AICDiff,BICDiff)))\\n\\nwhere my disagreement metric only reasonably applies when the observations are notable.  For example,\\n\\n    are.diff <- sum(sign(c(AICDiff,BICDiff)))\\n    notable <- ifelse(are.diff == 0 & AICDiff != 0,TRUE,FALSE)\\n\\nHowever in cases where AIC and BIC disagreed, the calculated disagreement value was always the same (and is a function of sample size).  Looking back at how AIC and BIC are calculated I can see why this might be the case computationally, but I'm not sure why it would be the case conceptually.  If someone could elucidate that issue as well, I'd appreciate it.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other\\n  [2]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation",removed my answer and made it an actual answer; added 2 characters in body,
2634,2,1203,77ca137a-357c-40ed-ac46-00a851c6c032,2010-08-03 22:23:31.0,196.0,"In an attempt to partially answer my own question, I read [Wikipedia's][1] description of leave-one-out cross validation \\n\\n> involves using a single observation\\n> from the original sample as the\\n> validation data, and the remaining\\n> observations as the training data.\\n> This is repeated such that each\\n> observation in the sample is used once\\n> as the validation data.\\n\\nIn R code, I suspect that that would mean something like this...\\n\\n    resid <- rep(NA, Nobs) \\n    for (lcv in 1:Nobs)\\n    	{\\n    		data.loo <- data[-lcv,] #drop the data point that will be used for validation\\n    		loo.model <- lm(y ~ a+b,data=data.loo) #construct a model without that data point\\n                resid[lcv] <- data[lcv,""y""] - (coef(loo.model)[1] + coef(loo.model)[2]*data[lcv,""a""]+coef(loo.model)[3]*data[lcv,""b""]) #compare the observed value to the value predicted by the loo model for each possible observation, and store that value\\n        }\\n\\n... is supposed to yield values in resid that is related to the AIC.  In practice the sum of squared residuals from each iteration of the LOO loop detailed above is a good predictor of the AIC for the notable.seeds, r^2 = .9776.  But, [elsewhere][1] a contributor suggested that LOO should be asymptotically equivalent to the AIC (at least for linear models), so I'm a little disappointed that r^2 isn't closer to 1.  Obviously this isn't really an answer - more like additional code to try to encourage someone to try to provide a better answer.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other\\n  [2]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation",,
2635,2,1204,07f51f02-a763-4300-85f0-ce9e01019b29,2010-08-03 22:25:14.0,287.0,"If you have the same sample size for each distribution you want to compare, and you're comparing them all to the same baseline, you could use the Mann-Whitney U statistic as a distribution similarity metric.\\n\\nhttp://en.wikipedia.org/wiki/Mann-Whitney_U\\n\\nTo calculate, from wikipedia:\\n\\n>1) Choose the sample for which the ranks seem to be smaller (The only reason to do this is to make computation easier). Call this ""sample 1,"" and call the other sample ""sample 2.""\\n\\n>2) Taking each observation in sample 1, count the number of observations in sample 2 that are smaller than it (count a half for any that are equal to it).\\n\\n>3) The total of these counts is U.\\n\\nIn R, it would be\\n\\n    wilcox.test(sample1, baseline)$statistic\\n\\n\\n",,
2636,12,1204,081d49c5-b065-4756-8421-ac4fdb0d7853,2010-08-03 22:31:16.0,287.0,"{""Voters"":[{""Id"":287,""DisplayName"":""JoFrhwld""}]}",,
2637,5,1204,b0876bdd-e55e-4d4e-8823-0ef68a2e7bef,2010-08-03 22:39:31.0,287.0,"Edit: I misunderstood the question at first. Your observations are actually paired. Sample1 Bin1 to Baseline Bin1 etc.\\n\\nWhat you could do is take the difference between sample and baseline for each bin, then use the Wilcoxon signed rank statistic on the differences.\\n\\nhttp://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\\n\\nIf \\n \\n- D is your sequence of differences, then \\n- R is the ranks of |D|, and \\n- psi = 0 if D < 0, 1 if D > 0. \\n- W = Sum(psi*R)\\n\\nIn R\\n\\n    wilcox.test(sample1-baseline)$statistic",deleted 226 characters in body,
2638,13,1204,a80ee374-b976-4648-87f4-08a719417473,2010-08-03 22:39:38.0,287.0,"{""Voters"":[{""Id"":287,""DisplayName"":""JoFrhwld""}]}",,
2639,2,1205,74624074-3e7d-4707-8e41-de8ec3d43529,2010-08-03 22:56:48.0,366.0,"I want to perform a two-sample T-test to test for a difference between two independent samples which each sample abides by the assumptions of the T-test (each distribution can be assumed to be independent and identically distributed as Normal with equal variance). The only complication from the basic two-sample T-test is that the data is weighted. I am using weighted means and standard deviations, but weighted N's will artificially inflate the size of the sample, hence bias the result. Is it simply a case of replacing the weighted Ns with the unweighted Ns?",,
2640,1,1205,74624074-3e7d-4707-8e41-de8ec3d43529,2010-08-03 22:56:48.0,366.0,Two-sample T-test with weighted data,,
2641,3,1205,74624074-3e7d-4707-8e41-de8ec3d43529,2010-08-03 22:56:48.0,366.0,<t-test>,,
2642,5,1203,d861f39b-b3cf-4b81-9d30-340ff50dbb82,2010-08-03 22:58:09.0,196.0,"In an attempt to partially answer my own question, I read [Wikipedia's][1] description of leave-one-out cross validation \\n\\n> involves using a single observation\\n> from the original sample as the\\n> validation data, and the remaining\\n> observations as the training data.\\n> This is repeated such that each\\n> observation in the sample is used once\\n> as the validation data.\\n\\nIn R code, I suspect that that would mean something like this...\\n\\n    resid <- rep(NA, Nobs) \\n    for (lcv in 1:Nobs)\\n    	{\\n    		data.loo <- data[-lcv,] #drop the data point that will be used for validation\\n    		loo.model <- lm(y ~ a+b,data=data.loo) #construct a model without that data point\\n                resid[lcv] <- data[lcv,""y""] - (coef(loo.model)[1] + coef(loo.model)[2]*data[lcv,""a""]+coef(loo.model)[3]*data[lcv,""b""]) #compare the observed value to the value predicted by the loo model for each possible observation, and store that value\\n        }\\n\\n... is supposed to yield values in resid that is related to the AIC.  In practice the sum of squared residuals from each iteration of the LOO loop detailed above is a good predictor of the AIC for the notable.seeds, r^2 = .9776.  But, [elsewhere][1] a contributor suggested that LOO should be asymptotically equivalent to the AIC (at least for linear models), so I'm a little disappointed that r^2 isn't closer to 1.  Obviously this isn't really an answer - more like additional code to try to encourage someone to try to provide a better answer.\\n\\nAddendum:  Since AIC and BIC for models of fixed sample size only vary by a constant, the correlation of BIC to squared residuals is the same as the correaltion of AIC to squared residuals, so the approach I took above appears to be fruitless.\\n\\n  [1]: http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other\\n  [2]: http://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation",added 247 characters in body,
2643,2,1206,73993b6f-8e5e-45ef-8572-a3aae663f5fd,2010-08-03 23:36:08.0,159.0,"One practical issue that arises here is variable selection in modelling. A variable can be an important explanatory variable (e.g., is statistically significant) but may not be useful for predictive purposes (i.e., its inclusion in the model leads to worse predictive accuracy). I see this mistake almost every day in published papers.",,
2644,5,1174,466107f3-053e-4e68-ac48-93bed0ae1617,2010-08-03 23:47:36.0,159.0,"I know of normality tests, but how do I test for ""Poisson-ness""?\\n\\nI have ~1000 sample of non-negative integers, which I suspect are taken from a Poisson distribution, and I would like to test that.\\n\\nThanks,\\n\\nDave",added 3 characters in body,
2645,5,1206,c4b49533-50ec-465e-8dc4-f3874a0d4158,2010-08-03 23:57:47.0,159.0,"One practical issue that arises here is variable selection in modelling. A variable can be an important explanatory variable (e.g., is statistically significant) but may not be useful for predictive purposes (i.e., its inclusion in the model leads to worse predictive accuracy). I see this mistake almost every day in published papers.\\n\\nAnother difference is in the distinction between principal components analysis and factor analysis. PCA is often used in prediction, but is not so useful for explanation. FA involves the additional step of rotation which is done to improve interpretation (and hence explanation). There is a [nice post today on Galit Shmueli's blog about this][1].\\n\\n\\n  [1]: http://blog.bzst.com/2010/08/pca-debate.html",added 408 characters in body,
2646,2,1207,b033a56c-638e-4b8f-a2e3-e2509155db09,2010-08-04 00:32:13.0,667.0,"This post is the continuation of another post related to a generic method for outlier detection in time series.\\nBasically, at this point I'm interested in a robust way to discover the periodicity/seasonality of a generic time series affected by a lot of noise.\\nFrom a developer point of view, I would like a simple interface such as:\\n\\nunsigned int discover_period(vector<double> v);\\n\\nWhere v is the array containing the samples, and the return value is the period of the signal.\\nThe main point is that, again, I can't make any assumption regarding the analyzed signal.\\nI already tried an approach based on the signal autocorrelation (detecting the peaks of a correlogram), but it's not robust as I would like.",,
2647,1,1207,b033a56c-638e-4b8f-a2e3-e2509155db09,2010-08-04 00:32:13.0,667.0,Period detection of a generic time series,,
2648,3,1207,b033a56c-638e-4b8f-a2e3-e2509155db09,2010-08-04 00:32:13.0,667.0,<time-series><algorithms><mathematical-statistics><real-time>,,
2651,5,1207,6b34c85c-2aa2-4603-ad45-13beacd9650d,2010-08-04 01:23:06.0,,"This post is the continuation of another post related to a [generic method for outlier detection in time series][1].\\nBasically, at this point I'm interested in a robust way to discover the periodicity/seasonality of a generic time series affected by a lot of noise.\\nFrom a developer point of view, I would like a simple interface such as:\\n\\nunsigned int discover_period(vector<double> v);\\n\\nWhere v is the array containing the samples, and the return value is the period of the signal.\\nThe main point is that, again, I can't make any assumption regarding the analyzed signal.\\nI already tried an approach based on the signal autocorrelation (detecting the peaks of a correlogram), but it's not robust as I would like.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series",added url to earlier post; removed mathematical-statistics tag; edited tags,user28
2652,6,1207,6b34c85c-2aa2-4603-ad45-13beacd9650d,2010-08-04 01:23:06.0,,<time-series><algorithms><real-time>,added url to earlier post; removed mathematical-statistics tag; edited tags,user28
2653,2,1210,faa64ebc-a055-4390-aba9-4cc3e1f314fc,2010-08-04 01:26:59.0,485.0,"You should look into ""partitioning chi-squared"".  This is similar in logic to performing post-hoc tests in ANOVA.  It will allow you to determine whether your significant overall test is primarily attributable to differences in particular categories or groups of categories.\\n\\nA quick google turned up this presentation, which at the end discusses methods for partitioning chi-squared.\\n\\nhttp://www.ed.uiuc.edu/courses/EdPsy490AT/lectures/2way_chi-ha-online.pdf",,
2654,2,1211,e06006ce-42af-49f7-aa4d-7096800fa1d0,2010-08-04 02:05:29.0,521.0,"It's like that old joke. When asked for directions the philosopher said ""Well, if I wanted to go there, I wouldn't start from here ...""\\n\\nWhile I think each ""culture"" should be open to learning from the other, they have <a href=""http://blog.revolutionanalytics.com/2009/09/the-difference-between-statistics-and-machine-learning.html"" rel=""nofollow"">different ways of looking at the world</a>.\\n\\nI think the problem with learning statistics through studying machine learning algorithms is that, whilst ML algorithms start with statistical concepts, statistics doesn't start with algorithms, but probability models.\\n\\n\\n\\n\\n\\n",,
2655,2,1212,6415ca03-cc17-4712-afa8-e6deccc8d738,2010-08-04 03:23:21.0,25692.0,"First of all my advice is you must refrain from trying out a Poisson distribution just as it is to the data. I suggest you must first make a theory as to why should Poisson distribution fit a particular dataset or a phenomenon. Once you have established this, the next question is whether the distribution is homogeneous or not. this means whether all parts of the data are handled by the same poisson distribution or is their a variation in this based on some aspect like time or space. Once you have convinced of these aspects, try the following three tests \\n\\n1. likelihood ratio test using a chi square variable \\n2. use of conditional chi-square statistic, this test is called the poisson dispersion test or the variance test\\n3. use of the neyman-scott statistic, this is based on a variance stabilizing transform of the poisson variable.\\n\\nsearch for these and you will find them easily on the net. ",,
2656,2,1213,46572c8b-3b0b-4476-a08b-7534b311567f,2010-08-04 03:28:53.0,25692.0,"Some useful R links : (find out the link that suits u)\\n\\nhttp://had.co.nz/plyr/plyr-intro-090510.pdf for data manipulation\\nhttp://www.stats.ox.ac.uk/~ruth/RCourse/Rcourse3.pdf\\nhttp://cran.r-project.org/doc/contrib/usingR.pdf for R basics\\nhttp://www.ats.ucla.edu/stat/r/dae/default.htm with annotated outputs in R\\nhttp://cran.r-project.org/doc/contrib/Rossiter-RIntro-ITC.pdf tutorial with info on plots\\nhttp://www.statmethods.net/stats/regression.html\\nhttp://www.rmetrics.org/ provides an Open Source framework for Financial Analysis.\\nhttp://www.econ.uiuc.edu/~econ472/e-Tutorial.html has lecture notes with R code\\nA brief guide to R and Economics http://people.su.se/~ma/R_intro/R_intro.pdf\\nhttp://www.stat.pitt.edu/stoffer/tsa2/index.html has a good beginner’s tutorial for Time Series\\nhttp://www.quantmod.com/ provides a great analysis and visualization framework for quantitative trading\\nhttp://www.wise.xmu.edu.cn/2007summerworkshop/download/Advanced%20Topics%20in%20Time%20Series%20Econometrics%20Using%20R1_ZongwuCAI.pdf advanced time series in R\\nInteresting time series packages in R http://robjhyndman.com/software\\nA Data Mining tool in R http://rattle.togaware.com/\\nAn online e-book for Data Mining with R http://www.liaad.up.pt/~ltorgo/DataMiningWithR/\\nAdvanced Statistics using R http://www.statmethods.net/advstats/index.html\\nGuide to Credit Scoring using R http://cran.r-project.org/doc/contrib/Sharma-CreditScoring.pdf\\nhttp://addictedtor.free.fr/graphiques/ is a graph gallery of R plots and charts with supporting code\\nA tutorial for Lattice http://osiris.sunderland.ac.uk/~cs0her/Statistics/UsingLatticeGraphicsInR.htm\\nGgplot R graphics http://had.co.nz/ggplot2/\\nGgplot Vs Lattice @ http://had.co.nz/ggplot/vs-lattice.html\\nMultiple tutorials for using ggplot2 and Lattice http://learnr.wordpress.com/tag/ggplot2/\\nIntroduction to the Text Mining package in R http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf\\nSocial Network Analysis http://www.r-project.org/conferences/useR-2008/slides/Bojanowski.pdf\\nWeb Scraping in R http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl\\nhttp://learnr.wordpress.com/2009/10/06/export-data-frames-to-multi-worksheet-excel-file/ to embed R data frames in Excel via multiple approaches.\\nhttp://www.statconn.com/ provides a tool to make R usable from Excel\\nConnect to MySQL from R http://erikvold.com/blog/index.cfm/2008/8/20/how-to-connect-to-mysql-with-r-in-wndows-using-rmysql\\nhttp://www.statmethods.net/input/importingdata.html provides info about pulling data from SAS, STATA, SPSS, etc.\\nThematic Maps with R http://stackoverflow.com/questions/1260965/developing-geographic-thematic-maps-with-r\\nhttp://smartdatacollective.com/Home/22052 for geographic maps in R\\nGoogle Charts with R http://www.iq.harvard.edu/blog/sss/archives/2008/04/google_charts_f_1.shtml\\nIntro to using RGoogleMaps @ http://cran.r-project.org/web/packages/RgoogleMaps/vignettes/RgoogleMaps-intro.pdf\\nhttp://www.stat.uni-muenchen.de/~leisch/Sweave/\\nR2HTML http://www.feferraz.net/en/P/R2HTML\\nPoor Man GUI for R http://wiener.math.csi.cuny.edu/pmg/\\nR Commander is a robust GUI for R http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html\\nJGR is a Java-based GUI for R http://jgr.markushelbig.org/Screenshots.html\\nTinn-R makes for a good R editor http://www.sciviews.org/Tinn-R/\\nAn Eclipse plugin for R @ http://www.walware.de/goto/statet\\nInstructions to install StatET in Eclipse http://www.splusbook.com/Rintro/R_Eclipse_StatET.pdf\\nKomodo Edit R editor http://www.sciviews.org/SciViews-K/index.html\\nhttp://www.omegahat.org/ has a very interesting list of packages that is seriously worth a look\\nCommercial versions of R @ http://www.revolution-computing.com/\\nA very informative blog http://blog.revolution-computing.com/\\nRed R for R tasks http://code.google.com/p/r-orange/\\nKNIME for R http://www.knime.org/introduction/screenshots and is worth a serious look. \\n",,
2657,16,1213,46572c8b-3b0b-4476-a08b-7534b311567f,2010-08-04 03:28:53.0,-1.0,,,
2658,5,1206,3d1be18b-a6a4-402d-bc62-6e4bdda4eac4,2010-08-04 05:17:43.0,159.0,"One practical issue that arises here is variable selection in modelling. A variable can be an important explanatory variable (e.g., is statistically significant) but may not be useful for predictive purposes (i.e., its inclusion in the model leads to worse predictive accuracy). I see this mistake almost every day in published papers.\\n\\nAnother difference is in the distinction between principal components analysis and factor analysis. PCA is often used in prediction, but is not so useful for explanation. FA involves the additional step of rotation which is done to improve interpretation (and hence explanation). There is a [nice post today on Galit Shmueli's blog about this][1].\\n\\nUpdate: a third case arises in time series when a variable may be an important explanatory variable but it just isn't available for the future. For example, home loans may be strongly related to GDP but that isn't much use for predicting future home loans unless we also have good predictions of GDP.\\n\\n  [1]: http://blog.bzst.com/2010/08/pca-debate.html",added 302 characters in body,
2659,2,1214,9a276d78-11a5-400b-a2a8-21cce176ffeb,2010-08-04 05:41:02.0,159.0,"If you really have no idea what the periodicity is, probably the best approach is to find the frequency corresponding to the maximum of the spectral density. However, the spectrum at low frequencies will be affected by trend, so you need to detrend the series first. The following R function should do the job for most series. It is far from perfect, but I've tested it on a few dozen examples and it seems to work ok. It will return 1 for data that have no strong periodicity, and the length of period otherwise.\\n\\n    find.freq <- function(x)\\n    {\\n        n <- length(x)\\n        tt <- 1:n\\n        # First de-trend. Trend not too flexible or it might capture seasonality.\\n        resid <- residuals(loess(x ~ tt))\\n    	if(n > 30)\\n    		spec <- spec.ar(c(resid),method='mle',plot=FALSE)\\n    	else\\n    		spec <- spec.ar(c(resid),method='yw',plot=FALSE)\\n    	if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.\\n    		period <- round(1/spec$freq[which.max(spec$spec)])\\n    	else\\n    		period <- 1\\n    	if(period > n)\\n    		period <- 1\\n    	return(period)\\n    }",,
2660,2,1215,3ba39a59-6791-4075-9396-6b453528b508,2010-08-04 05:41:51.0,698.0,"Statistical analysis is not my field, so forgive me if this is a very stupid question.\\n\\nI have a list of sold items by size. Shoes in this case\\n\\n	Size	Qty\\n	35	2\\n	36	1\\n	37	4\\n	38	4\\n	39	32\\n	40	17\\n	41	23\\n	42	57\\n	43	95\\n	44	90\\n	45	98\\n	46	33\\n	47	16\\n	48	4\\n	total:	476\\n\\nI have to tell the owner how much of every size to buy. The problem is, I can't say him.\\n - You should buy 95 shoes size 43 for every one of size 36...   \\n\\nThe usual practice is to buy the whole size curve and buy extras for the most selling sizes.  \\nThis is about a year worth of data. \\nHow should I present this information in an easy to understand way?\\nWhat I want to present is a general rule. Something like ""for every size curve, you should buy x additional shoes of size x"".  \\nThe idea would be to later apply this approach to other clothing items.\\n\\n\\n",,
2661,1,1215,3ba39a59-6791-4075-9396-6b453528b508,2010-08-04 05:41:51.0,698.0,Analysis of sells (what to buy),,
2662,3,1215,3ba39a59-6791-4075-9396-6b453528b508,2010-08-04 05:41:51.0,698.0,<statistical-analysis><data-visualization>,,
2663,2,1216,d38cd001-703a-4e36-97e8-0d65d879f062,2010-08-04 06:12:22.0,159.0,"I suggest he stocks $y_s$ pairs of shoes of size $s$ where $y_s$ is chosen so that the probability of running out of stock before the next delivery is set to some acceptable level (e.g., 5%).\\n\\nIt seems reasonable to assume $y_s$ is Poisson with rate $\\lambda_s$. You can estimate $\\lambda_s$ as the average sales of that size over the last few delivery periods. Then all that remains is to find the 95th percentile of the Poisson distribution for the estimated rate for each size.\\n",,
2664,2,1217,44d7e2ec-8949-49a9-8df5-1d89f3802f00,2010-08-04 06:16:41.0,183.0,"**Example:** A classic example that I have seen is in the context of predicting human performance.\\nSelf-efficacy (i.e., the degree to which a person thinks that they can perform a task well) is often a strong predictor of task performance. Thus, if you put self-efficacy into a multiple regression along with other variables such as intelligence and degree of prior experience, you often find that self-efficacy is a strong predictor.\\n\\nThis has lead some researchers to suggest that self-efficacy causes task performance. And that effective interventions are those which focus on increasing a person's sense of self-efficacy.\\n\\nHowever, the alternative theoretical model sees self-efficacy largely as a consequence of task performance. I.e., If you are good, you'll know it. In this framework interventions should focus on increasing actual competence and not perceived competence.\\n\\nThus, including a variable like self-efficacy might increase prediction, but assuming you adopt the self-efficacy-as-consequence model, it should not be included as a predictor if the aim of the model is to elucidate causal processes influencing performance.\\n\\nThis of course raises the issue of how to develop and validate a causal theoretical model. This clearly relies on multiple studies, ideally with some experimental manipulation, and a coherent argument about dynamic processes.\\n\\n**Proximal versus distal**: I've seen similar issues when researchers are interested in the effects of distal and proximal causes. Proximal causes tend to predict better than distal causes. However, theoretical interest may be in understanding the ways in which distal and proximal causes operate.\\n\\n**Variable selection issue**: Finally, a huge issue in social science research is the variable selection issue.\\nIn any given study, there is an infinite number of variables that could have been measured\\n but weren't. Thus, interpretation of models need to consider the implications of this when making theoretical interpretations.\\n\\n\\n",,
2665,2,1218,ccec067d-ebf6-4440-be53-95fe6f4a532b,2010-08-04 06:51:12.0,,I would strongly enjoin you to avoid red as an indicator: there are many sorts of colour-deficiency that make this choice problematic (see eg http://en.wikipedia.org/wiki/Color_blindness#Design_implications_of_color_blindness ).\\n\\nThe high-contrast option is I believe the best choice.\\n,,Damien Warman
2666,5,763,e2c5460a-5552-4e75-8701-297b3d87de83,2010-08-04 07:04:32.0,438.0,"The final year of the NetFlix Prize competition (2009) seemed to me to have sharply changed the general community-wide presumption against combining multiple learning algorithms. \\n\\nFor instance, my formal training (university courses) and later on-the-job oversight/mentoring taught us to avoid algorithm combination unless we had an explicit reason to do so--and ""to improve resolution of my current algorithm"", wasn't really deemed a good reason. (Others might have a different experience--of course i'm inferring a community-wide view based solely on my own experience, though my experience in coding poorly-performing ML algorithms is substantial.) \\n\\nStill, there were a few ""patterns"" in which combining algorithms in one way or another was accepted, and actually improved performance. For me, the most frequent example involved some ML algorithm configured in machine mode (assigning a class label to each data point) and in which there were more than two classes (usually many more). When for instance, using a supervised-learning algorithm  to resolve four classes, and we would see excellent separation *except for* let's say Class III versus Class IV. So out of those six decision boundaries, only one resolved below the required threshold. Particularly when classes III and IV together accounted for a small percent of the data, adding an additional algorithm *optimized just on the resolution of those two classes*, was a fairly common solution to this analytical problem type. (Usually that 'blind spot' was an inherent limitation of the primary algorithm--e.g., it was a linear classifier and the III/IV decision boundary was non-linear. \\n\\nIn other words, when we had a reliable algorithm suited to the processing environment (which was usually streaming data) and that performed within the spec except for a single blind spot that caused it to fail to resolve two (or more) classes that accounted for a small fraction of the data, then it was always better to 'bolt-on' another specialized algorithm to catch what the main algorithm was systematically missing.\\n\\nLastly, on this topic, i would like to recommend highly Chapter 17, *Combining Multiple Learners*, in ***Introduction to Machine Learning***, 2d, by Ethem Alpaydin, MIT Press, 2010. Note that this is the *second edition* published a few months ago; the first edition was published in 2004 and i doubt it has the same coverage of this topic. (Actually i recommend the entire text, but that chapter in particular since it relates to Shane's Question.)\\n\\nIn 25 pages, the author summarizes probably every ML algorithm-combination scheme whose utility has been demonstrated in the academic literature or practice--e.g., bagging, boosting, mixture of experts, stacked generalization, cascading, voting, error-correcdting,....\\n\\n",added a reference plus annotations on that reference in light of OP's comment below this answer.,
2667,5,763,84a16177-d2ee-4623-b006-0503c9301820,2010-08-04 07:39:52.0,438.0,"The final year of the NetFlix Prize competition (2009) seemed to me to have sharply changed the general community-wide presumption against combining multiple learning algorithms. \\n\\nFor instance, my formal training (university courses) and later on-the-job oversight/mentoring taught us to avoid algorithm combination unless we had an explicit reason to do so--and ""to improve resolution of my current algorithm"", wasn't really deemed a good reason. (Others might have a different experience--of course i'm inferring a community-wide view based solely on my own experience, though my experience in coding poorly-performing ML algorithms is substantial.) \\n\\nStill, there were a few ""patterns"" in which combining algorithms in one way or another was accepted, and actually improved performance. For me, the most frequent example involved some ML algorithm configured in machine mode (assigning a class label to each data point) and in which there were more than two classes (usually many more). When for instance, using a supervised-learning algorithm  to resolve four classes, and we would see excellent separation *except for* let's say Class III versus Class IV. So out of those six decision boundaries, only one resolved below the required threshold. Particularly when classes III and IV together accounted for a small percent of the data, adding an additional algorithm *optimized just on the resolution of those two classes*, was a fairly common solution to this analytical problem type. (Usually that 'blind spot' was an inherent limitation of the primary algorithm--e.g., it was a linear classifier and the III/IV decision boundary was non-linear. \\n\\nIn other words, when we had a reliable algorithm suited to the processing environment (which was usually streaming data) and that performed within the spec except for a single blind spot that caused it to fail to resolve two (or more) classes that accounted for a small fraction of the data, then it was always better to 'bolt-on' another specialized algorithm to catch what the main algorithm was systematically missing.\\n\\nLastly, on this topic, i would like to recommend highly Chapter 17, *Combining Multiple Learners*, in ***[Introduction to Machine Learning][1]***, 2d, by Ethem Alpaydin, MIT Press, 2010. Note that this is the *second edition* published a few months ago; the first edition was published in 2004 and i doubt it has the same coverage of this topic. (Actually i recommend the entire text, but that chapter in particular since it relates to Shane's Question.)\\n\\nIn 25 pages, the author summarizes probably every ML algorithm-combination scheme whose utility has been demonstrated in the academic literature or practice--e.g., bagging, boosting, mixture of experts, stacked generalization, cascading, voting, error-correcdting,....\\n\\n\\n  [1]: http://www.cmpe.boun.edu.tr/~ethem/i2ml2e/",added link to textbook i added earlier,
2668,5,1216,934bdaba-0a0f-4ab8-b9c6-29db9ca7da01,2010-08-04 08:13:41.0,159.0,"I suggest he stocks $y_s$ pairs of shoes of size $s$ where $y_s$ is chosen so that the probability of running out of stock before the next delivery is set to some acceptable level (e.g., 5%).\\n\\nIt seems reasonable to assume $y_s$ is Poisson with rate $\\lambda_s$. You can estimate $\\lambda_s$ as the average sales of that size over the last few delivery periods. Then all that remains is to find the 95th percentile of the Poisson distribution with mean equal to the estimated rate for each size.\\n",added 15 characters in body,
2669,2,1219,2e19bce7-f769-403d-befe-5276dad496c6,2010-08-04 08:20:14.0,702.0,"A very good article explaining the general approach of LMMs and their advantage over ANOVA is:\\n\\n - Baayen, R. H., Davidson, D. J., & Bates, D. M. (2008). [Mixed-effects modeling with crossed random effects for subjects and items][1]. *Journal of Memory and Language*, *59*, 390-412.\\n\\nLinear mixed-effects models (LMMs) generalize regression models to have residual-like components, random effects, at the level of, e.g., people or items and not only at the level of individual observations. The models are very flexible, for instance allowing the modeling of varying slopes and intercepts.\\n\\nLMMs work by using a likelihood function of some kind, the probability of your data given some parameter, and a method for maximizing this (Maximum Likelihood Estimation; MLE) by fiddling around with the parameters.  MLE is a very general technique allowing lots of different models, e.g., those for binary and count data, to be fitted to data, and is explained in a number of places, e.g.,\\n\\n - Agresti, A. (2007). *An Introduction to Categorical Data Analysis (2nd Edition)*. John Wiley & Sons.\\n\\nLMMs, however, can't deal with non-Gaussian data like binary data or counts; for that you need Generalized Linear Mixed-effects Models (GLMMs).  One way to understand these is first to look into GLMs; also see Agresti (2007).\\n\\n\\n  [1]: http://www.ualberta.ca/~baayen/publications/baayenDavidsonBates.pdf",,
2670,2,1220,bc6cbeea-37b2-4383-bf56-1ec356c5dfe4,2010-08-04 08:20:54.0,8.0,"Following on from Rob's answers, you could estimate an acceptable level using a normal approximation. So if an acceptable rate of running out is 5%, then use the following rule:\\n\\nQty + 1.644 * Qty^(0.5)\\n\\nThe value 1.644 comes from the [Normal][1] or Gaussian distribution.\\n\\nOur acceptable rates could be:\\n\\n1. 1%: change 1.644 to 2.326\\n1. 10%: change 1.644 to 1.282\\n\\n**Further Rationale**\\n\\nRob suggested that you could model your data using a Poisson distribution. When the rate (or in you case Qty) is large (say 10), then the Normal distribution gives a good approximation. The following graph shows the Poisson 95th percentile (red) and the Normal approximation in green. As you can see they are fairly close. The raw data is shown as points.\\n\\n![Normal approximation][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [2]: http://img821.imageshack.us/img821/3636/tmps.jpg",,
2671,5,1220,c1e06e5e-8401-4653-97ac-15265f467de9,2010-08-04 08:33:07.0,8.0,"Following on from Rob's answer, you could estimate an acceptable level using a Normal approximation. So if an acceptable rate of running out is 5%, then use the following rule:\\n\\nQty + 1.644 * Qty^(0.5)\\n\\nThe value 1.644 comes from the [Normal][1] or Gaussian distribution.\\n\\nOther acceptable rates could be:\\n\\n1. 1%: change 1.644 to 2.326\\n1. 10%: change 1.644 to 1.282\\n\\n**Further Rationale**\\n\\nRob suggested that you could model your data using a Poisson distribution. When the rate (or in your case Qty) is large (say 10), then the Normal distribution gives a good approximation. The following graph shows the Poisson 95th percentile (red) and the Normal approximation in green. As you can see they are fairly close. The raw data is shown as points.\\n\\n![Normal approximation][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution\\n  [2]: http://img821.imageshack.us/img821/3636/tmps.jpg",added 2 characters in body,
2672,2,1221,6d6cfee8-6d41-4eaf-8899-5c06c4b5e486,2010-08-04 08:39:21.0,521.0,"In the twins example it is not just the correlation that suggests causality, but also the associated information or prior knowledge. \\n\\nSuppose I add one further piece of information. Assume that the diligent twin spent 6 hours studying for a stats exam, but due to an unfortunate error the exam was in history. Would we still conclude the study was the cause of the superior performance?\\n\\nDetermining causality is as much a philosophical question as a scientific one, hence the tendency to invoke philosophers such as David Hume and Karl Popper when causality is discussed. \\n\\nNot surprisingly medicine has made significant contributions to establishing causality through heuristics, such as <a href=""http://en.wikipedia.org/wiki/Koch%27s_Postulates"" rel=""nofollow"">Koch's postulates for establishing the causal relationship between microbes and disease</a>. These have been extended to ""molecular Koch's postulates"" required to show that a gene in a pathogen encodes a product that contributes to the disease caused by the pathogen. \\n\\nUnfortunately I can't post a second hyperlink for ""molecular Koch's postulates"" supposedly beCAUSE I'm a new user (not true) and don't have enough ""reputation points"". The real reason is anybody's guess.\\n ",,
2673,5,1221,79345ac3-a8e6-48c3-80b0-ba97e78a956d,2010-08-04 08:44:43.0,521.0,"In the twins example it is not just the correlation that suggests causality, but also the associated information or prior knowledge. \\n\\nSuppose I add one further piece of information. Assume that the diligent twin spent 6 hours studying for a stats exam, but due to an unfortunate error the exam was in history. Would we still conclude the study was the cause of the superior performance?\\n\\nDetermining causality is as much a philosophical question as a scientific one, hence the tendency to invoke philosophers such as David Hume and Karl Popper when causality is discussed. \\n\\nNot surprisingly medicine has made significant contributions to establishing causality through heuristics, such as <a href=""http://en.wikipedia.org/wiki/Koch%27s_Postulates"" rel=""nofollow"">Koch's postulates</a> for establishing the causal relationship between microbes and disease. \\nThese have been extended to ""molecular Koch's postulates"" required to show that a gene in a pathogen encodes a product that contributes to the disease caused by the pathogen. \\n\\nUnfortunately I can't post a hyperlinks supposedly beCAUSE I'm a new user (not true) and don't have enough ""reputation points"". The real reason is anybody's guess.\\n",deleted 201 characters in body; deleted 2 characters in body; added 164 characters in body,
2674,5,397,d55956cb-2f1d-4428-b00f-ad6a6cf1168b,2010-08-04 08:50:40.0,8.0,"Just to add a bit to honk's answer, the [Diehard Test Suite][1] (developed by George Marsaglia) are the standard tests for PRNG.\\n\\nThere's a nice [Diehard C library][2] that gives you access to these tests. As well as the standard Diehard tests it also provides functions for a few other PRNG tests involving (amongst other things) checking bit order. There is also a facilty for testing the speed of the RNG and writing your own tests.\\n\\nThere is a R interface to the Diehard library, called [RDieHarder][3]:\\n\\n\\n    library(RDieHarder)\\n    dhtest = dieharder(rng=""randu"", test=10, psamples=100, seed=12345)\\n    print(dhtest)\\n\\n	Diehard Count the 1s Test (byte)\\n\\n           data:  Created by RNG `randu' with seed=12345, \\n                  sample of size 100 p-value < 2.2e-16\\n\\nThis shows that the [RANDU][4] RNG generator fails the minimum-distance / 2dsphere test. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Diehard_tests\\n  [2]: http://www.phy.duke.edu/~rgb/General/dieharder.php\\n  [3]: http://dirk.eddelbuettel.com/code/rdieharder.html\\n  [4]: http://en.wikipedia.org/wiki/RANDU",deleted 7 characters in body; added 380 characters in body,
2675,2,1222,9ae010d0-6e9d-4630-b69b-b4835198203c,2010-08-04 09:07:44.0,521.0,"I can't post a comment (the appropriate place for this comment) as I don't have enough reputation, but the answer accepted as the best answer by the question owner misses the point.\\n\\n""If statistics is all about maximizing likelihood, then machine learning is all about minimizing loss.""\\n\\nThe likelihood is a loss function. Maximising likelihood is the same as minimising a loss function: the deviance, which is just -2 times the log-likelihood function. Similarly finding a least squares solution is about minimising the loss function describing the residual sum of squares. \\n\\nBoth ML and stats use algorithms to optimise the fit of some function (in the broadest terms) to data. Optimisation necessarily involves minimising some loss function.",,
2676,5,270,622677a3-70ab-44fa-a5c6-98c45bd22f36,2010-08-04 09:50:49.0,90.0,"Due to the factorial in a poisson distribution, it becomes unpractical to estimate poisson models (for example, using maximum likelihood) when the observations are large. So, for example, if I am trying to estimate a model to explain the number of suicides in a given year (only annual data are available), and say, there are thousands of suicides every year, is it wrong to express suicides in hundreds, so that 2998 would be 29.98 ~= 30? In other words, is it wrong to change the unit of measurement to make the data manageable? ",edited body,
2677,2,1223,fea53eec-e7c3-4edf-b762-5ec40944ab5b,2010-08-04 10:02:35.0,127.0,"I'm looking for some robust techniques to remove outliers and errors (whatever the cause) from financial time-series data (i.e. tickdata). \\n\\nTick-by-tick financial time-series data is very messy. It contains huge (time) gaps when the exchange is closed, and make huge jumps when the exchange opens again. When the exchange is open, all kinds of factors introduce trades at price levels that are wrong (they did not occur) and/or not representative of the market (a spike because of an incorrectly entered bid or ask price for example). [This paper by tickdata.com][1] (PDF) does a good job of outlining the problem, but offers few concrete solutions.\\n\\nMost papers I can find online that mention this problem either ignore it (the tickdata is assumed filtered) or include the filtering as part of some huge trading model which hides any useful filtering steps.\\n\\nIs anybody aware of more in-depth work in this area?\\n\\n\\n  [1]: http://www.tickdata.com/pdf/Tick_Data_Filtering_White_Paper.pdf",,
2678,1,1223,fea53eec-e7c3-4edf-b762-5ec40944ab5b,2010-08-04 10:02:35.0,127.0,Robust outlier detection in financial timeseries,,
2679,3,1223,fea53eec-e7c3-4edf-b762-5ec40944ab5b,2010-08-04 10:02:35.0,127.0,<time-series><outliers>,,
2680,2,1224,334479be-1df6-4f16-89ae-d17296aa6a53,2010-08-04 10:38:03.0,,With data from two centres I want to account for potential heterogeneity or confounders between two centers. So the analysis will initially be stratified by clinical center and a chi square test performed with one degree of freedom.  Is this appropriate with just two centres? Or is there an alternative?\\n,,user712
2681,1,1224,334479be-1df6-4f16-89ae-d17296aa6a53,2010-08-04 10:38:03.0,,Heterogeneity with two studies,,user712
2682,3,1224,334479be-1df6-4f16-89ae-d17296aa6a53,2010-08-04 10:38:03.0,,<hypothesis-testing>,,user712
2683,2,1225,107ec969-3220-4d67-82f3-cafb661c3659,2010-08-04 10:55:31.0,686.0,"*Statistics as Principled Argument* by Abelson is a good side book to learning statistics, particularly if your substantive field is in the social sciences.  It won't teach you how to do analysis, but it will teach you about statistical thinking.\\n\\nI reviewed this book [here][1]\\n\\n\\n  [1]: http://www.associatedcontent.com/article/2225261/book_review_statistics_as_principled.html?cat=58",,
2684,16,1225,107ec969-3220-4d67-82f3-cafb661c3659,2010-08-04 10:55:31.0,-1.0,,,
2685,16,1126,d5fff605-793c-4939-bb21-1d81b57326a7,2010-08-04 11:38:07.0,660.0,,,
2686,2,1226,3fe78c0c-5d69-4463-818d-e4a91141dc59,2010-08-04 11:48:13.0,319.0,"There are two parts to testing a random number generator. If you're only concerned with testing a uniform generator, then yes, something like the DIEHARD test suite is a good idea.\\n\\nBut often you need to test a transformation of a uniform generator.  For example, you might use a uniform generator to create exponentially or normally distributed values.  You may have a high-quality uniform generator -- say you have a trusted implementation of a well-known algorithm such as Mersenne Twister -- but you need to test whether the transformed output has the right distribution.  In that case you need to do some sort of goodness of fit test such as Kolmogorov-Smirnov.  But for starters, you could verify that the sample mean and variance have the values you expect.\\n\\nMost people don't -- and shouldn't -- write their own uniform random number generator from scratch.  It's hard to write a good generator and easy to fool yourself into thinking you've written a good one when you haven't.  For example, Donald Knuth tells the story in [TAOCP volume 2][1] of a random number generator he wrote that turned out to be awful. But it's common for people to have to write their own code to produce random values from a new distribution. \\n\\n\\n  [1]: http://www.amazon.com/gp/product/0201896842?ie=UTF8&tag=theende-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=0201896842",,
2688,5,1099,c8c6ca19-a76e-4ae4-b123-355e8a840d7a,2010-08-04 12:00:04.0,588.0,"I am working on some kind of disease infection data, and I am puzzled on how to handle the data as ""categorical"" or ""continuous"".\\n\\n""Infection Count"" \\n\\n - the number of infection cases found in a specific period of time, the count is generated from categorical data (i.e. no. of patient tagged as ""infected"")\\n\\n\\n\\n""Patient Bed Days"" \\n\\n - sum of total number of day stay in the ward by all patients in that ward, again, the count is generated from categorical data (i.e. no. of patient tagged as ""staying in that particular ward"")\\n\\n""infection per patient bed days""\\n\\n - ""infection count"" / ""patient bed days""\\nboth were originally count data, but now becomes a rate\\n\\nCan I still use Chi-Square here to find if the difference in ""infection per patient bed days"" is statistically significant or not?\\n\\nThanks!\\n\\n***\\nUpdates\\n\\nThanks for the comments!\\n\\nI have googled a bit and found that I can compare the incidence rate (or call it infection rate), but doing something like ""incidence rate difference"" (IRD) or ""incidence rate ratio"" (IRR). (I found it from [here][1])\\n\\nWhat is the difference between IRD and t-test? And is there any statistical test complementary for IRR?\\n\\nThanks again!\\n\\n\\n  [1]: http://www.statsdirect.com/help/rates/incidence_rates.htm",added 459 characters in body; edited tags,
2689,6,1099,c8c6ca19-a76e-4ae4-b123-355e8a840d7a,2010-08-04 12:00:04.0,588.0,<beginner><categorical-data><count-data>,added 459 characters in body; edited tags,
2690,2,1228,d348a938-e8fa-41ba-8b25-f883c914f8db,2010-08-04 12:05:30.0,588.0,"as title, I am using control chart to try to work on some infection data, and will raise alert if the infection is considered ""out of control"".\\n\\nProblems arrive when I come to a set of data that most of the time points have zero infection, with only a few occasion of one to two infections, but it already exceed the control limit of the chart, and raise alert also.\\n\\nHow should I work on the control chart if the data set is having very few infection count?\\n\\nThanks!",,
2691,1,1228,d348a938-e8fa-41ba-8b25-f883c914f8db,2010-08-04 12:05:30.0,588.0,how deal interpret control chart if the status of statistical control have the value of zero?,,
2692,3,1228,d348a938-e8fa-41ba-8b25-f883c914f8db,2010-08-04 12:05:30.0,588.0,<statistical-analysis><data-visualization>,,
2693,2,1229,839cb448-8461-44f8-8f23-815e6ab96c35,2010-08-04 12:32:08.0,442.0,"The best intro in my eyes is the following one:\\n\\n> David Howell - Statistical Methods for\\n> Psychology\\n\\n\\nIt is the BEST in making statistical concepts understandable for non mathematicians so that they get the math afterwards!\\nUnfortunately it is updated every year and, hence, pricey.",,
2694,16,1229,839cb448-8461-44f8-8f23-815e6ab96c35,2010-08-04 12:32:08.0,-1.0,,,
2695,2,1230,26d53169-71b7-4f95-a831-e4c94eea878c,2010-08-04 12:32:12.0,358.0,"I am a big fan of [Statistical Models - Theory and Practice][1] by David Friedman. It  succeeds remarkably well to introduce and motivate the different concepts of statistical modeling through concrete, and historically important problems (cholera in London, Yule on the causes of poverty, Political repression in the McCarty era..). \\n\\nFriedman illustrates the principles of modeling, and the pitfalls. In some sense, the discussion shows how to think about the critical issues and is honest about the connection between the statistical models and the real world phenomena.\\n\\n  [1]: http://www.amazon.com/Statistical-Models-Practice-David-Freedman/dp/0521671051",,
2696,16,1230,26d53169-71b7-4f95-a831-e4c94eea878c,2010-08-04 12:32:12.0,-1.0,,,
2697,5,1228,8289e79f-39db-4b66-a4df-63def9f24bd2,2010-08-04 12:34:06.0,159.0,"I am using a control chart to try to work on some infection data, and will raise an alert if the infection is considered ""out of control"".\\n\\nProblems arrive when I come to a set of data where most of the time points have zero infection, with only a few occasions of one to two infections, but these already exceed the control limit of the chart, and raise an alert.\\n\\nHow should I work on the control chart if the data set is having very few positive infection counts?\\n\\nThanks!",Tried to fix grammar.,
2698,4,1228,8289e79f-39db-4b66-a4df-63def9f24bd2,2010-08-04 12:34:06.0,159.0,How to interpret a control chart containing a majority of zero values?,Tried to fix grammar.,
2699,6,1228,8289e79f-39db-4b66-a4df-63def9f24bd2,2010-08-04 12:34:06.0,159.0,<statistical-analysis><data-visualization><control-chart>,Tried to fix grammar.,
2700,2,1231,af38187f-7a0e-4e78-8f56-50a361a27d0b,2010-08-04 12:52:05.0,8.0,"You are asking quite a tricky question!\\n\\nThis is outside my area of expertise, but I know that [Prof Farrington][1] does some work on this problem. So I would look at a some of his papers and follow a few of his references. To get you started, this [report][2] looks relevant.\\n\\n\\n  [1]: http://www.mcs.open.ac.uk/People/c.p.farrington\\n  [2]: http://stats-www.open.ac.uk/TechnicalReports/Cusum.pdf",,
2701,2,1232,fa9f4ef6-bac4-40ba-ab06-850df70ff21f,2010-08-04 12:56:10.0,210.0,Would it make sense to plot the control chart based on an average of the weekly infections or another similar floating average? Would this then 'damp' out spikes due to daily high values whilst ensuring that changes in trends are picked up in a relatively timely manner. ,,
2702,2,1233,f75c84c2-6b08-45f6-b884-fd37411331bc,2010-08-04 13:18:12.0,,"Perhaps, you can build an edge case in your routine/software to deal with the situation. If you detect several zeros in the dataset then you set a separate control for that particular situation. This is obviously a hack and not a principled solution but may serve your present needs till you can come up with something better. ",,user28
2703,2,1234,1acf0a45-b9f6-4d9d-ad89-a0d6af2fce19,2010-08-04 13:32:07.0,603.0,"Convert your price data to (percentage) returns,\\nthen remove all the points lying at more than 3 MAD from the median.\\n\\n\\nhttp://en.wikipedia.org/wiki/Mean_absolute_deviation\\n\\n",,
2704,5,1223,66e3ad09-0d3f-4009-8e88-f463c21db4b7,2010-08-04 13:43:26.0,127.0,"I'm looking for some robust techniques to remove outliers and errors (whatever the cause) from financial time-series data (i.e. tickdata). \\n\\nTick-by-tick financial time-series data is very messy. It contains huge (time) gaps when the exchange is closed, and make huge jumps when the exchange opens again. When the exchange is open, all kinds of factors introduce trades at price levels that are wrong (they did not occur) and/or not representative of the market (a spike because of an incorrectly entered bid or ask price for example). [This paper by tickdata.com][1] (PDF) does a good job of outlining the problem, but offers few concrete solutions.\\n\\nMost papers I can find online that mention this problem either ignore it (the tickdata is assumed filtered) or include the filtering as part of some huge trading model which hides any useful filtering steps.\\n\\nIs anybody aware of more in-depth work in this area?\\n\\n**Update:** [this questions][2] seems similar on the surface but:\\n\\n - Financial time series is (at least at the tick level) non-periodic.\\n - The opening effect is a big issue because you can't simply use the last day's data as initialisation even though you'd really like to (because otherwise you have nothing). External events might cause the new day's opening to differ dramatically both in absolute level, and in volatility from the previous day.\\n - Wildly irregular frequency of incoming data. Near open and close of the day the amount of datapoints/second can be 10 times higher than the average during the day. The other question deals with regularly sampled data.\\n - The ""outliers"" in financial data exhibit some specific patterns that could be detected with specific techniques not applicable in other domains and I'm -in part- looking for those specific techniques.\\n - In more extreme cases (e.g. the flash crash) the outliers might amount to more than 75% of the data over longer intervals (> 10 minutes). In addition, the (high) frequency of incoming data contains some information about the outlier aspect of the situation.\\n\\n  [1]: http://www.tickdata.com/pdf/Tick_Data_Filtering_White_Paper.pdf\\n  [2]: http://stats.stackexchange.com/questions/1142/simple-algorithm-for-online-outlier-detection-of-a-generic-time-series",updated to address comments.,
2705,2,1235,2bcf1483-7614-47e1-a3b2-576cd6f8f011,2010-08-04 13:47:36.0,5.0,Definitely start by working with returns. This is critical to deal with the irregular spacing where you can naturally get big price gaps (especially around weekends). Then you can apply a simple filter to remove returns well outside the norm (eg. vs a high number of standard deviations). The returns will adjust to the new absolute level so large real changes will result in the loss of only one tick. I suggest using a two-pass filter with returns taken from 1 step and *n* steps to deal with clusters of outliers. ,,
2706,5,1235,2907c846-e63a-491e-a844-8f39c2fe36d9,2010-08-04 13:55:08.0,5.0,"I'll add some paper references when I'm back at a computer, but here are some simple suggestions:\\n\\nDefinitely start by working with returns. This is critical to deal with the irregular spacing where you can naturally get big price gaps (especially around weekends). Then you can apply a simple filter to remove returns well outside the norm (eg. vs a high number of standard deviations). The returns will adjust to the new absolute level so large real changes will result in the loss of only one tick. I suggest using a two-pass filter with returns taken from 1 step and *n* steps to deal with clusters of outliers. ",added 101 characters in body,
2707,2,1236,19803b30-e87e-4241-8f9e-de677dd22f08,2010-08-04 14:02:21.0,124.0,"As a biologist, I found the Sokal and Rohlf text to be quite readable, despite its voluminous-ness. It's not so great as a quick reference, but does walk one through statistical theory.\\n\\nR. R. Sokal and F. J. Rohlf, Biometry the principles and practice of statistics in biological research, Third. (New York: W.J. Freeman and Company, 1995).\\n\\n",,
2708,16,1236,19803b30-e87e-4241-8f9e-de677dd22f08,2010-08-04 14:02:21.0,-1.0,,,
2709,2,1237,9c3cbacd-2a33-4ee0-abba-400e23f515e2,2010-08-04 14:32:12.0,196.0,"I usually like to run simulations to answer questions like this, but without confirmed details of the algorithm the question asker wants evaluated and no obvious implementation of the Holm-Sidak procedure available in R, that is not possible.  For my answer I eyeballed the code provided [here][1].\\n\\nFeel free to correct me. I usually like to run simulations on such things. But, without the ability to readily do that, I can't check my answer. So, I might be entirely wrong here.  My answer is that usually the Holm-Sidak will demonstrate greater power, but that the answer in a strict sense is ""it depends"".  Both methods use the pooled error term and assume homogeneity of variance, so there is no difference in the procedures there.  However, since Holm-Sidak adjusts in a stepwise manner early comparisons are more likely to pass a threshold of significance than later comparisons, especially since the freedom to assess later comparisons is dependent on the outcome of previous comparisons.  Thus, it seems likely that in situations where the differences in means between groups to be compared is roughly equal (and meets the Tukey HSD threshold for significance) and the number of groups is sufficiently large (purposely vague without a simulation), that Holm-Sidak will fail to reach significance for the later comparisons.  Thus, in these situations Tukey's HSD will have more power than Holm-Sidak.\\n\\n  [1]: http://www.mathworks.com/matlabcentral/fileexchange/12786",,
2710,5,1237,fae4b498-64c3-475a-ae62-adb7160d6676,2010-08-04 14:38:22.0,196.0,"I usually like to run simulations to answer questions like this, but without confirmed details of the algorithm the question asker wants evaluated and no obvious implementation of the Holm-Sidak procedure available in R, that is not possible.  For my answer I eyeballed the code provided [here][1].  Assuming that is the right procedure, and assuming the null hypothesis is that all group means are equal:\\n\\nFeel free to correct me. I usually like to run simulations on such things. But, without the ability to readily do that, I can't check my answer. So, I might be entirely wrong here.  My answer is that usually the Holm-Sidak will demonstrate greater power, but that the answer in a strict sense is ""it depends"".  Both methods use the pooled error term and assume homogeneity of variance, so there is no difference in the procedures there.  However, since Holm-Sidak adjusts in a stepwise manner early comparisons are more likely to pass a threshold of significance than later comparisons, especially since the freedom to assess later comparisons is dependent on the outcome of previous comparisons.  Thus, it seems likely that in situations where the differences in means between groups to be compared is roughly equal (and meets the Tukey HSD threshold for significance) and the number of groups is sufficiently large (purposely vague without a simulation), that Holm-Sidak will fail to reach significance for the later comparisons.  Thus, in these situations Tukey's HSD will have more power than Holm-Sidak.\\n\\n  [1]: http://www.mathworks.com/matlabcentral/fileexchange/12786",added 107 characters in body,
2711,2,1238,26d2ac7d-3491-4ce9-bd9c-3d8dcf5c9507,2010-08-04 14:47:27.0,729.0,"The [NIST publishes a list of statistical tests][1] with a reference implementation in C.\\n\\nThere is also TestU01by some smart folks, including respected PRNG researcher Pierre L'Ecuyer. Again, there is a reference implementation in C.\\n \\nAs pointed out by other commenters, these are for testing the generation of pseudo random bits. If you transform these bits into a different random variable (e.g. Box-Muller transform from uniform to Normal), you'll need additional tests to confirm the correctness of the transform algorithm.\\n\\n\\n  [1]: http://csrc.nist.gov/groups/ST/toolkit/rng/stats_tests.html\\n",,
2712,5,1238,d7904bd2-60ff-4b67-9120-3cedab4e6ac4,2010-08-04 14:49:15.0,,"The [NIST publishes a list of statistical tests][1] with a reference implementation in C.\\n\\nThere is also [TestU01][2] by some smart folks, including respected PRNG researcher Pierre L'Ecuyer. Again, there is a reference implementation in C.\\n \\nAs pointed out by other commenters, these are for testing the generation of pseudo random bits. If you transform these bits into a different random variable (e.g. Box-Muller transform from uniform to Normal), you'll need additional tests to confirm the correctness of the transform algorithm.\\n\\n\\n  [1]: http://csrc.nist.gov/groups/ST/toolkit/rng/stats_tests.html\\n  [2]: http://www.iro.umontreal.ca/~simardr/testu01/tu01.html",fixed the link to TestU01,user28
2716,2,1240,8c6033d8-bf85-4cb9-ac08-f7a264674494,2010-08-04 15:11:13.0,334.0,"Small correction to Colin's post: the CRAN package \\n[RDieHarder](http://cran.r-project.org/package=RDieHarder) is an interface to \\n[**DieHarder**](http://www.phy.duke.edu/~rgb/General/dieharder.php), the Diehard rewrite / extension / overhaul done by [Robert G. Brown](http://www.phy.duke.edu/~rgb/) (who kindly lists me as a coauthor based on my RDieHarder wrappers) with recent contribution by David Bauer.  \\n\\nAmong other things, DieHarder includes the [NIST battery of tests](http://csrc.nist.gov/groups/ST/toolkit/rng/stats_tests.html) mentioned in Mark's post as well as some new ones.  This is ongoing research and has been for a while.  I gave a talk at useR! 2007 about RDieHarder which you can get from [here](http://dirk.eddelbuettel.com/presentations.html).",,
2717,2,1241,2a81ea43-c077-4d41-b9d0-0a59f1c20c65,2010-08-04 15:13:27.0,684.0,"When would you tend to use ROC curves over some other tests to determine the predictive ability of some measurement on an outcome?\\n\\nWhen dealing with discrete outcomes (alive/dead, present/absent), what makes ROC curves more or less powerful than something like a chi-square?",,
2718,1,1241,2a81ea43-c077-4d41-b9d0-0a59f1c20c65,2010-08-04 15:13:27.0,684.0,What do ROC curves tell you that traditional inference wouldn't?,,
2719,3,1241,2a81ea43-c077-4d41-b9d0-0a59f1c20c65,2010-08-04 15:13:27.0,684.0,<regression>,,
2721,2,1243,afd5ddfd-9afd-4e1b-aed3-d6f12ebe7d19,2010-08-04 15:24:11.0,732.0,"I think the main point is the one sidmaestro raises...does the experimental setup or data generation mechanism support the premise that the data might arise from a Poisson distribution. \\n\\nI'm not a big fan of testing for distributional assumptions, since those tests typically aren't very useful. What seems more useful to me is to make distributional or model assumptions that are flexible and reasonably robust to deviations from the model, typically for purposes of inference. In my experience, it is not that common to see mean=variance, so often the negative binomial model seems more appropriate, and includes the Poisson as a special case. \\n\\nAnother point that is important in going for distributional testing, if that's what you want to do, is to make sure that there aren't strata involved which would make your observed distribution a mixture of other distributions. Individual stratum-specific distributions might appear Poisson, but the observed mixture might not be. An analogous situation from regression only assumes that the **conditional** distribution of Y|X  is normally distributed, and not really the distribution of Y itself.",,
2722,5,1235,bd1cb757-0183-4d73-8e83-db01a0baccd0,2010-08-04 15:37:05.0,5.0,"I'll add some paper references when I'm back at a computer, but here are some simple suggestions:\\n\\nDefinitely start by working with returns. This is critical to deal with the irregular spacing where you can naturally get big price gaps (especially around weekends). Then you can apply a simple filter to remove returns well outside the norm (eg. vs a high number of standard deviations). The returns will adjust to the new absolute level so large real changes will result in the loss of only one tick. I suggest using a two-pass filter with returns taken from 1 step and *n* steps to deal with clusters of outliers.\\n\\n*Edit 1:* Regarding the usage of prices rather than returns: asset prices tend to not be stationary, so IMO that can pose some additional challenges. To account for the irregularity and power law effects, I would advise some kind of adjustment if you want to include them in your filter. You can scale the price changes by the time interval or by volatility.  You can refer to the ""realized volatility"" literture for some discussion on this.  Also discussed in Dacorogna et. al.   ",added 484 characters in body,
2723,2,1244,a3726668-cf02-406d-b2b8-195eb57123d4,2010-08-04 15:37:40.0,732.0,"The way I think about this really is in terms of information. Say each of X1 and X2 has some information about Y. The more correlated X1 and X2 are with each other, the more the information content about Y from X1 and X2 are similar or overlapping, to the point that for perfectly correlated X1 and X2, it really is the same information content. If we now put X1 and X2 in the same (regression) model to explain Y, the model tries to ""apportion"" the information that (X1,X2) contains about Y to each of X1 and X2, in a somewhat arbitrary manner. There is no really good way to apportion this, since any split of the information still leads to keeping the total information from (X1,X2) in the model (for perfectly correlated X's, this really is a case of non-identifiability). This leads to unstable individual estimates for the individual coefficients of X1 and X2, though if you look at the predicted values b1X1+b2X2 over many runs and estimates of b1 and b2, these will be quite stable. ",,
2724,5,1235,00d0a72a-b18b-4f8b-addb-61c60b2ae951,2010-08-04 15:45:37.0,5.0,"I'll add some paper references when I'm back at a computer, but here are some simple suggestions:\\n\\nDefinitely start by working with returns. This is critical to deal with the irregular spacing where you can naturally get big price gaps (especially around weekends). Then you can apply a simple filter to remove returns well outside the norm (eg. vs a high number of standard deviations). The returns will adjust to the new absolute level so large real changes will result in the loss of only one tick. I suggest using a two-pass filter with returns taken from 1 step and *n* steps to deal with clusters of outliers.\\n\\n*Edit 1:* Regarding the usage of prices rather than returns: asset prices tend to not be stationary, so IMO that can pose some additional challenges. To account for the irregularity and power law effects, I would advise some kind of adjustment if you want to include them in your filter. You can scale the price changes by the time interval or by volatility.  You can refer to the ""realized volatility"" literture for some discussion on this.  Also discussed in Dacorogna et. al.   \\n\\nTo account for the changes in volatility, you might try basing your volatility calculation from the same time of the day over the past week (using the seasonality).  ",added 170 characters in body,
2725,2,1245,a4e65f4e-2b0f-4e90-aa96-a6edfc611e61,2010-08-04 15:51:53.0,334.0,"The problem is definitely *hard*. \\n\\nMechanical rules like the +/- *N1* times standard deviations, or +/ *N2* times MAD,  or +/- *N3* IQR or ... *will* fail because there are always some series that are different as for example:\\n\\n - fixings like interbank rate may be constant for some time and then jump all of a sudden\\n - similarly for *e.g.* certain foreign exchanges coming off a peg\\n - certain instrument are implicitly spreads; these may be near zero for periods and all of a sudden jump manifold\\n\\nBeen there, done that, ... in a previous job.  You could try to bracket each series using arbitrage relations ships (*e.g.* assuming USD/EUR and EUR/JPY are presumed good, you can work out bands around what USD/JPY should be; likewise for derivatives off an underlying etc pp.\\n\\nCommercial data vendors expand some effort on this, and those of use who are clients of theirs know ... it still does not exclude errors.\\n\\n",,
2726,2,1246,83eb6fa2-ab35-4c30-8e32-ed2e438485a3,2010-08-04 15:55:11.0,279.0,"An ROC curve is used when the predictor is continuous and the outcome is discrete, so a chi-square test would not be applicable. In fact, ROC analysis is in some sense equivalent to the Mann-Whitney test: the area under the curve is P(X>Y) which is the quantity being tested by the M-W test. However Mann-Whitney analysis does not emphasize selecting a cutoff, while that is the main point of the ROC analysis. Additionally, ROC curves are often used as just a visual display of the predictive ability of a covariate.",,
2727,2,1247,df948340-ad8e-465c-9f29-6c980b8d964f,2010-08-04 15:56:39.0,601.0,The shortest answer is that traditional tests of signal detection only give you a single point on the ROC (receiver operating characteristic) while the curve allows you to see responses through a range of values.  It's possible that the criteria and d' shift as one shifts throughout the curve.  It's like the difference between a t-test generated by selecting two classes of predictor variables and two regression lines generated by looking at parametric manipulations of each predictor variable.,,
2728,2,1248,894327e3-b418-428a-93d8-8a7e85f61d6b,2010-08-04 15:57:30.0,279.0,"Use regression methods. A simple linear regression with group coded as 0-1 (or 1-2, etc) is equivalent to a t-test, but regression software usually has the capability to incorporate weigths correctly.",,
2729,2,1249,5c8a3d7b-570f-4159-bdef-27e1d4652a29,2010-08-04 16:04:56.0,168.0,"How to find a non-trivial upper bound on $E[\\exp(Z^2)]$ when $Z \\sim {\\rm Bin}(n, n^{-\\beta})$ with $\\beta \\in (0,1)$? A trivial bound is obtained for substituting $Z$ with $n$.\\n\\nA background on this question. In the paper by Baraud, 2002 -- Non-asymptotic minimax rates of testing in signal detection, if one is to substitute the model in Eq. (1), by a random effects model, then the above quantity appears in the computation of a lower bound. ",,
2730,1,1249,5c8a3d7b-570f-4159-bdef-27e1d4652a29,2010-08-04 16:04:56.0,168.0,"Non-trivial bound for $E[\\exp(Z^2)]$ when $Z \\sim {\\rm Bin}(n, n^{-\\beta})$ with $\\beta \\in (0,1)$",,
2731,3,1249,5c8a3d7b-570f-4159-bdef-27e1d4652a29,2010-08-04 16:04:56.0,168.0,<binomial>,,
2733,2,1251,0000232e-1cd7-43ce-991a-4810c1d81e86,2010-08-04 16:29:26.0,8.0,"Thanks for all you answers. For completeness I thought I should include what I usually do. I tend to do a combination of the suggestions given: dots, boxplots (when n is large), and se (or sd) ranges.\\n\\n![Dot plot][1]\\n\\nFrom the dot plot, it is clear that data is far more spread out than what ""handle bar"" plots suggest. In fact there is a negative value in A3. \\n\\n\\n\\n\\n\\n  [1]: http://img594.imageshack.us/img594/6883/tmpsc.jpg",,
2734,2,1252,f4611157-f524-4375-ac43-3c23318cdef6,2010-08-04 16:39:49.0,59.0,"I was wondering if you know if a stastical model ""cheat sheet(s)"" that lists any or more information:\\n\\n - When to use the model\\n - When not to use the model\\n - required and optional inputs\\n - expected outputs\\n - has the model been tested in different fields (policy, bio, engineering, manufacturing, etc)\\n - is it accepted in practice or research\\n - expected variation / accuracy / precision\\n - caveats\\n - scalability\\n - deprecated model, avoid or don't use\\n - etc ..\\n\\nI've seen hierarchies before on various websites, and some simplistic model cheat sheets in various textbooks; however, it'll be nice if there is a larger one that encompasses various types of models based on different types of analysis and theories\\n\\n",,
2735,1,1252,f4611157-f524-4375-ac43-3c23318cdef6,2010-08-04 16:39:49.0,59.0,statistical models cheat sheet,,
2736,3,1252,f4611157-f524-4375-ac43-3c23318cdef6,2010-08-04 16:39:49.0,59.0,<statistical-analysis><modeling><methodology><best-practices>,,
2737,2,1253,7742bb16-e6ed-4b98-b498-763235047a06,2010-08-04 16:53:04.0,738.0,"I'm trying to compute item-item similarity using [Jaccard(specifically Tanimoto)][1] on a large list of data in the format\\n\\n    (userid, itemid)\\n\\nI have about 800k users and 7900 items, and 3.57 million 'ratings'.  I've restricted my data to users who have rated at least n items(usually 10).  However, I'm wondering if I should place an upper limit on number of items rated.  The reason is that when a user rates 1000 items, that generates 999000 pairwise-combinations of items to use in my calc assuming the calculation\\n\\n    n! / (n-r)!\\n\\nAdding this much input data slows the calculating process down tremendously, even when the workload is distributed(using hadoop).  I'm thinking that the users who rate many, many items are not my core users and might be diluting my similarity calculations.  \\n\\nMy gut tells me to limt the data to customers who have rated between 10 and 150-200 items but I'm not sure if there is a better way to statistically determine these boundaries.\\n\\nHere are some more details about my source data's distribution.  Please feel free to enlighten me on any statistical terms that I might have butchered!\\n\\nThe distribution of my users' itemCounts:\\n![alt text][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Jaccard_index#Tanimoto_coefficient_.28extended_Jaccard_coefficient.29\\n  [2]: http://www.neilkodner.com/images/littlesnapper/itemsRated.png\\n\\n    > summary(raw)\\n       itemsRated      \\n     Min.   :   1.000  \\n     1st Qu.:   1.000  \\n     Median :   1.000  \\n     Mean   :   4.466  \\n     3rd Qu.:   3.000  \\n     Max.   :2069.000  \\n\\n    > sd(raw)\\n    itemsRated \\n      16.46169 \\n\\nIf I limit my data to users who have rated at least 10 items:\\n\\n    > above10<-raw[raw$itemsRated>=10,]\\n    > summary(above10)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   34.04   35.00 2069.00 \\n    > sd(above10)\\n    [1] 48.64679\\n    > length(above10)\\n    [1] 64764\\n\\nIf I further limit my data to users who have rated between 10 and 150 items:\\n\\n    > above10less150<-above10[above10<=150]\\n    > summary(above10less150)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   28.17   33.00  150.00 \\n    > sd(above10less150)\\n    [1] 24.32098\\n    > length(above10less150)\\n    [1] 63080\\n\\n",,
2738,1,1253,7742bb16-e6ed-4b98-b498-763235047a06,2010-08-04 16:53:04.0,738.0,How to limit my input data for Jaccard item-item similarity calc,,
2739,3,1253,7742bb16-e6ed-4b98-b498-763235047a06,2010-08-04 16:53:04.0,738.0,<distributions><statistics><data-mining>,,
2740,6,1241,7602a171-07b5-40fa-ad11-cc3b9aa9cba0,2010-08-04 16:53:24.0,88.0,<regression><roc>,edited tags,
2741,5,1253,f80c42a7-a4ed-4ab0-816d-85855f570bf4,2010-08-04 17:02:34.0,738.0,"I'm trying to compute item-item similarity using [Jaccard(specifically Tanimoto)][1] on a large list of data in the format\\n\\n    (userid, itemid)\\n\\nI have about 800k users and 7900 items, and 3.57 million 'ratings'.  I've restricted my data to users who have rated at least n items(usually 10).  However, I'm wondering if I should place an upper limit on number of items rated.  The reason is that when a user rates 1000 items, that generates 999000 pairwise-combinations of items to use in my calc assuming the calculation\\n\\n    n! / (n-r)!\\n\\nAdding this much input data slows the calculating process down tremendously, even when the workload is distributed(using hadoop).  I'm thinking that the users who rate many, many items are not my core users and might be diluting my similarity calculations.  \\n\\nMy gut tells me to limt the data to customers who have rated between 10 and 150-200 items but I'm not sure if there is a better way to statistically determine these boundaries.\\n\\nHere are some more details about my source data's distribution.  Please feel free to enlighten me on any statistical terms that I might have butchered!\\n\\nThe distribution of my users' itemCounts:\\n![alt text][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Jaccard_index#Tanimoto_coefficient_.28extended_Jaccard_coefficient.29\\n  [2]: http://www.neilkodner.com/images/littlesnapper/itemsRated.png\\n\\n    > summary(raw)\\n       itemsRated      \\n     Min.   :   1.000  \\n     1st Qu.:   1.000  \\n     Median :   1.000  \\n     Mean   :   4.466  \\n     3rd Qu.:   3.000  \\n     Max.   :2069.000  \\n\\n    > sd(raw)\\n    itemsRated \\n      16.46169 \\n\\nIf I limit my data to users who have rated at least 10 items:\\n\\n    > above10<-raw[raw$itemsRated>=10,]\\n    > summary(above10)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   34.04   35.00 2069.00 \\n    > sd(above10)\\n    [1] 48.64679\\n    > length(above10)\\n    [1] 64764\\n\\nIf I further limit my data to users who have rated between 10 and 150 items:\\n\\n    > above10less150<-above10[above10<=150]\\n    > summary(above10less150)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   28.17   33.00  150.00 \\n    > sd(above10less150)\\n    [1] 24.32098\\n    > length(above10less150)\\n    [1] 63080\\n\\nEdit: I dont think this is an issue of outliers as much as the data is positively skewed. ",added comment about outliers and skew,
2742,5,1253,11c22dac-471a-4b1d-b39d-08e2bcaf7b0e,2010-08-04 17:25:58.0,738.0,"I'm trying to compute item-item similarity using [Jaccard(specifically Tanimoto)][1] on a large list of data in the format\\n\\n    (userid, itemid)\\n\\nI have about 800k users and 7900 items, and 3.57 million 'ratings'.  I've restricted my data to users who have rated at least n items(usually 10).  However, I'm wondering if I should place an upper limit on number of items rated.  When users rate 1000 or more items, each user generates 999000 pairwise-combinations of items to use in my calc, assuming the calculation\\n\\n    n! / (n-r)!\\n\\nAdding this much input data slows the calculating process down tremendously, even when the workload is distributed(using hadoop).  I'm thinking that the users who rate many, many items are not my core users and might be diluting my similarity calculations.  \\n\\nMy gut tells me to limt the data to customers who have rated between 10 and 150-200 items but I'm not sure if there is a better way to statistically determine these boundaries.\\n\\nHere are some more details about my source data's distribution.  Please feel free to enlighten me on any statistical terms that I might have butchered!\\n\\nThe distribution of my users' itemCounts:\\n![alt text][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Jaccard_index#Tanimoto_coefficient_.28extended_Jaccard_coefficient.29\\n  [2]: http://www.neilkodner.com/images/littlesnapper/itemsRated.png\\n\\n    > summary(raw)\\n       itemsRated      \\n     Min.   :   1.000  \\n     1st Qu.:   1.000  \\n     Median :   1.000  \\n     Mean   :   4.466  \\n     3rd Qu.:   3.000  \\n     Max.   :2069.000  \\n\\n    > sd(raw)\\n    itemsRated \\n      16.46169 \\n\\nIf I limit my data to users who have rated at least 10 items:\\n\\n    > above10<-raw[raw$itemsRated>=10,]\\n    > summary(above10)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   34.04   35.00 2069.00 \\n    > sd(above10)\\n    [1] 48.64679\\n    > length(above10)\\n    [1] 64764\\n\\nIf I further limit my data to users who have rated between 10 and 150 items:\\n\\n    > above10less150<-above10[above10<=150]\\n    > summary(above10less150)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   28.17   33.00  150.00 \\n    > sd(above10less150)\\n    [1] 24.32098\\n    > length(above10less150)\\n    [1] 63080\\n\\nEdit: I dont think this is an issue of outliers as much as the data is positively skewed. ",deleted 7 characters in body,
2743,5,1252,0e15bc49-566a-4556-a527-8ce75fe06257,2010-08-04 17:27:29.0,88.0,"I was wondering if you know if a stastical model ""cheat sheet(s)"" that lists any or more information:\\n\\n - When to use the model\\n - When not to use the model\\n - required and optional inputs\\n - expected outputs\\n - has the model been tested in different fields (policy, bio, engineering, manufacturing, etc)\\n - is it accepted in practice or research\\n - expected variation / accuracy / precision\\n - caveats\\n - scalability\\n - deprecated model, avoid or don't use\\n - etc ..\\n\\nI've seen hierarchies before on various websites, and some simplistic model cheat sheets in various textbooks; however, it'll be nice if there is a larger one that encompasses various types of models based on different types of analysis and theories.\\n\\n",Spelling fixed.,
2744,4,1252,0e15bc49-566a-4556-a527-8ce75fe06257,2010-08-04 17:27:29.0,88.0,Statistical models cheat sheet,Spelling fixed.,
2745,5,1031,0ee94f96-18c3-460b-9e22-d20aedfb9bdf,2010-08-04 17:27:55.0,223.0,"KL has a deep meaning when you visualize a set of **dentities as a manifold** within the fisher metric tensor, it gives the geodesic distance between two ""close"" distributions. Formally: \\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\nThe following lines are here to explain with details what is meant by this las mathematical formulae. \\n\\n**Definition of the Fisher metric.** \\n\\n\\nConsider a parametrized family of probability distributions $D=(f(x, \\theta ))$ (given by densities in $R^n$), where $x$ is a random variable and theta is a parameter in $R^p$. You may all knnow that the fisher information matrix $F=(F_{ij})$ is \\n \\n$F_{ij}=E[d(\\log f(x,\\theta))/d \\theta_i d(\\log f(x,\\theta))/d \\theta_j]$\\n\\nWith this notation $D$ is a riemannian manifold and $F(\\theta)$ is a Riemannian metric tensor. (The interest of this metric is given by cramer Rao lower bound theorem)\\n\\nYou may say ... OK mathematical abstraction but where is KL ? \\n\\nIt is not mathematical abstraction, if $p=1$ you can really imagine your parametrized density as a curve (instead of a subset of a space of infinite dimension) and $F_{11}$ is connected to the curvature of that curve...\\n(see the seminal paper of Bradley Efron http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176343282) \\n\\n**The geometric answer to part of point a/ in your question :** the squared distance $ds^2$ between two (close) distributions $p(x,\\theta)$ and $p(x,\\theta+d \\theta)$ on the manifold (think of geodesic distance on earth of two points that are close, it is related to the curvature of the earth) is given by the quadratic form:\\n\\n$ds^2= \\sum F_{ij} d \\theta^i d \\theta^j$\\n\\nand it is known to be twice the Kullback Leibler Divergence:\\n\\n$ds^2=2KL(p(x, \\theta ),p(x,\\theta + d \\theta))$\\n\\nIf you want to learn more about that I suggest reading the paper from Amari\\nhttp://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176345779\\n(I think there is also a book from Amari about riemannian geometry in statistic but I don't remember the name) ",I mentionned Amari's work,
2746,2,1254,ee6712f7-e5b9-4b62-a2e6-7b0defa22d0d,2010-08-04 17:30:03.0,253.0,"Do you mean a statistical analysis decision tree? ([google search][1]), like this (only with extensions):\\n![alt text][2]\\n\\n?\\n\\nBTW, notice that the chart in wrong in that the tests it offers for median are not for median but for rank...\\n\\n\\n\\n  [1]: http://www.google.com/images?um=1&hl=en&biw=1024&bih=628&tbs=isch:1&sa=1&q=statistical+analysis+decision+tree&aq=f&aqi=&aql=&oq=&gs_rfai=\\n  [2]: http://www.processma.com/resource/images/htest.gif",,
2747,2,1255,c875abc9-53ea-4680-9dca-20e4bb77d07b,2010-08-04 17:35:42.0,303.0,"I'm confused: shouldn't you only need the 7900^2 item similarities, for which you use ratings from all users, which is still quite sparse?",,
2748,2,1256,901f1e3a-0251-4c24-bc8e-4835731c68ae,2010-08-04 17:39:29.0,196.0,"I have a colleague who calculates correlations in which one set of scores for a subject (e.g. 100 scores) is correlated with another set of scores for that same subject. The resulting correlation reflects the degree to which those sets of scores are associated for that subject.  He needs to do this for N subjects. Consider the following dataset:\\n\\n    ncol <- 100\\n    nrow <- 100\\n    x <- matrix(rnorm(ncol*nrow),nrow,ncol)\\n    y <- matrix(rnorm(ncol*nrow),nrow,ncol)\\n\\nThe correct output vector of correlations would be:\\n\\n    diag(cor(t(x),t(y)))\\n\\nIs there a faster way to do this without using a multicore package in R?",,
2749,1,1256,901f1e3a-0251-4c24-bc8e-4835731c68ae,2010-08-04 17:39:29.0,196.0,How can one speed up this correlation calculation in R without multicore?,,
2750,3,1256,901f1e3a-0251-4c24-bc8e-4835731c68ae,2010-08-04 17:39:29.0,196.0,<r><correlation><efficiency>,,
2751,5,386,98fea326-f108-4d08-b790-792fbf230ba8,2010-08-04 17:47:33.0,223.0,"I would do some sort of ""leave one out testing algorithm"" (n is the number of data):\\n\\nfor i=1 to n\\n\\n 1. **compute a density estimation of the data set obtained by throwing Xi away**. (This density estimate should be done with some assumption if the dimension is high, for example, a gaussian assumption for which the density estimate is easy: mean and covariance)\\n 2. **Calculate the likelihood of Xi for the density estimated in step 1**. call it $L_i$. \\n\\nend for \\n\\nsort the $L_i$ (for i=1,..,n) and use a multiple hypothesis testing procedure to say which are not good ... \\n \\nThis will work if n is sufficiently large... you can also use ""leave k out strategy"" which can be more relevent when you have ""groups"" of outliers ...",added 6 characters in body,
2754,5,1253,3272515e-82db-44d9-bd60-9f6f2dc93609,2010-08-04 18:01:45.0,738.0,"I'm trying to compute item-item similarity using [Jaccard(specifically Tanimoto)][1] on a large list of data in the format\\n\\n    (userid, itemid)\\n\\nAn item is considered as rated if i have a userid-itemid pair. I have about 800k users and 7900 items, and 3.57 million 'ratings'.  I've restricted my data to users who have rated at least n items(usually 10).  However, I'm wondering if I should place an upper limit on number of items rated.  When users rate 1000 or more items, each user generates 999000 pairwise-combinations of items to use in my calc, assuming the calculation\\n\\n    n! / (n-r)!\\n\\nAdding this much input data slows the calculating process down tremendously, even when the workload is distributed(using hadoop).  I'm thinking that the users who rate many, many items are not my core users and might be diluting my similarity calculations.  \\n\\nMy gut tells me to limt the data to customers who have rated between 10 and 150-200 items but I'm not sure if there is a better way to statistically determine these boundaries.\\n\\nHere are some more details about my source data's distribution.  Please feel free to enlighten me on any statistical terms that I might have butchered!\\n\\nThe distribution of my users' itemCounts:\\n![alt text][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Jaccard_index#Tanimoto_coefficient_.28extended_Jaccard_coefficient.29\\n  [2]: http://www.neilkodner.com/images/littlesnapper/itemsRated.png\\n\\n    > summary(raw)\\n       itemsRated      \\n     Min.   :   1.000  \\n     1st Qu.:   1.000  \\n     Median :   1.000  \\n     Mean   :   4.466  \\n     3rd Qu.:   3.000  \\n     Max.   :2069.000  \\n\\n    > sd(raw)\\n    itemsRated \\n      16.46169 \\n\\nIf I limit my data to users who have rated at least 10 items:\\n\\n    > above10<-raw[raw$itemsRated>=10,]\\n    > summary(above10)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   34.04   35.00 2069.00 \\n    > sd(above10)\\n    [1] 48.64679\\n    > length(above10)\\n    [1] 64764\\n\\nIf I further limit my data to users who have rated between 10 and 150 items:\\n\\n    > above10less150<-above10[above10<=150]\\n    > summary(above10less150)\\n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \\n      10.00   13.00   19.00   28.17   33.00  150.00 \\n    > sd(above10less150)\\n    [1] 24.32098\\n    > length(above10less150)\\n    [1] 63080\\n\\nEdit: I dont think this is an issue of outliers as much as the data is positively skewed. ",added 63 characters in body,
2755,2,1257,d98a7314-beb2-43ec-a94b-355e72d083fe,2010-08-04 18:04:52.0,,"Consider the following sequential, adaptive data generating process for $Y_1$, $Y_2$, $Y_3$. (By sequential I mean that we generate $Y_1$, $Y_2$, $Y_3$ in sequence and by adaptive I mean that $Y_3$ is generated depending on the observed values of $Y_1$ and $Y_2$.):\\n\\n$Y_1 = X_1\\  \\beta + \\epsilon_1$\\n\\n$Y_2 = X_2\\  \\beta + \\epsilon_2$\\n\\n$Y_3 = X_3\\  \\beta + \\epsilon_3$\\n\\n$\\nX_3 = \\n\\begin{cases} \\n X_{31} & \\mbox{if }Y_1 Y_2  \\gt  0 \\\\\\ X_{32} & \\mbox{if }Y_1 Y_2 \\le 0\\n\\end{cases}$\\n\\nwhere,\\n\\n$X_1$, $X_2$, $X_{31}$ and $X_{32}$ are all 1 x 2 vectors.\\n\\n$\\beta$ is a 2 x 1 vector\\n\\n$\\epsilon_i \\sim N(0,\\sigma^2)$ for $i$ = 1, 2, 3\\n\\nSuppose we observe the following sequence: {$Y_1 = y_1,\\ Y_2 = y_2,\\ X_3 = X_{31},\\ Y_3 = y_3$} and wish to estimate the parameters $\\beta$ and $\\sigma$. \\n\\nIn order to write down the likelihood function note that we have four random variables: $Y_1$, $Y_2$, $X_3$ and $Y_3$. Therefore, the joint density of $Y_1$, $Y_2$, $X_3$ and $Y_3$ is given by:\\n\\n$f(Y_1, Y_2, X_3, Y_3 |-) = f(Y_1|-)\\ f(Y_2|-)\\ [\\ f(Y_3|X_{31},-)\\ P(X_3=X_{31}|-)\\ +  \\ f(Y_3|X_{32},-)\\ P(X_3={X_{32}}|-)\\ ]$\\n\\n(Note: I am suppressing the dependency of the density on $\\beta$ and $\\sigma$.)\\n\\nSince the likelihood conditions on the observed data and our sequence is such that $y_1 y_2 >0$. Therefore, we have:\\n\\n$L(\\beta,\\ \\sigma | X_1,\\ X_2,\\ X_{31}, y_1, y_2, y_3) = f(Y_1|-)\\ f(Y_2|-)\\ f(Y_3|X_{31},-)\\ P(X_3=X_{32}) $\\n\\nIs the above the correct likelihood function for this data generating process?\\n",,user28
2756,1,1257,d98a7314-beb2-43ec-a94b-355e72d083fe,2010-08-04 18:04:52.0,,"What is the correct likelihood function for an sequential, adaptive data generation process?",,user28
2757,3,1257,d98a7314-beb2-43ec-a94b-355e72d083fe,2010-08-04 18:04:52.0,,<likelihood-function><data-generating-process>,,user28
2758,2,1258,af87b86b-f281-4b2f-b47d-219611c4c304,2010-08-04 18:12:23.0,287.0,"As someone who has learned a little bit of statistics for my own research, I'll guess that the reasons are pedagogical and inertial.\\n\\nI've observed within my own field that the order in which topics are taught reflects the history of the field. Those ideas which came first are taught first, and so on. For people who only dip into stats for cursory instruction, this means they'll learn classical stats first, and probably last. Then, even if they learn more, the classical stuff with stick with them better due to primacy effects.\\n\\nAlso, everyone knows what a two sample t-test is. Less than everyone knows what a Mann-Whitney or Wilcoxon Rank Sum test is. This means that I have to exert just a little bit of energy on explaining what my robust test is, versus not having to exert any with a classical test. Such conditions will obviously result in fewer people using robust methods than should.",,
2759,5,1257,acf6a09e-078d-427d-8701-6806e199207a,2010-08-04 18:13:40.0,,"Consider the following sequential, adaptive data generating process for $Y_1$, $Y_2$, $Y_3$. (By sequential I mean that we generate $Y_1$, $Y_2$, $Y_3$ in sequence and by adaptive I mean that $Y_3$ is generated depending on the observed values of $Y_1$ and $Y_2$.):\\n\\n$Y_1 = X_1\\  \\beta + \\epsilon_1$\\n\\n$Y_2 = X_2\\  \\beta + \\epsilon_2$\\n\\n$Y_3 = X_3\\  \\beta + \\epsilon_3$\\n\\n$\\nX_3 = \\n\\begin{cases} \\n X_{31} & \\mbox{if }Y_1 Y_2  \\gt  0 \\\\\\ X_{32} & \\mbox{if }Y_1 Y_2 \\le 0\\n\\end{cases}$\\n\\nwhere,\\n\\n$X_1$, $X_2$, $X_{31}$ and $X_{32}$ are all 1 x 2 vectors.\\n\\n$\\beta$ is a 2 x 1 vector\\n\\n$\\epsilon_i \\sim N(0,\\sigma^2)$ for $i$ = 1, 2, 3\\n\\nSuppose we observe the following sequence: {$Y_1 = y_1,\\ Y_2 = y_2,\\ X_3 = X_{31},\\ Y_3 = y_3$} and wish to estimate the parameters $\\beta$ and $\\sigma$. \\n\\nIn order to write down the likelihood function note that we have four random variables: $Y_1$, $Y_2$, $X_3$ and $Y_3$. Therefore, the joint density of $Y_1$, $Y_2$, $X_3$ and $Y_3$ is given by:\\n\\n$f(Y_1, Y_2, X_3, Y_3 |-) = f(Y_1|-)\\ f(Y_2|-)\\ [\\ f(Y_3|X_{31},-)\\ P(X_3=X_{31}|-)$\\n\\n$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad + \\ f(Y_3|X_{32},-)\\ P(X_3={X_{32}}|-)\\ ]$ \\n\\n(Note: I am suppressing the dependency of the density on $\\beta$ and $\\sigma$.)\\n\\nSince the likelihood conditions on the observed data and our sequence is such that $y_1 y_2 >0$. Therefore, we have:\\n\\n$L(\\beta,\\ \\sigma | X_1,\\ X_2,\\ X_{31}, y_1, y_2, y_3) = f(Y_1|-)\\ f(Y_2|-)\\ f(Y_3|X_{31},-)\\ P(X_3=X_{31}) $\\n\\nIs the above the correct likelihood function for this data generating process?\\n",fixed typo in last equation,user28
2760,5,1255,f3ea6578-cb5d-4c53-aea0-7c34103e41c4,2010-08-04 18:26:22.0,303.0,"I'm confused: shouldn't you only need the 7900^2 item similarities, for which you use ratings from all users, which is still quite sparse?\\n\\n**UPDATE**\\n\\nI still think there's a more efficient way to do this, but maybe I'm just being dense. Specifically, consider item A and item B. For item A, generate a U-dimensional vector of 0's and 1's, where U is the number of users in your data set, and there's a 1 in dimension i if and only if user i rated item A. Do the same thing for item B. Then you can easily generate the AB, A and B terms for your equation from these vectors. Importantly, these vectors are very sparse, so they can produce a very small data set if encoded properly.\\n\\n 1. Iterate over the item ID's to generate their cross product: (ItemAID, ItemBID)\\n 2. Map this pair to this n-tuple: (ItemAID, ItemBID, ItemAVector, ItemBVector)\\n 3. Reduce this n-tuple to your similarity measure: (ItemAID,ItemBID,SimilarityMetric)\\n\\nIf you set up a cache of the ItemXVector's at the start, this computation should be very fast.",Gave a full reply.,
2761,2,1259,6ba09faa-9e91-4187-949d-e94168e254e1,2010-08-04 18:29:57.0,743.0,All information looks like noise until you break the code.\\n—Hiro in Neal Stephenson's *Snow Crash* (1992),,
2762,16,1259,6ba09faa-9e91-4187-949d-e94168e254e1,2010-08-04 18:29:57.0,-1.0,,,
2763,2,1260,d8f352fa-56f7-4fc8-8487-8bf3fddbd5c2,2010-08-04 18:35:22.0,253.0,It might be one of those cases where using a different [BLAS engine][1] would help.  But I am not sure of it - it needs testing (and depends on your machine)\\n\\n\\n  [1]: http://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-use-a-fast-BLAS_003f,,
2764,6,1249,e54f487d-183b-4459-a8f6-38b63d0241ec,2010-08-04 18:44:27.0,223.0,<probability><mathematical-statistics><binomial>,edited tags,
2765,2,1261,049f6e10-e5ce-4355-938f-604962a4234a,2010-08-04 19:06:50.0,287.0,"Suppose I have a table of counts that look like this\\n\\n                A    B     C\\n    Success  1261  230  3514\\n    Failure   381  161  4012\\n\\nI have a hypothesis that there is some probability $p$ such that $P(Success_A) = p^i$, $P(Success_B) = p^j$ and $P(Success_C) = p^k$.\\n\\nIs there some way to produce estimates for $p$, $i$, $j$ and $k$? The idea I have is to iteratively try values for $p$ between 0 and 1, and values for $i$, $j$ and $k$ between 1 and 5. Given the column totals, I could produce expected values, then calculate $\\chi^2$ or $G^2$.\\n\\nThis would produce a best fit, but it wouldn't give any confidence interval for any of the values. It's also not particularly computationally efficient.\\n\\nAs a side question, if I wanted to test the goodness of fit of a particular set of values for $i$, $j$ and $k$ (specifically 1, 2, and 3), once I've calculated $\\chi^2$ or $G^2$, I'd want to calculate significance on the $\\chi^2$ distribution with 1 degree of freedom, correct? This isn't a normal contingency table since relationship of each column to the others is fixed to a single value. Given $p$, $i$, $j$ and $k$, filling in a single value in a cell fixes what the values of the other cells must be. ",,
2766,1,1261,049f6e10-e5ce-4355-938f-604962a4234a,2010-08-04 19:06:50.0,287.0,"Fitting a fixed, exponential relationship between categories with categorical data.",,
2767,3,1261,049f6e10-e5ce-4355-938f-604962a4234a,2010-08-04 19:06:50.0,287.0,<modeling><categorical-data><chi-squared>,,
2768,2,1262,8071188d-5b59-4374-a0ec-5b0dea7a2b78,2010-08-04 19:19:23.0,,"You could write the likelihood function like so:\\n\\n$L(p,i,j,k|-) \\propto (p^i)^{1261} (1-p^i)^{381} (p^j)^{230} (1-p^j)^{161} (p^k)^{3514} (1-p^k)^{4012}$\\n\\nMaximize the above likelihood function to estimate your parameters. Constructing confidence intervals and hypothesis testing should be straight forward once you have the estimates.",,user28
2769,2,1263,e6d60e99-80c5-4d9f-9a8a-728ca70c91f6,2010-08-04 19:30:30.0,88.0,"While making a call to `diag` you throw out a lot of information, so you can save time by simply not calculating it. You code is equivalent to:\\n\\n    sapply(1:100,function(i) cor(x[i,],y[i,]))",,
2770,2,1264,846b0518-1c1a-4f96-ba84-27148c78f110,2010-08-04 19:31:22.0,447.0,"The ROC function (it is not necessarily a curve) allows you to assess the discrimination ability provided by a a specific statistical model (comprised of a predictor variable or a set of them). \\n\\nA main consideration of ROCs is that model predictions do not only stem from the model's ability to discriminate/make predictions based on the evidence provided by predictor variables. Also operating is a response criteria that defines how much evidence is necessary for the model to predict a response, and what is the outcome of these responses. The value that is established for the response criteria will greatly influence the model predictions, and ultimately the type of mistakes that it will make.\\n\\nConsider a generic model with predictor variables and a response criteria. This model is trying to predict the Presence of X,by responding Yes or No.\\nSo you have the following confusion matrix:\\n\\n                                    **X present               X absent**\\n     **Model Predicts X Present**       Hit                   False Alarm\\n\\n     **Model Predicts X Absent**      Miss                 Correct Rejection\\n\\n\\nIn this matrix, you only need to consider the proportion of Hits and the False Alarms (because the others can be derived from these, given that they have to some to 1). For each response criteria, you wil ave a different confusion matrix. The errors (Misses and False Alarms) are negatively related, which means that a response criteria that minimizes false alarms maximizes misses and vice-versa. The message is: there is no free lunch. \\n\\nSo, in order to understand how well the model discriminates cases/makes predictions, independently of the response criteria established, you plot the Hits and False rates produced across the range of possible response criteria.\\n\\nWhat you get from this plot is the ROC function. The area under the function provides an unbiased, and non-parametric measure of the discrimination ability of the model. This measure is very important because it is free of any confounds that could have been produced by the response criteria.\\n\\nA second important aspect, is that by analyzing the function, one can define what response criteria is better for your objectives. What types of errors you want to avoid, and what are errors are OK. For instance, consider an HIV test: it is a test that looks up some sort of evidence (in this case antibodies) and  makes a discrimination/prediction based on the comparison of the evidence with response criterion. This response criterion is usually set very low, so that you minimize Misses. Of course this will result in more False Alarms, which have a cost, but a cost that is negligible when compared to the Misses.\\n\\n\\nWith ROCs you can assess some model's discrimination ability, independently of the response criteria, and also establish the optimal response criteria, given the needs and constraints of whatever that you are measuring.\\nTests like hi-square cannot help at all in this because even if your testing if the predictions are at chance level, many different Hit-False Alarm pairs are consistent with chance level.\\n\\n\\nSome frameworks, like signal detection theory, assume a priori that the evidence available for discrimination has specific distribuiton (e.g., normal distribution, or gamma distribution). When these assumptions hold (or are pretty close), some really nice measures are available that make your life easier.\\n\\nhope this helps to elucidate you on the advantages of ROCs \\n\\n\\n",,
2771,2,1265,3d53c105-7666-465c-beab-336125324543,2010-08-04 19:35:08.0,643.0,"This really depends on the relative numbers of ""scores"" and ""subjects"". The method you use calculates lots of cross-correlations which are not required. However, if there are relatively few ""subjects"" relative to ""scores"", then this probably doesn't matter too much, and the method you suggest is probably as good as anything, as it uses a small number of efficient blas operations. However, if there are a large number of ""subjects"" relative to scores, then it may well be quicker to loop over the rows computing the correlation for each pair separately, using the code suggested by ""mbq"". ",,
2772,2,1266,cc8c7268-da61-4535-ba72-11e49ae5cb0d,2010-08-04 20:01:07.0,253.0,"The following question is one of those holy grails for me for some time now, I hope someone might be able to offer a good advice.\\n\\n\\nI wish to perform a non-parametric repeated measures multiway anova using R.\\n\\nI have been doing some online searching and reading for some time, and so far was able to find solutions for only some of the cases: friedman test for one way nonparametric repeated measures anova, ordinal regression with {car} Anova function for multi way nonparametric anova, and so on.  The partial solutions is NOT what I am looking for in this question thread.  I have summarized my findings so far in a post I published some time ago (titled: [Repeated measures ANOVA with R (functions and tutorials)][1], in case it would help anyone) \\n\\n.\\n\\nIf what I read online is true, this task might be achieved using a mixed Ordinal Regression model (a.k.a: Proportional Odds Model).\\n\\nI found two packages that seems relevant, but couldn't find any vignette on the subject:\\n\\n - http://cran.r-project.org/web/packages/repolr/\\n - http://cran.r-project.org/web/packages/ordinal/\\n\\nSo being new to the subject matter, I was hoping for some directions from people here.\\n\\nAre there any tutorials/suggested-reading on the subject?  Even better, can someone suggest a simple example code for how to run and analyse this in R (e.g: ""non-parametric repeated measures multiway anova"") ?\\n\\n\\nThanks for any help,\\nTal\\n\\n\\n  [1]: http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/",,
2773,1,1266,cc8c7268-da61-4535-ba72-11e49ae5cb0d,2010-08-04 20:01:07.0,253.0,A non-parametric repeated-measures multi-way Anova in R ?,,
2774,3,1266,cc8c7268-da61-4535-ba72-11e49ae5cb0d,2010-08-04 20:01:07.0,253.0,<r><nonparametric><anova>,,
2776,2,1268,0ab00095-2b66-4e5c-9cdb-3d9bf1474454,2010-08-04 20:51:04.0,170.0,"I am using Singular Value Decomposition as a dimensionality reduction technique.\\n\\nGiven `N` vectors of dimension `D`, the idea is to represent the features in a transformed space of uncorrelated dimensions, which condenses most of the information of the data in the eigenvectors of this space in a decreasing order of importance.\\n\\nNow I am trying to apply this procedure to time series data. The problem is that not all the sequences have the same length, thus I cant really build the `num-by-dim` matrix and apply SVD. My first thought was to pad the matrix with zeros by building a `num-by-maxDim` matrix and filling the empty spaces with zeros, but I'm not so sure if that is the correct way.\\n\\nMy question is how do you the SVD approach of dimensionality reduction to time series of different length? Alternatively are there any other similar methods of eigenspace representation usually used with time series?\\n\\nBelow is a piece of MATLAB code to illustrate the idea:\\n\\n    X = randn(100,4);                       % data matrix of size N-by-dim\\n\\n    X0 = bsxfun(@minus, X, mean(X));        % standarize\\n    [U S V] = svd(X0,0);                    % SVD\\n    variances = diag(S).^2 / (size(X,1)-1); % variances along eigenvectors\\n    \\n    KEEP = 2;                               % number of dimensions to keep\\n    newX = U(:,1:KEEP)*S(1:KEEP,1:KEEP);    % reduced and transformed data\\n\\n\\n(I am coding mostly in MATLAB, but I'm comfortable enough to read R/Python/.. as well)\\n",,
2777,1,1268,0ab00095-2b66-4e5c-9cdb-3d9bf1474454,2010-08-04 20:51:04.0,170.0,Time Series dimensionality reduction using SVD,,
2778,3,1268,0ab00095-2b66-4e5c-9cdb-3d9bf1474454,2010-08-04 20:51:04.0,170.0,<machine-learning><time-series><pca><data-transformation>,,
2779,2,1269,15de07fe-d7b2-450c-b45d-6ce7e415127d,2010-08-04 21:10:02.0,223.0,Filling with zero is bad. Try filling with resampling using observations from the past.  ,,
2780,2,1270,c4d105c1-11d4-4799-995e-df5872182fb0,2010-08-04 21:12:03.0,749.0,"If I have a (financial) time series, and I sample it with two different periods, at 5 and at 60 minute intervals, can I create an exponential moving average on the 5 minute sampled data which is the same as an exponential moving average on the 60 minute sampled data?\\n\\nSomething like this:  \\ne1 = EMA(a1) applied on sampled_data(60 min)  \\ne2 = EMA(a2) applied on sampled_data(5 min)\\n\\na1 and a2 are the smoothing factors of the exponential moving average (the period)\\n\\nCan I compute the a2 value for any a1 value, such that e1 = e2?\\n\\nWhen I say that e1 = e2 I mean that if I graph the values of the EMA computed from 5 min data on top of the 60 min data chart and EMA, the two EMAs should be superposed. This means that in between two data points for EMA(60 min) there will be 60/5=12 data points for EMA(5 min).\\n",,
2781,1,1270,c4d105c1-11d4-4799-995e-df5872182fb0,2010-08-04 21:12:03.0,749.0,Exponential moving averages of a time series with varying sampling,,
2782,3,1270,c4d105c1-11d4-4799-995e-df5872182fb0,2010-08-04 21:12:03.0,749.0,<time-series>,,
2783,2,1271,3de4d3f7-bc7f-4b72-ab41-5b9cb742b7be,2010-08-04 21:16:37.0,485.0,"When in doubt, bootstrap!  Really, I don't know of a canned procedure to handle such a scenario.\\n\\nBootstrapping is a generally applicable way of generating some error parameters from the data at hand.  Rather than relying on the typical parametric assumptions, bootstrap procedures capitalize on the characteristics of the sample to generate and empirical distribution against which your sample estimates can be compared.\\n\\nGoogle scholar is gold...it's been done before...at least once.\\n\\nLunneborg, Clifford E.; Tousignant, James P.; \\n1985\\n""Efron's Bootstrap with Application to the Repeated Measures Design.""\\nMultivariate Behavioral Research; Apr85, Vol. 20 Issue 2, p161, 18p",,
2784,5,1271,b97c48bd-a337-46f6-9c6c-f213123dc092,2010-08-04 21:27:49.0,485.0,"When in doubt, bootstrap!  Really, I don't know of a canned procedure to handle such a scenario.\\n\\nBootstrapping is a generally applicable way of generating some error parameters from the data at hand.  Rather than relying on the typical parametric assumptions, bootstrap procedures capitalize on the characteristics of the sample to generate an empirical distribution against which your sample estimates can be compared.\\n\\nGoogle scholar is gold...it's been done before...at least once.\\n\\nLunneborg, Clifford E.; Tousignant, James P.; \\n1985\\n""Efron's Bootstrap with Application to the Repeated Measures Design.""\\nMultivariate Behavioral Research; Apr85, Vol. 20 Issue 2, p161, 18p",deleted 1 characters in body,
2785,2,1272,84a3212a-90c0-4cd1-8ef8-72af188761e8,2010-08-04 21:30:19.0,,"I have a set of data which consists of many different types (measurable, categorical)\\nFor example:\\nname   measurable_attribute_1   categorical_attribute_1   measurable_attribute_2   categorical_attribute_2 ...\\n\\nNumber of attributes may grow quite quickly during my study: into my spreadsheet, I can as many new entries as attribute... I have about a hundred of entries in this classification scheme, about 70 attribute, so far, and I am at the beginning of my data collection.\\n\\nI would like to perform statistical analysis of this data set. For example, what are the common features of the entries that have a similar categorical_attribute and this range of values of measurable_attribute.\\n\\nWell, I would like to generate relationships between attributes in order to create training images. \\nHowever, I am not sure of how to organize the data prior to classification. Even though, should I organize the data?... (referring to this <a href=""http://stats.stackexchange.com/questions/47/clustering-of-large-heavy-tailed-dataset"">question</a>) \\n\\nAlso, I can hardly gather entries into classes.\\n\\nI do not want to introduce any bias obviously.\\n\\nI am also quite new to statistical analysis (but eager to learn).",,MarcO
2786,1,1272,84a3212a-90c0-4cd1-8ef8-72af188761e8,2010-08-04 21:30:19.0,,How to organize a dataset with many attributes,,MarcO
2787,3,1272,84a3212a-90c0-4cd1-8ef8-72af188761e8,2010-08-04 21:30:19.0,,<algorithms><categorical-data><classification>,,MarcO
2788,2,1273,2cf39f4a-7c4d-40e1-a427-fc3116287e77,2010-08-04 21:54:28.0,666.0,"Change the variable.\\n\\nRun a control chart for the ""time between infections"" variable.",,
2789,5,1273,52861747-a90d-4346-ac59-b66b1b637103,2010-08-04 22:06:19.0,666.0,"Change the variable. Run a control chart for the ""time between infections"" variable.\\n\\nThis procedure was recommended by Donald Wheeler in [Understanding Variation: The Key to Managing Chaos][1].\\n\\n\\n  [1]: http://www.amazon.com/Understanding-Variation-Key-Managing-Chaos/dp/0945320531/ref=sr_1_1?s=books&ie=UTF8&qid=1280959429&sr=1-1",added 249 characters in body,
2790,2,1274,94706984-f08f-4f6d-bc42-a99e31e8b888,2010-08-04 22:17:59.0,273.0,"\\n[0,1,0,2,4,1,0,1,5,1,4,2,1,3,1,1,1,1,0,1,1,0,2,0,2,0,0,1,0,1,2,2,1,2,4,1,4,1,0,0,4,1,0,1,0,1,1,2,1,1,0,0]\\n\\nWhat is the best way to convince myself that these data are correlated? that no univariate discrete distribution would approximate them well? that a time series model is necessary to better estimate the future distribution of counts?\\n",,
2791,1,1274,94706984-f08f-4f6d-bc42-a99e31e8b888,2010-08-04 22:17:59.0,273.0,Best way to show these or similar count data are not independent?,,
2792,3,1274,94706984-f08f-4f6d-bc42-a99e31e8b888,2010-08-04 22:17:59.0,273.0,<correlation><count-data>,,
2793,16,1197,00000000-0000-0000-0000-000000000000,2010-08-04 22:36:49.0,88.0,,,
2794,16,1199,00000000-0000-0000-0000-000000000000,2010-08-04 22:36:49.0,88.0,,,
2795,16,1200,00000000-0000-0000-0000-000000000000,2010-08-04 22:36:49.0,88.0,,,
2796,16,1206,00000000-0000-0000-0000-000000000000,2010-08-04 22:36:49.0,88.0,,,
2797,16,1217,00000000-0000-0000-0000-000000000000,2010-08-04 22:36:49.0,88.0,,,
2798,16,1194,00000000-0000-0000-0000-000000000000,2010-08-04 22:36:49.0,88.0,,,
2799,2,1275,13b306cb-33e7-4c08-9aa5-ac627db67098,2010-08-04 22:46:30.0,,You could estimate univariate time series models for the 'short' series and extrapolate them into the future to 'align' all the series.,,user28
2800,6,712,0b049253-c92a-4d33-a8d9-077f06ef8e69,2010-08-04 22:51:14.0,88.0,<untagged>,More appropriate tag.,
2801,2,1276,d8baf451-69cf-476b-8634-09cf75a94ff7,2010-08-04 23:36:10.0,61.0,"The trivial and non-helpful answer is ""Yes, downsample your 5-minute data to 60-minute data.""\\n\\nMore practically, without throwing out 90% of your data, the answer is generally ""No, unless you get extremely lucky with sampling your five-minute data.""  You should get an answer that's *close* (and under most noise models I suspect they'll be equal in expectation) just by rescaling your smooth by a factor of 12, but any source of randomness in your data is going to cause some difference in the two curves on a point-by-point basis.",,
2802,2,1277,cd7fed33-c110-4e9f-b0c4-cdf0053b16df,2010-08-04 23:46:28.0,159.0,"You could just plot the ACF and check if the first coefficient is inside the critical values. The critical values are ok for non-Gaussian time series (at least asymptotically).\\n\\nAlternatively, fit a simple count time series model such as the INAR(1) and see if the coefficient is significantly different from zero.",,
2803,5,1273,2cf28631-6b0b-496f-b85f-60c20ce1003f,2010-08-04 23:48:08.0,666.0,"Change the variable. Run a control chart for the ""time between infections"" variable. That way, instead of a discrete variable with a very small range of values, you have a continuous variable with an adequate range of values. If the interval between infections gets too small, the chart will give an ""out of control"" indication.\\n\\nThis procedure was recommended by Donald Wheeler in [Understanding Variation: The Key to Managing Chaos][1].\\n\\n\\n  [1]: http://www.amazon.com/Understanding-Variation-Key-Managing-Chaos/dp/0945320531/ref=sr_1_1?s=books&ie=UTF8&qid=1280959429&sr=1-1",added 244 characters in body,
2804,2,1278,28a48798-9e51-4658-87c9-95992fe0bf52,2010-08-05 01:24:22.0,588.0,"(This is part-2 of my long question, you can have a look at part-1 [here][1])\\n\\nI am going to do a quasi-experiment, with measuring the base line of a sample (actually not quite a sample, but a ward, with high patient turn-over rate), and then we do a intervention, and measure the variables (i.e. infection rate) again.\\n\\nI googled a bit and found that this is something called a single case experiment, and it was said that single case experiment doesn't have very solid statistics because you don't have the control, you can't conclude on the causality in a solid manner.\\n\\nI have googled a bit again and found that I can compare the incidence rate (or call it infection rate), but doing something like ""incidence rate difference"" (IRD) or ""incidence rate ratio"" (IRR). (I found it from [here][2])\\n\\nWhat is the difference between IRD and t-test? And is there any statistical test complementary for IRR?\\n\\nBut mostly importantly, is it appropriate for me to use this test (does it have a name?) for single case experiment? Because the patients in the ward keep changing, this is what I worried about.\\n\\n\\nThanks again!\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1099/how-to-handle-count-data-categorical-data-when-it-has-been-converted-to-a-rate\\n  [2]: http://www.statsdirect.com/help/rates/incidence_rates.htm",,
2805,1,1278,28a48798-9e51-4658-87c9-95992fe0bf52,2010-08-05 01:24:22.0,588.0,Is calculating Incidence Rate Difference/Ratio appropriate for single case experimental design?,,
2806,3,1278,28a48798-9e51-4658-87c9-95992fe0bf52,2010-08-05 01:24:22.0,588.0,<experiment><epidemiology>,,
2807,2,1279,c0c94799-a78e-4600-9691-8c8ec7afbbbe,2010-08-05 02:36:08.0,183.0,One resource I found recently was the UCCS video archive.\\nIt has a couple of archived video series for subjects called Mathematical Statistics I and  Mathematical Statistics II. It requires a free registration to access.\\n\\nhttp://www.uccs.edu/~math/vidarchive.html,,
2808,16,1279,c0c94799-a78e-4600-9691-8c8ec7afbbbe,2010-08-05 02:36:08.0,-1.0,,,
2809,2,1280,102e0081-f01a-4faa-ae79-b581077d3ab0,2010-08-05 02:42:11.0,553.0,87% of statistics are made up on the spot\\n\\n-Unknown,,
2810,16,1280,102e0081-f01a-4faa-ae79-b581077d3ab0,2010-08-05 02:42:11.0,-1.0,,,
2811,2,1281,a82ece2b-fcd8-47a4-bbb9-3564f6d7aba8,2010-08-05 02:57:24.0,74.0,You could look at Exploratory Factor Analysis. It will tell you which attributes are the most similar to each other.,,
2812,5,1214,346cae46-2dcb-4845-b793-25b6f61aa1e2,2010-08-05 02:59:12.0,159.0,"If you really have no idea what the periodicity is, probably the best approach is to find the frequency corresponding to the maximum of the spectral density. However, the spectrum at low frequencies will be affected by trend, so you need to detrend the series first. The following R function should do the job for most series. It is far from perfect, but I've tested it on a few dozen examples and it seems to work ok. It will return 1 for data that have no strong periodicity, and the length of period otherwise.\\n\\n**Update:** Version 2 of function. This is much faster and seems to be more robust.\\n\\n\\n    find.freq <- function(x)\\n    {\\n        n <- length(x)\\n        spec <- spec.ar(c(x),plot=FALSE)\\n        if(max(spec$spec)>10) # Arbitrary threshold chosen by trial and error.\\n    	{\\n            period <- round(1/spec$freq[which.max(spec$spec)])\\n    		if(period==Inf) # Find next local maximum\\n    		{\\n    			j <- which(diff(spec$spec)>0)\\n    			if(length(j)>0)\\n    			{\\n    				nextmax <- j[1] + which.max(spec$spec[j[1]:500])\\n    				period <- round(1/spec$freq[nextmax])\\n    			}\\n    			else\\n    				period <- 1\\n    		}\\n    	}\\n        else\\n            period <- 1\\n        return(period)\\n    }\\n\\n",added 134 characters in body,
2813,2,1282,e8bd3f41-eb2c-40b7-84f0-95f9230d0150,2010-08-05 03:06:36.0,279.0,"Following up on my comment, this question would be very simple if i, j, and k were not restricted to be integers. The reason is as follows:  pA, pB, and pC denote the observed probability of success in the three groups. Then let p=pA, i=1, j=log(pB)/log(pA), and k=log(pC)/log(pA). These will easily satisfy the required conditions (except for j and k being between 1 and 5, but that looks like an ad-hoc simplifying assumption instead of a real constraint).\\nIn fact, if you do this with the given data, you get j=2.009 and k=2.884 which I think prompted the original question.\\n\\nIt is even possible to get standard errors for these quantities (or rather their logarithm). Note that if pB = p^j, then log(-log(pB)) = log(j) + log(-log(p)), so one can use logistic regression with a complimentary log-log link for the number of failures (the complimentary log-log function is log(-log(1-x)) and this link is built in for most statitical software such as R or SAS). Then one could check whether the 95% CIs include integers, or perhaps run a likelihood-ratio (or other) test comparing the fit of the unrestricted model to one where j and k are rounded to the nearest integer.\\n\\nThe above assumes that i=1. Something similar could probably be done for other integer i's (probably by having an offset of log(i) in the model - I have not thought it through).\\n\\nIn the end, I want to note that you should make sure that your hypothesis is meaningful by itself, and did not come from playing with the data. Otherwise any statistical test is biased because you picked a form of the null hypothesis (out all the possible weird forms that you could have imagined) that is likely to fit.",,
2814,2,1283,47f183ce-f189-402a-889e-f269d623cedb,2010-08-05 03:23:53.0,485.0,"Immediately, a runs test comes to mind.  That is, if this is an ordered set and you are interested in knowing if the ordered observations are independent.  It tests whether the sequence is likely due to randomness rather than systematic variation.  Without more detail, that's the best I can come up with.",,
2815,2,1284,66d6b6e8-78b4-424d-a0e7-278fa8904a88,2010-08-05 03:43:12.0,196.0,"Consider checking out [gmpm][1], but it is still in development.\\n\\n\\n  [1]: https://r-forge.r-project.org/projects/gmpm/",,
2816,2,1285,eb4a01c3-7faf-4286-924b-56f3ef823263,2010-08-05 03:55:13.0,183.0,"One option would involve using optimal scaling principal components analysis.\\nThe approach allows you to state your measurement assumptions about each variable (e.g., nominal, ordinal, numeric).\\nI've used it in SPSS: see the Categories Add-On module (i.e., Analyze - Dimension Reduction - Optimal Scaling).\\nI'm not sure, but the homals package in R may also implement the procedure.\\n\\nA quick Google ( http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=optimal+scaling+principal+components) \\nthrough up this reference:\\nhttp://takane.brinkster.net/Yoshio/p009.pdf\\n",,
2817,5,1285,ee007be0-2f39-40c8-a8ee-ebd0e3a9d4fa,2010-08-05 04:14:12.0,183.0,"One option would involve using **optimal scaling principal components analysis**.\\nThe approach allows you to state your measurement assumptions about each variable (e.g., nominal, ordinal, numeric).\\nI've used it in SPSS: see the Categories Add-On module (i.e., Analyze - Dimension Reduction - Optimal Scaling).\\nI'm not sure, but the homals package in R may also implement the procedure.\\n\\nA quick Google ( http://www.google.com/search?sourceid=chrome&ie=UTF-8&q=optimal+scaling+principal+components) \\nrevealed this reference:\\nhttp://takane.brinkster.net/Yoshio/p009.pdf\\n",added 2 characters in body,
2818,2,1286,1fd2d00c-5b4b-4bbe-b7bc-03a7b2647da5,2010-08-05 04:54:46.0,196.0,"Sometimes I want to do an exact test by examining all possible combinations of the data to build an empirical distribution against which I can test my observed differences between means.  To find the possible combinations I'd typically use the combn function.  The choose function can show me how many possible combinations there are.  It is very easy for the number of combinations to get so large that it is not possible to store the result of the combn function, e.g. combn(28,14) requires a 2.1 Gb vector.  So I tried writing an object that steped through the same logic as the combn function in order to provide the values off an imaginary ""stack"" one at a time.  However, this method (as I instantiated it) is easily 50 times slower than combn at reasonable combination sizes, leading me to think it will also be painfully slow for larger combination sizes.  \\n\\nIs there a better algorithm for doing this sort of thing than the algorithm used in combn?Specifically is there a way to generate and pull the Nth possible combination without calculating through all previous combinations?  Also, I'm new to S4 programing, so - is there a way to allow a method of an S4 object to directly adjust the values inside the slots of that object without copying the entire object into memory and having to re-write it to the parent environment at the end of the method?",,
2819,1,1286,1fd2d00c-5b4b-4bbe-b7bc-03a7b2647da5,2010-08-05 04:54:46.0,196.0,How can I obtain some of all possible combinations in R?,,
2820,3,1286,1fd2d00c-5b4b-4bbe-b7bc-03a7b2647da5,2010-08-05 04:54:46.0,196.0,<r><nonparametric><permutation><combination>,,
2821,2,1287,b796f151-b5e4-4dd8-b43d-26330dbd1a15,2010-08-05 04:58:19.0,10229.0,"Reading ""Using Multivariate Statistics (4th Edition) Barbara G. Tabachnick"" \\nI found these decision trees based on major research question. I think they are quite useful. Following this link you'll find an extract of the book\\nhttp://www.psychwiki.com/images/d/d8/TF2.pdf\\nsee pages 29 to 31",,
2822,5,609,fed285b4-95e8-41af-a961-7f9702702503,2010-08-05 05:35:45.0,223.0,"**As a summary**, my answer is : if you have an explicit expression or can figure out some how what your distance is measuring (what ""differences"" it gives weigth to), then you can say what it is better for. An other complementary way to analyse and compare such test is the minimax theory. \\n\\nAt the end some test will be good for some alternatives and some for others. For a given set of alternatives it is sometime possible to show if your test has optimal property in the worst case: this is the minimax theory. \\n\\n\\n----------\\n **Some details** \\n\\n Hence You can tell about the properties of two different test by regarding the set of alternative for which they are minimax (if such alternative exist) i.e. (using the word of Donoho and Jin) by comparing their ""optimal detection boudary"" http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1085408492.\\n\\n\\nLet me go distance by distance:\\n\\n 1. KS distance is obtained calculating supremum of difference between empirical cdf and cdf. Being a suppremum it will be highly sensitive to local alternatives (local change in the cdf) but not with global change (at least using L2 distance between cdf would be less local (Am I openning open door ?)). However, the most important thing is that is uses the cdf. This implies an asymetry: you give more importance to the changes in the tail of your distribution.\\n\\n \\n 2. Wassertein metric  (what you meant by Kantorovitch Rubinstein ? )  http://en.wikipedia.org/wiki/Wasserstein_metric is ubiquitous and hence hard to compare. \\n   - For the particular case of W2 it has been uses in http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1017938923  and it is related to the L2 distance to inverse of cdf. My understanding is that it gives even more weight to the tails but I think you should read the paper to know more about it. \\n   - For the case of the L1 distance between density function it will highly depend on how you estimate your dentity function from the data... but otherwise it seems to be a ""balanced test"" not giving importance to tails. \\n\\n\\n\\n\\n----------\\n\\n\\nTo recall and extend the comment I made which complete the answer: \\n\\nI know you did not meant to be exhaustive but you could add Anderson darling statistic (see http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test). This made me remind of a paper fromo Jager and Wellner (see http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1194461721) which extands/generalises Anderson darling statistic (and include in particular higher criticism of Tukey). Higher criticism was already shown to be minimax for a wide range of alternatives and the same is done by Jager and Wellner for their extention. I don't think that minimax property has been shown for Kolmogorov test. Anyway, understanding for which type of alternative your test is minimax helps you to know where is its strength, so you should read the paper above.. \\n\\n",added 1 characters in body,
2823,5,725,d14c522d-4adc-48c6-80c1-5f4ef9c6c2c2,2010-08-05 05:47:10.0,223.0,"An **hyperspectral image is** a multidimensional image with more than 200 spectral bands i.e. and image for which each pixel is a vector of dimension 200 (most often it is a smapled spectral curve). \\n\\n\\nWhat are the **implemented package** (I am especially interested in R packages but if other free algorithm exist, I will try them) for **frontier detection** and (unsupervised ) **segmentation** of this type of images?  \\n",added 2 characters in body,
2824,5,1286,001b73a2-5505-4085-a4b0-620fae0fa727,2010-08-05 08:21:17.0,196.0,"Sometimes I want to do an exact test by examining all possible combinations of the data to build an empirical distribution against which I can test my observed differences between means.  To find the possible combinations I'd typically use the combn function.  The choose function can show me how many possible combinations there are.  It is very easy for the number of combinations to get so large that it is not possible to store the result of the combn function, e.g. combn(28,14) requires a 2.1 Gb vector.  So I tried writing an object that steped through the same logic as the combn function in order to provide the values off an imaginary ""stack"" one at a time.  However, this method (as I instantiated it) is easily 50 times slower than combn at reasonable combination sizes, leading me to think it will also be painfully slow for larger combination sizes.  \\n\\nIs there a better algorithm for doing this sort of thing than the algorithm used in combn?Specifically is there a way to generate and pull the Nth possible combination without calculating through all previous combinations?","Dropped the most ""non-stats"" R part of the question to feed the grumpy masses.",
2825,2,1288,adb4ce87-de59-43a0-a324-4dad93a4edc7,2010-08-05 09:15:34.0,438.0,"So there are plenty of answers here paraphrased from statistics/probability textbooks, Wikipedia, etc. I believe we have ""laypersons"" where i work; i think they are in the marketing department. If i ever have to explain anything technical to them, i apply the rule ""show don't tell."" With that rule in mind, i would probably show them something like this.\\n\\nThe idea here is to try to code an algorithm that i can teach to spell--not by learning all of the hundreds (thousands?) of rules like *When adding an ending to a word that ends with a silent e, drop the final e if the ending begins with a vowel*. One reason that won't work is i don't know those rules (i'm not even sure the one i just recited is correct). Instead i am going to teach it to spell by showing it a bunch of correctly spelled words and letting it extract the rules from those words, which is more or less the essence of Machine Learning, regardless of the algorithm--pattern extraction and pattern recognition.\\n\\nThe success criterion is correctly spelling a word the algorithm has never seen before (i realize that can happen by pure chance, but that won't occur to the marketing guys, so i'll ignore--plus i am going to have the algorithm attempt to spell not one word, but a lot, so it's not likely we'll be deceived by a few lucky guesses).\\n\\nAn hour or so ago, i downloaded (as a plain text file) from the excellent Project Gutenberg Site, the Herman Hesse novel *Siddhartha*. I'll use the words in this novel to teach the algorithm how to spell.\\n\\nSo i coded the algorithm below that scanned this novel, three letters at a time (each word has one additional character at the end, which is 'whitespace', or the end of the word). Three-letter sequences can tell you a lot--for instance, the letter 'q' is nearly always followed by 'u'; the sequence 'ty' usually occurs at the end of a word; z rarely does, and so forth. (Note: i could just as easily have fed it entire words in order to train it to speak in complete sentences--exactly the same idea, just a few tweaks to the code.)\\n\\nNone of this involves MCMC though, that happens after training, when we give the algorithm a few random letters (as a seed) and it begins forming 'words'. How does the algorithm build words? Imagine that it has the block 'qua'; what letter does it add next? During training, the algorithm constructed a massive l*etter-sequence frequency matrix* from all of the thousands of words in the novel. Somewhere in that matrix is the three-letter block 'qua' and the frequencies for the characters that could follow the sequence. The algorithm selects a letter based on those frequencies that could possibly follow it. So the letter that the algorithm selects next depends on--and solely on--the last three in its word-construction queue.\\n\\nSo that's a Markov Chain Monte Carlo algorithm. \\n\\nI think perhaps the best way to illustrate how it works is to show the results based on different levels of training. Training level is varied by changing the number of passes the algorithm makes though the novel--the more passes thorugh the greater the fidelity of its letter-sequence frequency matrices.  Below are the results--in the form of 100-character strings output by the algorithm--after training on the novel 'Siddharta'.<hr> \\n\\n\\n\\n**A single pass through the novel, *Siddhartha*:**\\n\\n> *then whoicks ger wiff all mothany stand ar you livid theartim mudded\\n> sullintionexpraid his sible his*\\n\\n\\n(Straight away, it's learned to speak almost perfect Welsh; i hadn't expected that.)<hr>\\n\\n**After two passes through the novel:**\\n\\n> *the ack wor prenskinith show wass an twor seened th notheady theatin land\\n> rhatingle was the ov there*\\n<hr>\\n\\n**After 10 passes:**\\n\\n> *despite but the should pray with ack now have water her dog lever pain feet\\n> each not the weak memory*\\n\\n<hr> \\n\\n\\n\\nAnd here's the code (in Python, i'm nearly certain that this could be done in R using an MCMC package, of which there are several, in just 3-4 lines)\\n\\n    def create_words_string(raw_string) :\\n      """""" in case i wanted to use training data in sentence/paragraph form; \\n          this function will parse a raw text string into a nice list of words;\\n          filtering: keep only words having  more than 3 letters and remove \\n          punctuation, etc.\\n      """"""\\n      pattern = r'\\b[A-Za-z]{3,}\\b'\\n      pat_obj = re.compile(pattern)\\n      words = [ word.lower() for word in pat_obj.findall(raw_string) ]\\n      pattern = r'\\b[vixlm]+\\b'\\n      pat_obj = re.compile(pattern)\\n      return "" "".join([ word for word in words if not pat_obj.search(word) ])\\n\\n    def create_markov_dict(words_string):\\n      # initialize variables\\n      wb1, wb2, wb3 = "" "", "" "", "" ""\\n      l1, l2, l3 = wb1, wb2, wb3\\n      dx = {}\\n      for ch in words_string :\\n        dx.setdefault( (l1, l2, l3), [] ).append(ch)\\n        l1, l2, l3 = l2, l3, ch\\n      return dx\\n\\n    def generate_newtext(markov_dict) :\\n      simulated_text = """"\\n      l1, l2, l3 = "" "", "" "", "" ""\\n      for c in range(100) :\\n        next_letter = sample( markov_dict[(l1, l2, l3)], 1)[0]\\n        simulated_text += next_letter\\n        l1, l2, l3 = l2, l3, next_letter\\n      return simulated_text\\n\\n    if __name__==""__main__"" :\\n      # n = number of passes through the training text\\n      n = 1\\n      q1 = create_words_string(n * raw_str)\\n      q2 = create_markov_dict(q1)\\n      q3 = generate_newtext(q2)\\n      print(q3)\\n\\n",,
2826,5,1263,09d2f262-3b5e-472d-97cc-f610f3ba76ca,2010-08-05 10:00:43.0,88.0,"While making a call to `diag` you throw out a lot of information, so you can save time by simply not calculating it. You code is equivalent to:\\n\\n    sapply(1:100,function(i) cor(x[i,],y[i,]))\\n\\n**Extended to reflect comments:** This code will be slower for small matrices since it does not use the full ""vectorization power"" of `cor`. So, if you'd like to make fast calculations on small matrices, write it as a C chunk. If one would like to parallelize it (again, will be profitable only for large matrices), may use this code replacing `sapply` with `mc.lapply` or something like this.",added 397 characters in body,
2827,2,1289,61bfa650-6695-42ed-8c1b-6faca89bc048,2010-08-05 10:03:34.0,760.0,"I am having difficulties to select the right way to visualize data. Let's say we have **bookstores** that sells **books**, and every book has at least one **category**.\\n\\nFor a bookstore, if we count all the categories of books, we acquire a histogram that shows the number of books that falls into a specific category for that bookstore.\\n\\nI want to visualize the bookstore behavior, I want to see if they favor a category over other categories. I don't want to see if they are favoring sci-fi all together, but I want to see if they are treating every category equally or not.\\n\\nI have ~1M bookstores.\\n\\nI have thought of 4 methods:\\n\\n 1. Sample the data, show only 500 bookstore's histograms. Show them in 5 separate pages using 10x10 grid. Example of a 4x4 grid:\\n![multiple histograms 1][1]\\n\\n 2. Same as #1. But this time sort x axis values according to their count desc, so if there is a favoring it will be seen easily.\\n\\n 3. Imagine putting the histograms in #2 together like a deck and showing them in 3D. Something like this: ![3D histogram][2]\\n \\n 4. Instead of using third axis suing color to represent colors, so using a heatmap (2D histogram): ![2D histogram][3]\\nIf generally bookstores prefer some categories to others it will be displayed as a nice gradient from left to right.\\n\\nDo you have any other visualization ideas/tools to represent multiple histograms?\\n\\n\\n  [1]: http://i37.tinypic.com/2aes2uo.jpg2.\\n  [2]: http://www.statsoft.com/textbook/popups/popup110.gif\\n  [3]: http://i38.tinypic.com/1ptfrm.png",,
2828,1,1289,61bfa650-6695-42ed-8c1b-6faca89bc048,2010-08-05 10:03:34.0,760.0,"Visualizing multiple ""histograms""",,
2829,3,1289,61bfa650-6695-42ed-8c1b-6faca89bc048,2010-08-05 10:03:34.0,760.0,<data-visualization>,,
2830,2,1290,2dc9777d-cadc-4020-87aa-44b3a8ecc4e1,2010-08-05 10:17:47.0,88.0,"Generating combinations is pretty easy, see for instance [this][1]; write this code in R and then process each combination at a time it appears.\\n\\n\\n  [1]: http://compprog.wordpress.com/2007/10/17/generating-combinations-1/",,
2831,5,1262,811535a4-7437-4913-982b-97890072608e,2010-08-05 10:19:24.0,,"You could write the likelihood function like so:\\n\\n$L(p,i,j,k|-) \\propto (p^i)^{1261} (1-p^i)^{381} (p^j)^{230} (1-p^j)^{161} (p^k)^{3514} (1-p^k)^{4012}$\\n\\nMaximize the above likelihood function to estimate your parameters. Constructing confidence intervals and hypothesis testing should be straight forward once you have the estimates.",added 156 characters in body; deleted 156 characters in body,user28
2832,2,1291,eeaaa792-ad51-4cb4-aa8e-c6f29073ce37,2010-08-05 10:31:54.0,8.0,"As you have found out there are no easy answers to your question!\\n\\nI presume that you interested in finding strange or different book stores? If this is the case then you could try things like [PCA][1] (see the wikipedia [cluster analysis][2] page for more details).\\n\\nTo give you an idea, consider this example. You have 26 bookshops (with names A, B,..Z). All bookshops are similar, except that shop Z sells only a few History books. A principal components plot highlights this shop for further investigation.\\n\\nHere's some sample R code:\\n\\n    > d = data.frame(Romance = rpois(26, 50), Horror = rpois(26, 100), \\n                   Science = rpois(26, 75), History = rpois(26, 125))\\n    > rownames(d) = LETTERS\\n    > d[26,][4] = rpois(1, 10)\\n    #look at the data\\n    > head(d, 2)\\n           Romance Horror Science History\\n     A      36    107      62     139\\n     B      47     93      64     118\\n    > books.PC.cov = prcomp(d)\\n    > books.scores.cov = predict(books.PC.cov)\\n    # Plot of PC1 vs PC2\\n    > plot(books.scores.cov[,1],books.scores.cov[,2],\\n           xlab=""PC 1"",ylab=""PC 2"", pch=NA)\\n    > text(books.scores.cov[,1],books.scores.cov[,2],labels=LETTERS)\\n\\nThis gives the following plot: \\n\\n![PCA plot][3]\\n\\nNotice that shop z is an outlying point.\\n\\n\\n**Other possibilities**\\n\\nYou could also look at [GGobi][4], I've never used it, but it looks interesting.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance/78#78\\n  [2]: http://en.wikipedia.org/wiki/Cluster_analysis\\n  [3]: http://img19.imageshack.us/img19/9119/tmpsr.jpg\\n  [4]: http://www.ggobi.org/",,
2833,5,1291,19e04189-55d5-4880-942a-6f834e79b36f,2010-08-05 10:39:06.0,8.0,"As you have found out there are no easy answers to your question!\\n\\nI presume that you interested in finding strange or different book stores? If this is the case then you could try things like [PCA][1] (see the wikipedia [cluster analysis][2] page for more details).\\n\\nTo give you an idea, consider this example. You have 26 bookshops (with names A, B,..Z). All bookshops are similar, except:\\n\\n1.Shop Z sells only a few History books. \\n1.Shops O-Y sell more romance books than average.\\n\\nA principal components plot highlights these shops for further investigation.\\n\\nHere's some sample R code:\\n\\n    > d = data.frame(Romance = rpois(26, 50), Horror = rpois(26, 100), \\n                   Science = rpois(26, 75), History = rpois(26, 125))\\n    > rownames(d) = LETTERS\\n    #Alter a few shops\\n    > d[15:25,][1] = rpois(11,150)\\n    > d[26,][4] = rpois(1, 10)\\n    #look at the data\\n    > head(d, 2)\\n           Romance Horror Science History\\n     A      36    107      62     139\\n     B      47     93      64     118\\n    > books.PC.cov = prcomp(d)\\n    > books.scores.cov = predict(books.PC.cov)\\n    # Plot of PC1 vs PC2\\n    > plot(books.scores.cov[,1],books.scores.cov[,2],\\n           xlab=""PC 1"",ylab=""PC 2"", pch=NA)\\n    > text(books.scores.cov[,1],books.scores.cov[,2],labels=LETTERS)\\n\\nThis gives the following plot: \\n\\n![PCA plot][3]\\n\\nNotice that:\\n\\n1. Shop z is an outlying point.\\n1. The others shops form two distinct groups.\\n\\n\\n**Other possibilities**\\n\\nYou could also look at [GGobi][4], I've never used it, but it looks interesting.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance/78#78\\n  [2]: http://en.wikipedia.org/wiki/Cluster_analysis\\n  [3]: http://img265.imageshack.us/img265/7263/tmplx.jpg\\n  [4]: http://www.ggobi.org/",added 174 characters in body,
2834,2,1292,871c4cae-3e8e-4b52-88ab-168b15e5355c,2010-08-05 10:42:44.0,217.0,[Decision trees][1] seems to be a very understandable machine learning method.\\nOnce created it can be easily inspected by a human which is a great advantage in some applications. \\n\\nWhat are the practical weak sides of Decision Trees?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Decision_tree,,
2835,1,1292,871c4cae-3e8e-4b52-88ab-168b15e5355c,2010-08-05 10:42:44.0,217.0,What is the weak side of decision trees?,,
2836,3,1292,871c4cae-3e8e-4b52-88ab-168b15e5355c,2010-08-05 10:42:44.0,217.0,<machine-learning><nonparametric><cart>,,
2837,2,1293,b41fd11b-8e37-4570-8f3f-18a728aeb963,2010-08-05 10:47:12.0,59.0,My girlfriend (B.B.A) is really interested in Actuarial science. She's looking at self teaching her self. She's good with basic math (Calculus 1 and 2) and stats\\n\\nWhat are some of the essential sources she needs to read in order to learn and excel in the field?,,
2838,1,1293,b41fd11b-8e37-4570-8f3f-18a728aeb963,2010-08-05 10:47:12.0,59.0,Essential reads for people interested in Actuary and Actuarial science,,
2839,3,1293,b41fd11b-8e37-4570-8f3f-18a728aeb963,2010-08-05 10:47:12.0,59.0,<beginner>,,
2840,2,1294,5f36089c-f573-4379-b7ac-582c07f8c99f,2010-08-05 11:22:54.0,88.0,"I would suggest something that hasn't got a defined name (probably ""parallel plot"") and looks like this:  \\n\\n![alt text][1]\\n\\n\\nBasically you plot all counts for all bookstores as points over categories listed on x axis and connect the results from each bookstore with a line. Still this may be too tangled for 1M lines, though. The concept comes from GGobi which was already mentioned by csgillespie.\\n\\n  [1]: http://rosuda.org/iPlots/plots/pcp.png",,
2841,2,1295,444d020a-9907-4a4f-a0cb-299b6c90d3d2,2010-08-05 11:58:42.0,159.0,"One disadvantage is that all terms are assumed to interact. That is, you can't have two explanatory variables that behave independently. Every variable in the tree is forced to interact with every variable further up the tree. This is extremely inefficient if there are variables that have no or weak interactions.",,
2842,2,1296,22732174-a79e-443c-97b3-70ee583747e9,2010-08-05 11:59:25.0,762.0,"I have detector which will detect an event with some probability *p*. If the detector says that an event occured, then that is always the case, so there are not false-positives. After I run it for some time, I get *k* events detected. I would like to calculate what the total number of events that occured was, detected or otherwise, with some confidence, say 95%.\\n\\nSo for example, let's say I get 13 events detected. I would like to be able to calculate that there were between 13 and 19 events with 95% confidence based on *p*.\\n\\nHere's what I've tried so far:\\n\\nThe probability of detecting _k_ events if there were _n_ total is:\\n\\n`binomial(n, k) * p^k * (1 - p)^(n - k)`\\n\\nThe sum of that over _n_ from _k_ to infinity is:\\n\\n`1/p`\\n\\nWhich means, that the probability of there being _n_ events total is:\\n\\n`f(n) = binomial(n, k) * p^(k + 1) * (1 - p)^(n - k)`\\n\\nSo if I want to be 95% sure I should find the first partial sum `f(k) + f(k+1) + f(k+2) ... + f(k+m)` which is at least 0.95 and the answer is `[k, k+m]`. Is this the correct approach? Also is there a closed formula for the answer?\\n\\n",,
2843,1,1296,22732174-a79e-443c-97b3-70ee583747e9,2010-08-05 11:59:25.0,762.0,How to find a confidence interval for the total number of events,,
2844,3,1296,22732174-a79e-443c-97b3-70ee583747e9,2010-08-05 11:59:25.0,762.0,<beginner><probability><confidence-interval>,,
2845,2,1297,c1559b74-5d20-4f63-ba3a-23b24d9566d4,2010-08-05 12:08:23.0,495.0,"Here are a couple I can think of:\\n\\n - They can be extremely sensitive to small perturbations in the data: a slight change can result in a drastically different tree.\\n - They can easily overfit. This can be negated by validation methods and pruning, but this is a grey area.\\n - They can have problems out-of-sample prediction (this is related to them being non-smooth).\\n\\nSome of these are related to the problem of [multicollinearity][1]: when two variables both explain the same thing, a decision tree will greedily choose the best one, whereas many other methods will use them both. Ensemble methods such as random forests can negate this to a certain extent, but you lose the ease of understanding to a certain extent.\\n\\nHowever the biggest problem, from my point of view at least, is the lack of a principled probabilistic framework. Many other methods have things like confidence intervals, posterior distributions etc., which give us some idea of how good a model is. A decision tree is ultimately an ad hoc heuristic, which can still be very useful (they are excellent for finding the sources of bugs in data processing), but there is the danger of people treating the output as ""the"" correct model (from my experience, this happens a lot in marketing).\\n\\n  [1]: http://en.wikipedia.org/wiki/Multicollinearity",,
2846,2,1298,a595dace-4169-49f9-8fbe-964f21177f90,2010-08-05 12:12:16.0,,"I think you misunderstood the purpose of confidence intervals. Confidence intervals allow you to assess where the true value of the parameter is located. So, in your case, you can construct a confidence interval for $p$. It does not make sense to construct an interval for the data.\\n\\nHaving said that, once you have an estimate of $p$ you can calculate the probability that you will observe different realizations such as 14, 15 etc using the binomial pdf.",,user28
2847,2,1299,60227756-2f16-4642-a72f-303a65d390f5,2010-08-05 12:47:45.0,438.0,"My answer is directed to CART (the C 4.5/C 5 implementations) though i don't think are limited to it. My guess is that this is what the OP has in mind--it's usually what someone means when they say ""Decision Tree.""\\n\\n\\n**Limitations of Decision Trees**:<hr>\\n\\n\\n**Low-Performance**\\n\\nBy 'performance' i don't mean resolution, but *execution speed*. The reason why it's poor is that you need to 'redraw the tree' every time you wish to update your CART model--data classified by an already-trained Tree, that you then want to add to the Tree (i.e., use as a training data point) requires that you start from over--training instances can not be added incrementally, as they can for most other supervised learning algorithms. Perhaps the best way to state this is that Decision Trees cannot be trained in online mode, rather only in batch mode. Obviously you won't notice this limitation if you don't update your classifier, but then i would expect that you see a drop in resolution.  \\n\\nThis is significant because for Multi-Layer Perceptrons for instance, once it's trained, then it can begin classifying data; that data can also be used to 'tune' the already-trained classifier, though with Decision Trees, you need to retrain with the entire data set (original data used in training plus any new instances).<hr>\\n\\n\\n**Poor Resolution on Data With Complex Relationships Among the Variables**\\n\\nDecision Trees classify by step-wise assessment of a data point of unknown class, one node at time, starting at the root node and ending with a terminal node.  And at each node, only two possibilities are possible (left-right), hence there are some variable relationships that Decision Trees just can't learn.<hr>\\n\\n\\n**Practically Limited to Classification** \\n\\nDecision Trees work best when they are trained to assign a data point to a class--preferably one of only a few possible classes. I don't believe i have ever had any success using a Decision Tree in regression mode (i.e., continuous output, such as price, or expected lifetime revenue). This is not a formal or inherent limitation but a practical one. Most of the time, Decision Trees are used for prediction of factors or discrete outcomes. \\n<hr>\\n\\n**Poor Resolution With Continuous Expectation Variables**\\n\\nAgain, in principle, it's ok to have independent variables like ""download time"" or ""number of days since previous online purchase""--just change your splitting criterion to variance (it's usually Information Entropy or Gini Impurity for discrete variables) but in my experience Decision Trees rarely work well in these instance. Exceptions are cases like ""student's age"" which looks continuous but in practice the range of values is quite small (particularly if they are reported as integers). \\n\\n",,
2848,5,1291,e82e6988-4658-477e-b61d-c2c106ded248,2010-08-05 12:58:40.0,8.0,"As you have found out there are no easy answers to your question!\\n\\nI presume that you interested in finding strange or different book stores? If this is the case then you could try things like [PCA][1] (see the wikipedia [cluster analysis][2] page for more details).\\n\\nTo give you an idea, consider this example. You have 26 bookshops (with names A, B,..Z). All bookshops are similar, except:\\n\\n1. Shop Z sells only a few History books.\\n1. Shops O-Y sell more romance books than average.\\n\\nA principal components plot highlights these shops for further investigation.\\n\\nHere's some sample R code:\\n\\n    > d = data.frame(Romance = rpois(26, 50), Horror = rpois(26, 100), \\n                   Science = rpois(26, 75), History = rpois(26, 125))\\n    > rownames(d) = LETTERS\\n    #Alter a few shops\\n    > d[15:25,][1] = rpois(11,150)\\n    > d[26,][4] = rpois(1, 10)\\n    #look at the data\\n    > head(d, 2)\\n           Romance Horror Science History\\n     A      36    107      62     139\\n     B      47     93      64     118\\n    > books.PC.cov = prcomp(d)\\n    > books.scores.cov = predict(books.PC.cov)\\n    # Plot of PC1 vs PC2\\n    > plot(books.scores.cov[,1],books.scores.cov[,2],\\n           xlab=""PC 1"",ylab=""PC 2"", pch=NA)\\n    > text(books.scores.cov[,1],books.scores.cov[,2],labels=LETTERS)\\n\\nThis gives the following plot: \\n\\n![PCA plot][3]\\n\\nNotice that:\\n\\n1. Shop z is an outlying point.\\n1. The others shops form two distinct groups.\\n\\n\\n**Other possibilities**\\n\\nYou could also look at [GGobi][4], I've never used it, but it looks interesting.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/53/pca-on-correlation-or-covariance/78#78\\n  [2]: http://en.wikipedia.org/wiki/Cluster_analysis\\n  [3]: http://img265.imageshack.us/img265/7263/tmplx.jpg\\n  [4]: http://www.ggobi.org/",added 1 characters in body,
2851,2,1302,494058ed-4bd6-49d8-bb15-6695091b813e,2010-08-05 13:04:27.0,,sources:\\n\\n1) statistics upto ANOVA\\n2) probability upto Central limit theorem\\n3) Basic programming and data analysis skills\\n4) Familiarity with business economics\\n5) Financial mathematics\\n\\nThis is a verrry basic smattering of things that she needs to read.,,Jatin Khanna
2852,10,109,25ad28f5-e4f0-4408-a967-2968c8b15046,2010-08-05 13:06:12.0,-1.0,"{""Voters"":[{""Id"":8,""DisplayName"":""csgillespie""},{""Id"":190,""DisplayName"":""Peter Smit""},{""Id"":88,""DisplayName"":""mbq""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":8,""DisplayName"":""csgillespie""}]}",4,
2854,5,1297,eba46583-40ed-415d-aada-b6eaad59131a,2010-08-05 13:31:59.0,495.0,"Here are a couple I can think of:\\n\\n - They can be extremely sensitive to small perturbations in the data: a slight change can result in a drastically different tree.\\n - They can easily overfit. This can be negated by validation methods and pruning, but this is a grey area.\\n - They can have problems out-of-sample prediction (this is related to them being non-smooth).\\n\\nSome of these are related to the problem of [multicollinearity][1]: when two variables both explain the same thing, a decision tree will greedily choose the best one, whereas many other methods will use them both. Ensemble methods such as random forests can negate this to a certain extent, but you lose the ease of understanding.\\n\\nHowever the biggest problem, from my point of view at least, is the lack of a principled probabilistic framework. Many other methods have things like confidence intervals, posterior distributions etc., which give us some idea of how good a model is. A decision tree is ultimately an ad hoc heuristic, which can still be very useful (they are excellent for finding the sources of bugs in data processing), but there is the danger of people treating the output as ""the"" correct model (from my experience, this happens a lot in marketing).\\n\\n  [1]: http://en.wikipedia.org/wiki/Multicollinearity",fix prose,
2855,2,1304,5b41d825-3fc9-4e64-b6c8-71785807debf,2010-08-05 13:35:59.0,56.0,"If you measure $k$ events and know your detection efficiency is $p$ you can automatically correct your measured result up to the ""true"" count $k_\\mathrm{true} = k/p$.\\n\\nYour question is then about finding the range of $k_\\mathrm{true}$ where 95%\\nof the observations will fall. You can use the [Feldman-Cousins method][1] to estimate this interval. If you have access to [ROOT][2] there is a class to do this calculation for you.\\n\\n    {\\n    gSystem->Load(""libPhysics"");\\n\\n    const double lvl = 0.95;\\n    TFeldmanCousins f(lvl);\\n\\n    const double p = 0.95;\\n    const double k = 13;\\n    const double k_true = k/p;\\n\\n    const double k_bg = 0;\\n\\n    const double upper = f.CalculateUperLimit(k_true, k_bg);\\n    const double lower = f.GetLowerLimit();\\n\\n    std::cout << ""[""\\n      lower <<""...""<<\\n      k_true <<""...""<<\\n      upper <<\\n      ""]"" << std::endl;\\n    }\\n\\n[1]: http://arxiv.org/abs/physics/9711021\\n[2]: http://root.cern.ch/drupal/",,
2856,2,1305,30202e1a-2b3e-436e-9a6f-698fd75df8cf,2010-08-05 13:37:05.0,713.0,"Can anyone point towards applications, SQL algorithms or similar that automate the production of standarized rates/ratios (ie directly/indirectly age/sex etc.-standardized rates/ratios) with confidence intervals from numerator, denominator & reference data?  The process would ideally prevent inappropriate calculations, eg from small counts.",,
2857,1,1305,30202e1a-2b3e-436e-9a6f-698fd75df8cf,2010-08-05 13:37:05.0,713.0,Automation of standardization,,
2858,3,1305,30202e1a-2b3e-436e-9a6f-698fd75df8cf,2010-08-05 13:37:05.0,713.0,<statistical-analysis><statistical-significance><confidence-interval><software><epidemiology>,,
2859,2,1306,c1eccf30-3f3c-408e-b772-ba178053cc4b,2010-08-05 13:56:03.0,449.0,"I have a suspicion that 'exact' tests and sampling weights are essentially incompatible concepts. I checked in Stata, which has good facilities for sample surveys and reasonable ones for exact tests, and its 8 possible test statistics for a crosstab with sample weights don't include any 'exact' tests such as Fisher's. \\n\\nThe relevant Stata manual entry (for *svy: tabulate twoway*) advises using its default test in all cases. This default method is based on the usual Pearson's chi-squared statistic. To quote: \\n\\n""To account for the survey design, the statistic is turned into an F statistic with noninteger degrees of freedom by using a second-order Rao and Scott (1981, 1984) correction"". \\n\\nRefs:\\n\\n* Rao, J. N. K., and A. J. Scott. 1981. The analysis of categorical data from complex sample surveys: Chi-squared tests for goodness of fit and independence in two-way tables. Journal of the American Statistical Association 76:221–230.\\n* Rao, J. N. K., and A. J. Scott. 1984. On chi-squared tests for multiway contingency tables with cell proportions estimated from survey data. Annals of Statistics 12: 46–60.",,
2860,2,1307,25e739ff-9bee-4d5b-a24b-827bf730e806,2010-08-05 13:57:42.0,247.0,"And for bedtime reading, ""Against the Gods: The Remarkable Story of Risk"" by Peter L. Bernstein. She'll find out how Lloyds of London started, among many other interesting bits. Highly recommended and only $13 from amazon.",,
2861,2,1308,dae1e021-9c72-43db-9edd-4aed192b7d2c,2010-08-05 14:00:07.0,765.0,"In the traditional birthday paradox the question is ""what are the chances that two or more people in a group of n people share a birthday"".  I'm stuck on a problem which is an extension of this.\\n\\nInstead of knowing the probability that two people share a birthday I need to extend the question to know what is the probability that x or more people share a birthday.  With x=2 you can do this by calculating the probability that no two people share a birthday and subtract that from 1, but I don't think I can extend this logic to larger numbers of x.\\n\\nTo further complicate this I also need a solution which will work for very large numbers for n (millions) and x (thousands).",,
2862,1,1308,dae1e021-9c72-43db-9edd-4aed192b7d2c,2010-08-05 14:00:07.0,765.0,Extending the birthday paradox to more than 2 people,,
2863,3,1308,dae1e021-9c72-43db-9edd-4aed192b7d2c,2010-08-05 14:00:07.0,765.0,<probability>,,
2865,2,1309,7a703d37-92b8-4140-bee7-deec9943fb97,2010-08-05 14:10:26.0,5.0,"You might have a look to [this generalized solution][1] on the wikipedia article.\\n\\nIt is also always possible to solve this problem with a monte-carlo solution, although that's far from the most efficient.\\n\\n  [1]: http://en.wikipedia.org/wiki/Birthday_problem#Generalization_to_multiple_types",,
2867,16,1302,00000000-0000-0000-0000-000000000000,2010-08-05 14:32:53.0,88.0,,,
2868,16,1307,00000000-0000-0000-0000-000000000000,2010-08-05 14:32:53.0,88.0,,,
2869,16,1293,00000000-0000-0000-0000-000000000000,2010-08-05 14:32:53.0,88.0,,,
2870,5,1309,e7987be1-722b-4a6c-96b6-fdb7b14cb567,2010-08-05 14:41:45.0,5.0,"It is always possible to solve this problem with a monte-carlo solution, although that's far from the most efficient.\\n\\n  [1]: http://en.wikipedia.org/wiki/Birthday_problem#Generalization_to_multiple_types",deleted 90 characters in body,
2871,6,1308,94ff88e6-1a45-4a69-a0a3-5d26c14a5180,2010-08-05 14:48:45.0,,<probability>,added tag,user28
2873,5,1309,6eafbd08-e71c-4408-a714-df5dcfb19d2d,2010-08-05 14:54:11.0,5.0,"It is always possible to solve this problem with a monte-carlo solution, although that's far from the most efficient.  Here's a simple example of the 2 person problem in R (from [a presentation I gave last year][1]; I used this as an example of inefficient code), which could be easily adjusted to account for more than 2:\\n\\n    birthday.paradox <- function(n.people, n.trials) {\\n    	matches <- 0\\n    	for (trial in 1:n.trials) {\\n    		birthdays <- cbind(as.matrix(1:365), rep(0, 365))\\n    		for (person in 1:n.people) {\\n    			day <- sample(1:365, 1, replace = TRUE)\\n    			if (birthdays[birthdays[, 1] == day, 2] == 1) {\\n    				matches <- matches + 1\\n    				break\\n    			}\\n    			birthdays[birthdays[, 1] == day, 2] <- 1\\n    		}\\n    		birthdays <- NULL\\n    	}\\n    	print(paste(""Probability of birthday matches = "", matches/n.trials))\\n    }\\n\\n\\n  [1]: http://www.meetup.com/nyhackr/calendar/10251302/?from=list&offset=0",added 734 characters in body,
2874,6,1308,2e6df122-6676-4444-80f4-dde97737191a,2010-08-05 15:00:32.0,8.0,<probability><bioinformatics>,edited tags,
2875,2,1311,b04abb9f-6047-4a71-81b3-d66697a5c3e3,2010-08-05 15:27:49.0,,"This is an attempt at a general solution. There may be some mistakes so use with caution!\\n\\nFirst some notation:\\n\\n$P(x,n)$ be the probability that $x$ or more people share a birthday among $n$ people,\\n\\n$P(y|n)$ be the probability that *exactly* $y$ people share a birthday among $n$ people.\\n\\nNotes:\\n\\n 1. Abuse of notation as $P(.)$ is being used in two different ways.\\n\\n 2. By definition $y$ cannot take the value of 1 as it does not make any sense and $y$ = 0 can be interpreted to mean that no one shares a common birthday.\\n\\nThen the required probability is given by:\\n\\n$P(x,n) = 1 - P(0|n) - P(2|n) - P(3|n) .... - P(x-1|n)$\\n\\nNow,\\n\\n$P(y|n) = {n \\choose y} (\\frac{365}{365})^y \\ \\prod_{k=1}^{k=n-y}\\(1 -\\frac{k}{365})$\\n\\nHere is the logic: You need the probability that exactly $y$ share a birthday. \\n\\nStep 1: You can pick $y$ people in ${n \\choose y}$ ways.\\n\\nStep 2: Since they share a birthday it can be any of the 365 days in a year. So, we basically have 365 choices which gives us $(\\frac{365}{365})^y$.\\n\\nStep 3: The remaining $n-y$ people should not share a birthday with the first $y$ people or with each other. This reasoning gives us $\\prod_{k=1}^{k=n-y}\\(1 -\\frac{k}{365})$.\\n\\nYou can check that for $y$ = 0 the above collapses to the standard birthday paradox solution.",,user28
2876,5,1311,2033f62d-f769-4e75-960f-ddaf1d37d416,2010-08-05 15:34:26.0,,"This is an attempt at a general solution. There may be some mistakes so use with caution!\\n\\nFirst some notation:\\n\\n$P(x,n)$ be the probability that $x$ or more people share a birthday among $n$ people,\\n\\n$P(y|n)$ be the probability that *exactly* $y$ people share a birthday among $n$ people.\\n\\nNotes:\\n\\n 1. Abuse of notation as $P(.)$ is being used in two different ways.\\n\\n 2. By definition $y$ cannot take the value of 1 as it does not make any sense and $y$ = 0 can be interpreted to mean that no one shares a common birthday.\\n\\nThen the required probability is given by:\\n\\n$P(x,n) = 1 - P(0|n) - P(2|n) - P(3|n) .... - P(x-1|n)$\\n\\nNow,\\n\\n$P(y|n) = {n \\choose y} (\\frac{365}{365})^y \\ \\prod_{k=1}^{k=n-y}\\(1 -\\frac{k}{365})$\\n\\nHere is the logic: You need the probability that exactly $y$ people share a birthday. \\n\\nStep 1: You can pick $y$ people in ${n \\choose y}$ ways.\\n\\nStep 2: Since they share a birthday it can be any of the 365 days in a year. So, we basically have 365 choices which gives us $(\\frac{365}{365})^y$.\\n\\nStep 3: The remaining $n-y$ people should not share a birthday with the first $y$ people or with each other. This reasoning gives us $\\prod_{k=1}^{k=n-y}\\(1 -\\frac{k}{365})$.\\n\\nYou can check that for $x$ = 2 the above collapses to the standard birthday paradox solution.",fixed typo; edited body,user28
2877,2,1312,a8bd6f1c-b87c-412e-8737-da180fc0cc8d,2010-08-05 16:04:34.0,643.0,"It depends on exactly what you mean - your notation is a bit ambiguous. If I understand your notation correctly, you don't actually have 4 independent random variables - $X_3$ is a deterministic function of $Y_1$ and $Y_2$, and so it shouldn't occur explicitly in the likelihood. $Y_3$ is a function of the deterministic node $X_3$, and hence is a function of $Y_1$ and $Y_2$ when you drop out the deterministic node to form the likelihood, which is:\\n\\n$f(Y_1)f(Y_2)f(Y_3|Y_1,Y_2)$\\n\\nprovided that $Y_1Y_2>0$ (for consistency with data on $X_3$) and is zero otherwise. Is that what you meant?\\n",,
2879,5,1272,db43c4a2-2ea9-4d3c-beb3-a6d53ca6e37b,2010-08-05 16:33:34.0,,"I have a set of data which consists of many different types (measurable, categorical)\\nFor example:\\nname   measurable_attribute_1   categorical_attribute_1   measurable_attribute_2   categorical_attribute_2 ...\\n\\nNumber of attributes may grows quite quickly during my study: into my spreadsheet, I can as many new entries as attribute... I have about a hundred of entries in this classification scheme, about 70 attribute, so far, and I am at the beginning of my data collection.\\n\\nI would like to perform statistical analysis of this data set. For example, what are the common features of the entries that have a similar categorical_attribute and this range of values of measurable_attribute.\\n\\nWell, I would like to generate relationships between attributes in order to create training images. \\nHowever, I am not sure of how to organize the data prior to classification. Even though, should I organize the data?... (referring to this <a href=""http://stats.stackexchange.com/questions/47/clustering-of-large-heavy-tailed-dataset"">question</a>) \\n\\nAlso, I can hardly gather entries into classes.\\n\\nI do not want to introduce any bias obviously.\\n\\nI am also quite new to statistical analysis (but eager to learn).",added 1 characters in body,MarcO
2880,5,1304,361ad05d-468b-4a4d-aabe-a9e0cfbd69c2,2010-08-05 16:34:40.0,56.0,"If you measure $k$ events and know your detection efficiency is $p$ you can automatically correct your measured result up to the ""true"" count $k_\\mathrm{true} = k/p$.\\n\\nYour question is then about finding the range of $k_\\mathrm{true}$ where 95%\\nof the observations will fall. You can use the [Feldman-Cousins method][1] to estimate this interval. If you have access to [ROOT][2] there is a class to do this calculation for you.\\n\\nYou would calculate the upper and lower limits with Feldman-Cousins from the\\n*uncorrected* number of events $k$ and then scale them up to 100% with $1/p$.\\nThis way the actual number of measurements determines your uncertainty, not\\nsome scaled number that wasn't measured.\\n\\n    {\\n    gSystem->Load(""libPhysics"");\\n\\n    const double lvl = 0.95;\\n    TFeldmanCousins f(lvl);\\n\\n    const double p = 0.95;\\n    const double k = 13;\\n    const double k_true = k/p;\\n\\n    const double k_bg = 0;\\n\\n    const double upper = f.CalculateUperLimit(k, k_bg) / p;\\n    const double lower = f.GetLowerLimit() / p;\\n\\n    std::cout << ""[""\\n      lower <<""...""<<\\n      k_true <<""...""<<\\n      upper <<\\n      ""]"" << std::endl;\\n    }\\n\\n[1]: http://arxiv.org/abs/physics/9711021\\n[2]: http://root.cern.ch/drupal/",Scale correctly,
2881,2,1313,a575eb02-056d-4db0-b765-61d4e5836915,2010-08-05 16:54:35.0,279.0,"If you cannot collect data on a different ward where you don't do the intervention, your conclusions will be weak, because you cannot rule out other causes that act simultaneously (change in weather, season, epidemic of something, etc, etc). However if you observe a large effect, your study would still contribute an interesting piece of evidence.\\n\\nThe rest of your questions are a bit confused. If your outcome is binary: infection yes/no in a bunch of patients (probably adjusted for length of stay so it becomes a rate?), then you could not even do a t-test, so there is no point in discussing its appropriateness. But in the sense that it looks at differences it is similar to a t-test when you have continuous outcomes.\\n\\nThere is a test loosely called ""ratio t-test"", which is a t-test conducted on log-transformed data that concentrates on ratios instead of differences. So it is in some sense the counterpart of IRR, however I don't think you could actually perform it, because you don't have a continuous outcome variable.\\n\\nSo pick either the IRD or IRR. The difference is usually more important from a public health point of view, while ratios tend to be more impressive especially for rare events.",,
2887,6,1293,1e65bbb0-d340-4c73-95dc-f3348ff0cbb8,2010-08-05 19:08:46.0,,<beginner><careers>,edited tags,user28
2888,2,1315,8e5b0973-99fd-4842-b65d-4485b3da42e4,2010-08-05 19:26:34.0,775.0,"i've sampled a real world process, network ping times. The ""round-trip-time"" is measured in milliseconds. Results are plotted in a histogram:\\n\\n![alt text][1]\\n\\nPing times have a minimum value, but a long upper tail.\\n\\ni want to know what statistical distribution this is, and how to estimate its parameters. \\n\\nEven though the distribution is not a normal distribution, i can still show what i am trying to achieve.\\n\\nThe normal distribution uses the function:\\n\\n![alt text][2]\\n\\nwith the two parameters\\n\\n- μ (mean)\\n- σ<sup>2 </sup> (variance)\\n\\nParameter estimation\\n--------------------\\n\\nThe formulas for estimating the two parameters are:\\n\\n![alt text][3]\\n\\nApplying these formulas against the data i have in Excel, i get:\\n\\n- μ = 10.9558 (mean)\\n- σ<sup>2 </sup> = 67.4578 (variance)\\n\\nWith these parameters i can plot the ""*normal*"" distribution over top my sampled data:\\n\\n![alt text][4]\\n\\nObviously it's not a normal distribution. A normal distribution has an infinite top and bottom tail, and is symmetrical. This distribution is not symmetrical.\\n\\n\\n----------\\n\\nWhat principles would i apply, what flowchart, would i apply to determine what kind of distribution this is?\\n\\nAnd cutting to the chase, what is the formula for that distribution, and what are the formulas to estimate its parameters?\\n\\n----------\\n\\ni want to get the distribution so i can get the ""average"" value, as well as the ""spread"":\\n![alt text][5]\\n\\ni am actually plotting the histrogram in software, and i want to overlay the theoretical distribution:\\n\\n![alt text][6]\\n\\n**Note:** Cross-posted from [math.stackexchange.com][7]\\n\\n\\n**Tags**: sampling, statistics, parameter-estimation, normal-distribution\\n\\n\\n  [1]: http://i36.tinypic.com/2qtfp6f.jpg\\n  [2]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [3]: http://upload.wikimedia.org/math/3/2/4/324081bc08a66fbeba64f43a3dbd5e1a.png\\n  [4]: http://i34.tinypic.com/2mqu3wl.jpg\\n  [5]: http://i38.tinypic.com/2i27sp4.jpg\\n  [6]: http://i38.tinypic.com/nqpnck.jpg\\n  [7]: http://math.stackexchange.com/questions/1648/how-do-i-figure-out-what-kind-of-distribution-this-is",,
2889,1,1315,8e5b0973-99fd-4842-b65d-4485b3da42e4,2010-08-05 19:26:34.0,775.0,How do i figure out what kind of distribution this is?,,
2890,3,1315,8e5b0973-99fd-4842-b65d-4485b3da42e4,2010-08-05 19:26:34.0,775.0,<distributions><normality><sample><sample-size>,,
2892,2,1316,b3e981b1-2735-4aac-8466-fb21b1335941,2010-08-05 19:33:56.0,,"Try the [gamma][1] distribution which is parametrized as $x \\sim Gamma(k,\\theta)$. If you see these [pdf plots][2] of the gamma from the wiki you will see that there are some plots that look similar to what you have.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Gamma_distribution\\n  [2]: http://en.wikipedia.org/wiki/File:Gamma_distribution_pdf.svg",,user28
2893,5,1315,818151b9-e237-49f6-b284-f008fe2b9128,2010-08-05 19:37:00.0,775.0,"i've sampled a real world process, network ping times. The ""round-trip-time"" is measured in milliseconds. Results are plotted in a histogram:\\n\\n![alt text][1]\\n\\nPing times have a minimum value, but a long upper tail.\\n\\ni want to know what statistical distribution this is, and how to estimate its parameters. \\n\\nEven though the distribution is not a normal distribution, i can still show what i am trying to achieve.\\n\\nThe normal distribution uses the function:\\n\\n![alt text][2]\\n\\nwith the two parameters\\n\\n- μ (mean)\\n- σ<sup>2 </sup> (variance)\\n\\nParameter estimation\\n--------------------\\n\\nThe formulas for estimating the two parameters are:\\n\\n![alt text][3]\\n\\nApplying these formulas against the data i have in Excel, i get:\\n\\n- μ = 10.9558 (mean)\\n- σ<sup>2 </sup> = 67.4578 (variance)\\n\\nWith these parameters i can plot the ""*normal*"" distribution over top my sampled data:\\n\\n![alt text][4]\\n\\nObviously it's not a normal distribution. A normal distribution has an infinite top and bottom tail, and is symmetrical. This distribution is not symmetrical.\\n\\n\\n----------\\n\\n> - What principles would i apply, what\\n> flowchart, would i apply to determine\\n> what kind of distribution this is?\\n> - Given that the distribution has no negative tail, and long positive tail: what distributions match that? \\n> - Is there a reference that matches distributions to the observations you're taking?\\n\\nAnd cutting to the chase, what is the formula for this distribution, and what are the formulas to estimate its parameters?\\n\\n----------\\n\\ni want to get the distribution so i can get the ""average"" value, as well as the ""spread"":\\n![alt text][5]\\n\\ni am actually plotting the histrogram in software, and i want to overlay the theoretical distribution:\\n\\n![alt text][6]\\n\\n**Note:** Cross-posted from [math.stackexchange.com][7]\\n\\n\\n**Tags**: sampling, statistics, parameter-estimation, normal-distribution\\n\\n\\n  [1]: http://i36.tinypic.com/2qtfp6f.jpg\\n  [2]: http://upload.wikimedia.org/math/9/e/1/9e1e4a3af93c9680ba75669a0b69fbf6.png\\n  [3]: http://upload.wikimedia.org/math/3/2/4/324081bc08a66fbeba64f43a3dbd5e1a.png\\n  [4]: http://i34.tinypic.com/2mqu3wl.jpg\\n  [5]: http://i38.tinypic.com/2i27sp4.jpg\\n  [6]: http://i38.tinypic.com/nqpnck.jpg\\n  [7]: http://math.stackexchange.com/questions/1648/how-do-i-figure-out-what-kind-of-distribution-this-is",added 209 characters in body,
2894,2,1317,59f00d27-5620-4edc-b18e-a5317ccdd252,2010-08-05 19:41:53.0,776.0,9 out of ten dentists think the 10th dentist is an idiot. \\n\\n- No idea who said it.,,
2895,16,1317,59f00d27-5620-4edc-b18e-a5317ccdd252,2010-08-05 19:41:53.0,-1.0,,,
2896,2,1318,bad8d1f8-fa2c-4839-82ad-1928aefd8b11,2010-08-05 19:51:02.0,247.0,"There is no reason to expect that any real world data set will fit a known distributional form...especially from such a known messy data source.\\n\\nWhat you want to do with the answers will largely indicate an approach. For example, if you want to know when the ping times have changed significantly, then trending the empirical distribution may be a way to go. If you want to identify outliers, other techniques may be more appropriate.",,
2897,5,1316,813d621b-c190-4b75-9f7b-9baf54f183af,2010-08-05 19:51:03.0,,"Try the [gamma][1] distribution which is parametrized as $x \\sim Gamma(k,\\theta)$. If you see these [pdf plots][2] of the gamma from the wiki you will see that there are some plots that look similar to what you have.\\n\\n**Update- Estimation Process**\\n\\nThe estimation via [maximum likelihood][3] is tricky but possible. I imagine you can start with the approximate solution given by the wiki for [$\\hat{\\theta}$][4] and [$\\hat{k}$][5] and if the plots look ok and if needed you can estimate $\\hat{k}$ more accurately using the details in the wiki. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Gamma_distribution\\n  [2]: http://en.wikipedia.org/wiki/File:Gamma_distribution_pdf.svg\\n  [3]: http://en.wikipedia.org/wiki/Gamma_distribution#Maximum_likelihood_estimation\\n  [4]: http://upload.wikimedia.org/math/3/2/0/3203c979914ef4e2f354617a7723df81.png\\n  [5]: http://upload.wikimedia.org/math/8/8/1/881fb8c7ad01bb1b1505dcacf092b4e4.png",added 587 characters in body,user28
2898,2,1319,4f3b243d-db16-4fc0-8bff-3ac4740b3551,2010-08-05 20:13:15.0,61.0,"Let me ask a more basic question:  what do you want to *do* with this distributional information?\\n\\nThe reason I ask is because it may well make more sense to approximate the distribution with some sort of kernel density estimator, rather than insist that it fit into one of the (possibly shifted) exponential family distributions.  You can answer almost all of the same sorts of questions that a standard distribution will let you answer, and you don't have to worry (as much) about whether you've selected the correct model.\\n\\nBut if there's a fixed minimum time, and you must have some sort of compactly parameterized distribution to go with it, then just eyeballing it I'd subtract off the minimum and fit a gamma, like others have suggested.  ",,
2900,2,1320,798f305c-df25-4144-a2df-c518e2f37c05,2010-08-05 20:38:41.0,778.0,"No statistican, but useful for the profession:\\n\\n> The perfect is the enemy of the good \\n> - Voltaire",,
2901,16,1320,798f305c-df25-4144-a2df-c518e2f37c05,2010-08-05 20:38:41.0,-1.0,,,
2902,2,1321,ecea309e-d550-416f-94c7-c551fd7b7668,2010-08-05 21:12:50.0,776.0,"What would be the best way to display changes in two scalar variables (x,y) over time (z), in one visualization?\\n\\nOne idea that I had was to plot x and y both on the vertical axis, with z as the horizontal. \\n\\nNote: I'll be using R and likely ggplot2",,
2903,1,1321,ecea309e-d550-416f-94c7-c551fd7b7668,2010-08-05 21:12:50.0,776.0,Visualizing two scalar variables over time. ,,
2904,3,1321,ecea309e-d550-416f-94c7-c551fd7b7668,2010-08-05 21:12:50.0,776.0,<r><data-visualization>,,
2905,12,1317,2611d156-646a-44ab-b43d-36ea2396f32c,2010-08-05 21:17:42.0,776.0,"{""Voters"":[{""Id"":776,""DisplayName"":""Brandon Bertelsen""}]}",,
2906,13,1317,728257e4-116b-4d7d-959a-6e1d7d767ec4,2010-08-05 21:17:45.0,776.0,"{""Voters"":[{""Id"":776,""DisplayName"":""Brandon Bertelsen""}]}",,
2907,2,1322,5f6952a0-7790-461f-bb7b-6200392e989e,2010-08-05 22:01:03.0,88.0,"The other idea is to plot one series as x and the second as y -- the time dependency will be hidden,  but this plots shows correlations pretty well. (Yet time can be shown to some extent by connecting points chronologically; if the series are quite short and continuous it should be readable.)",,
2911,4,1321,05458894-7353-476d-a9e3-bc0b0baedb2c,2010-08-05 22:02:35.0,88.0,Visualizing two scalar variables over time,"Added ts tag, title fixed.",
2912,6,1321,05458894-7353-476d-a9e3-bc0b0baedb2c,2010-08-05 22:02:35.0,88.0,<r><time-series><data-visualization>,"Added ts tag, title fixed.",
2914,2,1324,fa078acc-f3ba-4e8e-a32d-237faa6e0f61,2010-08-05 22:16:20.0,1356.0,"The title is quite self-explanatory - I'd like to know if there's any other parametric technique apart from repeated-measures ANOVA, that can be utilized in order to compare several (more than 2) repeated measures?",,
2915,1,1324,fa078acc-f3ba-4e8e-a32d-237faa6e0f61,2010-08-05 22:16:20.0,1356.0,Parametric techniques for n-related samples,,
2916,3,1324,fa078acc-f3ba-4e8e-a32d-237faa6e0f61,2010-08-05 22:16:20.0,1356.0,<statistical-analysis><repeated-measures><parametric>,,
2917,2,1325,7b419d4c-e477-48ef-ac0c-5ec108eb0665,2010-08-05 22:17:29.0,603.0,"Weibull is sometimes used for modelling ping time. try a weibull distribution. To fit one in R:\\n\\n    x<-rweibull(n=1000,shape=2,scale=100)\\n    #generate a weibull (this should be your data).\\n    hist(x)\\n    #this is an histogram of your data.\\n    library(survival)\\n    a1<-survreg(Surv(x,rep(1,1000))~1,dist='weibull')\\n    exp(a1$coef) #this is the ML estimate of the scale parameter\\n    1/a1$scale     #this is the ML estimate of the shape parameter\\n\\nIf you're wondering for the goofy names (i.e. $scale to get the inverse of the shape)\\nit's because ""survreg"" uses another parametrization.",,
2918,2,1326,d27d18ac-619d-4d43-9b69-1ca50fde12cc,2010-08-05 22:35:46.0,71.0,"Multilevel/hierarchical linear models can be used for this.  Essentially, each repetition of the measure is clustered within the individual; individuals can then be clustered within other hierarchies.\\n\\nThe canonical text is [Raudenbush and Bryk][1]; I'm also really fond of [Gelman and Hill][2].  [Here's a tutorial I read some time ago][3] - you may or may not find the tutorial itself useful (that's so often a matter of personal taste, training and experience), but the bibliography at the end is good.\\n\\n\\n  [1]: http://www.amazon.com/Hierarchical-Linear-Models-Applications-Quantitative/dp/076191904X\\n  [2]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?s=books&ie=UTF8&qid=1281047453&sr=1-1\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.5130&rep=rep1&type=pdf",,
2919,5,1326,4ec43ab8-4419-4d12-87ee-528b49140567,2010-08-05 22:43:34.0,71.0,"Multilevel/hierarchical linear models can be used for this.  Essentially, each repetition of the measure is clustered within the individual; individuals can then be clustered within other hierarchies.  For me, at least, it's more intuitive than repeated-measures ANOVA.\\n\\nThe canonical text is [Raudenbush and Bryk][1]; I'm also really fond of [Gelman and Hill][2].  [Here's a tutorial I read some time ago][3] - you may or may not find the tutorial itself useful (that's so often a matter of personal taste, training and experience), but the bibliography at the end is good.\\n\\nI should note that Gelman and Hill doesn't have a ton on multilevel models specifically for repeated measures; I can't remember if that's the case or not for Raudenbush and Bryk.  \\n\\nEdit: Found a book I was looking for - [Applied Longitudinal Data Analysis by Singer and Willett][4] has an explicit focus on multilevel models for repeated measures.  I haven't had a chance to read very far into it, but it might be worth looking into.\\n\\n\\n  [1]: http://www.amazon.com/Hierarchical-Linear-Models-Applications-Quantitative/dp/076191904X\\n  [2]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?s=books&ie=UTF8&qid=1281047453&sr=1-1\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.5130&rep=rep1&type=pdf\\n  [4]: http://www.amazon.com/Applied-Longitudinal-Data-Analysis-Occurrence/dp/0195152964",Added book recommendation,
2920,5,1326,62502d8f-be79-4fb1-854a-3fad26389618,2010-08-05 23:06:58.0,71.0,"Multilevel/hierarchical linear models can be used for this.  Essentially, each repetition of the measure is clustered within the individual; individuals can then be clustered within other hierarchies.  For me, at least, it's more intuitive than repeated-measures ANOVA.\\n\\nThe canonical text is [Raudenbush and Bryk][1]; I'm also really fond of [Gelman and Hill][2].  [Here's a tutorial I read some time ago][3] - you may or may not find the tutorial itself useful (that's so often a matter of personal taste, training and experience), but the bibliography at the end is good.\\n\\nI should note that Gelman and Hill doesn't have a ton on multilevel models specifically for repeated measures; I can't remember if that's the case or not for Raudenbush and Bryk.  \\n\\nEdit: Found a book I was looking for - [Applied Longitudinal Data Analysis by Singer and Willett][4] has (I believe) an explicit focus on multilevel models for repeated measures.  I haven't had a chance to read very far into it, but it might be worth looking into.\\n\\n\\n  [1]: http://www.amazon.com/Hierarchical-Linear-Models-Applications-Quantitative/dp/076191904X\\n  [2]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?s=books&ie=UTF8&qid=1281047453&sr=1-1\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.110.5130&rep=rep1&type=pdf\\n  [4]: http://www.amazon.com/Applied-Longitudinal-Data-Analysis-Occurrence/dp/0195152964",Must qualify every statement on the Internet,
2921,2,1327,acbbe148-b9ea-402b-b7e5-81a48d95396c,2010-08-05 23:22:01.0,582.0,Looking at it I would say a skew-normal or possibly a binormal distribution may fit it well.\\n\\nIn R you could use the `sn` library to deal with skew-normal distribution and use `nls` or `mle` to do a non-linear least square or a maximum likelihood extimation fit of your data.,,
2923,2,1328,30bcb462-3bcd-4dd8-a98c-4a7cd87de2da,2010-08-06 00:29:25.0,781.0,"""If you think that statistics has nothing to say about what you do or how you could do it better, then you are either wrong or in need of a more interesting job."" - Stephen Senn (Dicing with Death: Chance, Risk and Health, Cambridge University Press, 2003)",,
2924,16,1328,30bcb462-3bcd-4dd8-a98c-4a7cd87de2da,2010-08-06 00:29:25.0,-1.0,,,
2927,9,1293,1d776a3d-ec1a-45ef-a1eb-0a7cfdc92cba,2010-08-06 01:04:22.0,,<beginner>,Rollback to [b41fd11b-8e37-4570-8f3f-18a728aeb963],user28
2929,2,1330,9e053b59-b24e-491f-af37-eabb599c01b0,2010-08-06 01:08:05.0,521.0,"""The best thing about being a statistician is that you get to play in everyone's backyard."" - John Tukey (This is MY favourite Tukey quote)",,
2930,16,1330,9e053b59-b24e-491f-af37-eabb599c01b0,2010-08-06 01:08:05.0,-1.0,,,
2931,2,1331,3941a1d7-f9b9-4527-9220-6af67611456a,2010-08-06 01:15:52.0,253.0,"There is a new resources forming these days for talks about R:\\n\\nhttp://www.r-bloggers.com/RUG/\\n\\nCompiled by the organizers of ""R Users Groups"" around the world (right now, mainly around the States).\\n\\nIt is a new project (just a few weeks old), but already got good content on it, and good people wanting to take part in it.\\n![alt text][1]\\n\\n\\n  [1]: http://www.r-bloggers.com/RUG/wp-content/uploads/2010/07/banner.jpg",,
2932,16,1331,3941a1d7-f9b9-4527-9220-6af67611456a,2010-08-06 01:15:52.0,-1.0,,,
2933,2,1332,2431a9fd-c4c0-4092-93b0-18dee52cbc72,2010-08-06 01:20:36.0,521.0,"""It is easy to lie with statistics. It is hard to tell the truth without statistics."" - <a href=""http://en.wikiquote.org/wiki/Andrejs_Dunkels"" rel=""nofollow"">Andrejs Dunkels</a>\\n",,
2934,16,1332,2431a9fd-c4c0-4092-93b0-18dee52cbc72,2010-08-06 01:20:36.0,-1.0,,,
2935,2,1333,86e3247d-55f1-498d-8440-caf94ccbd634,2010-08-06 01:26:54.0,521.0,"""Statisticians, like artists, have the bad habit of falling in love with their models."" - George Box",,
2936,16,1333,86e3247d-55f1-498d-8440-caf94ccbd634,2010-08-06 01:26:54.0,-1.0,,,
2937,2,1334,8b06cc33-a9b6-4ee5-8097-028614aa0958,2010-08-06 01:29:56.0,521.0,"""New methods always look better than old ones. Neural nets are better than logistic regression, support vector machines are better than neural nets, etc."" - Brad Efron",,
2938,16,1334,8b06cc33-a9b6-4ee5-8097-028614aa0958,2010-08-06 01:29:56.0,-1.0,,,
2939,2,1335,5962568e-87b8-4fee-9fbc-1c9d758a54a3,2010-08-06 01:40:54.0,729.0,"If you wish to trade processing speed for memory (which I think you do), I would suggest the following algorithm:\\n\\n - Set up a loop from 1 to N Choose K, indexed by i\\n - Each i can be considered an index to a [combinadic][1], decode as such\\n - Use the combination to perform your test statistic, store the result, discard the combination\\n - Repeat\\n\\nThis will give you all N Choose K possible combinations without having to create them explicitly. I have code to do this in R if you'd like it (you can email me at mark dot m period fredrickson at-symbol gmail dot com).\\n\\n  [1]: http://msdn.microsoft.com/en-us/library/aa289166(VS.71).aspx",,
2940,2,1336,4b5ba149-819e-402f-beac-287f9150a495,2010-08-06 01:43:46.0,521.0,"""In the long run, we're all dead."" - John Maynard Keynes\\nA reference to survival analysis?!",,
2941,16,1336,4b5ba149-819e-402f-beac-287f9150a495,2010-08-06 01:43:46.0,-1.0,,,
2942,2,1337,e161200b-7be7-4d74-86d2-a71767e9dcfa,2010-08-06 01:53:47.0,521.0,"Well we've got favourite statistics quotes. What about statistics jokes?\\n\\nSo, what's your favourite statistics joke?",,
2943,1,1337,e161200b-7be7-4d74-86d2-a71767e9dcfa,2010-08-06 01:53:47.0,521.0,Statistics Jokes,,
2944,3,1337,e161200b-7be7-4d74-86d2-a71767e9dcfa,2010-08-06 01:53:47.0,521.0,<humor>,,
2945,2,1338,302d3bc2-ab9f-4f0a-90e1-5341faf46bdb,2010-08-06 01:54:55.0,521.0,"I thought I'd start the ball rolling with my favourite.\\n\\n""Being a statistician means never having to say you are certain.""",,
2946,16,1338,00000000-0000-0000-0000-000000000000,2010-08-06 02:30:56.0,159.0,,,
2947,16,1337,00000000-0000-0000-0000-000000000000,2010-08-06 02:30:56.0,159.0,,,
2948,5,1336,6bc36a7f-19db-4690-9214-d27d92fce953,2010-08-06 02:44:13.0,521.0,"""In the long run, we're all dead."" - John Maynard Keynes. \\nA reference to survival analysis?!",added 2 characters in body,
2949,2,1339,9848ea75-02cc-4115-ab57-cd800f436ba9,2010-08-06 02:48:09.0,781.0,"You may want to define what you want more clearly (to yourself, if not here). If what you're looking for is the most statistically significant stationary period contained in your noisy data, there's essentially two routes to take:\\n\\n1) compute a robust autocorrelation estimate, and take the maximum coefficient<br>\\n2) compute a robust power spectral density estimate, and take the maximum of the spectrum\\n\\nThe problem with #2 is that for any noisy time series, you will get a large amount of power in low frequencies, making it difficult to distinguish. There are some techniques for resolving this problem (i.e. pre-whiten, then estimate the PSD), but if the true period from your data is long enough, automatic detection will be iffy.\\n\\nYour best bet is probably to implement a robust autocorrelation routine such as can be found in chapter 8.6, 8.7 in *Robust Statistics - Theory and Methods* by Maronna, Martin and Yohai. Searching Google for ""robust durbin-levinson"" will also yield some results.\\n\\nIf you're just looking for a simple answer, I'm not sure that one exists. Period detection in time series can be complicated, and asking for an automated routine that can perform magic may be too much. ",,
2950,2,1340,f457046e-c865-451b-a49b-30c3172702dd,2010-08-06 03:06:41.0,781.0,"Anyone trained in statistical data analysis at a reasonable level uses the *concepts* of robust statistics on a regular basis. Most researchers know enough to look for serious outliers and data recording errors; the policy of removing suspect data points goes back well into the 19th century with Lord Rayleigh, G.G. Stokes, and others of their age. If the question is: \\n\\n*Why don't researchers use the more modern methods for computing location, scale, regression, etc. estimates?*\\n\\nthen the answer is given above -- the methods have largely been developed in the last 25 years, say 1985 - 2010. The lag for learning new methods factors in, as well as inertia compounded by the 'myth' that there is nothing wrong with blindly using classical methods. John Tukey comments that *just which\\nrobust/resistant methods you use is not important—what is important is that you\\nuse some. It is perfectly proper to use both classical and robust/resistant methods\\nroutinely, and only worry when they differ enough to matter. But when they **differ**,\\nyou should think **hard**.*\\n\\nIf instead, the question is:\\n\\n*Why don't researchers stop and ask questions about their data, instead of blindly applying highly unstable estimates?*\\n\\nthen the answer really comes down to training. There are far too many researchers who were never trained in statistics properly, summed up by the general reliance on p-values as the be-all and end-all of 'statistical significance'.\\n\\nAs far as the above discussion, Huber's estimates from the 1970s *are* robust, in the classical sense of the word: they resist outliers. And redescending estimators actually date well before the 1980s: the Princeton robustness study (of 1971) included the bisquare estimate of location, a redescending estimate.",,
2951,2,1341,9e144d8d-7488-4aa1-9c63-7aef9d6e0dd5,2010-08-06 03:14:53.0,781.0,"The classic ""orange horror"" remains an excellent introduction: *Exploratory Data Analysis* by John Tukey. \\n\\nhttp://www.amazon.com/Exploratory-Data-Analysis-John-Tukey/dp/0201076160",,
2952,16,1341,9e144d8d-7488-4aa1-9c63-7aef9d6e0dd5,2010-08-06 03:14:53.0,-1.0,,,
2953,2,1342,2e838754-adc4-4aed-bbc1-216e58147f7c,2010-08-06 03:17:17.0,,"> Do not make things easy for yourself\\n> by speaking or thinking of data as if\\n> they were different from what they\\n> are; and do not go off from facing\\n> data as they are, to amuse your\\n> imagination by wishing they were\\n> different from what they are. Such\\n> wishing is pure waste of nerve force,\\n> weakens your intellectual power, and\\n> gets you into habits of mental\\n> confusion.\\n\\n--Mary Everest Boole",,Steve Kass
2954,16,1342,2e838754-adc4-4aed-bbc1-216e58147f7c,2010-08-06 03:17:17.0,-1.0,,,
2955,2,1343,f1c028b8-4594-4878-a3a7-2b9290b39a04,2010-08-06 03:22:13.0,183.0,"I sometimes make the x-axis time and plot both scalar variables on the y-axis.\\nWhen the two scalar variables are on a different metric, I rescale one or both of the scalar variables so they can be displayed on the same plot.\\nI use things like colour and shape to discriminate the two scalar variables.\\nI've often used `xyplot` from `lattice` for this purpose. \\n\\nHere's an example:\\n\\n    require(lattice)\\n    xyplot(dv1 + dv2  ~ iv, data = x, col = c(""black"", ""red""))\\n\\n",,
2956,5,1340,6f4941ff-ce6e-45c0-a980-ac1b36b895fe,2010-08-06 03:36:09.0,781.0,"Anyone trained in statistical data analysis at a reasonable level uses the *concepts* of robust statistics on a regular basis. Most researchers know enough to look for serious outliers and data recording errors; the policy of removing suspect data points goes back well into the 19th century with Lord Rayleigh, G.G. Stokes, and others of their age. If the question is: \\n\\n*Why don't researchers use the more modern methods for computing location, scale, regression, etc. estimates?*\\n\\nthen the answer is given above -- the methods have largely been developed in the last 25 years, say 1985 - 2010. The lag for learning new methods factors in, as well as inertia compounded by the 'myth' that there is nothing wrong with blindly using classical methods. John Tukey comments that *just which\\nrobust/resistant methods you use is not important—what is important is that you\\nuse some. It is perfectly proper to use both classical and robust/resistant methods\\nroutinely, and only worry when they differ enough to matter. But when they **differ**,\\nyou should think **hard**.*\\n\\nIf instead, the question is:\\n\\n*Why don't researchers stop and ask questions about their data, instead of blindly applying highly unstable estimates?*\\n\\nthen the answer really comes down to training. There are far too many researchers who were never trained in statistics properly, summed up by the general reliance on p-values as the be-all and end-all of 'statistical significance'.\\n\\nAs far as the discussion below, Huber's estimates from the 1970s *are* robust, in the classical sense of the word: they resist outliers. And redescending estimators actually date well before the 1980s: the Princeton robustness study (of 1971) included the bisquare estimate of location, a redescending estimate.",edited body,
2957,2,1344,c33ff3dd-9c3d-49d3-8028-e96692ce762a,2010-08-06 03:47:26.0,25.0,"A simpler approach might be to transform the data. After transforming, it might be close to Gaussian. \\n\\nOne common way to do so is by taking the logarithm of all values. \\n\\nMy guess is that in this case the distribution of the reciprocal of the round-trip times will be more symmetrical and perhaps close to Gaussian. By taking the reciprocal, you are essentially tabulating velocities instead of times, so it still is easy to interpret the results (unlike logarithms or many transforms).",,
2958,2,1345,c257db1c-7e90-4d64-8565-d148fee28ac9,2010-08-06 04:11:29.0,782.0,"Based on your comment ""Really i want to draw the mathematical curve that follows the distribution. Granted it might not be a known distribution; but i can't imagine that this hasn't been investigated before."" I am providing a function that sort of fits.\\n\\nTake a look at [ExtremeValueDistribution][1]\\n\\nI added an amplitude and made the two betas different. I figure your function's center is closer to 9.5 then 10.\\n\\nNew function:\\na E^(-E^(((-x + alpha)/b1)) + (-x + alpha)/b2)/((b1 + b2)/2)\\n\\n{alpha->9.5, b2 -> 0.899093, a -> 5822.2, b1 -> 0.381825}\\n\\n[Wolfram alpha][2]:\\nplot 11193.8 E^(-E^(1.66667 (10 - x)) + 1.66667 (10 - x)) ,x 0..16, y from 0 to 4500\\n\\nSome points around 10ms:<br>\\n{{9, 390.254}, {10, 3979.59}, {11, 1680.73}, {12, 562.838}}\\n\\nTail does not fit perfectly though. The tail can be fit better if b2 is lower and the peak is chosen to be closer to 9.\\n\\n  [1]: http://reference.wolfram.com/mathematica/ref/ExtremeValueDistribution.html?q=ExtremeValueDistribution&lang=en\\n  [2]: http://www.wolframalpha.com/input/?i=plot+11193.8+E%5E%28-E%5E%281.66667+%2810+-+x%29%29+%2B+1.66667+%2810+-+x%29%29+%2Cx+0..16%2C+y+from+0+to+4500",,
2959,2,1346,cca64426-f366-49b4-87c6-e930f29efb80,2010-08-06 04:50:59.0,287.0,"I saw this posted as a comment on here somewhere:\\n\\nhttp://xkcd.com/552/\\n\\nA: I used to think correlation implied causation. Then I took a statistics class. Now I don't.\\n\\nB: Sounds like the class helped.\\n\\nA: Well, maybe.",,
2960,16,1346,cca64426-f366-49b4-87c6-e930f29efb80,2010-08-06 04:50:59.0,-1.0,,,
2961,2,1347,52f5fc29-b130-4884-ba4e-bc0e9c95eb4b,2010-08-06 05:15:22.0,521.0,"""Extraordinary claims demand extraordinary evidence."" \\n\\nOften attributed to Carl Sagan, but he was paraphrasing sceptic Marcello Truzzi. Doubtless the concept is even more ancient. \\n\\nDavid Hume said, ""A wise man, therefore, proportions his belief to the evidence"". \\n\\nOne could argue this is not a quote about statistics. However, applied statistics is ultimately in the business of evaluating the quality of evidence for or against some proposition.\\n",,
2962,16,1347,52f5fc29-b130-4884-ba4e-bc0e9c95eb4b,2010-08-06 05:15:22.0,-1.0,,,
2963,2,1348,d156a22d-5cf3-4b79-80f4-4f50b5a2a83a,2010-08-06 05:27:57.0,521.0,An old favourite of mine as an introduction to biostatistics is Armitage & Berry's (& now Matthew's):\\n\\n*Statistical Methods in Medical Research* ,,
2964,16,1348,d156a22d-5cf3-4b79-80f4-4f50b5a2a83a,2010-08-06 05:27:57.0,-1.0,,,
2965,2,1349,d49c8df2-4c3e-46d3-94f8-a82ae139af11,2010-08-06 05:33:06.0,521.0,"I have previously found UCLA's ""What statistical analysis should I use?"" to be helpful:\\nhttp://www.ats.ucla.edu/stat/mult_pkg/whatstat/default.htm\\n\\nIt also gives examples of how to do the analysis in SAS, Stata and SPSS - although not R!",,
2966,5,1327,fa572fde-b8a1-4d4e-a416-abffed785537,2010-08-06 06:19:32.0,582.0,"Looking at it I would say a skew-normal or possibly a binormal distribution may fit it well.\\n\\nIn R you could use the `sn` library to deal with skew-normal distribution and use `nls` or `mle` to do a non-linear least square or a maximum likelihood extimation fit of your data.\\n\\n===\\n\\nEDIT: rereading your question/comments I would add something more\\n\\nIf what you're interested into is just drawing a pretty graph over the bars forget about distributions, who cares in the end if you're not doing anything with it. Just draw a B-spline over your data point and you're good.\\n\\nAlso, with this approach you avoid having to implement a MLE fit algorithm (or similar), and you're covered in the case of a distribution that is not skew-normal (or whatever you choose to draw) ",added 500 characters in body,
2967,2,1350,9dbfb420-b95c-441c-95a4-42995c126e58,2010-08-06 06:32:06.0,189.0,"I am working with a large data set (approximately 50K observations) and trying to running a Maximum likelihood estimation on 5 unknowns in Stata. \\n\\nI encountered an error message of ""Numerical Overflow"". How can I overcome this? ",,
2968,1,1350,9dbfb420-b95c-441c-95a4-42995c126e58,2010-08-06 06:32:06.0,189.0,How to get around Numerical Overflow in Stata? ,,
2969,3,1350,9dbfb420-b95c-441c-95a4-42995c126e58,2010-08-06 06:32:06.0,189.0,<statistical-analysis><software>,,
2971,2,1351,307ab5c3-9eff-4f7e-a47a-7bacac3d2a47,2010-08-06 07:44:34.0,789.0,"Assuming you want to pick a distribution for n, p(n) you can apply Bayes law.\\n\\nYou know that the probability of k events occuring given that n have actually occured is governed by a binomial distribtion \\n\\n$p(k|n) = {n \\choose k} p^k (1-p)^{(n-k)}$\\n\\nThe thing you really want to know is the probability of n events having actually occured, given that you observed k. By Bayes lay:\\n\\n$p(n|k) = \\frac{p(k|n)p(n)}{p(k)}$\\n\\nBy applying the theorem of total probability, we can write:\\n\\n$p(n|k) = \\frac{p(k|n)p(n)}{\\sum_{n'} p(k|n')p(n')}$\\n\\nSo without further information, about the distribution of $p(n)$ you can't really go any further. \\n\\nHowever, if you want to pick a distribution for $p(n)$ for which there is a value $n$ greater than which $p(n) = 0$, or sufficiently close to zero, then you can do a bit better. For example, assume that the distribution of $n$ is uniform in the range $[0,n_{max}]$. this case:\\n\\n$p(n) = \\frac{1}{n_{max}}$\\n\\nThe Bayesian formulation simplifies to:\\n\\n$p(n|k) = \\frac{p(k|n)}{\\sum_{n'} p(k|n')}$\\n\\nAs for the final part of the problem, I agree that the best approach is to perform a cumulative summation over $p(n|k)$, to generate the cummulative probability distribution function, and iterate until the 0.95 limit is reached.\\n\\nGiven that this question migrated from SO, toy sample code in python is attached below\\n\\n    import numpy.random\\n    \\n    p = 0.8\\n    nmax = 200\\n    \\n    def factorial(n):\\n        if n == 0:\\n            return 1\\n        return reduce( lambda a,b : a*b, xrange(1,n+1), 1 )\\n    \\n    def ncr(n,r):\\n        return factorial(n) / (factorial(r) * factorial(n-r))\\n    \\n    def binomProbability(n, k, p):\\n        p1 = ncr(n,k)\\n        p2 = p**k\\n        p3 = (1-p)**(n-k)\\n        return p1*p2*p3\\n    \\n    def posterior( n, k, p ):\\n        def p_k_given_n( n, k ):\\n            return binomProbability(n, k, p)\\n        def p_n( n ):\\n            return 1./nmax\\n        def p_k( k ):\\n            return sum( [ p_n(nd)*p_k_given_n(nd,k) for nd in range(k,nmax) ] )\\n        return (p_k_given_n(n,k) * p_n(n)) / p_k(k)\\n    \\n    \\n    observed_k   = 80\\n    p_n_given_k  = [ posterior( n, observed_k, p ) for n in range(0,nmax) ]\\n    cp_n_given_k = numpy.cumsum(p_n_given_k)\\n    for n in xrange(0,nmax):\\n        print n, p_n_given_k[n], cp_n_given_k[n]\\n\\n",,
2973,2,1352,f2a1520d-12c6-442b-a707-c1cf6311b885,2010-08-06 08:06:13.0,144.0,"In an average (median?) conversation about statistics you will often find yourself discussing this or that method of analyzing this or that type of data. In my experience, careful study design with special thought with regards to the statistical analysis is often neglected (working in biology/ecology, this seems to be a prevailing occurrence). Statisticians often find themselves in a gridlock with insufficient (or outright wrong) collected data. To paraphrase Ronald Fisher, they are forced to do a post-mortem on the data, which often leads to weaker conclusions, if at all.\\n\\nI would like to know which references you use to construct a successful study design, preferably for a wide range of methods (e.g. *t*-test, GLM, GAM, ordination techniques...) that helps you avoid pitfalls mentioned above.",,
2974,1,1352,f2a1520d-12c6-442b-a707-c1cf6311b885,2010-08-06 08:06:13.0,144.0,planning a study,,
2975,3,1352,f2a1520d-12c6-442b-a707-c1cf6311b885,2010-08-06 08:06:13.0,144.0,<statistical-analysis>,,
2976,16,1352,f2a1520d-12c6-442b-a707-c1cf6311b885,2010-08-06 08:06:13.0,144.0,,,
2977,6,1350,da2c7d40-3322-4cc0-b552-7d8d4f304de3,2010-08-06 08:09:20.0,8.0,<statistical-analysis><software><stata>,edited tags,
2979,2,1353,337cfc64-1765-4ee7-b137-463f9c2d7693,2010-08-06 09:01:29.0,339.0,"I would choose to use the negative binomial distribution, which returns the probability that there will be X failures before the k_th success, when the constant probability of a success is p.\\n\\nUsing an example\\n\\n    k=17 # number of successes\\n    p=.6 # constant probability of success\\n\\nthe mean and sd for the failures are given by\\n\\n    mean.X <- k*(1-p)/p\\n    sd.X <- sqrt(k*(1-p)/p^2) \\n\\nThe distribution of the failures X, will have approximately that shape\\n\\n    plot(dnbinom(0:(mean.X + 3 * sd.X),k,p),type='l')\\n\\nSo, the number of failures will be (with 95% confidence) approximately between\\n\\n    qnbinom(.025,k,p)\\n    [1] 4\\n\\nand\\n\\n    qnbinom(.975,k,p)\\n    [1] 21\\n\\nSo you inerval would be [k+qnbinom(.025,k,p),k+qnbinom(.975,k,p)] (using the example's numbers [21,38] )",,
2980,4,1352,a7f3377e-fef2-4c16-8fbc-9b0fa830c592,2010-08-06 09:12:50.0,88.0,Planning a study,Title fixed.,
2981,2,1354,c3c3ae80-e738-49cf-a52a-6ae8e65eea86,2010-08-06 09:34:37.0,521.0,"How many variables do you have? Are you using a built in command?\\n\\nFive years ago I used mlogit (multinomial logit) in Stata 8  to model 60,000 observations and about 40 variables with no difficulty.\\n\\nI'd recommend searching the Stata websites (http://www.stata.com/) and even asking technical support.\\n\\nFor example, here are two answers:\\nhttp://www.stata.com/statalist/archive/2007-04/msg00390.html\\nhttp://statalist.1588530.n2.nabble.com/st-Reproducing-xtlogit-with-xtmelogit-td3418488.html\\n\\nStata provide the best technical support of any stats software I know.",,
2982,2,1355,f25e97b7-9d28-4455-ac0f-cd3382898ccf,2010-08-06 09:48:14.0,114.0,"I want to predict the results of a simple card game, to judge on average, how long a game will last.\\n\\nMy 'simple' game is;\\n\\n - Cards are dealt from a randomised\\n   deck to n players (typically 2-4)\\n - Each player gets five cards\\n - The top\\n   card from the deck is turned over\\n - Each player takes it in turns to\\n   either place a card of the same face\\n   value (i.e 1-10, J, Q, K, A), the\\n   same suit (i.e Hearts, Diamonds,\\n   Spades, Clubs) or any suit of magic\\n   card (a jack) \\n - If the player can place\\n   a card they do, otherwise they must\\n   take a card from the deck \\n - Play\\n   continues in turn until all but one\\n   player has no cards left\\n\\nI'm guessing that I could write code to play a mythical game and report the result, then run that code thousands of times.\\n\\nHas anyone done this ?  Can they suggest code that does a similar job (my favoured language is R, but anything would do) ?  Is there a better way ?",,
2983,1,1355,f25e97b7-9d28-4455-ac0f-cd3382898ccf,2010-08-06 09:48:14.0,114.0,How could I predict the results of a simple card game ?,,
2984,3,1355,f25e97b7-9d28-4455-ac0f-cd3382898ccf,2010-08-06 09:48:14.0,114.0,<r><probability>,,
2985,2,1356,c89a87d8-95e6-443a-8b5e-7d0e8dea508d,2010-08-06 09:52:51.0,114.0,"Two statisticians were traveling in an airplane from LA to New York.\\nAbout an hour into the flight, the pilot announced that they had lost\\nan engine, but don’t worry, there are three left. However, instead of 5\\nhours it would take 7 hours to get to New York. \\n\\nA little later, he\\nannounced that a second engine failed, and they still had two left,\\nbut it would take 10 hours to get to New York. \\n\\nSomewhat later, the\\npilot again came on the intercom and announced that a third engine\\nhad died. Never fear, he announced, because the plane could fly on a\\nsingle engine. However, it would now take 18 hours to get to New\\nYork.\\n\\nAt this point, one statistician turned to the other and said, “Gee, I\\nhope we don’t lose that last engine, or we’ll be up here forever!”",,
2986,16,1356,c89a87d8-95e6-443a-8b5e-7d0e8dea508d,2010-08-06 09:52:51.0,-1.0,,,
2987,6,1355,0b3927be-afe2-4e54-9f6a-646bcc807bcd,2010-08-06 10:01:52.0,8.0,<r><probability><games>,edited tags,
2988,2,1357,8c5d2bb5-b28b-40e6-b622-089171184ac5,2010-08-06 10:48:52.0,,I am trying to compare it to Euclidean distance and Pearson correlation,,CLOCK
2989,1,1357,8c5d2bb5-b28b-40e6-b622-089171184ac5,2010-08-06 10:48:52.0,,"Is mutual information invariant to scaling, i.e. multiplying all elements by a nonzero constant?",,CLOCK
2990,3,1357,8c5d2bb5-b28b-40e6-b622-089171184ac5,2010-08-06 10:48:52.0,,<correlation>,,CLOCK
2991,5,1353,f9778152-ebe9-45f5-817f-69dd913e186c,2010-08-06 10:55:02.0,339.0,"I would choose to use the [negative binomial distribution][1], which returns the probability that there will be X failures before the k_th success, when the constant probability of a success is p.\\n\\nUsing an example\\n\\n    k=17 # number of successes\\n    p=.6 # constant probability of success\\n\\nthe mean and sd for the failures are given by\\n\\n    mean.X <- k*(1-p)/p\\n    sd.X <- sqrt(k*(1-p)/p^2) \\n\\nThe distribution of the failures X, will have approximately that shape\\n\\n    plot(dnbinom(0:(mean.X + 3 * sd.X),k,p),type='l')\\n\\nSo, the number of failures will be (with 95% confidence) approximately between\\n\\n    qnbinom(.025,k,p)\\n    [1] 4\\n\\nand\\n\\n    qnbinom(.975,k,p)\\n    [1] 21\\n\\nSo you inerval would be [k+qnbinom(.025,k,p),k+qnbinom(.975,k,p)] (using the example's numbers [21,38] )\\n\\n\\n  [1]: http://www.math.ntu.edu.tw/~hchen/teaching/StatInference/notes/lecture16.pdf",added 94 characters in body,
2992,2,1358,d2c59958-46bd-4cc4-8cb7-dc9e505445da,2010-08-06 10:57:18.0,650.0,"In circular statistics, the expectation value of a random variable $Z$ with values on the circle $S$ is defined as\\n$$\\nm_1(Z)=\\int_S z P^Z(\\theta)\\textrm{d}\\theta\\n$$\\n(see [wikipedia](http://en.wikipedia.org/wiki/Circular_statistics#Moments)).\\nThis is a very natural definition, as is the definition of the variance\\n$$\\n\\mathrm{Var}(Z)=1-|m_1(Z)|.\\n$$\\nSo we didn't need a second moment in order to define the variance!\\n\\nNonetheless, we define the higher moments\\n$$\\nm_n(Z)=\\int_S z^n P^Z(\\theta)\\textrm{d}\\theta.\\n$$\\nI admit that this looks rather natural as well at first sight, and very similar to the definition in linear statistics. But still I feel a little bit uncomfortable, and have the following\\n\\n**Questions:**\\n\\n1.\\n**What is measured by the higher moments** defined above (intuitively)? Which properties of the distribution can be characterized by their moments?\\n\\n2.\\nIn the computation of the higher moments we use multiplication of complex numbers, although we think of the values of our random variables merely as vectors in the  plane or as angles. I know that complex multiplication is essentially addition of angles in this case, but still:\\n**Why is complex multiplication a meaningful operation for circular data?**\\n\\n(Feel free to retag this question appropriately. Maybe creating a tag [directional-statistics] might be worthwile?)",,
2993,1,1358,d2c59958-46bd-4cc4-8cb7-dc9e505445da,2010-08-06 10:57:18.0,650.0,Intuition for higher moments in circular statistics,,
2994,3,1358,d2c59958-46bd-4cc4-8cb7-dc9e505445da,2010-08-06 10:57:18.0,650.0,<statistical-analysis><statistics><data>,,
2995,4,1352,56664fdf-e596-4330-b12a-2b1344becb24,2010-08-06 11:06:15.0,253.0,References for how to plan a study,changing title,
2996,2,1359,fd49d30e-ae30-43a0-8e76-01a2bebb8e4f,2010-08-06 11:09:39.0,253.0,"In general, I would say any book that has DOE (design of experiments) in the title would fit the bill (and there are MANY).\\n\\nMy rule of thumb for such resource would be to start with the [wiki page][1], in particular to your question, notice the [Principles of experimental design, following Ronald A. Fisher][2]\\n\\nBut a more serious answer would be domain specific (clinical trial has a huge manual, but for a study on mice, you'd probably go with some other field-related book)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Design_of_experiments\\n  [2]: http://en.wikipedia.org/wiki/Design_of_experiments#Principles_of_experimental_design.2C_following_Ronald_A._Fisher",,
2997,16,1359,fd49d30e-ae30-43a0-8e76-01a2bebb8e4f,2010-08-06 11:09:39.0,-1.0,,,
2998,2,1360,8bd26d20-592d-4a3a-8909-fa08bc76f83d,2010-08-06 11:11:00.0,778.0,> Those who ignore Statistics are condemned to reinvent it.\\n- Brad Efron,,
2999,16,1360,8bd26d20-592d-4a3a-8909-fa08bc76f83d,2010-08-06 11:11:00.0,-1.0,,,
3000,2,1361,be4ce824-a359-4e73-82bb-4382827b44d6,2010-08-06 11:15:17.0,778.0,>  My thesis is simply this: probability does not exist.\\n- Bruno de Finetti,,
3001,16,1361,be4ce824-a359-4e73-82bb-4382827b44d6,2010-08-06 11:15:17.0,-1.0,,,
3002,2,1362,2557cacd-8067-4376-ade0-272e5e0591b1,2010-08-06 11:17:09.0,88.0,"My rule of thumb is ""repeat more than you think its sufficient"". ",,
3003,16,1362,2557cacd-8067-4376-ade0-272e5e0591b1,2010-08-06 11:17:09.0,-1.0,,,
3004,2,1363,e2c74e18-26c4-4306-a5cb-c0ab7a93be03,2010-08-06 11:23:15.0,114.0,"""...Statistics used as a catalyst to engineering creation will, I believe, always result in the fastest and most economical progress.""  George Box 1992",,
3005,16,1363,e2c74e18-26c4-4306-a5cb-c0ab7a93be03,2010-08-06 11:23:15.0,-1.0,,,
3007,6,1358,28b442e7-f35c-4725-8b73-b7b0031d0607,2010-08-06 12:10:04.0,,<data><intuition><directional-statistics><moments>,edited tags,user28
3008,2,1364,f38191ee-0c47-4a61-a6ca-17a15996698e,2010-08-06 12:13:05.0,8.0,"The easiest way is just to simulate the game lots of times. The R code below simulates a single game.\\n\\n    nplayers = 4\\n    #Create an empty data frame to keep track\\n    #of card number, suit and if it's magic\\n    empty.hand = data.frame(number = numeric(52),\\n      suit = numeric(52),\\n      magic  = numeric(52))\\n    \\n    #A list of players who are in the game\\n    players =list()\\n    for(i in 1:nplayers)\\n      players[[i]] = empty.hand\\n    \\n    #Simulate shuffling the deck\\n    deck = empty.hand\\n    deck$number = rep(1:13, 4)\\n    deck$suit = as.character(rep(c(""H"", ""C"", ""S"", ""D""), each=13))\\n    deck$magic = rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0), each=4)\\n    deck = deck[sample(1:52, 52),]\\n    \\n    #Deal out five cards per person\\n    for(i in 1:length(players)){\\n      r = (5*i-4):(5*i)\\n      players[[i]][r,] = deck[r,]\\n    }\\n    \\n    #Play the game\\n    i = 5*length(players)+1\\n    current = deck[i,]\\n    while(i < 53){\\n      for(j in 1:length(players)){\\n        playersdeck = players[[j]]\\n        #Need to test for magic and suit also - left as an exercise!\\n        if(is.element(current$number, playersdeck$number)){\\n          #Update current card\\n          current = playersdeck[match(current$number,\\n            playersdeck$number),]\\n          #Remove card from players deck\\n          playersdeck[match(current$number, playersdeck$number),] = c(0,\\n                       0, 0)\\n        } else {\\n          #Add card to players deck\\n          playersdeck[i,] = deck[i,]\\n          i = i + 1\\n        }\\n        #Has someone won\\n        if(sum(playersdeck$number) == 0)\\n          break\\n        players[[j]] = playersdeck\\n      }\\n    }\\n    \\n    #How many cards are left for each player\\n    for(i in 1:length(players))\\n    {\\n      cat(sum(players[[i]]$number !=0), ""\\n"") \\n     }\\n\\n\\n\\n**Some comments**\\n\\n1. You will need to add a couple of lines for magic cards and suits, but data structure is still there. I presume you didn't want a complete solution? ;)\\n1. To estimate the average game length, just place the above code in a function and call lots of times.\\n1. Rather than dynamically increasing a vector when a player gets a card, I find it easier just to create a sparse data frame that is more than sufficient. In this case, each player has a data frame with 52 rows, which they will never fill (unless it's a 1 player game).\\n\\n\\n\\n",,
3009,2,1365,b5be1363-4816-4e88-935a-5432209a37bf,2010-08-06 12:38:35.0,89.0,"The moments are the Fourier coefficients of the probability measure $P^Z$.  Suppose (for the sake of intuition) that $Z$ has a density.  Then the argument (angle from $1$ in the complex plane) of $Z$ has a density on $[0,2\\pi)$, and the moments are the coefficients when that density is expanded in a Fourier series.  Thus the usual intuition about Fourier series applies -- these measure the strengths of frequencies in that density.\\n\\nAs for your second question, I think you already gave the answer: ""complex multiplication is essentially addition of angles in this case"".",,
3010,2,1366,35728920-a53a-471b-bc31-6b0d774b6a80,2010-08-06 12:46:57.0,247.0,"Another approach, that is more justified by network considerations, is to try to fit a sum of independent exponentials with different parameters. A reasonable assumption would be that each node in the path of the ping the delay would be an independent exponential, with different parameters. A reference to the distributional form of the sum of independent exponentials with differing parameters is http://www.math.bme.hu/~balazs/sumexp.pdf.\\n\\nYou should probably also look at the ping times vs the number of hops.",,
3011,5,1346,01c9b13a-63b8-44fb-8065-3ce94b5ec0e7,2010-08-06 12:47:14.0,253.0,"I saw this posted as a comment on here somewhere:\\n\\nhttp://xkcd.com/552/\\n\\n![alt text][1]\\n\\nA: I used to think correlation implied causation. Then I took a statistics class. Now I don't.\\n\\nB: Sounds like the class helped.\\n\\nA: Well, maybe.\\n\\n\\n  [1]: http://imgs.xkcd.com/comics/correlation.png",added 74 characters in body,
3012,6,1357,5738c690-1121-4a42-8586-dd1f0972f8b9,2010-08-06 13:04:47.0,8.0,<correlation><mutual-information>,edited tags,
3013,2,1367,3d99845b-4a38-4b72-87a2-436f018cfd09,2010-08-06 13:52:50.0,634.0,"One passed by Gary Ramseyer:\\n\\nStatistics play an important role in genetics. For instance, statistics prove that numbers of offspring is an inherited trait. If your parent didn't have any kids, odds are you won't either.\\n",,
3014,16,1367,3d99845b-4a38-4b72-87a2-436f018cfd09,2010-08-06 13:52:50.0,-1.0,,,
3015,5,1364,0cab271e-7249-4e1d-85c5-5ee0bfb25b0c,2010-08-06 13:57:56.0,8.0,"The easiest way is just to simulate the game lots of times. The R code below simulates a single game.\\n\\n    nplayers = 4\\n    #Create an empty data frame to keep track\\n    #of card number, suit and if it's magic\\n    empty.hand = data.frame(number = numeric(52),\\n      suit = numeric(52),\\n      magic  = numeric(52))\\n    \\n    #A list of players who are in the game\\n    players =list()\\n    for(i in 1:nplayers)\\n      players[[i]] = empty.hand\\n    \\n    #Simulate shuffling the deck\\n    deck = empty.hand\\n    deck$number = rep(1:13, 4)\\n    deck$suit = as.character(rep(c(""H"", ""C"", ""S"", ""D""), each=13))\\n    deck$magic = rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0), each=4)\\n    deck = deck[sample(1:52, 52),]\\n    \\n    #Deal out five cards per person\\n    for(i in 1:length(players)){\\n      r = (5*i-4):(5*i)\\n      players[[i]][r,] = deck[r,]\\n    }\\n    \\n    #Play the game\\n    i = 5*length(players)+1\\n    current = deck[i,]\\n    while(i < 53){\\n      for(j in 1:length(players)){\\n        playersdeck = players[[j]]\\n        #Need to test for magic and suit also - left as an exercise!\\n        if(is.element(current$number, playersdeck$number)){\\n          #Update current card\\n          current = playersdeck[match(current$number,\\n            playersdeck$number),]\\n          #Remove card from players deck\\n          playersdeck[match(current$number, playersdeck$number),] = c(0,\\n                       0, 0)\\n        } else {\\n          #Add card to players deck\\n          playersdeck[i,] = deck[i,]\\n          i = i + 1\\n        }\\n        #Has someone won\\n        if(sum(playersdeck$number) == 0)\\n          break\\n        players[[j]] = playersdeck\\n      }\\n    }\\n    \\n    #How many cards are left for each player\\n    for(i in 1:length(players))\\n    {\\n      cat(sum(players[[i]]$number !=0), ""\\n"") \\n     }\\n\\n\\n\\n**Some comments**\\n\\n1. You will need to add a couple of lines for magic cards and suits, but data structure is already there. I presume you didn't want a complete solution? ;)\\n1. To estimate the average game length, just place the above code in a function and call lots of times.\\n1. Rather than dynamically increasing a vector when a player gets a card, I find it easier just to create a sparse data frame that is more than sufficient. In this case, each player has a data frame with 52 rows, which they will never fill (unless it's a 1 player game).\\n\\n\\n\\n",Fixed a typo,
3016,2,1368,a7bc37ca-2c43-472e-a971-c40abb64c7dd,2010-08-06 14:00:40.0,,"A statistic professor plans to travel to a conference by plane. When he passes the security check, they discover a bomb in his carry-on-baggage. Of course, he is hauled off immediately for interrogation.\\n\\n""I don't understand it!"" the interrogating officer exclaims. ""You're an accomplished professional, a caring family man, a pillar of your parish - and now you want to destroy that all by blowing up an airplane!""\\n\\n""Sorry"", the professor interrupts him. ""I had never intended to blow up the plane.""\\n\\n""So, for what reason else did you try to bring a bomb on board?!""\\n\\n""Let me explain. Statistics shows that the probability of a bomb being on an airplane is 1/1000. That's quite high if you think about it - so high that I wouldn't have any peace of mind on a flight.""\\n\\n""And what does this have to do with you bringing a bomb on board of a plane?""\\n\\n""You see, since the probability of one bomb being on my plane is 1/1000, the chance that there are two bombs is 1/1000000. If I already bring one, the chance of another bomb being around is actually 1/1000000, and I am much safer..."" ",,scdef
3017,16,1368,a7bc37ca-2c43-472e-a971-c40abb64c7dd,2010-08-06 14:00:40.0,-1.0,,,
3018,2,1369,a2143e43-ee7a-4dd3-891d-7c094ea87964,2010-08-06 14:01:57.0,791.0,I have a now distance and a standard deviation. I have simulated now a few 100 distances and would like to sample from these distances sample of 10-20 re-sampling the original distribution. Is there any standardized way of doing so? \\n\\n,,
3019,1,1369,a2143e43-ee7a-4dd3-891d-7c094ea87964,2010-08-06 14:01:57.0,791.0,Sampling according to a normal distribtuion,,
3020,3,1369,a2143e43-ee7a-4dd3-891d-7c094ea87964,2010-08-06 14:01:57.0,791.0,<sample>,,
3021,2,1370,ad91507b-dd8a-456d-a3bc-010d9d1326e2,2010-08-06 14:10:01.0,,"I think the answer is yes to your question. I will show this for the discrete case only and I think the basic idea carries over to the continuous case. MI is defined as:\\n\\n$I(X;Y) = \\sum_{y\\in Y}\\sum_{x\\in X}\\Bigg(p(x,y) log(\\frac{p(x,y)}{p(x)p(y)})\\Bigg)$\\n\\nDefine:\\n\\n$Z_x = \\alpha X$\\n\\nand \\n\\n$Z_y = \\alpha Y$.\\n\\nSo, the question is: Does $I(Z_x;Z_y)$ equal $I(X;Y)$?\\n\\nSince scaling is a one-to-one transformation it must be that:\\n\\n$p(z_x) = p(x)$, \\n\\n$p(z_y) = p(y)$ and\\n\\n$p(z_x,z_y) = p(x,y)$ \\n\\nTherefore, the mutual information remains the same and hence the answer is to your question is yes.",,user28
3022,2,1371,72a35062-fa8c-44af-86c0-da8203a96632,2010-08-06 14:12:21.0,666.0,"George Burns said that ""If you live to be one hundred, you've got it made. Very few people die past that age.""",,
3023,16,1371,72a35062-fa8c-44af-86c0-da8203a96632,2010-08-06 14:12:21.0,-1.0,,,
3024,5,1369,0c999bf0-16cd-4b40-b38c-7b2bfe24efe1,2010-08-06 14:14:37.0,791.0,I have a given distance with a standard deviation. I have simulated now a few 100 distances and would like to draw from these distances a sample of 10-20 resembling the original distribution. Is there any standardized way of doing so? \\n\\n,added 2 characters in body; edited title,
3025,4,1369,0c999bf0-16cd-4b40-b38c-7b2bfe24efe1,2010-08-06 14:14:37.0,791.0,Sampling according to a normal distribution,added 2 characters in body; edited title,
3026,6,1350,f009ca9d-b858-46e2-9a2e-d0b662ed7b40,2010-08-06 15:09:20.0,,<statistical-analysis><software><stata><numerical-overflow>,added tag,user28
3027,2,1372,da268499-401b-4dc3-90d0-2fe8f5bef0a2,2010-08-06 15:14:46.0,88.0,"You mean you want to draw 10-20 numbers from a normal distribution? In R, use `rnorm` function; for a generic solution, see [Wikipedia][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution#Generating_values_from_normal_distribution",,
3028,2,1373,0e33c37e-8db2-401b-b809-dec41cad9d80,2010-08-06 15:22:43.0,88.0,"Intuitive explanation is such: multiplying by constant does not change information content of X and Y, so also their mutual information -- and thus it is invariant to scaling. Still Srikant gave you a strict proof of this fact.",,
3029,5,1362,98c128b3-874d-4a3c-9ba8-ab668f4a5b33,2010-08-06 15:23:24.0,88.0,"My rule of thumb is ""repeat more than you think it's sufficient"". ",added 1 characters in body,
3030,5,1340,b6973052-6cd6-4f51-9173-d71a76ccd77a,2010-08-06 15:49:38.0,781.0,"Anyone trained in statistical data analysis at a reasonable level uses the *concepts* of robust statistics on a regular basis. Most researchers know enough to look for serious outliers and data recording errors; the policy of removing suspect data points goes back well into the 19th century with Lord Rayleigh, G.G. Stokes, and others of their age. If the question is: \\n\\n*Why don't researchers use the more modern methods for computing location, scale, regression, etc. estimates?*\\n\\nthen the answer is given above -- the methods have largely been developed in the last 25 years, say 1985 - 2010. The lag for learning new methods factors in, as well as inertia compounded by the 'myth' that there is nothing wrong with blindly using classical methods. John Tukey comments that *just which\\nrobust/resistant methods you use is not important—what is important is that you\\nuse some. It is perfectly proper to use both classical and robust/resistant methods\\nroutinely, and only worry when they differ enough to matter. But when they **differ**,\\nyou should think **hard**.*\\n\\nIf instead, the question is:\\n\\n*Why don't researchers stop and ask questions about their data, instead of blindly applying highly unstable estimates?*\\n\\nthen the answer really comes down to training. There are far too many researchers who were never trained in statistics properly, summed up by the general reliance on p-values as the be-all and end-all of 'statistical significance'.\\n\\n@Kwak: Huber's estimates from the 1970s *are* robust, in the classical sense of the word: they resist outliers. And redescending estimators actually date well before the 1980s: the Princeton robustness study (of 1971) included the bisquare estimate of location, a redescending estimate.",deleted 25 characters in body,
3031,2,1374,a09f9f7d-6275-4e09-8e52-cec08cae7ec6,2010-08-06 16:10:48.0,88.0,"-How many statisticians does it take to change a light bulb?  \\n-5-7, with p-value 0.01",,
3032,16,1374,a09f9f7d-6275-4e09-8e52-cec08cae7ec6,2010-08-06 16:10:48.0,-1.0,,,
3033,5,1325,2b0d6b96-7232-45af-972b-203e5279d80d,2010-08-06 16:20:50.0,603.0,"Weibull is sometimes used for modelling ping time. try a weibull distribution. To fit one in R:\\n\\n    x<-rweibull(n=1000,shape=2,scale=100)\\n    #generate a weibull (this should be your data).\\n    hist(x)\\n    #this is an histogram of your data.\\n    library(survival)\\n    a1<-survreg(Surv(x,rep(1,1000))~1,dist='weibull')\\n    exp(a1$coef) #this is the ML estimate of the scale parameter\\n    1/a1$scale     #this is the ML estimate of the shape parameter\\n\\nIf you're wondering for the goofy names (i.e. $scale to get the inverse of the shape)\\nit's because ""survreg"" uses another parametrization (i.e. it is parametrized in terms of the ""inverse weibull"" which is more comon in actuarial sciences).",added 102 characters in body,
3034,5,1182,22555fd9-a5de-4cae-88f2-f56e75b5506a,2010-08-06 17:00:04.0,8.0,"From a psychological perspective, I advocate plotting the data plus your uncertainty about the data. Thus, in a plot like you show, I would never bother with extending the bars all the way to zero, which only serves to minimize the eye's ability to distinguish differences in the range of the data.\\n\\nAdditionally, I'm frankly anti-bargraph; bar graphs map two variables to the same aesthetic attribute (x-axis location), which can cause confusion. A better approach is to avoid redundant aesthetic mapping by mapping one variable to the x-axis and another variable to another aesthetic attribute (eg. point shape or color or both).\\n\\nFinally, in your plot above, you only include error bars above the value, which hinders one's ability to compare the intervals of uncertainty relative to bars above and below the value.\\n\\nHere's how I would plot the data (via the ggplot2 package). Note that I add lines connecting points in the same series; some argue that this is only appropriate when the series across which the lines are connected are numeric (as seems to be in this case), however as long as there is any reasonable ordinal relationship among the levels of the x-axis variable, I think connecting lines are useful for helping the eye associate points across the x-axis. This can become particularly useful for detecting interactions, which really stand out with lines.\\n\\n    library(ggplot2)\\n    a = data.frame(names,prevs,se)\\n    a$let = substr(a$names,1,1)\\n    a$num = substr(a$names,2,2)\\n    ggplot(data = a)+\\n    layer(\\n        geom = 'point'\\n        , mapping = aes(\\n            x = num\\n            , y = prevs\\n            , colour = let\\n            , shape = let\\n        )\\n    )+\\n    layer(\\n        geom = 'line'\\n        , mapping = aes(\\n            x = num\\n            , y = prevs\\n            , colour = let\\n            , linetype = let\\n            , group = let\\n        )    \\n    )+\\n    layer(\\n        geom = 'errorbar'\\n        , mapping = aes(\\n            x = num\\n            , ymin = prevs-se\\n            , ymax = prevs+se\\n            , colour = let\\n        )\\n        , alpha = .5\\n        , width = .5\\n    )\\n\\n![alt text][1]\\n\\n\\n  [1]: http://img517.imageshack.us/img517/8002/tmpa.jpg",Making the graph a bit smaller,
3035,5,1374,70edb703-80df-4f1e-a39b-27c1b737d389,2010-08-06 17:54:34.0,88.0,"How many statisticians does it take to change a light bulb?  \\n5–7, with p-value 0.01",deleted 2 characters in body,
3036,2,1375,ff3c6ebf-24e2-4c0a-881c-0c93601ae278,2010-08-06 18:42:26.0,253.0,"Here is a list of many fun statistics jokes ([link][1])\\n\\nHere are just a few:\\n\\n------\\n\\nDid you hear the one about the statistician? Probably....\\n\\n------\\nIt is proven that the celebration of birthdays is healthy. Statistics show that those people who celebrate the most birthdays become the oldest. -- S. den Hartog, Ph D. Thesis Universtity of Groningen.\\n\\n------\\n\\nA statistician is a person who draws a mathematically precise line from an unwarranted assumption to a foregone conclusion.\\n\\n\\n------\\n\\n\\nThe average statistician is just plain mean.\\n\\n------\\n\\n\\nAnd there is also the one from a TED talk:\\n\\n""A friend ask my wife what I do.  She answered that I model.  Model what she was asked - he model's genes - she answered""\\n\\n\\n  [1]: http://www.btinternet.com/~se16/hgb/statjoke.htm",,
3037,16,1375,ff3c6ebf-24e2-4c0a-881c-0c93601ae278,2010-08-06 18:42:26.0,-1.0,,,
3038,2,1376,f74218a0-5922-41a9-af98-a2d132a962c4,2010-08-06 19:02:08.0,795.0,"I am looking for a robust version of Hotelling's T^2 test for the mean of a vector. As data, I have a m x n matrix, X, each row an i.i.d. sample of an n-dimensional RV, x. The null hypothesis I wish to test is E[x] = mu, where mu is a fixed n-dimensional vector. The classical Hotelling test appears to be susceptible to non-normality in the distribution of x (just as the 1-d analogue, the Student t-test is susceptible to skew and kurtosis). \\n\\nwhat is the state of the art robust version of this test? I am looking for something relatively fast and conceptually simple. There was a paper in COMPSTAT 2008 on the topic, but I do not have access to the proceedings. any help? ",,
3039,1,1376,f74218a0-5922-41a9-af98-a2d132a962c4,2010-08-06 19:02:08.0,795.0,robust version of Hotelling T^2 test. ,,
3040,3,1376,f74218a0-5922-41a9-af98-a2d132a962c4,2010-08-06 19:02:08.0,795.0,<statistical-analysis><t-test><test><robust>,,
3041,5,1376,a2ce8a18-de60-4f8f-94f8-75e42ef55ceb,2010-08-06 19:07:21.0,,"I am looking for a robust version of Hotelling's $T^2$ test for the mean of a vector. As data, I have a $m\\ \\times\\  n$ matrix, $X$, each row an i.i.d. sample of an $n$-dimensional RV, $x$. The null hypothesis I wish to test is $E[x] = \\mu$, where $\\mu$ is a fixed $n$-dimensional vector. The classical Hotelling test appears to be susceptible to non-normality in the distribution of $x$ (just as the 1-d analogue, the Student t-test is susceptible to skew and kurtosis). \\n\\nwhat is the state of the art robust version of this test? I am looking for something relatively fast and conceptually simple. There was a paper in COMPSTAT 2008 on the topic, but I do not have access to the proceedings. any help? ",changed notation to latex,user28
3042,4,1376,a2ce8a18-de60-4f8f-94f8-75e42ef55ceb,2010-08-06 19:07:21.0,,robust version of Hotelling $T^2$ test. ,changed notation to latex,user28
3043,2,1377,2a8e03dc-57ea-4404-9dee-3c14d88cee0d,2010-08-06 20:07:17.0,236.0,"""If you torture data enough it will confess"" one of my professors ",,
3044,16,1377,2a8e03dc-57ea-4404-9dee-3c14d88cee0d,2010-08-06 20:07:17.0,-1.0,,,
3045,2,1378,78598efc-4ab7-4bff-a5f4-fb94265acde8,2010-08-06 21:54:11.0,71.0,"I have a dataset that contains ~7,500 blood tests from ~2,500 individuals.  I'm trying to find out if variability in the blood tests increases or decreases with the time between two tests.  For example - I draw your blood for the baseline test, then immediately draw a second sample.  Six months later, I draw another sample.  One might expect the difference between the baseline and the immediate repeat tests to be smaller than the difference between the baseline and the six-month test.\\n\\nEach point on the plot below reflects the difference between two tests.  X is the number of days between two tests; Y is the size of the difference between the two tests.  As you can see, tests aren't evenly distributed along X - the study wasn't designed to address this question, really.  Because the points are so heavily stacked at the mean, I've included 95% (blue) and 99% (red) quantile lines, based on 28-day windows.  These are obviously pulled around by the more extreme points, but you get the idea.\\n\\n![alt text][1]\\n\\nIt looks to me like the variability is fairly stable.  If anything, it's higher when the test is repeated within a short period - that's terribly counterintuitive.  How can I address this in a systematic way, accounting for varying n at each time point (and some periods with no tests at all)?  Your ideas are greatly appreciated.\\n\\nJust for reference, this is the distribution of the number of days between test and retest:\\n\\n![alt text][2]\\n\\n\\n  [1]: http://a.imageshack.us/img175/6595/diffsbydays.png\\n  [2]: http://a.imageshack.us/img697/6572/testsateachtimepoint.png",,
3046,1,1378,78598efc-4ab7-4bff-a5f4-fb94265acde8,2010-08-06 21:54:11.0,71.0,Estimating variability over time,,
3047,3,1378,78598efc-4ab7-4bff-a5f4-fb94265acde8,2010-08-06 21:54:11.0,71.0,<repeated-measures><variability>,,
3048,2,1379,4f5c4916-2dbd-4aac-a250-11f911ef1bdd,2010-08-06 22:10:44.0,795.0,"you could compute a [Kolmogorov-Smirnov][1] statistic based on your binned data. This would work by first computing an empirical CDF based on your bins (just a cumulative sum with rescaling), then compute the $\\infty$-norm of the differences.\\n\\nI don't know R well enough to give you code in R, but can quote the Matlab very simply:\\n\\n    %let base be the 1 x 1024 vector of binned observed data\\n    %let dists be the 1000 x 1024 matrix of binned distributions to be checked\\n    emp_base = cumsum(base,2) ./ 1024;\\n    emp_dists = cumsum(dists,2) ./ 1024;\\n    emp_diff = bsxfun(@minus,emp_base,emp_dists);   %subtract the cdfs; R does this transparently, IIRC\\n    KS_stat = max(abs(emp_diff),[],2);   %take the maximum absolute difference along dim 2\\n    %KS_stat is now a 1000 x 1 vector of the KS statistic. you can convert to a p-value as well.\\n    %but you might as well just rank them.\\n    [dum,dist_ranking] = sort(KS_stat);\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kolmogorov_Smirnov_Test ""Kolmogorov Smirnov""",,
3049,2,1380,657e76b7-1b0e-4443-9bd2-17e8cb5d2358,2010-08-06 22:16:01.0,795.0,"(migrating from math overflow, where no answers were posted)\\n\\nsuppose I have `$K$` different methods for forecasting a binary random variable, which I test on independent sets of data, resulting in `$K$` contingency tables of values `$n_{ijk}$` for $i,j=1,2$ and $k=1,2,...,K$. How can I compare these methods based on the contingency tables?  The general case would be nice, but `$K=2$` is also very interesting.\\n\\nI can think of a few approaches:\\n\\n - compute _some_ statistic on each of the tables, and compare those random variables (I'm not sure if this is a standard problem or not),\\n - something like [Goodman's improvement of Stouffer's method][1], but I cannot access this paper, and was hoping for something a little more recent (more likely to have the latest-greatest, plus computer simulations).\\n\\nany ideas?\\n\\n[1]: http://www.jstor.org/pss/2982447 ""On methods of comparing contingency tables""",,
3050,1,1380,657e76b7-1b0e-4443-9bd2-17e8cb5d2358,2010-08-06 22:16:01.0,795.0,"comparing multiple contingency tables, independent data",,
3051,3,1380,657e76b7-1b0e-4443-9bd2-17e8cb5d2358,2010-08-06 22:16:01.0,795.0,<forecasting><contingency-tables>,,
3052,4,1268,4874ac75-7399-4a57-a900-8969d9cbbe5b,2010-08-06 22:19:31.0,170.0,SVD dimensionality reduction for time series of different length,changed title,
3053,2,1381,df32d923-d9d1-4cca-9129-3ed88ba237b6,2010-08-06 22:34:27.0,795.0,"The Baumgartner-Weiss-Schindler statistic appears to be the modern alternative to the K-S test, and appears to be more powerful in certain situations. A few links:\\n\\n - [A Nonparametric Test for the General Two-Sample Problem][2] (the original B.W.S. paper)\\n - M. Neuhauser, ['Exact Tests Based on the Baumgartner-Weiss-Schindler Statistic--A Survey'][1], Statistical Papers, Vol 46 (2005), pp. 1-30. (perhaps not relevant to your large sample case...)\\n - H. Murakami, 'K-Sample Rank Test Based on Modified Baumgartner Statistic and its Power Comparison', J. Jpn. Comp. Statist. Vol 19 (2006), pp. 1-13.\\n - M. Neuhauser, 'One-Sided Two-Sample and Trend Tests Based on a Modified Baumgartner-Weiss-Schindler Statistic', J. Nonparametric Statistics, Vol 13 (2001) pp 729-739.\\n \\n\\n  [1]: http://www.springerlink.com/content/j7270477094575hn/\\n  [2]: http://www.jstor.org/stable/2533862",,
3054,5,786,ac5632dd-5394-4751-97cb-18b74769f0df,2010-08-06 23:23:57.0,509.0,"> The death of one man is a tragedy. \\n> The death of millions is a statistic.\\n\\n  ~Joseph Stalin, comment to Churchill at Potsdam, 1945",added 3 characters in body,
3055,5,1380,f65709fd-c92f-4e7e-bbf4-22f9d9262002,2010-08-07 02:17:45.0,159.0,"(migrating from math overflow, where no answers were posted)\\n\\nsuppose I have $K$ different methods for forecasting a binary random variable, which I test on independent sets of data, resulting in $K$ contingency tables of values $n_{ijk}$ for $i,j=1,2$ and $k=1,2,...,K$. How can I compare these methods based on the contingency tables?  The general case would be nice, but $K=2$ is also very interesting.\\n\\nI can think of a few approaches:\\n\\n - compute _some_ statistic on each of the tables, and compare those random variables (I'm not sure if this is a standard problem or not),\\n - something like [Goodman's improvement of Stouffer's method][1], but I cannot access this paper, and was hoping for something a little more recent (more likely to have the latest-greatest, plus computer simulations).\\n\\nany ideas?\\n\\n[1]: http://www.jstor.org/pss/2982447 ""On methods of comparing contingency tables""",Fixed LaTeX,
3056,2,1382,09252319-e87f-4770-92f3-ff60464d237f,2010-08-07 02:33:35.0,159.0,"A statistics major was completely hung over the day of his final exam. It was a true/false test, so he decided to flip a coin for the answers. The statistics professor watched the student the entire two hours as he was flipping the coin … writing the answer … flipping the coin … writing the answer. At the end of the two hours, everyone else had left the final except for the one student. The professor walks up to his desk and interrupts the student, saying, “Listen, I have seen that you did not study for this statistics test, you didn’t even open the exam. If you are just flipping a coin for your answer, what is taking you so long?”\\nThe student replies bitterly (as he is still flipping the coin), “Shhh! I am checking my answers!”\\n\\nI've posted a few others on [my blog][1].\\n\\n [1]: http://robjhyndman.com/researchtips/statistical-jokes/",,
3057,16,1382,09252319-e87f-4770-92f3-ff60464d237f,2010-08-07 02:33:35.0,-1.0,,,
3058,5,1379,6de2da56-e8c6-4054-9011-36012128a3a3,2010-08-07 03:10:45.0,795.0,"you could compute a [Kolmogorov-Smirnov][1] statistic based on your binned data. This would work by first computing an empirical CDF based on your bins (just a cumulative sum with rescaling), then compute the $\\infty$-norm of the differences.\\n\\nI don't know R well enough to give you code in R, but can quote the Matlab very simply:\\n\\n    %let base be the 1 x 1024 vector of binned observed data\\n    %let dists be the 1000 x 1024 matrix of binned distributions to be checked\\n    emp_base = cumsum(base,2) ./ 1024;\\n    emp_dists = cumsum(dists,2) ./ 1024;\\n    emp_diff = bsxfun(@minus,emp_base,emp_dists);   %subtract the cdfs; R does this transparently, IIRC\\n    KS_stat = max(abs(emp_diff),[],2);   %take the maximum absolute difference along dim 2\\n    %KS_stat is now a 1000 x 1 vector of the KS statistic. you can convert to a p-value as well.\\n    %but you might as well just rank them.\\n    [dum,sort_idx] = sort(KS_stat);  %Matlab does not have a builtin ranking; PITA!\\n    dist_ranks = nan(numel(sort_idx),1);\\n    dist_ranks(sort_idx) = (1:numel(sort_idx))';\\n    %dist_ranks are now the ranks of each of the distributions (ignoring ties!)\\n    %if you want the best match, it is indexed by sort_idx(1);\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kolmogorov_Smirnov_Test ""Kolmogorov Smirnov""",embellish Matlab code: rank vs. sort order.,
3059,2,1383,4624b1a4-fe27-4717-b3e8-a06850f9bc1b,2010-08-07 03:47:26.0,799.0,"There's a lot of work done in statistics,\\nwhile state-of-art in lossless data compression is apparently this:\\nhttp://mattmahoney.net/dc/dce.html#Section_4\\n\\nPlease suggest good methods/models applicable for data compression.\\nTo be specific:<br>\\n1) How to predict the next bit in a bit string?<br>\\n2) How to integrate predictions of different models?<br>",,
3060,1,1383,4624b1a4-fe27-4717-b3e8-a06850f9bc1b,2010-08-07 03:47:26.0,799.0,Suggest a method for statistical data compression,,
3061,3,1383,4624b1a4-fe27-4717-b3e8-a06850f9bc1b,2010-08-07 03:47:26.0,799.0,<modeling>,,
3062,5,1383,baf49331-1021-4075-b2ea-fde0dc604154,2010-08-07 03:54:12.0,799.0,"There's a lot of work done in statistics,\\nwhile state-of-art in lossless data compression is apparently this:\\nhttp://mattmahoney.net/dc/dce.html#Section_4\\n\\nPlease suggest good methods/models applicable for data compression.\\nTo be specific:<br>\\n1) How to estimate the probability of the next bit in a bit string?<br>\\n2) How to integrate predictions of different models?<br>",added 20 characters in body,
3063,2,1384,dd6b7007-0364-4652-8e39-7120729160ac,2010-08-07 03:55:36.0,183.0,"1. I agree with the point that statistics consultants are often brought in later on a project when it's too late to remedy design flaws. It's also true that many statistics book give scant attention to study design issues.\\n\\n2. You say you want designs ""preferably for a wide range of methods (e.g. t-test, GLM, GAM, ordination techniques..."". I see designs as relatively independent of statistical method: e.g., experiments (between subjects and within subjects factors) versus observational studies; longitudinal versus cross-sectional; etc. There are also a lot of issues related to measurement, domain specific theoretical knowledge, and domain specific study design principles that need to be understood in order to design a good study.\\n\\n3. In terms of books, I'd be inclined to look at domain specific books. In psychology (where I'm from) this means books on psychometrics for measurement, a book on research methods, and a book on statistics, as well as a range of even more domain specific research method books. You might want to check out [Research Methods Knowledge Base][1] for a free online resource for the social sciences.\\n\\n4. Published journal articles are also a good guide to what is best practice in a particular domain. \\n\\n  [1]: http://www.socialresearchmethods.net/kb/index.php",,
3064,16,1384,dd6b7007-0364-4652-8e39-7120729160ac,2010-08-07 03:55:36.0,-1.0,,,
3065,2,1385,1cae1ae4-c11d-4e91-bab1-acd838fda51d,2010-08-07 05:07:27.0,438.0,"My initial reason for asking this question was a problem directly in front of me; however, before posting my Question, i thought it might be more useful if i re-phrased it so the answers would be useful to a larger portion of the Community.\\n\\nAs a simple heuristic, let's divide these data-handling techniques based on when during the processing flow they are employed--before input to the classifier or during (i.e., the technique is inside the classifier).\\n \\nThe best example i can think of for the latter is the clever 'three-way branching' technique used in Decision Trees.\\n\\nNo doubt, the former category is far larger. The techniques i am aware of all fall into one of the groups below. \\n\\nWhile recently reviewing my personal notes on ""missing data handling"" i noticed that i had quite an impressive list of techniques. I just maintain these notes for general peace of mind and in case a junior colleague asks me how to deal with missing data. In actual practice, i don't actually use any of them, except for the last one.\\n\\n 1. **Imputation**: a broad rubric for a set of techniques which whose common\\n    denominator (i believe) is that the\\n    missing data is supplied directly by\\n    the same data set--substitution\\n    rather than estimation/prediction.\\n\\n 2. **Reconstruction**: estimate the missing data points using an\\n    auto-associative network (just a\\n    neural network in which the sizes of\\n    the input and output layers are\\n    equal--in other words, the output\\n    has the same dimension as the\\n    input); the idea here is to train\\n    this network on complete data, then\\n    feed it incomplete patterns, and\\n    read the missing values from the\\n    output nodes.\\n\\n 3. **Bootstrapping**: (no summary necessary i shouldn't think, given\\n    it's use elsewhere in statistical\\n    analysis).\\n\\n 4. **Denial**: quietly remove the data points with missing/corrupt elements\\n    from your training set and pretend\\n    they never existed.\\n\\n",,
3066,1,1385,1cae1ae4-c11d-4e91-bab1-acd838fda51d,2010-08-07 05:07:27.0,438.0,Defensible Techniques for Handling Incomplete/Missing Data during Training/Fitting,,
3067,3,1385,1cae1ae4-c11d-4e91-bab1-acd838fda51d,2010-08-07 05:07:27.0,438.0,<data><methodology><data-generating-process>,,
3068,2,1386,f68d0522-3430-4721-9e78-538562e7390c,2010-08-07 05:18:58.0,795.0,"I am trying to test the null $E[X] = 0$, against the local alternative $E[X] > 0$, for a random variable $X$, subject to mild to medium skew and kurtosis of the random variable. Following suggestions by Wilcox in 'Introduction to Robust Estimation and Hypothesis Testing', I have looked at tests based on the trimmed mean, the median, as well as the M-estimator of location (Wilcox' ""one-step"" procedure). These robust tests do outperform the standard t-test, in terms of power, when testing with a distribution that is non-skewed, but leptokurtotic.\\n\\nHowever, when testing with a distribution that is skewed, these one-sided tests are either far too liberal or far too conservative under the null hypothesis, depending on whether the distribution is left- or right-skewed, respectively. For example, with 1000 observations, the test based on the median will actually reject ~40% of the time, at the nominal 5% level. The reason for this is obvious: for skewed distributions, the median and the mean are rather different. However, in my application, I really need to test the mean, not the median, not the trimmed mean.\\n\\nThe question, then: is there a more robust version of the t-test that actually tests for the mean, but is impervious to skew and kurtosis? Ideally the procedure would work well in the no-skew, high-kurtosis case as well. The 'one-step' test is almost good enough, with the 'bend' parameter set relatively high, but it is less powerful than the trimmed mean tests when there is no skew, and has some troubles maintaining the nominal level of rejects under skew.\\n\\n**background:** the reason I really care about the mean, and not the median, is that the test would be used in a financial application. For example, if you wanted to test whether a portfolio had positive expected log returns, the mean is actually appropriate because if you invest in the portfolio, you will experience all the returns (which is the mean times the number of samples), instead of $n$ duplicates of the median.  That is, I really care about the sum of $n$ draws from the R.V. $X$. ",,
3069,1,1386,f68d0522-3430-4721-9e78-538562e7390c,2010-08-07 05:18:58.0,795.0,a robust t-test for the mean,,
3070,3,1386,f68d0522-3430-4721-9e78-538562e7390c,2010-08-07 05:18:58.0,795.0,<hypothesis-testing><t-test><finance><test><robust>,,
3071,2,1387,d8823cfa-f77a-44dc-b456-40d1c0b6677f,2010-08-07 06:03:39.0,795.0,there was the one about the two statisticians who tried to use grant money to pay for their bill at a strip club.  They were vindicated when it was explained they were performing a 'posterior analysis'.  (groan),,
3072,16,1387,d8823cfa-f77a-44dc-b456-40d1c0b6677f,2010-08-07 06:03:39.0,-1.0,,,
3073,2,1388,aaa17624-a035-4204-81b4-f5315e2cace9,2010-08-07 07:15:20.0,,"> A statistician's wife had twins. He\\n> was delighted. He rang the minister\\n> who was also delighted. ""Bring them to\\n> church on Sunday and we'll baptize\\n> them,"" said the minister. ""No,""\\n> replied the statistician. ""Baptize\\n> one. We'll keep the other as a\\n> control.""\\n\\n*STATS: The Magazine For Students of Statistics, Winter 1996, Number 15*\\n",,hadast85
3074,16,1388,aaa17624-a035-4204-81b4-f5315e2cace9,2010-08-07 07:15:20.0,-1.0,,,
3075,5,1350,22c8414c-a3c0-43d5-8fd2-43598d0b656e,2010-08-07 07:21:34.0,189.0,"I am working with a large data set (approximately 50K observations) and trying to running a Maximum likelihood estimation on 5 unknowns in Stata. \\n\\nI encountered an error message of ""Numerical Overflow"". How can I overcome this? \\n\\nI am trying to run a Stochastic Frontier analysis using the built in Stata command ""frontier"". The dependent variable is log of output and the independent variable is log of intermediate inputs, capital, labour, and utlities. ",added 230 characters in body,
3076,2,1389,42a688e2-ee21-4211-8e55-ad41323e5bd2,2010-08-07 07:23:07.0,189.0,I came across an error of numerical overflow when running a maximum likelihood estimation on a log-linear specification. \\n\\nWhat does numerical overflow mean?,,
3077,1,1389,42a688e2-ee21-4211-8e55-ad41323e5bd2,2010-08-07 07:23:07.0,189.0,What is numerical overflow?,,
3078,3,1389,42a688e2-ee21-4211-8e55-ad41323e5bd2,2010-08-07 07:23:07.0,189.0,<estimation><maximum-likelihood>,,
3079,2,1390,1a413ae9-1231-4a35-957a-10cb6a79f67d,2010-08-07 07:23:55.0,485.0,"Why are you looking at non-parametric tests?  Are the assumptions of the t-test violated?  Namely, ordinal or non-normal data and inconstant variances?  Of course, if your sample is large enough you can justify the parametric t-test with its greater power despite the lack of normality in the sample.  Likewise if your concern is unequal variances, there are corrections to the parametric test that yield accurate p-values (the Welch correction).\\n\\nOtherwise, comparing your results to the t-test is not a good way to go about this, because the t-test results are biased when the assumptions are not met.  The Mann-Whitney U is an appropriate non-parametric alternative, if that's what you really need.  You only lose power if you are using the non-parametric test when you could justifiably use the t-test (because the assumptions are met).",,
3080,5,1390,8566f9e8-7c8e-4e13-8554-180f7471c72f,2010-08-07 07:30:04.0,485.0,"Why are you looking at non-parametric tests?  Are the assumptions of the t-test violated?  Namely, ordinal or non-normal data and inconstant variances?  Of course, if your sample is large enough you can justify the parametric t-test with its greater power despite the lack of normality in the sample.  Likewise if your concern is unequal variances, there are corrections to the parametric test that yield accurate p-values (the Welch correction).\\n\\nOtherwise, comparing your results to the t-test is not a good way to go about this, because the t-test results are biased when the assumptions are not met.  The Mann-Whitney U is an appropriate non-parametric alternative, if that's what you really need.  You only lose power if you are using the non-parametric test when you could justifiably use the t-test (because the assumptions are met).\\n\\nAnd, just for some more background, go here...\\n\\nhttp://www.jerrydallal.com/LHSP/STUDENT.HTM",added 97 characters in body,
3081,2,1391,9d7b3ef0-04e0-488c-9971-0ff313baba81,2010-08-07 07:34:44.0,183.0,"I agree that if you want to actually test whether the group means are different (as opposed to testing differences between group medians or trimmed means, etc.), then you don't want to use a nonparametric test that tests a different hypothesis.\\n\\n1. In general p-values from a t-test tend to be fairly accurate given moderate departure in the assumption of normality of residuals.\\nCheck out this applet to get an intuition on this robustness: http://onlinestatbook.com/stat_sim/robustness/index.html\\n\\n2. If you're still concerned about the violation of the normality assumption, \\nyou might want to **bootstrap**.\\ne.g., http://biostat.mc.vanderbilt.edu/wiki/pub/Main/JenniferThompson/ms_mtg_18oct07.pdf\\n\\n3. You could also **transform** the skewed dependent variable to resolve issues with departures from normality.\\n\\n\\n",,
3082,2,1392,35d5a247-5f51-408e-9fc0-305326e1dc20,2010-08-07 07:35:58.0,582.0,"It means that the algorithm generated a variable that is greater than the maximum allowed for that type of variable. That is due to the fact that computers use a finite number of bits to represent numbers, so it is not possible to represent ANY number, but only a limited subset of them.\\n\\nThe actual value depends on the type of variable and the architecture of the system.\\n\\nWhy that happens during a MLE I'm not sure, my best call would be that you should change the starting parameters.",,
3083,2,1393,c4c7aeb4-89a3-4468-b4f7-c1b86b44530e,2010-08-07 08:12:09.0,352.0,"There is a reasonably new area of research called *Matrix Completion*, that probably does what you want. A really nice introduction to this is given in this [lecture][1] by Emmanuel Candes \\n\\n\\n  [1]: http://videolectures.net/mlss09us_candes_mccota/",,
3085,5,1393,0b8be16f-85b1-40f9-94c0-0be35f06568e,2010-08-07 09:29:45.0,352.0,"There is a reasonably new area of research called *Matrix Completion*, that probably does what you want. A really nice introduction is given in this [lecture][1] by Emmanuel Candes \\n\\n\\n  [1]: http://videolectures.net/mlss09us_candes_mccota/",deleted 8 characters in body,
3086,5,1391,e9d849a1-a308-4c02-907e-f4aa035c0c9c,2010-08-07 09:39:58.0,183.0,"I agree that if you want to actually test whether the group means are different (as opposed to testing differences between group medians or trimmed means, etc.), then you don't want to use a nonparametric test that tests a different hypothesis.\\n\\n1. In general p-values from a t-test tend to be fairly accurate given moderate departures of the assumption of normality of residuals.\\nCheck out this applet to get an intuition on this robustness: http://onlinestatbook.com/stat_sim/robustness/index.html\\n\\n2. If you're still concerned about the violation of the normality assumption, \\nyou might want to **bootstrap**.\\ne.g., http://biostat.mc.vanderbilt.edu/wiki/pub/Main/JenniferThompson/ms_mtg_18oct07.pdf\\n\\n3. You could also **transform** the skewed dependent variable to resolve issues with departures from normality.\\n\\n\\n",added 1 characters in body,
3087,5,880,c5189e65-ee77-4c54-8ef6-44f7ea89d246,2010-08-07 09:52:20.0,223.0,"My question is about binary classification in very high dimension (more features than observation).\\n\\n **Problem:** Assume that for each variable $i=1,\\dots,p$ you have a measure of importance $T[i]$ than exactly measure the interest of feature $i$ for the classification problem. The problem of selecting a subset of feature to reduce optimally the classification error is then reduced to that of finding the number of features. \\n\\n**Question:** What is the most efficient way to run cross validation in this case (cross validation scheme)? My question is not about how to write the code but on the version of cross validation to use when trying to find the number of selected feature (to minimize the classification error) but how to deal with the high dimension when doing cross validation (hence the problem above may be a bit like a 'toy problem' to discuss CV in high dimension). \\n\\n\\n**Notations** $n$ is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example $p=10000$ and $n=100$). ",math edit + trying to rephrase the question to focus on CV  in very high dimension,
3088,4,880,c5189e65-ee77-4c54-8ef6-44f7ea89d246,2010-08-07 09:52:20.0,223.0,Cross validation in very high dimension (to select the number of used variables in very high dimensional classification),math edit + trying to rephrase the question to focus on CV  in very high dimension,
3089,6,880,c5189e65-ee77-4c54-8ef6-44f7ea89d246,2010-08-07 09:52:20.0,223.0,<machine-learning><classification><cross-validation>,math edit + trying to rephrase the question to focus on CV  in very high dimension,
3090,5,725,0c96484a-22ca-4179-9863-d21a69d8ff8c,2010-08-07 09:53:37.0,223.0,"An **hyperspectral image is** a multidimensional image with more than 200 spectral bands i.e. an image for which each pixel is a vector of dimension 200 (most often it is a sampled spectral curve that is encoutered in satellite imagery or medical imagery). \\n\\n\\nWhat are the **implemented package** (I am especially interested in R packages but if other free algorithms exist, I will try them) for **frontier detection** and (unsupervised ) **segmentation** of this type of images?  \\n",edited body; added 59 characters in body; edited title,
3091,4,725,0c96484a-22ca-4179-9863-d21a69d8ff8c,2010-08-07 09:53:37.0,223.0,hyperspectral images : Frontier estimation - or Segmentation - R package,edited body; added 59 characters in body; edited title,
3092,5,973,73d7b017-4ac8-42e6-81a1-876d48e0ce3f,2010-08-07 09:56:31.0,223.0,"What are the **freely available data set for classification with more than 1000 features** (or sample points if it is about curves)? \\n\\nThere is already a community wiki about free data sets :\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nbut here, it would be nice to have a **more focused list that can be used more conveniently**, also I propose the following rules:\\n\\n - One post **per dataset** \\n - **No** link to set of dataset (however, many data set can be given in a post )\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n\\n    3 - a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n\\n",added 8 characters in body; deleted 272 characters in body,
3095,5,878,13366060-664d-494c-99c0-fc9042db0bbf,2010-08-07 10:06:32.0,380.0,> Absence of evidence is not evidence of\\n> absence.\\n\\nCarl Sagan,blockquote. correct Sagan's name,
3096,5,732,b28c3440-0cae-4078-8446-5068150e7b9b,2010-08-07 10:08:04.0,380.0,> Strange events permit themselves the\\n> luxury of occurring.\\n\\n-- [Charlie Chan][1]\\n\\n\\n  [1]: http://gutenberg.net.au/ebooks02/0200691.txt,formatting,
3097,5,737,3eec0a85-fc38-421a-9064-d1bef7a1d982,2010-08-07 10:09:08.0,380.0,"> There are no routine statistical\\n> questions, only questionable\\n> statistical routines.\\n\\nD.R. Cox",formatting,
3098,5,1358,af850d49-6f14-4af8-8215-a7692a9b21f1,2010-08-07 11:05:53.0,650.0,"In circular statistics, the expectation value of a random variable $Z$ with values on the circle $S$ is defined as\\n$$\\nm_1(Z)=\\int_S z P^Z(\\theta)\\textrm{d}\\theta\\n$$\\n(see [wikipedia](http://en.wikipedia.org/wiki/Circular_statistics#Moments)).\\nThis is a very natural definition, as is the definition of the variance\\n$$\\n\\mathrm{Var}(Z)=1-|m_1(Z)|.\\n$$\\nSo we didn't need a second moment in order to define the variance!\\n\\nNonetheless, we define the higher moments\\n$$\\nm_n(Z)=\\int_S z^n P^Z(\\theta)\\textrm{d}\\theta.\\n$$\\nI admit that this looks rather natural as well at first sight, and very similar to the definition in linear statistics. But still I feel a little bit uncomfortable, and have the following\\n\\n**Questions:**\\n\\n1.\\n**What is measured by the higher moments** defined above (intuitively)? Which properties of the distribution can be characterized by their moments?\\n\\n2.\\nIn the computation of the higher moments we use multiplication of complex numbers, although we think of the values of our random variables merely as vectors in the  plane or as angles. I know that complex multiplication is essentially addition of angles in this case, but still:\\n**Why is complex multiplication a meaningful operation for circular data?**\\n",removed obsolete comment on tags,
3099,5,1383,22e0426f-71ca-4818-974c-3b806e660e62,2010-08-07 11:33:26.0,799.0,"There's a lot of work done in statistics,\\nwhile state-of-art in lossless data compression is apparently this:\\nhttp://mattmahoney.net/dc/dce.html#Section_4\\n\\nPlease suggest good methods/models applicable for data compression.\\nTo be specific:<br>\\n1) How to estimate the probability of the next bit in a bit string?<br>\\n2) How to integrate predictions of different models?<br>\\n\\nUpdate:\\n\\n> you should include a better description of what data you\\n> want to compress...\\n\\nWhy, I'm talking about universal compression obviously.\\nFor data with known structure its really not a mathematical problem,\\nso there's no sense to discuss it here.\\nIn other words, the first question is: given a string of bits, \\nwhat do we do to determine the probability of the next bit, as\\nprecisely as possible?\\n\\n> otherwise we will have 10 different answers trying to\\n> summarize different part of the huge theory of compression\\n\\nI'd written quite a few statistical compressors, and I'm\\nnot interested in that.\\nI'm asking how a statistician would approach this task,\\ndetect correlations in given data, and compute a probability\\nestimation for the next bit.\\n\\n> In addition, the two point you give to be more specific\\n> are not detailed enough to be understood.\\n\\nWhat's not detailed in there? I'm even talking about bits,\\nnot some vague ""symbols"". I'd note though, that I'm talking\\nabout ""probability of a bit"" because computing a probability\\nof bit==0 or bit==1 is a matter of convenience.\\n\\nAlso, I'm obviously not talking about some ""random data compression"",\\nor methods with infinite complexity, like ""Kolmogorov compression"".\\nAgain, I want to know how a good statistician would approach this\\nproblem, given a string of bits.\\n\\nHere's an example, if you need one: hxxp://encode.ru/threads/482-Bit-guessing-game\\n",added 1442 characters in body; added 3 characters in body,
3100,2,1395,4751a2d1-6001-4a1e-b05f-3d1f3d00d934,2010-08-07 13:53:16.0,807.0,Can anyone recommend me an open source graphic library to create forest and funnel plots?\\n\\nI was aiming at using it on a Java desktop application.,,
3101,1,1395,4751a2d1-6001-4a1e-b05f-3d1f3d00d934,2010-08-07 13:53:16.0,807.0,Libraries for forest and funnel plots,,
3102,3,1395,4751a2d1-6001-4a1e-b05f-3d1f3d00d934,2010-08-07 13:53:16.0,807.0,<data-visualization>,,
3103,2,1396,c27f1eb1-f6ff-4e26-aa56-3dd9a83447db,2010-08-07 14:36:40.0,319.0,You can probably avoid your overflow problems by working with the log of the likelihood function rather than the likelihood function itself.  Both have the same maximum.,,
3104,2,1397,d8051577-11a0-4c45-ad42-65eba21a3dff,2010-08-07 14:40:50.0,603.0,"Sure: two answers\\n\\na) If by robustness, you mean robust to outliers, then run Hottelling's T-test using a robust estimation of scale/scatter: you will find all the explications and R code here:\\nhttp://www.r-bloggers.com/a-robust-hotelling-test%E2%80%A6/\\n\\nb) if by robustness you mean optimal under large group of distributions, then you should go for a sign based T2 (ask if this what you want, by the tone of your question i think note).\\n\\nPS: this is the paper you want;\\nRoelant, E., Van Aelst, S., and Willems, G. (2008), “Fast Bootstrap for Robust Hotelling Tests,” COMPSTAT 2008: Proceedings in Computational Statistics (P. Brito, Ed.) Heidelberg: Physika-Verlag, to appear.\\n",,
3105,5,1193,4a36e70a-2872-4d6f-a6e2-9d4bcf72ee3b,2010-08-07 14:44:30.0,174.0,"A variation on the Fisher quotation given [here][1] is\\n\\n>Hiring a statistician after the data have been collected is like hiring a physician when your patient is in the morgue.  He may be able to tell you what went wrong, but he is unlikely to be able to fix it.\\n\\nBut I heard this attributed to Box, not Fisher.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/726/famous-statistician-quotes/739#739","changed first ""physician"" to ""statistician"" and block-quoted the quote",
3106,5,1397,ab6d5e5c-35b3-4820-a821-3d7982ad9327,2010-08-07 14:46:10.0,603.0,"Sure: two answers\\n\\na) If by robustness, you mean robust to outliers, then run Hottelling's T-test using a robust estimation of scale/scatter: you will find all the explications and R code here:\\nhttp://www.r-bloggers.com/a-robust-hotelling-test%E2%80%A6/\\n\\nb) if by robustness you mean optimal under large group of distributions, then you should go for a sign based T2 (ask if this what you want, by the tone of your question i think not).\\n\\nPS: this is the paper you want;\\nRoelant, E., Van Aelst, S., and Willems, G. (2008), “Fast Bootstrap for Robust Hotelling Tests,” COMPSTAT 2008: Proceedings in Computational Statistics (P. Brito, Ed.) Heidelberg: Physika-Verlag, to appear.\\n",deleted 1 characters in body,
3107,2,1398,8be2a80e-3c01-49dd-b817-1f5f9991b5f4,2010-08-07 15:24:01.0,812.0,There was an article on LFT (liver function tests) and how it seemed to vary in a cyclic basis (in months). I think it was in Annals of Internal Medicine. ,,
3108,5,1397,5e1cb87a-06f5-4203-a805-53b695a72f33,2010-08-07 15:39:38.0,603.0,"Sure: two answers\\n\\na) If by robustness, you mean robust to outliers, then run Hottelling's T-test using a robust estimation of scale/scatter: you will find all the explications and R code here:\\nhttp://www.statsravingmad.com/blog/statistics/a-robust-hotelling-test/\\n\\nb) if by robustness you mean optimal under large group of distributions, then you should go for a sign based T2 (ask if this what you want, by the tone of your question i think not).\\n\\nPS: this is the paper you want;\\nRoelant, E., Van Aelst, S., and Willems, G. (2008), “Fast Bootstrap for Robust Hotelling Tests,” COMPSTAT 2008: Proceedings in Computational Statistics (P. Brito, Ed.) Heidelberg: Physika-Verlag, to appear.\\n",added 11 characters in body,
3109,5,1379,2332ece9-eba4-4e44-bc27-7ae6b1fd45e4,2010-08-07 16:00:25.0,795.0,"you could compute a [Kolmogorov-Smirnov][1] statistic based on your binned data. This would work by first computing an empirical CDF based on your bins (just a cumulative sum with rescaling), then compute the $\\infty$-norm of the differences.\\n\\nI don't know R well enough to give you code in R, but can quote the Matlab very simply:\\n\\n    %let base be the 1 x 1024 vector of binned observed data\\n    %let dists be the 1000 x 1024 matrix of binned distributions to be checked\\n    emp_base = cumsum(base,2);emp_base = emp_base ./ emp_base(end);  %cumsum, then normalize\\n    emp_dists = cumsum(dists,2);\\n    emp_dists = bsxfun(@rdivide,emp_dists,emp_dists(:,end));   %normalize for the top sum.\\n    emp_diff = bsxfun(@minus,emp_base,emp_dists);   %subtract the cdfs; R does this transparently, IIRC\\n    KS_stat = max(abs(emp_diff),[],2);   %take the maximum absolute difference along dim 2\\n    %KS_stat is now a 1000 x 1 vector of the KS statistic. you can convert to a p-value as well.\\n    %but you might as well just rank them.\\n    [dum,sort_idx] = sort(KS_stat);  %Matlab does not have a builtin ranking; PITA!\\n    dist_ranks = nan(numel(sort_idx),1);\\n    dist_ranks(sort_idx) = (1:numel(sort_idx))';\\n    %dist_ranks are now the ranks of each of the distributions (ignoring ties!)\\n    %if you want the best match, it is indexed by sort_idx(1);\\n\\nthe `bsxfun` nonsense here is Matlab's way of doing proper vector recycling, which R (and numpy, IIRC) does transparently.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kolmogorov_Smirnov_Test ""Kolmogorov Smirnov""","oopsie: was dividing by # of bins, not the # of counts. would have been OK if # of counts were all equal.",
3110,5,1376,e8d7d61f-29ff-4178-8a4e-7eec26a0bceb,2010-08-07 17:27:44.0,88.0,"I am looking for a robust version of Hotelling's $T^2$ test for the mean of a vector. As data, I have a $m\\ \\times\\  n$ matrix, $X$, each row an i.i.d. sample of an $n$-dimensional RV, $x$. The null hypothesis I wish to test is $E[x] = \\mu$, where $\\mu$ is a fixed $n$-dimensional vector. The classical Hotelling test appears to be susceptible to non-normality in the distribution of $x$ (just as the 1-d analogue, the Student t-test is susceptible to skew and kurtosis). \\n\\nwhat is the state of the art robust version of this test? I am looking for something relatively fast and conceptually simple. There was a paper in COMPSTAT 2008 on the topic, but I do not have access to the proceedings. Any help? ",Spelling fixed.,
3111,4,1376,e8d7d61f-29ff-4178-8a4e-7eec26a0bceb,2010-08-07 17:27:44.0,88.0,Robust version of Hotelling $T^2$ test,Spelling fixed.,
3113,6,1296,ad9efb03-c1eb-4dc5-9c07-20ee3b6c884d,2010-08-07 17:47:06.0,88.0,<probability><confidence-interval>,edited tags,
3114,6,1293,4c950335-90c6-447f-834e-a5e02bda443d,2010-08-07 17:47:26.0,88.0,<books>,edited tags,
3115,6,1184,0e091e33-5e3d-463b-a660-ae7add1a8b6c,2010-08-07 17:47:43.0,88.0,<untagged>,edited tags,
3116,6,1099,2db0e114-ae96-4140-a274-cf5bffdbe4a4,2010-08-07 17:47:54.0,88.0,<categorical-data><count-data>,edited tags,
3117,6,1063,25ed2e6a-bd1c-4432-bebc-a1fcc71a3cbd,2010-08-07 17:48:15.0,88.0,<probability>,edited tags,
3118,6,1062,c55af1c6-2568-4eaa-90f0-c9bac2543aa2,2010-08-07 17:48:25.0,88.0,<multivariable>,edited tags; edited title,
3119,6,942,331ce649-adf4-4336-9506-e25c10449c58,2010-08-07 17:48:38.0,88.0,<statistical-analysis><time-series>,edited tags,
3120,6,928,ca8ea1ad-7d12-45dc-a305-9d315bad03fc,2010-08-07 17:48:50.0,88.0,<measurement>,edited tags,
3121,4,1062,c55af1c6-2568-4eaa-90f0-c9bac2543aa2,2010-08-07 17:48:25.0,88.0,Orthogonal parametrization,edited tags; edited title,
3122,6,897,b497b7e0-7094-4a99-beb8-9c9a883361ec,2010-08-07 17:50:51.0,88.0,<machine-learning>,edited tags,
3124,6,884,8430c64b-d31b-4fa8-815d-bdaade8ba152,2010-08-07 17:51:33.0,88.0,<degrees-of-freedom>,edited tags,
3125,6,846,11594ec4-e0c4-4fbb-b31d-20d2a8039c54,2010-08-07 17:51:42.0,88.0,<untagged>,edited tags,
3126,6,834,478daf8a-8392-46dc-b2bc-ff261036f31c,2010-08-07 17:51:51.0,88.0,<aic>,edited tags,
3127,6,820,5603c6b5-08f7-400e-8d65-b581b316471f,2010-08-07 17:52:01.0,88.0,<algorithms>,edited tags,
3128,6,764,a4e59659-b837-428a-9490-e236b3bf8cca,2010-08-07 17:52:12.0,88.0,<mixed-model>,edited tags,
3129,6,672,160d686f-2c14-4ee5-bd8c-32092c680a80,2010-08-07 17:52:24.0,88.0,<probability><bayesian><theory>,edited tags,
3130,6,665,bf50c6ef-6367-4f8d-bbfd-c97329cb5f6c,2010-08-07 17:52:31.0,88.0,<probability>,edited tags,
3131,6,652,d80a9fbc-8c78-4b9f-b28d-9b53311f1eeb,2010-08-07 17:52:40.0,88.0,<machine-learning><bayesian><books>,edited tags,
3132,6,645,ce6244f6-d5e2-463d-8fa9-a829809a42e9,2010-08-07 17:53:02.0,88.0,<data><analysis>,edited tags,
3133,6,328,84fba914-685f-4495-91c3-13800c50bf48,2010-08-07 17:53:13.0,88.0,<finance><analysis>,edited tags,
3134,6,269,0aac65b1-58eb-4d8e-bd47-a9fc39723f20,2010-08-07 17:55:39.0,88.0,<standard-deviation><variance><sample><population>,edited tags,
3135,6,222,be7cab10-2cab-48c2-9f7c-1c1277739a80,2010-08-07 17:55:46.0,88.0,<pca><scores>,edited tags,
3136,6,138,0e5317e6-cd46-4e21-95c5-af0ff5736f2a,2010-08-07 17:56:04.0,88.0,<r>,edited tags,
3137,6,50,01b9ee20-afb1-4305-b117-6adea51aad24,2010-08-07 17:56:13.0,88.0,<random-variable>,edited tags,
3138,6,31,15ab7dbc-6fbf-4e80-97f1-ed20ff3d3336,2010-08-07 17:56:25.0,88.0,<hypothesis-testing><t-test><p-value>,edited tags,
3139,6,26,1ffc8b18-b145-41f9-8dd3-e859087f017a,2010-08-07 17:56:37.0,88.0,<statistics><standard-deviation>,edited tags,
3140,6,2,352b5de3-8cb5-4cf5-bd94-420d3bb573e8,2010-08-07 17:56:44.0,88.0,<distributions><normality>,edited tags,
3141,6,73,130000cc-576b-4948-98b2-2f66cb28b21e,2010-08-07 18:00:09.0,88.0,<r>,edited tags,
3143,6,726,a81cc057-bb67-46b6-8d18-6de83750efaa,2010-08-07 18:00:32.0,88.0,<statistical-analysis>,edited tags,
3144,6,130,94f3989a-a6a0-4a7f-9781-e1c77e621f32,2010-08-07 18:00:44.0,88.0,<r>,edited tags,
3145,2,1399,86ccb021-d255-4ed2-b118-c99d99b999fe,2010-08-07 18:10:02.0,364.0,"I'm interested in obtaining a bootstrapped confidence interval on quantity X, when this quantity is measured 10 times in each of 10 individuals.\\n\\nOne approach is to obtain the mean per individual, then bootstrap the means (eg. resample the means with replacement).\\n\\nAnother approach is to do the following on each iteration of the bootstrapping procedure: within each individual, resample that individual's 10 observations with replacement, then compute a new mean for that individual, and finally compute a new group mean. In this approach, each individual observed in the original data set always contribute to the group mean on each iteration of the bootstrap procedure. \\n\\nFinally, a third approach is to combine the above two approaches: resample individuals then resample within those individuals. This approach differs from the preceding approach in that it permits the same individual to contribute multiply to the group mean on each iteration, though because each contribution is generated via an independent resampling procedure, these contributions may be expected to vary slightly from eachother.\\n\\nIn practice, I find that these approaches yield different estimates for the confidence interval (ex. with one data set, I find that the third approach yields much larger confidence intervals than the first two approaches), so I'm curious what each might be interpreted to represent. ",,
3146,1,1399,86ccb021-d255-4ed2-b118-c99d99b999fe,2010-08-07 18:10:02.0,364.0,Obtaining and interpreting bootstrapped confidence intervals from hierarchical data,,
3147,3,1399,86ccb021-d255-4ed2-b118-c99d99b999fe,2010-08-07 18:10:02.0,364.0,<confidence-interval><credible-interval><bootstrap>,,
3149,2,1401,d504f4d5-edb8-4187-a1a1-2f78936d9e94,2010-08-07 18:31:32.0,601.0,Your first approach is wrong for what you want.\\n\\nThe second approach would only apply to those 10 individuals.\\n\\nThe last approach is the correct one. Any increases in the CI are because your CI is more representative of a CI that could be applied to the population instead of those 10 S's.,,
3151,5,1401,6bf3ff18-21ca-444d-b943-b7549ce0326c,2010-08-07 18:40:21.0,601.0,Your first approach is about a between S CI.  If you wanted to measure within S then that's the wrong approach.\\n\\nThe second approach would generate a within S CI that would only apply to those 10 individuals.\\n\\nThe last approach is the correct one for the within S CI. Any increases in the CI are because your CI is more representative of a CI that could be applied to the population instead of those 10 S's.,added 25 characters in body; added 93 characters in body,
3152,5,1385,c998c35f-acf8-4f3c-8284-a32ac71d8f7e,2010-08-07 19:44:45.0,438.0,"My question is directed to techniques to deal with *incomplete* data during the classifier/model training/fitting.\\n\\nFor instance, in a dataset w/ a few hundred rows, each row having let's say five dimensions and a class label as the last item, most data points will look like this:\\n\\n[0.74, 0.39, 0.14, 0.33, 0.34, 0]\\n\\nA few might look something like this:\\n\\n**[0.21, 0.68, ?, 0.82, 0.58, 1]**\\n\\nSo it's those types of data points that are the focus of this Question.\\n\\nMy initial reason for asking this question was a problem directly in front of me; however, before posting my Question, i thought it might be more useful if i re-phrased it so the answers would be useful to a larger portion of the Community.\\n\\nAs a simple heuristic, let's divide these data-handling techniques based on when during the processing flow they are employed--before input to the classifier or during (i.e., the technique is inside the classifier).\\n \\nThe best example i can think of for the latter is the clever 'three-way branching' technique used in Decision Trees.\\n\\nNo doubt, the former category is far larger. The techniques i am aware of all fall into one of the groups below. \\n\\nWhile recently reviewing my personal notes on ""missing data handling"" i noticed that i had quite an impressive list of techniques. I just maintain these notes for general peace of mind and in case a junior colleague asks me how to deal with missing data. In actual practice, i don't actually use any of them, except for the last one.\\n\\n 1. **Imputation**: a broad rubric for a set of techniques which whose common\\n    denominator (i believe) is that the\\n    missing data is supplied directly by\\n    the same data set--substitution\\n    rather than estimation/prediction.\\n\\n 2. **Reconstruction**: estimate the missing data points using an\\n    auto-associative network (just a\\n    neural network in which the sizes of\\n    the input and output layers are\\n    equal--in other words, the output\\n    has the same dimension as the\\n    input); the idea here is to train\\n    this network on complete data, then\\n    feed it incomplete patterns, and\\n    read the missing values from the\\n    output nodes.\\n\\n 3. **Bootstrapping**: (no summary necessary i shouldn't think, given\\n    it's use elsewhere in statistical\\n    analysis).\\n\\n 4. **Denial**: quietly remove the data points with missing/corrupt elements\\n    from your training set and pretend\\n    they never existed.\\n\\n",edited title for clarification and added an example,
3153,4,1385,c998c35f-acf8-4f3c-8284-a32ac71d8f7e,2010-08-07 19:44:45.0,438.0,Techniques for Handling Incomplete/Missing Data,edited title for clarification and added an example,
3154,2,1402,f1149dda-9f80-478d-b7ad-817a6babf0fc,2010-08-07 21:14:56.0,438.0,"Well, i use **[graphviz][1]**, which has Java bindings [(Grappa][2]).\\n\\nAlthough the dot language (graphviz's syntax) is simple, i prefer to use graphviz as a library through the excellent and production-stable python bindings, [pygraphviz][3], and [networkx][4].\\n\\nHere's the code for a simple 'funnel diagram' using those tools; it's not the most elaborate diagram, but it is complete--it initializes the graph object, creates all of the necessary components, styles them, renders the graph, and writes it to file.\\n\\n    import networkx as NX\\n    import pygraphviz as PV\\n\\n    G = PV.AGraph(strict=False, directed=True)     # initialize graph object\\n\\n    # create graph components:\\n    node_list = [""Step1"", ""Step2"", ""Step3"", ""Step4""]\\n    edge_list = [(""Step1, Step2""), (""Step2"", ""Step3""), (""Step3"", ""Step4"")]\\n    G.add_nodes_from(node_list)\\n    G.add_edge(""Step1"", ""Step2"")\\n    G.add_edge(""Step2"", ""Step3"")\\n    G.add_edge(""Step3"", ""Step4"")\\n\\n    # style them:\\n    nak = ""fontname fontsize fontcolor shape style fill color size"".split()\\n    nav = ""Arial 11 white invtrapezium filled cornflowerblue cornflowerblue 1.4"".split()\\n    nas = dict(zip(nak, nav))\\n    for k, v in nas.iteritems() :\\n        G.node_attr[k] = v\\n\\n    eak = ""fontname fontsize fontcolor dir arrowhead arrowsize arrowtail"".split()\\n    eav = ""Arial 10 red4 forward normal 0.8 inv"".split()\\n    eas = dict(zip(eak, eav))\\n    for k, v in eas.iteritems() :\\n        G.edge_attr[k] = v\\n\\n    n1 = G.get_node(""Step1"")\\n    n1.attr['fontsize'] = '11'\\n    n1.attr['fontcolor'] = 'red4'\\n    n1.attr['label'] = '1411'\\n    n1.attr['shape'] = 'rectangle'\\n    n1.attr['width'] = '1.4'\\n    n1.attr['height'] = '0.05'\\n    n1.attr['color'] = 'firebrick4'\\n    n4 = G.get_node(""Step4"")\\n    n4.attr['shape'] = 'rectangle'\\n\\n    # it's simple to scale graph features to indicate 'flow' conditions, e.g., scale \\n    # each container size based on how many items each holds in a given time snapshot:\\n    # (instead of setting node attribute ('width') to a static quantity, you would\\n    # just bind 'n1.attr['width']' to a variable such as 'total_from_container_1'\\n\\n    n1 = G.get_node(""Step2"")\\n    n1.attr['width'] = '2.4'\\n    \\n    # likewise, you can do the same with edgewidth (i.e., make the arrow thicker\\n    # to indicate higher 'flow rate')\\n\\n    e1 = G.get_edge(""Step1"", ""Step2"")\\n    e1.attr['label'] = '  1411'\\n    e1.attr['penwidth'] = 2.6\\n\\n    # and you can easily add labels to the nodes and edges to indicate e.g., quantities: \\n    e1 = G.get_edge(""Step2"", ""Step3"")\\n    e1.attr['label'] = '  392'\\n\\n    G.write(""conv_fnl.dot"")      # save the dot file\\n    G.draw(""conv_fnl.png"")       # save the rendered diagram\\n\\n\\n\\n  [1]: http://www.graphviz.org\\n  [2]: http://www2.research.att.com/~john/Grappa/\\n  [3]: http://networkx.lanl.gov/pygraphviz/\\n  [4]: http://networkx.lanl.gov/",,
3155,2,1403,c6c5de68-bec5-48b0-b4b8-a55336367229,2010-08-07 22:28:44.0,666.0,One common reason for numerical overflow is trying to divide by a very small (close to zero) number.,,
3156,5,1403,b39a6862-bd42-409e-881b-c688e01d1ab9,2010-08-07 22:38:33.0,666.0,"As stated by nico, numerical overflow is when computation finds a number that is too great for the limited number of bits allocated by software to store the number. For example, if your software uses 32 bits to store integers, then computing an integer that is greater than 2,147,483,648 (or smaller than -2,147,483,648) will cause overflow.\\n\\nOne common reason for numerical overflow is trying to divide by a very small (close to zero) number. If the absolute values of your numbers are not too large, Look at your data and try to figure out where you might be dividing by a very small number.",added 495 characters in body,
3157,2,1404,6abd6eca-1227-4769-baa1-57325783dbe2,2010-08-07 22:57:34.0,352.0,"I gave this answer to [another question][1], but it might apply here too.\\n\\n""There is a reasonably new area of research called *Matrix Completion*, that probably does what you want. A really nice introduction is given in this [lecture][2] by Emmanuel Candes""\\n\\nEssentially, if your dataset has low rank (or approximately low rank) i.e. you have 100 rows, but the actual matrix has some small rank, say 10 (or only 10 large singular values), then you can use Matrix Completion to fill the missing data.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1268/svd-dimensionality-reduction-for-time-series-of-different-length/1393#1393\\n  [2]: http://videolectures.net/mlss09us_candes_mccota/",,
3158,2,1405,66387fb2-a5d6-444d-ae68-0398ba01f2fd,2010-08-08 00:44:57.0,,I am attempting to compare two diagnostic odds ratios (DORs). I would like to know of a statistical test which will allow me to do this. Please help! Thank you!,,Jay
3159,1,1405,66387fb2-a5d6-444d-ae68-0398ba01f2fd,2010-08-08 00:44:57.0,,Statistical test for difference between two odds ratios?,,Jay
3160,3,1405,66387fb2-a5d6-444d-ae68-0398ba01f2fd,2010-08-08 00:44:57.0,,<statistics>,,Jay
3161,2,1406,59069b76-d1f9-4e15-8e62-ef9595061afb,2010-08-08 04:15:18.0,352.0,"This is unlikely to be a popular quote, but anyway,\\n\\n> If your experiment needs statistics, you ought to have done a better experiment.\\n\\nErnest Rutherford",,
3162,16,1406,59069b76-d1f9-4e15-8e62-ef9595061afb,2010-08-08 04:15:18.0,-1.0,,,
3163,2,1407,14fb4943-4766-4417-8ec6-40373f2ea3d4,2010-08-08 04:50:18.0,189.0,"After a day of searching, I found out that the issue was due to starting values. Thought I should just post the answer for future reference. \\n\\nThe frontier command in Stata obtains its starting values using method of moments. The initial values might have produced negative infinity for the log likelihood. To get around the problem I needed to specify the starting values myself, which were obtained from a linear regression. ",,
3164,2,1408,42277c4a-c019-465c-93b1-b7fc03020a31,2010-08-08 05:55:39.0,61.0,"Discrete wavelet basis functions (as distinct from the CWT described in Robin's answer) have two nice properties that make them useful for anomaly detection:\\n\\n1.  They're compactly supported.\\n2.  They act as band-pass filters with the pass-band determined by their support.\\n\\nWhat this means in practical terms is that your discrete wavelet decomposition looks at local changes in the signal across a variety of scales and frequency bands.  If you have (for instance) large-magnitude, high-frequency noise superimposed across a function that displays a low-magnitude shift over a longer period, the wavelet transform will efficiently separate these two scales and let you see the baseline shift that many other techniques will miss; a shift in this baseline can suggest a disease outbreak or some other change of interest.  In a lot of ways, you can treat the decomposition itself as a smoother (and there's been quite a bit of work done on efficient shrinkage for wavelet coefficients in nonparametric estimation, see e.g. pretty much anything on wavelets by Donoho).  Unlike pure frequency-based methods, the compact support means that they're capable of handling non-stationary data.  Unlike purely time-based methods, they allow for some frequency-based filtering.\\n\\nIn practical terms, to detect anomalies or change points, you would apply a discrete wavelet transform (probably the variant known either as the ""Maximum Overlap DWT"" or ""shift invariant DWT"", depending on who you read) to the data, and look at the lower-frequency sets of coefficients to see if you have significant shifts in the baseline.  This will show you when a long-term change is occurring underneath any day-to-day noise.  Percival and Walden (see references below) derive a few tests for statistically significant coefficients that you could use to see if a shift like this is significant or not.\\n\\nAn excellent reference work for discrete wavelets is Percival and Walden, ""Wavelet Methods for Time Series Analysis"".  A good introductory work is ""Introduction to wavelets and wavelet transforms, a primer"" by Burrus, Gopinath, and Guo.  If you're coming from an engineering background, then ""Elements of wavelets for engineers and scientists"" is a good introduction from a signal-processing point of view.",,
3166,2,1409,eac6ef60-130b-423a-a7e8-05abd857f23b,2010-08-08 11:30:18.0,183.0,The `rmeta`  package in R can produce forest and funnel plots.\\n\\nhttp://cran.r-project.org/web/packages/rmeta/index.html\\n,,
3167,5,1384,85092135-3121-4d8f-bc84-ed8d4c3560b3,2010-08-08 11:31:31.0,183.0,"1. I agree with the point that statistics consultants are often brought in later on a project when it's too late to remedy design flaws. It's also true that many statistics books give scant attention to study design issues.\\n\\n2. You say you want designs ""preferably for a wide range of methods (e.g. t-test, GLM, GAM, ordination techniques..."". I see designs as relatively independent of statistical method: e.g., experiments (between subjects and within subjects factors) versus observational studies; longitudinal versus cross-sectional; etc. There are also a lot of issues related to measurement, domain specific theoretical knowledge, and domain specific study design principles that need to be understood in order to design a good study.\\n\\n3. In terms of books, I'd be inclined to look at domain specific books. In psychology (where I'm from) this means books on psychometrics for measurement, a book on research methods, and a book on statistics, as well as a range of even more domain specific research method books. You might want to check out [Research Methods Knowledge Base][1] for a free online resource for the social sciences.\\n\\n4. Published journal articles are also a good guide to what is best practice in a particular domain. \\n\\n  [1]: http://www.socialresearchmethods.net/kb/index.php",added 1 characters in body,
3168,5,138,760a4bea-4729-4a38-ac64-91f5bb7604f5,2010-08-08 12:46:01.0,509.0,I'm looking to learn [R][1] on the cheap. What's the best free resource/book/tutorial for learning R?\\n\\n  [1]: http://en.wikipedia.org/wiki/R_%28programming_language%29\\n,Changed to sentence casing for the title.,
3169,4,138,760a4bea-4729-4a38-ac64-91f5bb7604f5,2010-08-08 12:46:01.0,509.0,Resources for learning R ,Changed to sentence casing for the title.,
3170,5,1213,36780acf-5719-4b5e-b1d5-aa6c1f7feb54,2010-08-08 12:51:01.0,509.0,"Some useful R links (find out the link that suits you):\\n\\n* http://had.co.nz/plyr/plyr-intro-090510.pdf for data manipulation\\n\\n* http://www.stats.ox.ac.uk/~ruth/RCourse/Rcourse3.pdf\\n\\n* http://cran.r-project.org/doc/contrib/usingR.pdf for R basics\\n\\n* http://www.ats.ucla.edu/stat/r/dae/default.htm with annotated outputs in R\\n\\n* http://cran.r-project.org/doc/contrib/Rossiter-RIntro-ITC.pdf tutorial with info on plots\\n\\n* http://www.statmethods.net/stats/regression.html\\n\\n* http://www.rmetrics.org/ provides an Open Source framework for Financial Analysis.\\n\\n* http://www.econ.uiuc.edu/~econ472/e-Tutorial.html has lecture notes with R code\\n\\n* A brief guide to R and Economics http://people.su.se/~ma/R_intro/R_intro.pdf\\n\\n* http://www.stat.pitt.edu/stoffer/tsa2/index.html has a good beginner’s tutorial for Time Series\\n\\n* http://www.quantmod.com/ provides a great analysis and visualization framework for quantitative trading\\n\\n* http://www.wise.xmu.edu.cn/2007summerworkshop/download/Advanced%20Topics%20in%20Time%20Series%20Econometrics%20Using%20R1_ZongwuCAI.pdf advanced time series in R\\n\\n* Interesting time series packages in R http://robjhyndman.com/software\\n\\n* A Data Mining tool in R http://rattle.togaware.com/\\n\\n* An online e-book for Data Mining with R http://www.liaad.up.pt/~ltorgo/DataMiningWithR/\\n\\n* Advanced Statistics using R http://www.statmethods.net/advstats/index.html\\n\\n* Guide to Credit Scoring using R http://cran.r-project.org/doc/contrib/Sharma-CreditScoring.pdf\\n\\n* http://addictedtor.free.fr/graphiques/ is a graph gallery of R plots and charts with supporting code\\n\\n* A tutorial for Lattice http://osiris.sunderland.ac.uk/~cs0her/Statistics/UsingLatticeGraphicsInR.htm\\n\\n* Ggplot R graphics http://had.co.nz/ggplot2/\\n\\n* Ggplot Vs Lattice @ http://had.co.nz/ggplot/vs-lattice.html\\n\\n* Multiple tutorials for using ggplot2 and Lattice http://learnr.wordpress.com/tag/ggplot2/\\n\\n* Introduction to the Text Mining package in R http://cran.r-project.org/web/packages/tm/vignettes/tm.pdf\\n\\n* Social Network Analysis http://www.r-project.org/conferences/useR-2008/slides/Bojanowski.pdf\\n\\n* Web Scraping in R http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl\\n\\n* http://learnr.wordpress.com/2009/10/06/export-data-frames-to-multi-worksheet-excel-file/ to embed R data frames in Excel via multiple approaches.\\n\\n* http://www.statconn.com/ provides a tool to make R usable from Excel\\n\\n* Connect to MySQL from R http://erikvold.com/blog/index.cfm/2008/8/20/how-to-connect-to-mysql-with-r-in-wndows-using-rmysql\\n\\n* http://www.statmethods.net/input/importingdata.html provides info about pulling data from SAS, STATA, SPSS, etc.\\n\\n* Thematic Maps with R http://stackoverflow.com/questions/1260965/developing-geographic-thematic-maps-with-r\\n\\n* http://smartdatacollective.com/Home/22052 for geographic maps in R\\n\\n* Google Charts with R http://www.iq.harvard.edu/blog/sss/archives/2008/04/google_charts_f_1.shtml\\n\\n* Introduction to using RGoogleMaps @ http://cran.r-project.org/web/packages/RgoogleMaps/vignettes/RgoogleMaps-intro.pdf\\n\\n* http://www.stat.uni-muenchen.de/~leisch/Sweave/\\n\\n* R2HTML http://www.feferraz.net/en/P/R2HTML\\n\\n* Poor Man GUI for R http://wiener.math.csi.cuny.edu/pmg/\\n\\n* R Commander is a robust GUI for R http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/installation-notes.html\\n\\n* JGR is a Java-based GUI for R http://jgr.markushelbig.org/Screenshots.html\\n\\n* Tinn-R makes for a good R editor http://www.sciviews.org/Tinn-R/\\n\\n* An Eclipse plugin for R @ http://www.walware.de/goto/statet\\n\\n* Instructions to install StatET in Eclipse http://www.splusbook.com/Rintro/R_Eclipse_StatET.pdf\\n\\n* Komodo Edit R editor http://www.sciviews.org/SciViews-K/index.html\\n\\n* http://www.omegahat.org/ has a very interesting list of packages that is seriously worth a look\\n\\n* Commercial versions of R @ http://www.revolution-computing.com/\\n\\n* A very informative blog http://blog.revolution-computing.com/\\n\\n* Red R for R tasks http://code.google.com/p/r-orange/\\n\\n* KNIME for R http://www.knime.org/introduction/screenshots and is worth a serious look.\\n\\n",Better formatting of the links.,
3171,2,1410,692187de-1c61-41cf-bc53-4f8f8729d82d,2010-08-08 14:28:10.0,183.0,"I suppose you could do a multidimensional scaling of the correlation or covariance matrix. It's not exactly structural equation modelling, but it might highlight patterns and structure in the correlation or covariance matrix. This could then be formalised with an appropriate model.",,
3172,2,1411,0a3e5a67-871d-4c4e-b716-a7fe15394271,2010-08-08 14:33:24.0,183.0,"[ggobi][1] ""is an open source visualization program for exploring high-dimensional data.""\\n\\nMat Kelcey has a good [5 minute intro to ggobi][2].\\n\\n\\n  [1]: http://www.ggobi.org/\\n  [2]: http://matpalm.com/blog/2010/06/04/5-minute-ggobi-demo/",,
3173,16,1411,0a3e5a67-871d-4c4e-b716-a7fe15394271,2010-08-08 14:33:24.0,-1.0,,,
3174,2,1412,90655b29-924a-46cf-ae96-6dfc013f7de3,2010-08-08 18:10:40.0,196.0,"Background:  In some cognitive psychology research areas N-alternative forced choice tasks are common.  The most common of these is a two alternative forced choice (2AFC).  This usually takes the form of participants being given a stimulus and asked to make one of two judgement, e.g. the target stimuli is present/absent, the stimulus on the left is the same/different than the the one on the right, etc.  Designs in which the experimental data is from a 2AFC but there is only one data point per subject are rare, but do exist, e.g. some eye-witness identification research.  Since the dependent variable (correct/incorrect) is binary, these experiments are reasonable places to use logistic regression.  \\n\\nMy question is this:  since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?  Specifically, the logistic function has a minimum value approaching 0% correct, but in practice participants in a 2AFC should be correct 50% of the time due to chance.  I imagine the following case in which it may present a problem, but I'm not sure:  an independent variable is assessing the difficulty of the discrimination (e.g. difficulty 1 (easy) - 5 (hard)), participants got 50% correct at 5 and 4, 75% correct at 3, 85% correct at 2, and 99% correct at 1.  Would using a standard logistic link function cause us to underestimate the slope?  What other consequences (if any) are there of using the generic logistic function with 2AFC data [presumably extensible to NAFC cases]?",,
3175,1,1412,90655b29-924a-46cf-ae96-6dfc013f7de3,2010-08-08 18:10:40.0,196.0,Consequences of an improper link function in N alternative forced choice procedures (e.g. 2AFC)?,,
3176,3,1412,90655b29-924a-46cf-ae96-6dfc013f7de3,2010-08-08 18:10:40.0,196.0,<logistic><link-function>,,
3177,2,1413,e9642577-eb60-43a6-90a1-044b7bb53bc7,2010-08-08 18:14:35.0,196.0,"It seems like the current revision of lmer does not allow for custom link functions.  \\n\\n 1. If one needs to fit a logistic\\n    linear mixed effect model with a\\n    custom link function what options\\n    are available in R?\\n\\n 2. If none - what options are available in other\\n    statistics/programming packages?\\n\\n 3. Are there conceptual reasons lmer\\n    does not have custom link functions,\\n    or are the constraints purely\\n    pragmatic/programmatic?",,
3178,1,1413,e9642577-eb60-43a6-90a1-044b7bb53bc7,2010-08-08 18:14:35.0,196.0,Mixed regression models and custom link functions in R?,,
3179,3,1413,e9642577-eb60-43a6-90a1-044b7bb53bc7,2010-08-08 18:14:35.0,196.0,<r><regression><mixed-model><link-function>,,
3180,2,1414,237f8bc4-3bbf-4163-9214-a07c930d7bf0,2010-08-08 18:27:45.0,603.0,"*My question is this: since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?*\\n\\nyes.\\n\\nThink of it this way: suppose you fit a logistic regression where your $y$ variable takes value 1 if subject i has flue, 0 otherwise.\\n\\nSo long as neither $y_i=1$ nor $y_i=0$ are rare events, then flue incidence (i.e. $n^{-1}\\sum_{i=1}^ny_i$) is not relevant, it will be absorbed by the intercept of your model.\\n\\nBest,",,
3181,5,1414,cdd8bcc2-a49f-46cd-b068-13e1a8321188,2010-08-08 18:35:16.0,603.0,"*My question is this: since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?*\\n\\nyes.\\n\\nThink of it this way: suppose you fit a logistic regression where your $y$ variable takes value 1 if subject i has flue, 0 otherwise.\\n\\nSo long as neither $y_i=1$ nor $y_i=0$ are rare events, then flue incidence (i.e. $n^{-1}\\sum_{i=1}^ny_i$) is not relevant, it will be absorbed by the intercept of your model.\\n\\n\\n*but in practice participants in a 2AFC should be correct 50% of the time due to chance*\\n\\nif this statement is true and all your exogenous variables have been de-meaned, then, \\nyou can expect your estimated constant to be $logit^{-1}(0.5)\\approx0.05$\\n\\nBest,",added 259 characters in body,
3182,6,942,6f2e3899-b0bd-4949-827a-7f91a567697b,2010-08-08 18:40:26.0,223.0,<statistical-analysis><time-series><outliers>,edited tags,
3183,2,1415,7953abe4-ad86-4fee-8849-383b92d1d001,2010-08-08 19:02:55.0,223.0,Prostate (gene expression array)\\n\\nk=2\\nn=48+52 \\np=6033\\n\\nAvailable via (among other) R package spls http://cran.r-project.org/web/packages/spls/\\nname of the dataset: prostate\\n\\n\\nerror rate =  3/102 (from http://www.stat.wisc.edu/%7Ekeles/Papers/C_SPLS.pdf)\\n,,
3184,16,1415,7953abe4-ad86-4fee-8849-383b92d1d001,2010-08-08 19:02:55.0,-1.0,,,
3185,5,1408,3c4aab51-e5a5-4e8f-afe7-c28bb1f95792,2010-08-08 19:22:55.0,61.0,"Most commonly used and implemented discrete wavelet basis functions (as distinct from the CWT described in Robin's answer) have two nice properties that make them useful for anomaly detection:\\n\\n1.  They're compactly supported.\\n2.  They act as band-pass filters with the pass-band determined by their support.\\n\\nWhat this means in practical terms is that your discrete wavelet decomposition looks at local changes in the signal across a variety of scales and frequency bands.  If you have (for instance) large-magnitude, high-frequency noise superimposed across a function that displays a low-magnitude shift over a longer period, the wavelet transform will efficiently separate these two scales and let you see the baseline shift that many other techniques will miss; a shift in this baseline can suggest a disease outbreak or some other change of interest.  In a lot of ways, you can treat the decomposition itself as a smoother (and there's been quite a bit of work done on efficient shrinkage for wavelet coefficients in nonparametric estimation, see e.g. pretty much anything on wavelets by Donoho).  Unlike pure frequency-based methods, the compact support means that they're capable of handling non-stationary data.  Unlike purely time-based methods, they allow for some frequency-based filtering.\\n\\nIn practical terms, to detect anomalies or change points, you would apply a discrete wavelet transform (probably the variant known either as the ""Maximum Overlap DWT"" or ""shift invariant DWT"", depending on who you read) to the data, and look at the lower-frequency sets of coefficients to see if you have significant shifts in the baseline.  This will show you when a long-term change is occurring underneath any day-to-day noise.  Percival and Walden (see references below) derive a few tests for statistically significant coefficients that you could use to see if a shift like this is significant or not.\\n\\nAn excellent reference work for discrete wavelets is Percival and Walden, ""Wavelet Methods for Time Series Analysis"".  A good introductory work is ""Introduction to wavelets and wavelet transforms, a primer"" by Burrus, Gopinath, and Guo.  If you're coming from an engineering background, then ""Elements of wavelets for engineers and scientists"" is a good introduction from a signal-processing point of view.\\n\\n(Edited to include Robin's comments)",Changes per Robin's comments.,
3186,2,1416,78532c67-8993-4016-af2a-6440ad71300e,2010-08-08 19:26:40.0,251.0,"Douglas Bates addressed this on the sig-ME list a while back:\\n\\n- [using glmer with user-defined link function][1]\\n\\nI'm not aware of significant changes since, but his recommendation (using a quasi family with specified link and variance) might be of use.  Hopefully this addresses your first and third questions.  I'm not aware of other packages - sorry.\\n\\n  [1]: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q4/001507.html\\n\\n",,
3187,2,1417,17b4dc37-91c5-48c3-86e5-d90f7b13f9b1,2010-08-08 19:59:34.0,223.0,"This site from Ecole normal Supérieure de Paris contains a lot of very interesting video \\n\\nhttp://www.diffusion.ens.fr/index.php?res=themes&idtheme=30\\n\\nI greatly encourage you to visit this site !! \\n\\nAmong other you will find there all video presentation from the conference ""Mathematical Foundations of Learning Theory"" that held in 2006. \\n",,
3188,16,1417,17b4dc37-91c5-48c3-86e5-d90f7b13f9b1,2010-08-08 19:59:34.0,-1.0,,,
3189,2,1418,7f98e816-acc8-41c0-b544-ac56a8d23b68,2010-08-08 19:59:40.0,251.0,"The following paper describes a couple of approaches for imputing right censored data in the same domain (i.e. topcoded wage data).  They use a truncated normal distribution and describe a single imputation model assuming homoscedasticity, and a multiple imputation model assuming heterscedasticity.  Also a second paper of interest, where a generalized beta distribution is assumed, might be closer to what you want.\\n\\n\\n- [Multiple Imputation Approaches for Right-Censored Wages in the German IAB-Employment Register][1]\\n- [Measuring Inequality Using Censored Data: A Multiple Imputation Approach][2]\\n\\n  [1]: http://ideas.repec.org/p/iab/iabdpa/200844.html\\n  [2]: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1431352\\n\\n",,
3190,2,1419,955d7ad6-9627-49fa-a517-74a939d163f4,2010-08-08 20:22:58.0,251.0,"The folks at SLAC put videos of their lecture series online.  Given that their audience is mostly physicists, they tend to be fairly mathematical.  \\n\\n- [SLUO Lecture Series][1] (see the ""Stat"" links)\\n\\n  [1]: http://www-group.slac.stanford.edu/sluo/lectures/lectureseries.html\\n  \\n",,
3191,16,1419,955d7ad6-9627-49fa-a517-74a939d163f4,2010-08-08 20:22:58.0,-1.0,,,
3192,2,1420,3e837bfd-4f65-4d61-a8c3-4b395d9d3186,2010-08-08 20:38:09.0,251.0,"I think relative distribution methods are a good candidate for the question you pose.  \\n\\nSince you're comparing data based on binning, this is very similar to the method of constructing a probability-probability plot.  Taking it a step further, you can can actually construct a relative CDF/PDF for two distributions based on empirical data.  From there, you can apply graphical and statistical techniques to explore the relative differences and perform inference on the relative distribution.  \\n\\nHandcock and Morris have an [interesting book][1] devoted to this topic and there's the [reldist package][2] in R available for applying these methods.  The following might be worth skimming to see if this is of any interest:\\n\\n- [Relative Distribution Methods][3]\\n- [Applying Relative Distribution Methods in R][4]\\n\\n  [1]: http://csde.washington.edu/~handcock/RelDist/\\n  [2]: http://cran.r-project.org/web/packages/reldist/index.html\\n  [3]: http://www.jstor.org/pss/270964\\n  [4]: http://www.csss.washington.edu/Papers/wp27.pdf\\n\\n\\n\\n",,
3193,5,1412,39e06a8c-75fb-4727-8e28-6bcfa344dc6e,2010-08-08 21:34:24.0,196.0,"Background:  In some cognitive psychology research areas N-alternative forced choice tasks are common.  The most common of these is a two alternative forced choice (2AFC).  This usually takes the form of participants being given a stimulus and asked to make one of two judgement, e.g. the target stimuli is present/absent, the stimulus on the left is the same/different than the the one on the right, etc.  Designs in which the experimental data is from a 2AFC but there is only one data point per subject are rare, but do exist, e.g. some eye-witness identification research.  Since the dependent variable (correct/incorrect) is binary, these experiments are reasonable places to use logistic regression.  \\n\\nMy question is this:  since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?  Specifically, the logistic function has a minimum value approaching 0% correct, but in practice participants in a 2AFC should be correct 50% of the time due to chance.  I imagine the following case in which it may present a problem, but I'm not sure:  an independent variable is assessing the difficulty of the discrimination (e.g. difficulty 1 (easy) - 5 (hard); please note this is introduces in ordinal terms only for ease of comprehension - this is legitimately an interval scale variable), participants got 50% correct at 5 and 4, 75% correct at 3, 85% correct at 2, and 99% correct at 1.  Would using a standard logistic link function cause us to underestimate the slope?  What other consequences (if any) are there of using the generic logistic function with 2AFC data [presumably extensible to NAFC cases]?",added 130 characters in body,
3194,2,1421,1fe0775b-5c4a-481b-b352-3244834cfa11,2010-08-08 21:57:57.0,601.0,"I don't see how the question in your example is sensible.  The slope of the values is the slope of the values. Using a logistic link function then you get the slope of the logit of the values.  There's no under or overestimating.  \\n\\nThe more interesting case in your (our) field is that of interactions in accuracy.  You might want to read [Dixon (2008)][1] as one of the more recent papers on this problem.  It also addresses many of your fundamental concerns.\\n\\nIn general, in cognitive and perceptual psychology a logit link function is better than any other standard link.  If you want to know the true effects of your independent variables, (i.e. whether they interact or are additive, whether they are linear or curvilinear) then you would need to know better the true underlying model.  Since you probably don't know that logistic regression is probably better than almost anything else and vastly better than just analyzing meaned accuracy scores.\\n\\nThe primary consequence of doing this is contradicting other findings where mean accuracy scores were put into an ANOVA or regression.\\n  [1]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6WK4-4RH8SFS-1&_user=10&_coverDate=11%2F30%2F2008&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1424726112&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=47db7bad266e5fff07e6fc8c0ceeac42",,
3199,2,1423,7987e332-280e-4ae6-bae0-5bcb4a7a0066,2010-08-08 23:05:17.0,749.0,"> Prediction is very difficult, especially about the future.\\n\\n-- Niels Bohr\\n",,
3200,16,1423,7987e332-280e-4ae6-bae0-5bcb4a7a0066,2010-08-08 23:05:17.0,-1.0,,,
3201,5,1412,82a98829-8731-49d0-9f02-e0a5bae812b6,2010-08-08 23:17:06.0,196.0,"Background:  In some cognitive psychology research areas N-alternative forced choice tasks are common.  The most common of these is a two alternative forced choice (2AFC).  This usually takes the form of participants being given a stimulus and asked to make one of two judgement, e.g. the target stimuli is present/absent, the stimulus on the left is the same/different than the the one on the right, etc.  Designs in which the experimental data is from a 2AFC but there is only one data point per subject are rare, but do exist, e.g. some eye-witness identification research.  Since the dependent variable (correct/incorrect) is binary, these experiments are reasonable places to use logistic regression.  \\n\\nMy question is this:  since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?  Specifically, the logistic function has a minimum value approaching 0% correct, but in practice participants in a 2AFC should be correct 50% of the time due to chance.  I imagine the following case in which it may present a problem:  an independent variable is assessing the difficulty of the discrimination (e.g. difficulty 1 (easy) - 5 (hard); please note this is introduced in ordinal terms only for ease of comprehension - this is legitimately an interval scale variable), participants got 50% correct at 5 and 4, 75% correct at 3, 85% correct at 2, and 99% correct at 1.  Would using a standard logistic link function cause us to underestimate the slope?  What other consequences (if any) are there of using the generic logistic function with 2AFC data [presumably extensible to NAFC cases]?\\n\\nEdit:  Those who have answered my question so far have expressed that the way in which I set up the problem was unclear.  I'm providing the sample below to help clear things up.\\n\\n    library(psyphy)\\n    make.data <- function(zero,one)\\n    	{\\n    		return(c(rep(0,zero),rep(1,one)))\\n    	}\\n    center <- function(x) {return(scale(x,scale=FALSE))}\\n    logit.data <- data.frame(Score=c(make.data(50,50),make.data(50,50),make.data(25,75),make.data(15,85),make.data(1,99)), Difficulty=rep(5:1,each=100))\\n    logit.data$Difficulty2 <- center(logit.data$Difficulty)^2\\n    standard <- glm(Score~center(Difficulty),data=logit.data,family=binomial) #standard link function\\n    standard.2 <- glm(Score~center(Difficulty)+Difficulty2,data=logit.data,family=binomial) #standard link function, but better with a quadradic\\n    revised.link <- glm(Score~center(Difficulty),data=logit.data,family=binomial(mafc.logit(2)))\\n    AIC(base)\\n    AIC(base.2)\\n    AIC(revised.link)\\n    coef(base)\\n    coef(base.2)\\n    coef(revised.link)\\n\\n![alt text][1]\\n\\nIn the above image the orange horizontal line marks 50% correct responses.  The jagged black line represents the data supplied to the estimation equation.  The blue line is the equation produced by a standard logistic link.  Note that it estimates below 50% accuracy when discrimination is most difficult (5).  The cyan line is the standard logistic link with a quadratic term.  The green line is a non-standard link that takes into account that the data comes from a 2AFC experiment where performance is very unlikely to fall below 50%.\\nNote that the AIC for a model fit using a non-standard link function is superior to the standard logistic link function.  Also note that the slope for the standard equation is less than the slope for the standard equation with the quadratic term (which more accurately reflects the real data).  Thus, using a logistic function blindly on 2AFC data does appear to underestimate the slope.\\n\\nIs there a problem with my demonstration above that means that I am not seeing what I think I am seeing?  If I'm correct, then what other consequences (if any) are there of using the generic logistic function with 2AFC data [presumably extensible to NAFC cases]?\\n\\n  [1]: http://psychlab2.ucr.edu/~russell/nAFCLogit.jpg",Added graph and vignette to make my point,
3202,5,1412,d7921cc4-4841-40d6-a1d0-e4ce8930f015,2010-08-08 23:24:20.0,196.0,"Background:  In some cognitive psychology research areas N-alternative forced choice tasks are common.  The most common of these is a two alternative forced choice (2AFC).  This usually takes the form of participants being given a stimulus and asked to make one of two judgement, e.g. the target stimuli is present/absent, the stimulus on the left is the same/different than the the one on the right, etc.  Designs in which the experimental data is from a 2AFC but there is only one data point per subject are rare, but do exist, e.g. some eye-witness identification research.  Since the dependent variable (correct/incorrect) is binary, these experiments are reasonable places to use logistic regression.  \\n\\nMy question is this:  since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?  Specifically, the logistic function has a minimum value approaching 0% correct, but in practice participants in a 2AFC should be correct at least 50% of the time due to chance.  I imagine the following case in which it may present a problem:  an independent variable is assessing the difficulty of the discrimination (e.g. difficulty 1, easy - 5, hard; please note this is introduced in ordinal terms only for ease of comprehension - for the sake of this discussion consider this variable as being interval) - participants got 50% correct at 5 and 4, 75% correct at 3, 85% correct at 2, and 99% correct at 1.  Would using a standard logistic link function cause us to underestimate the slope? [I think so, but please correct me if I'm wrong, see below]\\n\\nEdit:  Those who have answered my question so far have expressed that the way in which I set up the problem was unclear.  I'm providing the sample below to help clear things up.\\n\\n    library(psyphy)\\n    make.data <- function(zero,one)\\n    	{\\n    		return(c(rep(0,zero),rep(1,one)))\\n    	}\\n    center <- function(x) {return(scale(x,scale=FALSE))}\\n    logit.data <- data.frame(Score=c(make.data(50,50),make.data(50,50),make.data(25,75),make.data(15,85),make.data(1,99)), Difficulty=rep(5:1,each=100))\\n    logit.data$Difficulty2 <- center(logit.data$Difficulty)^2\\n    standard <- glm(Score~center(Difficulty),data=logit.data,family=binomial) #standard link function\\n    standard.2 <- glm(Score~center(Difficulty)+Difficulty2,data=logit.data,family=binomial) #standard link function, but better with a quadradic\\n    revised.link <- glm(Score~center(Difficulty),data=logit.data,family=binomial(mafc.logit(2)))\\n    AIC(base)\\n    AIC(base.2)\\n    AIC(revised.link)\\n    coef(base)\\n    coef(base.2)\\n    coef(revised.link)\\n\\n![alt text][1]\\n\\nIn the above image the orange horizontal line marks 50% correct responses.  The jagged black line represents the data supplied to the estimation equation (note the values for 4 and 5 disappear behind the orange 50% marker).  The blue line is the equation produced by a standard logistic link.  Note that it estimates below 50% accuracy when discrimination is most difficult (5).  The cyan line is the standard logistic link with a quadratic term.  The green line is a non-standard link that takes into account that the data comes from a 2AFC experiment where performance is very unlikely to fall below 50%.\\nNote that the AIC for a model fit using a non-standard link function is superior to the standard logistic link function.  Also note that the slope for the standard equation is less than the slope for the standard equation with the quadratic term (which more accurately reflects the real data).  Thus, using a logistic function blindly on 2AFC data does (at least) appear to underestimate the slope.\\n\\nIs there a problem with my demonstration that means that I am not seeing what I think I am seeing?  If I'm correct, then what other consequences (if any) are there of using the generic logistic function with 2AFC data [presumably extensible to NAFC cases]?\\n\\n  [1]: http://psychlab2.ucr.edu/~russell/nAFCLogit.jpg",added 30 characters in body,
3204,2,1424,5ca10f29-fe45-4443-8831-fbf5eee1556d,2010-08-09 00:00:13.0,1356.0,"This definitely sounds like a homework, but I assure you that it's not. You're probably familiar with the [Risk][1] game. Now, friend of mine rolled 3 aces in one hand. I reckon that probability of such event is C(n,k) = (n-1+k)!/[(n-1)!k!], so that's 8!/(5!*3!) = 56, so the prob. is 1/56. Am I correct?\\n\\nProblem starts here: he rolled 3 aces in his 2nd attack, so he asked me: now, tell me 'bout the odds, you do statistics! And I must admit that I'm still stunned by his tremendous attacks (he lost 3 tanks both times).\\n\\nIs the probability of such two consecutive events 1/56<sup>2</sup>?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Risk_%28game%29",,
3205,1,1424,5ca10f29-fe45-4443-8831-fbf5eee1556d,2010-08-09 00:00:13.0,1356.0,"The ""Risk"" game dice problem",,
3206,3,1424,5ca10f29-fe45-4443-8831-fbf5eee1556d,2010-08-09 00:00:13.0,1356.0,<probability><combination><games>,,
3207,2,1425,0c766fa2-5ab5-4a77-8d49-3b6e015d1eb9,2010-08-09 00:37:45.0,805.0,"You should not do a calculation of probability for an event deemed surprising *post hoc* as if it were an event specified before it was rolled (observed).\\n\\nIt's very difficult to to do a proper calculation of *post hoc* probability, because what *other* events would have been deemed at least as surprising depends on what the context is. \\n\\nWould three ones twice in a row at an earlier or later stage of the game have been as surprising? Would *you* rolling three ones have been as surprising as him rolling them? Would three sixes be as surprising as three ones? and so on... What is the totality of all the events would have been surprising enough to generate a post like this one?\\n\\n(Even if it were legitimate to do the calculation *as if it were a pre-specified event*, it looks like you have that calculation incorrect.)\\n\\n\\n\\n",,
3208,2,1426,6455fe7f-651e-41b0-8199-4662d3d15b4d,2010-08-09 01:44:41.0,319.0,"""It's easy to lie with statistics; it is easier to lie without them."" -- Frederick Mosteller",,
3209,16,1426,6455fe7f-651e-41b0-8199-4662d3d15b4d,2010-08-09 01:44:41.0,-1.0,,,
3210,5,1425,2024ea96-d4ef-4ec6-82c4-14b1957cd288,2010-08-09 01:56:08.0,805.0,"You should not do a calculation of probability for an event deemed surprising *post hoc* as if it were an event specified before it was rolled (observed).\\n\\nIt's very difficult to to do a proper calculation of *post hoc* probability, because what *other* events would have been deemed at least as surprising depends on what the context is. \\n\\nWould three ones twice in a row at an earlier or later stage of the game have been as surprising? Would *you* rolling three ones have been as surprising as him rolling them? Would three sixes be as surprising as three ones? and so on... What is the totality of all the events would have been surprising enough to generate a post like this one?\\n\\n(Even if it were legitimate to do the calculation *as if it were a pre-specified event*, it looks like you have that calculation incorrect. Specifically, the probability (*for an event specified before the roll*) of taking three dice and rolling (1,1,1) is 1/216, not 1/56, and the probability of doing it twice *out of a total of two rolls* is the square of that - but neither the condition of being pre-specified nor the ""out of two rolls"" hold)\\n\\n\\n\\n",(added minor detail of probability calculation),
3211,5,1421,6492d1fa-5669-4603-a013-7124463aea4e,2010-08-09 02:18:52.0,601.0,"I don't see how the question in your example is sensible.  The slope of the values is the slope of the values. Using a logistic link function then you get the slope of the logit of the values.  There's no under or overestimating.  \\n\\nThe more interesting case in your (our) field is that of interactions in accuracy.  You might want to read [Dixon (2008)][1] as one of the more recent papers on this problem.  It also addresses many of your fundamental concerns.\\n\\nIn general, in cognitive and perceptual psychology a logit link function is better than any other standard link.  If you want to know the true effects of your independent variables, (i.e. whether they interact or are additive, whether they are linear or curvilinear) then you would need to know better the true underlying model.  Since you probably don't know that logistic regression is probably better than almost anything else and vastly better than just analyzing meaned accuracy scores.\\n\\nThe primary consequence of doing this is contradicting other findings where mean accuracy scores were put into an ANOVA or regression.\\n\\n*** EDIT***\\n\\nNow that you've added some data it looks like you're trying to model a floor effect that you shouldn't be.  At some point the task becasue impossible.  It looks like that already happened at your level 4 difficulty.  Modelling level 5 is useless.  What if you had a level 6 or 7 difficulty?\\n\\nIt looks like a logistic will fit points 1-4 pretty well.\\n\\nAnd, you should be looking at residuals to assess fit, not just the curves overlaid.\\n\\n  [1]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6WK4-4RH8SFS-1&_user=10&_coverDate=11%2F30%2F2008&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1424726112&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=47db7bad266e5fff07e6fc8c0ceeac42",added 447 characters in body; added 15 characters in body; deleted 2 characters in body,
3212,5,1425,ff9e9a7b-9c26-48c7-88ff-678a57bc4379,2010-08-09 03:11:02.0,805.0,"You should not do a calculation of probability for an event deemed surprising *post hoc* as if it were an event specified before it was rolled (observed).\\n\\nIt's very difficult to to do a proper calculation of *post hoc* probability, because what *other* events would have been deemed at least as surprising depends on what the context is. \\n\\nWould three ones twice in a row at an earlier or later stage of the game have been as surprising? Would *you* rolling three ones have been as surprising as him rolling them? Would three sixes be as surprising as three ones? and so on... What is the totality of all the events would have been surprising enough to generate a post like this one?\\n\\n(Even if it were legitimate to do the calculation *as if it were a pre-specified event*, it looks like you have that calculation incorrect. Specifically, the probability (*for an event specified before the roll*) of taking three dice and rolling (1,1,1) is 1/216 (1/6)^3, because the three rolls are independent, not 1/56, and the probability of doing it twice *out of a total of two rolls* is the square of that - but neither the condition of being pre-specified nor the ""out of two rolls"" hold)\\n\\n\\n\\n",added 49 characters in body,
3213,2,1427,2960f6ac-aa9f-4657-87f4-6c557c7edba2,2010-08-09 03:57:50.0,394.0,"I'm exploring the use of changepoint detection or other methods (am slowly becoming aware of wavelet transformation, etc. but have tons to learn in this area) to identify key shifts in health care performance patterns over time. However, many of the metrics I'm seeking to analyze (e.g., health care quality metrics) are both generally calculated and more reasonably interpreted as rolling-12 month aggregates. For example, it's important to me to track on a monthly basis the proportion of patients who are up-to-date on a certain lab test within the 12-month period ending that month, but I'm not particularly concerned with how many of these tests occurred specifically in August. So it's something sort of like having a moving average to work with as a raw starting point.\\n\\nThat said, there are also reasons why this rolling-12 aggregation does not result in a stationary process either.\\n\\nMy thought was to account for the data structure and seasonality by modeling it as a function of a 1-month and 12-month lag. Is this the proper way to think about this data? Is there anything else or a better approach I should be doing/considering? Again, my general goal is surveillance of the general trend as well as breaks -- so if it affects the answer, I'm looking at this in the context of using the R strucchange package, CUSUM statistics, or some other approach to identify good and bad anomalies.\\n\\nThanks,\\n\\nShelby",,
3214,1,1427,2960f6ac-aa9f-4657-87f4-6c557c7edba2,2010-08-09 03:57:50.0,394.0,Time-series data pre-aggregated into non-stationary rolling 12-month periods: are there special considerations for modeling?,,
3215,3,1427,2960f6ac-aa9f-4657-87f4-6c557c7edba2,2010-08-09 03:57:50.0,394.0,<modelling><time-series>,,
3216,2,1428,230b4113-4a8f-4668-8247-f9dd9135a4f1,2010-08-09 04:05:27.0,159.0,"The 12-month rolling aggregation will remove seasonality which makes the task easier. For non-seasonal time series, the methods in the [strucchange][1] package for R are excellent.\\n\\nFor seasonal time series, you might look at the BFAST method which is implemented in the [bfast][2] package for R. This method involves applying strucchange to the trend and seasonal components obtained from a decomposition of the data (applied iteratively to allow for the breaks discovered). You could apply bfast on the original data (without the 12-month aggregation).\\n\\nNeither of these methods requires stationarity.\\n\\nI would think that a direct modelling approach such as the one you propose would be less capable of finding general breaks due to the additional assumptions being made.\\n\\n\\n  [1]: http://cran.r-project.org/package=strucchange\\n  [2]: http://cran.r-project.org/package=bfast",,
3217,5,1428,e89502fe-3f55-407b-b5d8-b1d609d7a977,2010-08-09 04:13:46.0,159.0,"The 12-month rolling aggregation will remove seasonality which makes the task easier. For non-seasonal time series, the methods in the [strucchange][1] package for R are excellent.\\n\\nFor seasonal time series, you might look at the BFAST (Breaks For Additive Seasonal and Trend) method which is implemented in the [bfast][2] package for R. This method involves applying strucchange to the trend and seasonal components obtained from a decomposition of the data (applied iteratively to allow for the breaks discovered). You could apply bfast on the original data (without the 12-month aggregation).\\n\\nNeither of these methods requires stationarity.\\n\\nI would think that a direct modelling approach such as the one you propose would be less capable of finding general breaks due to the additional assumptions being made.\\n\\n\\n  [1]: http://cran.r-project.org/package=strucchange\\n  [2]: http://cran.r-project.org/package=bfast",added 41 characters in body,
3218,2,1429,0485c79f-d595-4ce1-820d-06309b1659ba,2010-08-09 04:22:42.0,795.0,"I'm somewhat confused by your example code, as it seems you drop the `V` variable from the computation of `newX`. Are you looking to model `X` as a reduced rank product, or are you interested in a reduced column space of `X`? in the latter case, I think an EM-PCA approach would work. you can find matlab code under the title [Probabilistic PCA with missing values][1]. \\n\\nhth,\\n\\n\\n\\n  [1]: http://lear.inrialpes.fr/~verbeek/software.php",,
3219,2,1430,f70b0d32-22c2-48e0-8c74-0c30d99dfe4b,2010-08-09 06:50:42.0,352.0,"Does anybody have a nice example of a stochastic process that is 2nd-order stationary, but is not strictly stationary?",,
3220,1,1430,f70b0d32-22c2-48e0-8c74-0c30d99dfe4b,2010-08-09 06:50:42.0,352.0,"Example of a 2nd order stationary, but not strictly stationary process",,
3221,3,1430,f70b0d32-22c2-48e0-8c74-0c30d99dfe4b,2010-08-09 06:50:42.0,352.0,<random-variable>,,
3222,6,1430,81385368-96ad-438e-bed7-4b60149d24e6,2010-08-09 06:57:00.0,223.0,<random-variable><stochastic><processes>,edited tags,
3223,6,1430,171da602-480a-4ca6-a8cb-a7df109a8275,2010-08-09 06:57:37.0,223.0,<random-variable><stochastic-processes>,edited tags,
3224,2,1431,7ad7fe86-24c9-489a-8f1e-2fba66b1b79f,2010-08-09 07:05:20.0,223.0,Take any process that has a constant first and second moment and put a varying third moment. ,,
3225,5,1412,2f967973-d020-4850-9979-de5250be44ad,2010-08-09 07:10:24.0,196.0,"Background:  In some cognitive psychology research areas N-alternative forced choice tasks are common.  The most common of these is a two alternative forced choice (2AFC).  This usually takes the form of participants being given a stimulus and asked to make one of two judgement, e.g. the target stimuli is present/absent, the stimulus on the left is the same/different than the the one on the right, etc.  Designs in which the experimental data is from a 2AFC but there is only one data point per subject are rare, but do exist, e.g. some eye-witness identification research.  Since the dependent variable (correct/incorrect) is binary, these experiments are reasonable places to use logistic regression.  \\n\\nMy question is this:  since chance performance is 50% in a 2AFC trial, is it still reasonable to use the standard logistic link function?  Specifically, the logistic function has a minimum value approaching 0% correct, but in practice participants in a 2AFC should be correct at least 50% of the time due to chance.  I imagine the following case in which it may present a problem:  an independent variable is assessing the difficulty of the discrimination (e.g. difficulty 1, easy - 5, hard; please note this is introduced in ordinal terms only for ease of comprehension - for the sake of this discussion consider this variable as being interval) - participants got 50% correct at 5 and 4, 75% correct at 3, 85% correct at 2, and 99% correct at 1.  Would using a standard logistic link function cause us to underestimate the slope? [I think so, but please correct me if I'm wrong, see below]\\n\\nEdit:  Those who have answered my question so far have expressed that the way in which I set up the problem was unclear.  I'm providing the sample below to help clear things up.\\n\\n    library(psyphy)\\n    make.data <- function(zero,one)\\n    	{\\n    		return(c(rep(0,zero),rep(1,one)))\\n    	}\\n    center <- function(x) {return(scale(x,scale=FALSE))}\\n    logit.data <- data.frame(Score=c(make.data(50,50),make.data(50,50),make.data(25,75),make.data(15,85),make.data(1,99)), Difficulty=rep(5:1,each=100))\\n    logit.data$Difficulty2 <- center(logit.data$Difficulty)^2\\n    standard <- glm(Score~center(Difficulty),data=logit.data,family=binomial) #standard link function\\n    standard.2 <- glm(Score~center(Difficulty)+Difficulty2,data=logit.data,family=binomial) #standard link function, but better with a quadradic\\n    revised.link <- glm(Score~center(Difficulty),data=logit.data,family=binomial(mafc.logit(2)))\\n    AIC(base)\\n    AIC(base.2)\\n    AIC(revised.link)\\n    coef(base)\\n    coef(base.2)\\n    coef(revised.link)\\n    #plot\\n    plot(diffs,plogis(coef(standard)[1] +coef(standard)[2]*center(diffs)),xlab=""Difficulty"",ylab=""Pr(Correct)"",ylim=c(0,1),col=""blue"",type=""l"");abline(.5,0,col=""Orange"");lines(diffs,plogis(coef(standard.2)[1]+coef(standard.2)[2]*center(diffs)+coef(standard.2)[3]*center(diffs)^2),col=""Cyan"");lines(diffs,(p2afc(coef(revised.link)[1]+coef(revised.link)[2]*center(diffs))),col=""Green"");lines(5:1,c(.55,.60,.75,.85,.99),col=""Black"")\\n\\n\\n![alt text][1]\\n\\nIn the above image the orange horizontal line marks 50% correct responses.  The jagged black line represents the data supplied to the estimation equation (note the values for 4 and 5 disappear behind the orange 50% marker).  The blue line is the equation produced by a standard logistic link.  Note that it estimates below 50% accuracy when discrimination is most difficult (5).  The cyan line is the standard logistic link with a quadratic term.  The green line is a non-standard link that takes into account that the data comes from a 2AFC experiment where performance is very unlikely to fall below 50%.\\nNote that the AIC for a model fit using a non-standard link function is superior to the standard logistic link function.  Also note that the slope for the standard equation is less than the slope for the standard equation with the quadratic term (which more accurately reflects the real data).  Thus, using a logistic function blindly on 2AFC data does (at least) appear to underestimate the slope.\\n\\nIs there a problem with my demonstration that means that I am not seeing what I think I am seeing?  If I'm correct, then what other consequences (if any) are there of using the generic logistic function with 2AFC data [presumably extensible to NAFC cases]?\\n\\n  [1]: http://psychlab2.ucr.edu/~russell/nAFCLogit.jpg",added plot code,
3226,5,1279,d11edbf1-8c94-4dcd-8d08-0799119b6104,2010-08-09 07:20:11.0,183.0,One resource I found recently was the UCCS video archive.\\nIt has a couple of archived video series for subjects called Mathematical Statistics I and  Mathematical Statistics II. It requires a free registration to access.\\n\\nhttp://www.uccs.edu/~math/vidarchive.html\\n\\nThis videos for [Math 481 Mathematical Stats I - Fall 2007][1] provide a good intro to mathematical statistics.\\n\\n\\n  [1]: http://cmes.uccs.edu/Fall2007/Math481/archive.php,added 172 characters in body; added 5 characters in body,
3227,5,812,fa9968a1-076b-4b7b-b6f0-69d00f0a8f21,2010-08-09 07:26:23.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions (since in applied mathematic everything comes to be a finite at some point) (can be polynomes, splines, wavelet, Fourier, ....). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT/UPDATE with the help of Simon Byrne's answer:\\n\\n - sparsity (S-sparse assumption, $l^p$ ball and weak $l^p$ ball for $p<1$) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used as a structural assumption in functional data analysis. \\n\\nOn the other hand, inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity by wavelet and fourier transform. This make the critical difference mentionned by Simon not so critical?",added 6 characters in body,
3228,2,1432,5016226e-1f86-469f-b5a2-4d9b070e37db,2010-08-09 07:32:32.0,196.0,"In answering [this][1] question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman & Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?\\n\\n  [1]: http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur",,
3229,1,1432,5016226e-1f86-469f-b5a2-4d9b070e37db,2010-08-09 07:32:32.0,196.0,What do the residuals in a logistic regression mean?,,
3230,3,1432,5016226e-1f86-469f-b5a2-4d9b070e37db,2010-08-09 07:32:32.0,196.0,<r><logistic><aic><error>,,
3231,6,942,f9c728ae-3fd4-48ca-bb6c-a0a059491cdd,2010-08-09 07:44:33.0,223.0,<statistical-analysis><time-series><outliers><wavelet>,edited tags,
3232,5,962,9ff09318-3c4b-438d-bbfe-08d1aeefa02d,2010-08-09 07:47:01.0,223.0,"I don't know much about anomaly detection but I know wavelet is usefull to detect singularities in a signal (see for example the paper http://www.math.univ-toulouse.fr/~bigot/Site/Publications_files/Spectrometry.pdf (see figure 3 for an illustration) and the references mentionned in the paper.  I guess singularities are sometime anomaly? \\n\\nThe idea here is that the Continuous wavelet transform has maxima lines that propagates along frequencies, the longuer the line is the higher is the singularity. See Figure 3 in the paper to see what I mean ! note that there is a free matlab code related to that paper... \\n\\n\\n----------\\n\\n Additionally, I can give you some heuristic about why the DISRCETE (preceding example is about the continuous one) wavelet transform (**DWT**) is interesting for a statistician (excuse non exhaustivity).   \\n\\n-	There is a wide class of (realistic) signal that are transformed into a sparse sequence by the wavelet transform. (compression property)\\n-	A wide class of (quasi stationary) process that are transform into a sequence with almost uncorrelated features (decorrelation property)\\n-	 Wavelet coefficients contain are containing an information that is localized in time and in frequency (at different scales). (multiscale property)\\n-	Wavelet coefficients of a signal concentrate on its singularities. \\n\\n\\n","I only wrote ""DWT"" after discrete wavelet transform in case people don't see it",
3233,5,1431,a59b5d7a-8f31-4cc6-a886-05f320172aa8,2010-08-09 08:04:20.0,223.0,"Take any process $(X_t)_t$ with independent components that has a constant first and second moment and put a varying third moment. \\n\\nIt is second order stationnary because $E[X_t X_{t+h}]=0$ and it is not strictly stationnary \\nbecause $P(X_t\\geq x_t, X_{t+1}\\geq x_{t+1})$ depends upon $t$",added 199 characters in body,
3234,5,490,51e7b53c-eff6-428f-8537-6f29346144fa,2010-08-09 08:11:31.0,223.0,"What are the **variable/feature  selection that you prefer** for binary classification when there are many more variables/feature than observations in the learning set? The aim here is to discuss what is the feature selection procedure that reduces the best the classification error. \\n\\nWe can **fix notations** for homogeneity: for i=0,1,  let x1^i,...,xni^i be the learning set of observations from group i.   So n0+n1=n is the size of the learning set. We set p the number of features (i.e. the dimension of the feature space). If x is a vector of R^p, x[i] is the ith coordinate. \\n\\nPlease give full references if you cannot give the details. \\n\\n\\nEDIT (updated continuously):  Procedures proposed in the answers below \\n\\n - **Greedy forward selection** http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/497#497\\n - **Backward elimination** http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/501#501\\n - **Metropolis scanning / MCMC** http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/505#505\\n - **penalized logistic regression** http://stats.stackexchange.com/questions/490/variable-selection-procedure-for-binary-classification/606#606\\n\\nAs this is community wiki there can be more discussion and update\\n\\nI have one remark: in a certain sence, you all give a procedure that permit ordering of variables but not variable selection (you are quite evasive on how to select the number of features, I guess you all use cross validation?) Can you improve the answers in this direction? (as this is community wiki you don't need to be the answer writter to add an information about how to select the number of variables? I have openned a question in this direction here http://stats.stackexchange.com/questions/880/cross-validation-to-select-the-number-of-used-variables-in-very-high-dimensional)\\n\\n\\n",added 29 characters in body; added 22 characters in body,
3235,5,840,09aaf646-f4f4-4b18-b658-a7f35ec9f7b3,2010-08-09 08:13:30.0,509.0,I work with both R and [MATLAB][1] and I use [R.matlab](http://cran.r-project.org/web/packages/R.matlab/index.html) a lot to transfer data between the two.\\n\\n  [1]: http://en.wikipedia.org/wiki/MATLAB\\n,edited body; added 53 characters in body,
3236,5,818,115ffe38-4378-43f5-8b8b-64e182b3f6a3,2010-08-09 08:15:05.0,509.0,We mostly use:\\n\\n* ggplot - for charts\\n* stats\\n* e1071 - for [SVMs][1]\\n\\n  [1]: http://en.wikipedia.org/wiki/Support_vector_machine\\n,added 60 characters in body,
3237,5,1431,85d82c07-d8aa-4e2a-a32d-78fe5c207dc8,2010-08-09 08:19:36.0,223.0,"Take any process $(X_t)_t$ with independent components that has a constant first and second moment and put a varying third moment. \\n\\nIt is second order stationnary because $E[X_t X_{t+h}]=0$ and it is not strictly stationnary \\nbecause $P(X_t\\geq x_t, X_{t+1} \\geq  x_{t+1})$ depends upon $t$",added 2 characters in body,
3238,5,973,8dd3211a-c644-4114-91be-ca7cac0beb8f,2010-08-09 08:19:59.0,509.0,"What are the **freely available data set for classification with more than 1000 features** (or sample points if it is about curves)? \\n\\nThere is already a community wiki about free data sets:\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nBut here, it would be nice to have a **more focused list that can be used more conveniently**, also I propose the following rules:\\n\\n - One post **per dataset** \\n - **No** link to set of dataset (however, many data set can be given in a post)\\n - each data set **must** be associated with\\n\\n    1- a name (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    2- the number of features (let say it is p) the size of the dataset (let say it is n) and the number of labels/class (let say it is k)    \\n\\n    3- a typical error rate from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n",deleted 5 characters in body; edited title,
3239,4,973,8dd3211a-c644-4114-91be-ca7cac0beb8f,2010-08-09 08:19:59.0,509.0,Free data set for very high dimensional classification,deleted 5 characters in body; edited title,
3240,5,1431,96a6ccf2-d53c-4ace-bf7b-d8b4c06ec5fe,2010-08-09 08:29:24.0,223.0,"Take any process $(X_t)_t$ with independent components that has a constant first and second moment and put a varying third moment. \\n\\nIt is second order stationnary because $E[ X_t X_{t+h} ]=0$ and it is not strictly stationnary \\nbecause $P( X_t \\geq x_t, X_{t+1} \\geq  x_{t+1})$ depends upon $t$",added 4 characters in body,
3242,2,1433,dd643ddb-5653-418e-bb40-5a93f569452c,2010-08-09 10:12:35.0,438.0,"What enforces more separation than there should be is each discipline's lexicon. \\n\\nThere are many instances where ML uses one term and Statistics uses a different term--but both refer to the same thing--fine, you would expect that, and it doesn't cause any permanent confusion (e.g., features/attributes versus expectation variables, or neural network/MLP versus project-pursuit).\\n\\nWhat's more troublesome, is the number of key terms in ML that are also used in Statistics, though in Statistics the meaning is completely different (homonyms, i suppose).\\n\\nA few examples:\\n\\n***Kernel Function***\\n\\nIn ML, kernel functions are used in classifiers (e.g., SVM) and of course in kernel machines. The term refers to a simple function (*cosine, sigmoidal, rbf, polynomial*) to map data not linearly separable to a new input space, then applying a linear model in that new space (instead of using a non-linear model to begin with).\\n\\nIn statistics, a kernel function is weighting function used in density estimation to smooth the density curve.<br><br>\\n\\n\\n***Regression***\\n\\nIn ML, predictive algorithms are run in either of two modes, 'machine' mode or 'regression' mode. 'Machine' refers to an algorithm that returns a class label--a classifier; 'Regression' mode returns continuous output--e.g., price or expected value. (Rarely do the algorithms have different names based on mode, though i sometimes see terms like 'SVR' and 'SVM' used in the same paper, to indicate/distinguish a support vector algorithm configured to return a class label versus one returning a continuous variable.\\n\\nIn Statistics, 'regression', AFAIK, if you are attempting to model the relationship between two more variables, you are doing regression analysis. It doesn't matter whether the output is a continuous variable or a class label (e.g., logistic regression).<br><br>\\n\\n***Bias***\\n\\nIn ML, the *bias* term in the algorithm is conceptually identical to the *intercept* term used by statisticians in regression modeling.\\n\\nIn Statistics,...well, i'm not sure but it appears to be used as a synonym for 'error' in some contexts (e.g., estimator error is referred to always as 'bias' rather than 'error.')\\n\\n \\n\\n\\n\\n\\n",,
3243,2,1434,37592232-2fa6-4040-9806-3cba666c4589,2010-08-09 10:23:41.0,830.0,"Apparently not here yet, so... ""[How to Tell the Liars from the Statisticians][1]"" by Hooke. I am fond of its way of explaining the concepts of statistics to laypersons.\\n\\nAs for explaining the motivations of statisticians, ""The Lady Tasting Tea"" is good reading.\\n\\n\\n  [1]: http://www.amazon.com/dp/0824718178",,
3244,16,1434,37592232-2fa6-4040-9806-3cba666c4589,2010-08-09 10:23:41.0,-1.0,,,
3245,2,1435,894a6f2d-36c3-49c1-9832-871f3ee56ae3,2010-08-09 10:26:35.0,521.0,"The easiest residuals to understand are the deviance residuals as when squared these sum to -2 times the log-likelihood. In its simplest terms logistic regression can be understood in terms of fitting the function p = logit(X*beta) for known X in such a way as to minimise the total deviance, which is the sum of squared deviance residuals of all the data points.\\n\\nThe (squared) deviance of each data point is equal to (-2 times) the logarithm of the difference between its predicted probability (logit(X*beta))  and the complement of its actual value (1 for a control; a 0 for a case) in absolute terms. A perfect fit of a point (which never occurs) gives a deviance of zero as log(1) is zero. A poorly fitting point has a large residual deviance as -2 times the log of a very small value is a large number. \\n\\nDoing logistic regression is akin to finding a beta value such that the sum of squared deviance residuals is minimised. \\n\\nThis can be illustrated with a plot, but I don't know how to upload one.\\n\\n\\n",,
3246,2,1436,c49109e3-c5fd-4343-9d0c-a22f4d1849e8,2010-08-09 10:29:38.0,830.0,"A mathematician, a physicist and a statistician went hunting for deer. When they chanced upon one buck lounging about, the mathematician fired first, missing the buck's nose by a few inches. The physicist then tried his hand, and missed the tail by a wee bit. The statistician started jumping up and down saying ""We got him! We got him!""",,
3247,16,1436,c49109e3-c5fd-4343-9d0c-a22f4d1849e8,2010-08-09 10:29:38.0,-1.0,,,
3249,6,1427,67ec627b-1228-4e03-ae38-225c9f6ae616,2010-08-09 10:34:57.0,8.0,<time-series><modeling>,edited tags,
3250,6,155,5a24cbfe-ba17-4fff-bb85-255066ca87d6,2010-08-09 10:36:33.0,8.0,<teaching>,Removing layman meta tag,
3251,6,414,13ea3a41-cbb7-4344-b023-2b4605516ce1,2010-08-09 10:37:43.0,8.0,<books>,Removing introductory tag,
3252,6,223,bcb35790-846c-4bb1-b074-519fd9f4302e,2010-08-09 10:37:57.0,8.0,<online><books>,Removing introductory tag,
3253,2,1437,430036d2-a7f8-4461-b6af-4d8dbd009385,2010-08-09 10:37:59.0,,Do You think it can be used instead of k means? I obtain correlation with the first 2 components as they carry over 90% of the weight. Would You agree on the technique?,,CLOCK
3254,1,1437,430036d2-a7f8-4461-b6af-4d8dbd009385,2010-08-09 10:37:59.0,,Can Principle Component Analysis be used alone to infer major patterns within data instead of k means clustering?,,CLOCK
3255,3,1437,430036d2-a7f8-4461-b6af-4d8dbd009385,2010-08-09 10:37:59.0,,<pca>,,CLOCK
3256,6,125,d4426735-e06d-49f9-984f-1c27e8aa9524,2010-08-09 10:38:15.0,8.0,<bayesian><best-of><books>,Removing introductory tag,
3257,6,1289,86f54266-fe88-454c-b0fc-9cc57bd4f8d8,2010-08-09 10:43:44.0,8.0,<data-visualization><pca>,edited tags,
3258,2,1438,c95255bd-e0bf-4240-ac48-c4c74b5242e0,2010-08-09 10:47:18.0,8.0,"I think it depends on your data set and what you want to do with it. If you look at my answer to this [question][1], you will see that it indicates groups/differences. However, it certainly doesn't *prove* differences - it just gives you an idea of where differences may lie.\\n\\nHow long would it take you run a quick k-means analysis on your data? When I have a large multivariate data set, I try many different techniques to get a handle on it.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1289/visualizing-multiple-histograms/1291#1291",,
3259,5,1437,3d79892b-16a3-47b1-8d48-3d6581a22f1c,2010-08-09 10:56:24.0,159.0,Do you think it can be used instead of k means? I obtain correlation with the first 2 components as they carry over 90% of the weight. Would you agree on the technique?,edited body; edited title,
3260,4,1437,3d79892b-16a3-47b1-8d48-3d6581a22f1c,2010-08-09 10:56:24.0,159.0,Can Principal Component Analysis be used alone to infer major patterns within data instead of k means clustering?,edited body; edited title,
3261,6,846,ec2fda4f-bb4b-4091-a94e-64843b557724,2010-08-09 10:56:42.0,8.0,<regression>,Retag regression to linear-regression,
3262,5,1016,6b7a4d13-e7bc-4b19-abff-f2d8a3a88c5a,2010-08-09 10:57:45.0,8.0,I've got a linear regression model with the sample and variable observations and I want to know:\\n\\n1. Whether a specific variable is significant enough to remain included in the model.\\n1. Whether another variable (with observations) ought to be included in the model.\\n\\nWhich statistics can help me out? How can get them most efficiently?,Retag regression to linear-regression,
3263,6,1016,6b7a4d13-e7bc-4b19-abff-f2d8a3a88c5a,2010-08-09 10:57:45.0,8.0,<regression>,Retag regression to linear-regression,
3264,5,1437,4c7bcce0-1ee5-477e-82f9-92286781b77b,2010-08-09 10:59:13.0,8.0,Do you think it can be used instead of k means? I obtained a correlation with the first 2 components as they carry over 90% of the weight. Would you agree on the technique?,added 5 characters in body; deleted 1 characters in body,
3265,6,866,7b6963f1-7151-4350-af76-6f2a605ed297,2010-08-09 11:00:06.0,8.0,<regression><lasso><ridge>,Adding lasso and ridge tags,
3266,5,1141,2ac4a83b-49ea-4e09-b2c2-81e2e49d1c89,2010-08-09 11:09:24.0,665.0,"I can't pick just one :)\\n\\n**Check out this great blog post by flowing data**: <a href=""http://flowingdata.com/2009/05/06/37-data-ish-blogs-you-should-know-about/"">37 Data-ish blogs you should know about</a>\\n\\n",deleted 53 characters in body,
3267,8,1141,da855cf3-1064-46f4-8695-cc7c47c03aef,2010-08-09 11:12:16.0,665.0,http://datavisualization.ch<br> \\nby Benjamin Wiederkehr and others (~15 links a month). If you want heaps of links you can subscribe to their twitter feed twitter slash datavis (~5 links a day) \\n\\nahhh... i'm a new member and so i can only post one link per post.,Rollback to [1ffa7509-b4d3-49c7-8dfe-b80f6bac889b],
3268,2,1439,be01c898-3e1b-4380-8e8c-eeb3e29819c3,2010-08-09 11:14:34.0,665.0,"I can't pick just one :)\\n\\n**Check out this great blog post by flowingdata:** <a href=""http://flowingdata.com/2009/05/06/37-data-ish-blogs-you-should-know-about/"">37 Data-ish blogs you should know about</a>",,
3269,16,1439,be01c898-3e1b-4380-8e8c-eeb3e29819c3,2010-08-09 11:14:34.0,-1.0,,,
3270,5,237,dbc6fe89-a27e-4573-b767-91bfcaace2c7,2010-08-09 12:18:10.0,8.0,"I'm going to leave the main question alone, because I think I will get it wrong (although I too analyse data for a healthcare provider, and to be honest, if I had these data, I would just analyse them using standard techniques and hope for the best, they look pretty okay to me).\\n\\nAs for R packages, I have found the TSA library and it's accompanying [book][1] very useful indeed. The `armasubsets` command, particularly, I think is a great time saver.\\n\\n\\n  [1]: http://www.amazon.co.uk/Time-Analysis-Applications-Springer-Statistics/dp/0387759581/ref=sr_1_2?ie=UTF8&s=books&qid=1279610835&sr=8-2",Changed link to a hyperlink,
3271,6,866,e2f483ab-2f28-47dd-9b65-285d3dde5d52,2010-08-09 12:25:13.0,159.0,<regression><lasso><ridge-regression>,edited tags,
3272,2,1440,d2e32966-1364-4fca-b2a2-95d636574498,2010-08-09 12:45:05.0,183.0,"It's possible that my background in psychological research is disguising some understanding of the broader application of PCA and K-means, but I'd say the following:\\n\\n* **PCA** is used to reduce a set of variables to a smaller number of dimensions\\n* **k-means** is used to group cases.\\n\\nFor **example**, take a study of 1000 participants who have completed 10 different ability tests (verbal, mathematics, spatial, etc.).\\nI could use PCA (or factor analysis) to group tests in order to identify the main underlying dimensions.\\nI could use k-means to identify types of cases. \\n\\nAs a side point, I often find one approach is [theoretically more interesting][1]. If the cluster analysis is just grouping in terms of high and low on the first principal component, then I find PCA to be the more meaningful analysis, and the whole concept of clusters as an arbitrary dichotomisation (or categorisation) of a continuous variable.\\n\\n\\n  [1]: http://jeromyanglim.blogspot.com/2009/09/cluster-analysis-and-single-dominant.html",,
3273,2,1441,b8024b1f-6080-4adc-84cb-006b2d6a5af2,2010-08-09 12:50:55.0,352.0,"This question follows from [my previous question][1] , where Robin answered the question in the case of weak stationary processes.  Here, I am asking a similar question for (strong?) stationary processes.  I'll define what this means (the definition can also be found [here][2]).\\n\\nLet $X(t)$ be a stochastic process. We say that $X(t)$ is Nth-order stationary if, for every $t_1, t_2, \\dots, t_N$ we have that the joint cumulative density functions\\n$$F_{X(t_1),X(t_2),\\dots,X(t_N)} = F_{X(t_1 + \\tau),X(t_2 + \\tau),\\dots,X(t_N + \\tau)}$$\\nfor all $\\tau$. This is quite a strong condition, it says that the joint statistics don't change at all as time shifts. \\n\\nFor example, a 1st order stationary process is such that $F_{X(t_1)} = F_{X(t_2)}$ for all $t_1$ and $t_2$. That is, the $X(t)$ are all identically distributed. It is quite easy to see that a 1st order stationary process need not be 2nd order stationary.  Simply assign a correlation structure to say $X(t)$, $X(t+1)$, $X(t+2)$ that *does not* correspond to a (symmetric) Toeplitz matrix. That is, in vector form, the covariance matrix of $[ X(t), X(t+1), X(t+3)]$ could be given as\\n$$\\left[\\begin{array}{cc} \\n\\sigma^2 & a & b \\newline \\na & \\sigma^2 & c \\newline\\nb & c& \\sigma^2\\n\\end{array}\\right]$$\\nfor $a,b,c$ distinct.  This is now not 2nd order stationary because $E[X(t)X(t+1)] = a$ and, time shifting by 1 we have $E[X(t+1)X(t+2)] = c \\neq a$.\\n\\nIn a similar way (presumably) a process that is 1st and 2nd order stationary need not be 3rd order stationary and this leads to my question:\\n\\n> Does somebody have a nice example of a stochastic process that is both 1st and 2nd order stationary, but not 3rd order stationary?\\n \\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1430/example-of-a-2nd-order-stationary-but-not-strictly-stationary-process\\n  [2]: http://en.wikipedia.org/wiki/Stationary_process",,
3274,1,1441,b8024b1f-6080-4adc-84cb-006b2d6a5af2,2010-08-09 12:50:55.0,352.0,"Example of a stochastic process that is 1st and 2nd order stationary, but not strictly stationary (Round 2)",,
3275,3,1441,b8024b1f-6080-4adc-84cb-006b2d6a5af2,2010-08-09 12:50:55.0,352.0,<stochastic-processes>,,
3276,16,6,c45d4a16-e06d-4fd0-9c2c-540cd4aafb53,2010-08-09 13:05:50.0,5.0,,,
3277,6,6,c45d4a16-e06d-4fd0-9c2c-540cd4aafb53,2010-08-09 13:05:50.0,5.0,<machine-learning><statistics>,edited tags,
3278,16,76,70eeed00-2087-48bb-911b-7aa62a9da504,2010-08-09 13:16:41.0,5.0,,,
3279,2,1442,4852ad8d-dd2d-440d-a3aa-e57d2927824a,2010-08-09 13:36:43.0,159.0,"To assess the historical trend, I'd use a gam with trend and seasonal components. For example\\n\\n    require(mgcv)\\n    require(forecast)\\n    x <- ts(rpois(100,1+sin(seq(0,3*pi,l=100))),f=12)\\n    tt <- 1:100\\n    season <- seasonaldummy(x)\\n    fit <- gam(x ~ s(tt,k=5) + season, family=""poisson"")\\n    plot(fit)\\n\\nThen summary(fit) will give you a test of significance of the change in trend and the plot will give you some confidence intervals. The assumptions here are that the observations are independent and the conditional distribution is Poisson. Because the mean is allowed to change smoothing over time, these are not particularly strong assumptions.\\n\\nTo forecast is more difficult as you need to project the trend into the future. If you are willing to accept a linear extrapolation of the trend at the end of the data (which is certainly dodgy but probably ok for a few months), then use\\n\\n    fcast <- predict(fit,se.fit=TRUE,\\n                   newdata=list(tt=101:112,season=seasonaldummyf(x,h=12)))\\n\\nTo see the forecasts on the same graph:\\n\\n    plot(x,xlim=c(0,10.5))\\n    lines(ts(exp(fcast$fit),f=12,s=112/12),col=2)\\n    lines(ts(exp(fcast$fit-2*fcast$se),f=12,s=112/12),col=2,lty=2)\\n    lines(ts(exp(fcast$fit+2*fcast$se),f=12,s=112/12),col=2,lty=2)\\n\\n",,
3280,2,1443,1a690d69-6328-4b59-a2b7-0e20391acd0e,2010-08-09 13:51:29.0,521.0,"Ideally one should have a thorough knowledge of both statsitics and machine learning before attempting to answer his question. I am very much a neophyte to ML, so forgive me if wat I say is naive.\\n\\nI have limited experience in SVMs and regression trees. What strikes me as lacking in ML from a stats point of view is a well developed concept of inference. \\n\\nInference in ML seems to boil down almost exclusively to the predictice accuracy, as measured by (for example) mean classification error (MCE), or balanced error rate (BER) or similar. ML is in the very good habit of dividing data randomly (usually 2:1) into a training set and a test set. Models are fit using the training set and performance (MCE, BER etc) is assessed using the test set. This is an excellent practice and is only slowly making its way into mainstream statistics. \\n\\nML also makes heavy use of resampling methods (especially cross-validation), whose origins appear to be in statistics.\\n\\nHowever, ML seems to lack a fully developed concept of inference - beyond predictive accuracy. This has two results.\\n \\n1) There does not seem to be an appreciation that any prediction (parameter estimation etc.) is subject to a random error and perhaps systemmatics error (bias). Statisticians will accept that this is an inevitable part of prediction and will try and estimate the error. Statistical techniques will try and find an estimate that has minimum bias and random error. Their techniques are usually driven by a model of the data process, but not always (eg. Bootstrap).\\n \\n2) There does not seem to be a deep understanding in ML of the limits of applying a model to new data to a new sample from the same population (in spite of what I said earlier about the training-test data set approach). Various statistical techniques, among them cross validation and penalty terms applied to likelihood-based methods guide statisticians in the trade-off between parsimony and model complexity. Such guidelines in ML seem much more ad hoc.\\n\\nI've seen several papers in ML where cross validation is used to optimise a fitting of many models on a training dataset - producing better and better fit as the model complexity increases. There appears little appreciation that the tiny gains in accuracy are not worth the extra complexity and this is naturally leads to over-fitting. Then all these models are applied to the test set as a check on predictive performance and to prevent overfitting. Two things have been forgotten (above). The predictive performance will have a stochastic component. Secondly multiple tests against a test set will again result in over-fitting. The ""best"" model will be choisen by the ML practitioner without a full appreciation he/she has cherry picked from one realisation of many possible outomes of this experiment. The best of several tested models will almost certainly not reflect the true performance on new data.\\n ",,
3281,16,1443,1a690d69-6328-4b59-a2b7-0e20391acd0e,2010-08-09 13:51:29.0,-1.0,,,
3282,5,1443,dd8d54ee-719a-4ea9-bda5-32cc2b460a41,2010-08-09 13:56:39.0,521.0,"Ideally one should have a thorough knowledge of both statsitics and machine learning before attempting to answer his question. I am very much a neophyte to ML, so forgive me if wat I say is naive.\\n\\nI have limited experience in SVMs and regression trees. What strikes me as lacking in ML from a stats point of view is a well developed concept of inference. \\n\\nInference in ML seems to boil down almost exclusively to the predictice accuracy, as measured by (for example) mean classification error (MCE), or balanced error rate (BER) or similar. ML is in the very good habit of dividing data randomly (usually 2:1) into a training set and a test set. Models are fit using the training set and performance (MCE, BER etc) is assessed using the test set. This is an excellent practice and is only slowly making its way into mainstream statistics. \\n\\nML also makes heavy use of resampling methods (especially cross-validation), whose origins appear to be in statistics.\\n\\nHowever, ML seems to lack a fully developed concept of inference - beyond predictive accuracy. This has two results.\\n \\n1) There does not seem to be an appreciation that any prediction (parameter estimation etc.) is subject to a random error and perhaps systemmatics error (bias). Statisticians will accept that this is an inevitable part of prediction and will try and estimate the error. Statistical techniques will try and find an estimate that has minimum bias and random error. Their techniques are usually driven by a model of the data process, but not always (eg. Bootstrap).\\n \\n2) There does not seem to be a deep understanding in ML of the limits of applying a model to new data to a new sample from the same population (in spite of what I said earlier about the training-test data set approach). Various statistical techniques, among them cross validation and penalty terms applied to likelihood-based methods, guide statisticians in the trade-off between parsimony and model complexity. Such guidelines in ML seem much more ad hoc.\\n\\nI've seen several papers in ML where cross validation is used to optimise a fitting of many models on a training dataset - producing better and better fit as the model complexity increases. There appears little appreciation that the tiny gains in accuracy are not worth the extra complexity and this naturally leads to over-fitting. Then all these optimised models are applied to the test set as a check on predictive performance and to prevent overfitting. Two things have been forgotten (above). The predictive performance will have a stochastic component. Secondly multiple tests against a test set will again result in over-fitting. The ""best"" model will be choisen by the ML practitioner without a full appreciation he/she has cherry picked from one realisation of many possible outomes of this experiment. The best of several tested models will almost certainly not reflect the true performance on new data.\\n\\nAny my 2 cents worth. We have much to learn from each other.\\n ",added 72 characters in body,
3283,2,1444,1ea2f2ba-07b4-461b-8092-ada7a6ac4d9c,2010-08-09 13:57:51.0,159.0,If I have highly skewed positive data I often take logs. But what should I do with highly skewed non-negative data that include zeros? I have seen two transformations used:\\n\\n - log(x+1) which has the neat feature that 0 maps to 0.\\n - log(x+c) where c is either estimated or set to be some very small positive value.\\n\\nAre there any other approaches? Are there any good reasons to prefer one approach over the others?,,
3284,1,1444,1ea2f2ba-07b4-461b-8092-ada7a6ac4d9c,2010-08-09 13:57:51.0,159.0,How should I transform non-negative data including zeros?,,
3285,3,1444,1ea2f2ba-07b4-461b-8092-ada7a6ac4d9c,2010-08-09 13:57:51.0,159.0,<data-transformation>,,
3286,2,1445,d0660565-e517-4957-8438-f7e4ca63e87e,2010-08-09 14:05:50.0,223.0,"I assume you have continuous data. \\n\\nIf the data include **zeroS** this means you have a spike on zero which may be due to some particular aspect of your data. It appears for example in wind energy, wind below 2m/s produce zero power (it is called cut in) and wind over (something around) 25m/5 also produce zero power (for security reason, it is called cut off).  While the distribution of produced wind energy seems continuous there is a spike in zero. \\n\\n**My solution:**  In this case, I suggest to treat the zeros separatly by working with a mixture of the spike in zero and the model you planned to use for the part of the distribution that is continuous (wrt lebesgue). ",,
3287,2,1446,8534f838-1809-48eb-95a6-becdd7879607,2010-08-09 14:22:11.0,,"It seems to me that the most appropriate choice of transformation is contingent on the model and the context. \\n\\nThe '0' point can arise from several different reasons each of which may have to be treated differently:\\n\\n - Truncation (as in Robin's example): Use appropriate models (e.g., mixtures, survival models etc)\\n - Missing data: Impute data / Drop observations if appropriate.\\n - Natural zero point (e.g., income levels; an unemployed person has zero income): Transform as needed\\n - Sensitivity of measuring instrument: Perhaps, add a small amount to data?\\n\\nI am not really offering an answer as I suspect there is no universal, 'correct' transformation when you have zeros.\\n\\n \\n\\n",,user28
3288,2,1447,ef61210b-aafa-44c4-b0c0-649431e60dea,2010-08-09 14:52:42.0,6967.0,"I want to fully grasp the notion of r^2 describing the amount of variation b/w variables.\\n\\nEvery web explanation is a bit mechanical/obtuse.\\n\\nI want to ""get"" the concept, not just mechanically use the numbers.\\n\\neg: Hours studied vs. test score\\n\\nr = .8\\n\\nr^2 = .64\\n\\nSo, what does this mean?\\n64% of the variability of scores can be explained by the model? (ie: hours)\\nWhy?  I am not sure what this is supposed to mean.   Thanks for any clarification.\\n",,
3289,1,1447,ef61210b-aafa-44c4-b0c0-649431e60dea,2010-08-09 14:52:42.0,6967.0,I have never fully grasped the notion of r vs. r-squared,,
3290,3,1447,ef61210b-aafa-44c4-b0c0-649431e60dea,2010-08-09 14:52:42.0,6967.0,<regression>,,
3291,2,1448,9d6f8093-748e-420c-9282-e7eafbb34209,2010-08-09 15:09:34.0,,A mathematical demonstration of the relationship between the two is here: [Pearson's correlation and least squares regression analysis][1].\\n\\nI am not sure if there is a geometric or any other intuition that can be offered apart from the math but if I can think of one I will update this answer.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Pearson.27s_correlation_and_least_squares_regression_analysis,,user28
3292,2,1449,f59a3cb6-f844-4699-9455-dff65233d237,2010-08-09 15:36:26.0,223.0,"> My greatest concern was what to call\\n> it. I thought of calling it\\n> 'information,' but the word was overly\\n> used, so I decided to call it\\n> 'uncertainty.' When I discussed it\\n> with John von Neumann, he had a better\\n> idea. Von Neumann told me, 'You should\\n> call it entropy, for two reasons. In\\n> the first place your uncertainty\\n> function has been used in statistical\\n> mechanics under that name, so it\\n> already has a name. In the second\\n> place, and more important, no one\\n> really knows what entropy really is,\\n> so in a debate you will always have\\n> the advantage.'\\n\\n\\nClaude Elwood Shannon",,
3293,16,1449,f59a3cb6-f844-4699-9455-dff65233d237,2010-08-09 15:36:26.0,-1.0,,,
3294,5,1447,087f4fa7-761d-416b-b923-dec731ad3c23,2010-08-09 15:43:07.0,6967.0,"I want to fully grasp the notion of r^2 describing the amount of variation b/w variables.\\n\\nEvery web explanation is a bit mechanical/obtuse.\\n\\nI want to ""get"" the concept, not just mechanically use the numbers.\\n\\neg: Hours studied vs. test score\\n\\nr = .8\\n\\nr^2 = .64\\n\\nSo, what does this mean?\\n64% of the variability of test scores can be explained by hours?\\nHow do we know that just by squaring?   Thanks for any clarification.\\n",deleted 24 characters in body,
3295,2,1450,0f414589-929c-4bf4-8ff1-8d4609e7923c,2010-08-09 15:44:50.0,485.0,"Start with the variance.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variance that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variance in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variance is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another variance (through we don't call it that).  This is the difference between each Y point and the regression line that we get from a model incorporating X.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line).  \\n\\nIf the regression line is anything but horizontal, we're going to get less distance when we us that instead of the mean.  The ratio between those two values--those two variances--is your R-squared.  It's the proportion of hte original variation in your response that is explained by fitting that regression line.     \\n\\nI have an image, but I am uncertain as to how to upload it.  Here is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n",,
3296,2,1451,641ec25f-dc4d-43b5-9436-482b79830086,2010-08-09 15:51:47.0,5.0,"The list in the presentation that you reference seems fairly arbitrary to me, and the technique that would be used will really depend on the specific problem.  You will note however that it also includes [Kalman filters][4], so I suspect that the intended usage is as a filtering technique.  Wavelet transforms generally fall under the subject of [signal processing][5], and will often be used as a pre-processing step with very noisy data.  An example is the ""[Multi-scale anomaly detection][6]"" paper by Chen and Zhan (see below).  The approach would be to run an analysis on the different spectrum rather than on the original noisy series.\\n\\nWavelets are often compared to a continuous-time fourier transform, although they have the benefit of being localized in both time and frequency.  Wavelets can be used both for signal compression and also for smoothing (wavelet shrinkage).  Ultimately, it could make sense to apply a further statistical after the wavelet transform has been applied (by looking in at the auto-correlation function for instance).  One further aspect of wavelets that could be useful for anomaly detection is the effect of localization: namely, a discontinuity will only influence the wavelet that is near it (unlike a fourier transform).  One application of this is to finding locally stationary time series (using an LSW).  \\n\\n[Guy Nason][1] has a nice book that I would recommend if you want to delve further into the practical statistical application: ""[Wavelet Methods in Statistics with R][2]"".  This is specifically targeting the application of wavelets to statistical analysis, and he provides many real world examples along with all the code (using the [wavethresh package][3]).  Nason's book does not address ""anomaly detection"" specifically, although it does do an admiral job of providing a general overview.\\n\\nLastly, [the wikipedia article][7] does provide many good introductory references, so it is worth going through it in detail.\\n\\n - Xiao-yun Chen, Yan-yan Zhan [""Multi-scale anomaly detection algorithm based on infrequent pattern of time series""][8] Journal of Computational and Applied Mathematics Volume 214 ,  Issue 1  (April 2008) \\n - G.P. Nason ""[Wavelet Methods in Statistics with R][9]"" Springer, 2008\\n\\n[As a side note: if you are looking for a good modern technique for change point detection, I would suggest trying a HMM before spending too much time with wavelet methods, unless you have good reason to be using wavelets in your particular field.  This is based on my personal experience.  There are of course many other nonlinear models that could be considered, so it really depends on your specific problem.]\\n\\n\\n  [1]: http://www.stats.bris.ac.uk/~magpn/\\n  [2]: http://www.amazon.com/Wavelet-Methods-Statistics-Use-Nason/dp/0387759603\\n  [3]: http://cran.r-project.org/web/packages/wavethresh/index.html\\n  [4]: http://en.wikipedia.org/wiki/Kalman_filter\\n  [5]: http://en.wikipedia.org/wiki/Category:Signal_processing\\n  [6]: http://portal.acm.org/citation.cfm?id=1343118.1343331\\n  [7]: http://en.wikipedia.org/wiki/Wavelet\\n  [8]: http://portal.acm.org/citation.cfm?id=1343118.1343331\\n  [9]: http://www.amazon.com/Wavelet-Methods-Statistics-Use-Nason/dp/0387759603",,
3297,5,1450,0f4d1dba-e371-4739-9e9b-d383c4683141,2010-08-09 15:54:29.0,485.0,"Start with the basic idea of variation.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variation that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variation in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variation is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another measure of variability after you have the regression line from the model.  This is the difference between each Y point and the regression line.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line).  \\n\\nIf the regression line is anything but horizontal, we're going to get less total distance when we us that instead of the mean.  The ratio between those two values--those two measures of variation--is your R-squared.  It's the proportion of the original variation in your response that is explained by fitting that regression line.     \\n\\nI have an image, but I am uncertain as to how to upload it.  Here is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n",added 27 characters in body,
3298,4,1447,1b8909b6-c998-4796-b96b-6c08d6c51a9c,2010-08-09 16:12:31.0,6967.0,r-squared:  I have never fully grasped the interpretation,edited title,
3299,5,1364,46ce858d-e46e-4f04-8c5b-f6c712971bf2,2010-08-09 16:31:07.0,8.0,"The easiest way is just to simulate the game lots of times. The R code below simulates a single game.\\n\\n    nplayers = 4\\n    #Create an empty data frame to keep track\\n    #of card number, suit and if it's magic\\n    empty.hand = data.frame(number = numeric(52),\\n      suit = numeric(52),\\n      magic  = numeric(52))\\n    \\n    #A list of players who are in the game\\n    players =list()\\n    for(i in 1:nplayers)\\n      players[[i]] = empty.hand\\n    \\n    #Simulate shuffling the deck\\n    deck = empty.hand\\n    deck$number = rep(1:13, 4)\\n    deck$suit = as.character(rep(c(""H"", ""C"", ""S"", ""D""), each=13))\\n    deck$magic = rep(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0), each=4)\\n    deck = deck[sample(1:52, 52),]\\n    \\n    #Deal out five cards per person\\n    for(i in 1:length(players)){\\n      r = (5*i-4):(5*i)\\n      players[[i]][r,] = deck[r,]\\n    }\\n    \\n    #Play the game\\n    i = 5*length(players)+1\\n    current = deck[i,]\\n    while(i < 53){\\n      for(j in 1:length(players)){\\n        playersdeck = players[[j]]\\n        #Need to test for magic and suit also - left as an exercise!\\n        if(is.element(current$number, playersdeck$number)){\\n          #Update current card\\n          current = playersdeck[match(current$number,\\n            playersdeck$number),]\\n          #Remove card from players deck\\n          playersdeck[match(current$number, playersdeck$number),] = c(0,\\n                       0, 0)\\n        } else {\\n          #Add card to players deck\\n          playersdeck[i,] = deck[i,]\\n          i = i + 1\\n        }\\n        players[[j]] = playersdeck\\n        #Has someone won or have we run out of card\\n        if(sum(playersdeck$number) == 0 | i > 52){\\n          i = 53\\n          break\\n        }\\n      }\\n    }\\n    \\n    #How many cards are left for each player\\n    for(i in 1:length(players))\\n    {\\n      cat(sum(players[[i]]$number !=0), ""\\n"") \\n    }\\n\\n**Some comments**\\n\\n1. You will need to add a couple of lines for magic cards and suits, but data structure is already there. I presume you didn't want a complete solution? ;)\\n1. To estimate the average game length, just place the above code in a function and call lots of times.\\n1. Rather than dynamically increasing a vector when a player gets a card, I find it easier just to create a sparse data frame that is more than sufficient. In this case, each player has a data frame with 52 rows, which they will never fill (unless it's a 1 player game).\\n1. There is a small element of strategy with this game. What should you do if you can play more than one card. For example, if 7H comes up, and you have in your hand 7S, 8H and the JC. All three of these cards are ""playable"".\\n\\n\\n\\n",Fixed a typo in the code.,
3300,5,1450,003ef7d4-0d5a-417b-8644-213692356ca6,2010-08-09 16:41:19.0,485.0,"Start with the basic idea of variation.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variation that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variation in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variation is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another measure of variability after you have the regression line from the model.  This is the difference between each Y point and the regression line.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line).  \\n\\nIf the regression line is anything but horizontal, we're going to get less total distance when we us that instead of the mean.  The ratio between those two values--those two measures of variation--is your R-squared.  It's the proportion of the original variation in your response that is explained by fitting that regression line.\\n\\nSee the (quick and dirty) image below...\\n\\n![http://img217.imageshack.us/img217/5536/rsquared.png][1]\\n\\nUploaded with [URL=http://imageshack.us]ImageShack.us[/URL]\\n\\nHere is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n\\n  [1]: http://img217.imageshack.us/img217/5536/rsquared.png",added 20 characters in body; added 34 characters in body; added 146 characters in body; deleted 36 characters in body,
3301,2,1452,610a559d-b321-4c25-b893-131c6b4e049f,2010-08-09 16:43:48.0,251.0,"The log transforms with shifts are special cases of the [Box-Cox transformations][1]:\\n\\n$y(\\lambda) = \\frac {(y+\\lambda_{2})^{\\lambda_1} - 1} {\\log (y + \\lambda_{2})}, when \\lambda_{1} \\neq 0$\\n\\n$y(\\lambda) = \\log (y + \\lambda_{2}), when \\lambda_{1} = 0$\\n\\nThese are the extended form for negative values, but also applicable to data containing zeros.  Box and Cox (1964) present an algorithm to find appropriate values for the $\\lambda$'s using maximum likelihood.  This gives you the ultimate transformation.  \\n\\nA reason to prefer Box-Cox transformations is that they're developed to ensure assumptions for the linear model.  There's some work done to show that even if your data cannot be transformed to normality, then the estimated $\\lambda$ still lead to a symmetric distribution.\\n\\nI'm not sure how well this addresses your data, since it could be that $\\lambda = 0$ which is just the log transform you mentioned, but it may be worth estimating the requried $\\lambda$'s to see if another transformation is appropriate.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation",,
3302,2,1453,dbdace1b-fdba-442e-8940-e8e472be03e9,2010-08-09 16:49:45.0,251.0,"The [Regression By Eye][1] applet could be of use if you're trying to develop some intuition.  \\n\\nIt lets you generate data then guess a value for *R*, which you can then compare with the actual value.  \\n\\n\\n  [1]: http://onlinestatbook.com/stat_sim/reg_by_eye/index.html",,
3303,5,1452,c332ba8a-40b2-475f-988e-5badef04323d,2010-08-09 16:51:59.0,251.0,"The log transforms with shifts are special cases of the [Box-Cox transformations][1]:\\n\\n$y(\\lambda) = \\frac {(y+\\lambda_{2})^{\\lambda_1} - 1} {\\log (y + \\lambda_{2})}, when \\lambda_{1} \\neq 0$\\n\\n$y(\\lambda) = \\log (y + \\lambda_{2}), when \\lambda_{1} = 0$\\n\\nThese are the extended form for negative values, but also applicable to data containing zeros.  Box and Cox (1964) present an algorithm to find appropriate values for the $\\lambda$'s using maximum likelihood.  This gives you the ultimate transformation.  \\n\\nA reason to prefer Box-Cox transformations is that they're developed to ensure assumptions for the linear model.  There's some work done to show that even if your data cannot be transformed to normality, then the estimated $\\lambda$ still lead to a symmetric distribution.\\n\\nI'm not sure how well this addresses your data, since it could be that $\\lambda = 0$ which is just the log transform you mentioned, but it may be worth estimating the required $\\lambda$'s to see if another transformation is appropriate.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation",edited body,
3304,5,1452,e04fac1d-000b-44d1-9ce5-c1e7f09688ee,2010-08-09 16:55:26.0,,"The log transforms with shifts are special cases of the [Box-Cox transformations][1]:\\n\\n$y(\\lambda) = \\n\\begin{cases} \\n \\frac {(y+\\lambda_{2})^{\\lambda_1} - 1} {\\log (y + \\lambda_{2})} & \\mbox{when } \\lambda_{1} \\neq 0 \\\\\\ \\log (y + \\lambda_{2}) & \\mbox{when } \\lambda_{1} = 0\\n\\end{cases}$\\n\\nThese are the extended form for negative values, but also applicable to data containing zeros.  Box and Cox (1964) present an algorithm to find appropriate values for the $\\lambda$'s using maximum likelihood.  This gives you the ultimate transformation.  \\n\\nA reason to prefer Box-Cox transformations is that they're developed to ensure assumptions for the linear model.  There's some work done to show that even if your data cannot be transformed to normality, then the estimated $\\lambda$ still lead to a symmetric distribution.\\n\\nI'm not sure how well this addresses your data, since it could be that $\\lambda = 0$ which is just the log transform you mentioned, but it may be worth estimating the requried $\\lambda$'s to see if another transformation is appropriate.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation",fixed to use begin cases,user28
3305,5,1450,383fd802-4826-436f-9f6d-ad759056a913,2010-08-09 17:01:55.0,485.0,"Start with the basic idea of variation.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variation that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variation in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variation is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another measure of variability after you have the regression line from the model.  This is the difference between each Y point and the regression line.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line).  \\n\\nIf the regression line is anything but horizontal, we're going to get less total distance when we us that instead of the mean. The ratio between the variation explained and the original variation is your R^2.  It's the proportion of the original variation in your response that is explained by fitting that regression line.\\n\\nSee the (quick and dirty) image below...\\n\\n![http://img217.imageshack.us/img217/5536/rsquared.png][1]\\n\\nUploaded with [URL=http://imageshack.us]ImageShack.us[/URL]\\n\\nHere is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n\\n  [1]: http://img217.imageshack.us/img217/5536/rsquared.png",deleted 7 characters in body,
3306,2,1454,a338a2f2-d229-4923-ad85-c640cda61dc0,2010-08-09 17:43:33.0,247.0,"I have a data set that contains two types of points. The first type of points come from an N(0,1) distribution. The second type of points come from an N(m,v) distribution for some real m and some positive, real v. The objective is to classify each point as type 1 or type 2, and to identify m & v. We have no apriori information about m & v. Any ideas?",,
3307,1,1454,a338a2f2-d229-4923-ad85-c640cda61dc0,2010-08-09 17:43:33.0,247.0,How to identify points and an unknown distribution in a two type clustering problem?,,
3308,3,1454,a338a2f2-d229-4923-ad85-c640cda61dc0,2010-08-09 17:43:33.0,247.0,<clustering>,,
3309,2,1455,fb9e64bc-d70a-49d7-9c1d-b4babcf1c531,2010-08-09 17:47:12.0,253.0,"I have a bunch of articles presenting ""OR"" with a- 95% CI (confidence intervals).\\n\\nI want to estimate from the articles the P value for the observed OR.  For that, I need an assumption regarding the OR distribution.  What distribution can I safely assume/use?",,
3310,1,1455,fb9e64bc-d70a-49d7-9c1d-b4babcf1c531,2010-08-09 17:47:12.0,253.0,What is the distribution of OR (odds ration) ?,,
3311,3,1455,fb9e64bc-d70a-49d7-9c1d-b4babcf1c531,2010-08-09 17:47:12.0,253.0,<distributions><oddsration>,,
3312,2,1456,c9940f13-11ba-4378-8e89-ec3e50886130,2010-08-09 18:00:24.0,251.0,"The log odds ratio has a Normal asymptotic distribution :\\n\\n$\\log(\\hat{OR}) \\sim N(\\log(OR), \\sigma_{OR}^2)$\\n\\nwith $\\sigma$ estimated from the contingency table.  See, for example, page 6 of the notes:\\n\\n- [Asymptotic Theory for Parametric Models][1]\\n\\n  [1]: http://faculty.business.utsa.edu/rtripath/7853/Chapter14/Lecture14_2.pdf\\n  \\n\\n\\n",,
3313,2,1457,8c1d943d-d719-405c-94fa-a97fe1de0c24,2010-08-09 18:03:31.0,,"You could use a [mixture model][1] to separate out the components. The data generating process  can be represented as follows:\\n\\nLet:\\n\\n$z_i$: be the type (1 or 2) for the $i^{th}$ observation,\\n\\n$y_i$ be the $i^{th}$ observation.\\n\\nThen you have:\\n\\n$f\\ (y_i|z_i=1) \\sim N(0,1)$\\n\\n$f\\ (y_i|z_i=2) \\sim N\\ (\\ m\\ ,\\ v)$\\n\\n$P\\ (z_i=1) = \\pi$ and\\n\\n$P\\ (z_i=2) = 1-\\pi$.\\n\\nThe likelihood function is given by:\\n\\n$L\\ (m,v,\\pi|-) = \\sum_{y_i} \\pi f(y_i|z_i=1) + (1-\\pi) f(y_i|z_i=2)$ \\n\\nYou can then use either [EM or MCMC][2] to estimate for the model parameters.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Mixture_model\\n  [2]: http://en.wikipedia.org/wiki/Mixture_model#Common_approaches_for_estimation_in_mixture_models",,user28
3314,2,1458,d4daf86d-8a4f-47de-b7f0-84069baf9746,2010-08-09 18:03:54.0,148.0,"I find it hard to understand what really is the issue with *multiple comparisons*.  With a simple analogy, it is said that a person who will make many decisions will make many mistakes. So very conservative precaution is applied, like Bonferroni correction, so as to make the probability that, this person will make any mistake at all, as low as possible. \\n\\nBut why do we care about whether the person has made any mistake at all among all decisions he/she has made, rather than the *percentage* of the wrong decisions?  \\n\\nLet me try to explain what confuses me with another analogy.  Suppose there are two judges, one is 60 years old, and the other is 20 years old.  Then Bonferroni correction tells the one which is 20 years old to be as conservative as possible, in deciding for execution, because he  will work for many more years as a judge, will make many more decisions, so he has to be careful.  But the one at 60 years old will possibly retire soon, will make fewer decisions, so he can be more careless compared to the other.  I think this analogy more or less translates to the real problems where Bonferroni correction is applied, which I find counterintuitive.  ",,
3315,1,1458,d4daf86d-8a4f-47de-b7f0-84069baf9746,2010-08-09 18:03:54.0,148.0,Why  is multiple comparison a problem?,,
3316,3,1458,d4daf86d-8a4f-47de-b7f0-84069baf9746,2010-08-09 18:03:54.0,148.0,<multiple-comparisons>,,
3317,2,1459,53a04fe1-4076-4aca-a052-17304f82b5e3,2010-08-09 18:10:39.0,833.0,"I am trying to build a time series regression forecasting model for an outcome variable, in dollar amount, in terms of other predictors/input variables and autocorrelated errors.  This kind of model is also called dynamic regression model.  I need to learn how to identify transfer functions for each predictor and would love to hear from you about ways to do just that.  Thanks.",,
3318,1,1459,53a04fe1-4076-4aca-a052-17304f82b5e3,2010-08-09 18:10:39.0,833.0,way to create time series forecasting models ,,
3319,3,1459,53a04fe1-4076-4aca-a052-17304f82b5e3,2010-08-09 18:10:39.0,833.0,<forecasting>,,
3320,5,1459,9088eb4a-aa74-4300-a445-be430796e6ae,2010-08-09 18:13:54.0,5.0,"I am trying to build a time series regression forecasting model for an outcome variable, in dollar amount, in terms of other predictors/input variables and autocorrelated errors.  This kind of model is also called dynamic regression model.  I need to learn how to identify transfer functions for each predictor and would love to hear from you about ways to do just that. ",Added tag and removed letter format.,
3321,6,1459,9088eb4a-aa74-4300-a445-be430796e6ae,2010-08-09 18:13:54.0,5.0,<time-series><forecasting>,Added tag and removed letter format.,
3322,5,1458,8205da5d-c34e-4d28-b5b1-e3ff802fac48,2010-08-09 18:16:16.0,148.0,"I find it hard to understand what really is the issue with *multiple comparisons*.  With a simple analogy, it is said that a person who will make many decisions will make many mistakes. So very conservative precaution is applied, like Bonferroni correction, so as to make the probability that, this person will make any mistake at all, as low as possible. \\n\\nBut why do we care about whether the person has made any mistake at all among all decisions he/she has made, rather than the *percentage* of the wrong decisions?  \\n\\nLet me try to explain what confuses me with another analogy.  Suppose there are two judges, one is 60 years old, and the other is 20 years old.  Then Bonferroni correction tells the one which is 20 years old to be as conservative as possible, in deciding for execution, because he  will work for many more years as a judge, will make many more decisions, so he has to be careful.  But the one at 60 years old will possibly retire soon, will make fewer decisions, so he can be more careless compared to the other.  But actually, both judges should be equally careful or conservative, regardless of the total number of decisions they will make.  I think this analogy more or less translates to the real problems where Bonferroni correction is applied, which I find counterintuitive. ",added 129 characters in body,
3323,2,1460,11921679-df2c-4929-bb0f-9d0c017ff86c,2010-08-09 18:16:21.0,834.0,"I am trying to figure out the best transformation of my consumption variable. I am running a probit regression to look at whether or not a household enrolls in health insurance.  Consumption per capita is an independent variable and in my current model I use both consumption and consumption squared (two separate variables) to show that consumption increases but with diminishing returns. This makes for fairly straightforward interpretation. However, using the log of consumption is a slightly better fit because it normalizes the distribution and contributes a bit more to the overall R2 for the model but it is more difficult to interpret. Which would you suggest I use - log of consumption or consumption plus the quadratic function? My research is focused on health economics so I'm not sure what the preference is in that discipline. Any insight would be much appreciated. Thank you!\\n\\n\\n \\n",,
3324,1,1460,11921679-df2c-4929-bb0f-9d0c017ff86c,2010-08-09 18:16:21.0,834.0,Ideal transformation for consumption variable in a probit model,,
3325,3,1460,11921679-df2c-4929-bb0f-9d0c017ff86c,2010-08-09 18:16:21.0,834.0,<statistical-analysis><data-transformation><econometrics>,,
3326,2,1461,137841eb-bb21-4f92-b27e-45a24f92ffa1,2010-08-09 18:27:59.0,,"It seems to me that you already have a 'partial' statistics answer (better $R^2$ as to the decision as to what to choose: log vs quadratic). You could use other data-driven metrics (e.g., out-of-sample hit rates, whether the parameters are reasonable etc) to judge which model structure is 'better'. \\n\\nPS: By the way, are these two models consistent with economic theory? Just asking as I do not know this area. Another way to select a model is to check its consistency with theory.\\n",,user28
3327,5,1457,9181c69c-a274-4df9-b82a-569f6e887d36,2010-08-09 18:36:17.0,,"You could use a [mixture model][1] to separate out the components. The data generating process  can be represented as follows:\\n\\nLet:\\n\\n$z_i$: be the type (1 or 2) for the $i^{th}$ observation,\\n\\n$y_i$ be the $i^{th}$ observation.\\n\\nThen you have:\\n\\n$f(y_i|z_i=1) \\sim N(0,1)$\\n\\n$f(y_i|z_i=2) \\sim N(m,v)$\\n\\n$P(z_i=1) = \\pi$ and\\n\\n$P(z_i=2) = 1-\\pi$.\\n\\nThe likelihood function is given by:\\n\\n$L(m,v,\\pi|-) = \\sum_{y_i} \\pi f(y_i|z_i=1) + (1-\\pi) f(y_i|z_i=2)$ \\n\\nYou can then use either [EM or MCMC][2] to estimate for the model parameters.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Mixture_model\\n  [2]: http://en.wikipedia.org/wiki/Mixture_model#Common_approaches_for_estimation_in_mixture_models",deleted 18 characters in body,user28
3328,5,1452,c67363d6-5959-4318-a222-2a6ef069e9cf,2010-08-09 18:51:20.0,251.0,"The log transforms with shifts are special cases of the [Box-Cox transformations][1]:\\n\\n$y(\\lambda_{1}, \\lambda_{2}) = \\n\\begin{cases} \\n \\frac {(y+\\lambda_{2})^{\\lambda_1} - 1} {\\lambda_{1}} & \\mbox{when } \\lambda_{1} \\neq 0 \\\\\\ \\log (y + \\lambda_{2}) & \\mbox{when } \\lambda_{1} = 0\\n\\end{cases}$\\n\\nThese are the extended form for negative values, but also applicable to data containing zeros.  Box and Cox (1964) present an algorithm to find appropriate values for the $\\lambda$'s using maximum likelihood.  This gives you the ultimate transformation.  \\n\\nA reason to prefer Box-Cox transformations is that they're developed to ensure assumptions for the linear model.  There's some work done to show that even if your data cannot be transformed to normality, then the estimated $\\lambda$ still lead to a symmetric distribution.\\n\\nI'm not sure how well this addresses your data, since it could be that $\\lambda = 0$ which is just the log transform you mentioned, but it may be worth estimating the requried $\\lambda$'s to see if another transformation is appropriate.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation",had the wrong formula for transformation,
3329,2,1462,8bbc0652-1635-4883-bbec-775c8626cb0e,2010-08-09 18:53:30.0,6967.0,"Let's say I want to make a football simulator based on real-life data.\\n\\nSay I have a player who averages 5.3 yards per carry with a SD of 1.7 yards.\\n\\nI'd like to generate a random variable that simulates the next few plays.\\neg:  5.7, 4.9, 5.3, etc.\\n\\nWhat stats terms to I need to look up to pursue this idea?  Density function? The normal curve estimates what boundaries the data generally fall within, but how do I translate that into simulation of subsequent data points?\\n\\nThanks for any guidance!",,
3330,1,1462,8bbc0652-1635-4883-bbec-775c8626cb0e,2010-08-09 18:53:30.0,6967.0,Using Std.Dev and Mean to generate hypothetical/additional data points?,,
3331,3,1462,8bbc0652-1635-4883-bbec-775c8626cb0e,2010-08-09 18:53:30.0,6967.0,<standard-deviation>,,
3332,2,1463,f0028404-7007-402a-adc4-51305d205024,2010-08-09 18:55:55.0,601.0,"You've stated something that is a classic counter argument to Bonferroni corrections.  Shouldn't I adjust my alpha criterion based on every test I will ever make?  This kind of ad absurdum implication is why some people do not believe in Bonferroni style corrections at all.  Sometimes the kind of data one deals with in their career is such that this is not an issue.  For judges who make one, or very few decisions on each new piece of evidence this is a very valid argument.  But what about the judge with 20 defendants and who is basing their judgment on a single large set of data (e.g. war tribunals)?\\n\\nYou're ignoring the kicks at the can part of the argument.  Generally scientists are looking for something — a p-value less than alpha.  Every attempt to find one is another kick at the can.  One will eventually find one if one takes enough shots at it.  Therefore, they should be penalized for doing that.\\n\\nThe way you harmonize these two arguments is to realize they are both true.  The simplest solution is to consider testing of differences within a single dataset as a kicks at the can kind of problem but that expanding the scope of correction outside that would be a slippery slope.  \\n\\nThis is a genuinely difficult problem in a number of fields, notably FMRI where there are thousands of data points being compared and there are bound to be some come up as significant by chance.  Given that the field has been historically very exploratory one has to do something to correct for the fact that hundreds of areas of the brain will look significant purely by chance.  Therefore, many methods of adjustment of criterion have been developed in that field.\\n\\nOn the other hand, in some fields one might at most be looking at 3 to 5 levels of a variable and always just test every combination if a significant ANOVA occurs.  This is known to have some problems (type 1 errors) but it's not particularly terrible.\\n\\nIt depends on your point of view.  The FMRI researcher recognizes a real need for a criterion shift.  The person looking at a small ANOVA may feel that there's clearly something there from the legitimate test and a bunch of t-tests are simply uncovering what that is.  The proper conservative point of view on the multiple comparisons is to always do something about them but only based on a single dataset.  Any new data resets the criterion... unless you're a Bayesian...\\n\\nAnd remember, it all gets worked out in the replications.",,
3333,2,1464,82559f45-41fd-49ae-b9e6-9dff17c1b408,2010-08-09 19:02:57.0,334.0,"You need a random number generator for the standard normal.  Either you can supply mean and standard deviation as arguments to the function, or you simply scale yourself by multiplying with the latter for the variability and adding the former for for the central location.\\n\\nHere is a quick example of the former approach:\\n\\n    > set.seed(42)\\n    > x <- rnorm(1000, 5.3, 1.7)        # 1000 draws of N(5.5, 1.7)\\n    > print(c(mean=mean(x), sd=sd(x)))\\n      mean     sd \\n    5.2561 1.7043 \\n    > \\n\\n",,
3334,2,1465,7cdac785-2033-47ff-a77b-2a5be0ebf1a7,2010-08-09 19:13:45.0,279.0,"I am not sure I understand your interpretation: a log-transformed predictor would imply that the effect is increasing with diminishing returns, while a quadratic function as a predictor would imply the existance of a peak in the effect (for ax^2+bx+c the peak is at -b/(2a)). I would assume the latter is less realistic in your context, and is also not supported by the better R-squared for the log-transformed predictor.",,
3335,2,1466,fc92907e-83ea-4215-ae1c-a3754ae39cfb,2010-08-09 19:13:48.0,56.0,"If you want a realistic simulation you need to find a distribution that describes the real process good enough (a model).\\n\\nWhen a real player makes a move he will on average (e.g.) throw `X` yards, with a standard deviation of `Y`. This does however not mean that the distribution of throws is a normal distribution. You should plot the (histogrammed) `throw` distribution and plot your *fit* (a Normal distribution with mean `X` and &sigma;=`Y`) and determine if it fits good enough. If not find a distribution that describes the real data better.\\n\\nOnce you have that down you need to generate random numbers from the distribution you determined.",,
3336,2,1467,c13540af-7470-466d-9687-c3123602f066,2010-08-09 19:53:30.0,291.0,"Of course you can use rnorm() in R, but it may be easier to understand how drawing from a pdf works by using the [probability integral transform][1].\\n\\nBasically, once we specify the structure of the pdf, we can transform this into a cdf (empirically, to ignore what the equation is), and because the values of the cdf have unique values from 0 to 1, we can back-calculate a draw from the original pdf by matching it with the cdf.\\n\\nThis way, you only need to have a RNG from 0 to 1, and the function of the pdf, and you're set. Here is the R code:\\n\\n    x <- seq(-4, 4, len = 1000)\\n    f <- function(x, mu = 0, sigma = 1) {\\n      out <- 1 / sqrt(2*pi*sigma^2) * exp(-(x - mu)^2 / (2*sigma^2))\\n      out\\n    }\\n    \\n    x.ecdf <- cumsum(f(x)) / sum(f(x))\\n    \\n    out <- vector()\\n    y <- runif(100)\\n    for (i in 1:length(y)) {\\n      out[i] <- which((y[i] - x.ecdf)^2 == min((y[i] - x.ecdf)^2))\\n    }\\n    \\n    par(mfrow = c(1,2))\\n    plot(x, x.ecdf)\\n    hist(x[out], breaks = 20)\\n![alt text][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution#Generating_values_from_normal_distribution\\n  [2]: http://probabilitynotes.files.wordpress.com/2010/08/rnormish.png",,
3337,5,1448,b82fbb46-af8d-4446-b726-4e7ef6a26924,2010-08-09 19:58:51.0,,"A mathematical demonstration of the relationship between the two is here: [Pearson's correlation and least squares regression analysis][1].\\n\\n<strike>I am not sure if there is a geometric or any other intuition that can be offered apart from the math but if I can think of one I will update this answer.</strike>\\n\\n**Update: Geometric Intuition**\\n\\nHere is a geometric intuition I came up with. Suppose that you have two variables $x$ and $y$ which are mean centered. (Assuming mean centered lets us ignore the intercept which simplifies the geometrical intuition a bit.) Let us first consider the geometry of linear regression. In linear regression, we model $y$ as follows:\\n\\n$y = x\\  \\beta + \\epsilon$.\\n\\nConsider the situation when we have two observations from the above data generating process given by the pairs ($y_1,y_2$) and ($x_1,x_2$). We can view them as vectors in two-dimensional space as shown in the figure below:\\n\\n![alt text][2]\\n \\nThus, in terms of the above geometry, our goal is to find a $\\beta$ such that the vector $x\\ \\beta$ is the closest possible to the vector $y$. Note that different choices of $\\beta$ scale $x$ appropriately. Let $\\hat{\\beta}$ be the value of $\\beta$ that is our best possible approximation of $y$ and denote $\\hat{y} = x\\ \\hat{\\beta}$. Thus,\\n\\n$y = \\hat{y} + \\hat{\\epsilon}$ \\n\\nFrom a geometrical perspective we have three vectors. $y$, $\\hat{y}$ and $\\hat{\\epsilon}$. A little thought suggests that we must choose $\\hat{\\beta}$ such that three vectors look like the one below:\\n![alt text][3]\\n\\n\\nIn other words, we need to choose $\\beta$ such that the angle between $x\\ \\beta$ and $\\hat{\\epsilon}$ is 90<sup>0</sup>. \\n\\nSo, how much variation in $y$ have we explained with this projection of $y$ onto the vector $x$. Since the data is mean centered the variance in $y$ is equals ($y_1^2+y_2^2$) which is the square of the distance between the point represented by the point $y$ and the origin. The variation in $\\hat{y}$ is similarly the distance from the point $\\hat{y}$ and the origin and so on.\\n\\nBy the Pythagorean theorem, we have:\\n\\n$y^2 = \\hat{y}^2 + \\hat{\\epsilon}^2$\\n\\nTherefore, the proportion of the variance explained by $x$ is $\\frac{\\hat{y}^2}{y^2}$. Notice also that $cos(\\theta) = \\frac{\\hat{y}}{y}$.  and the wiki tells us that the [geometrical interpretation of correlation][4] is that correlation equals the cosine of the angle between the mean-centered vectors.\\n\\nTherefore, we have the required relationship:\\n\\n(Correlation)<sup>2</sup> = Proportion of variation in $y$ explained by $x$.\\n\\nHope that helps.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Pearson.27s_correlation_and_least_squares_regression_analysis\\n  [2]: http://a.imageshack.us/img202/669/linearregression1.png\\n  [3]: http://a.imageshack.us/img19/9524/intuitionlinearregressi.png\\n  [4]: http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Geometric_interpretation",added geometrical intuition,user28
3338,5,1467,b26f1adb-1dc6-4cbf-a0ec-c42dd0afc35a,2010-08-09 20:28:40.0,291.0,"Of course you can use rnorm() in R, but it may be easier to understand how drawing from a pdf works by using the [probability integral transform][1].\\n\\nBasically, once we specify the structure of the pdf, we can transform this into a cdf (empirically, to ignore what the equation is), and because the values of the cdf have unique values from 0 to 1, we can back-calculate a draw from the original pdf by matching random draws from 0 to 1, with the cdf.\\n\\nThis way, you only need to have a RNG from 0 to 1, and the function of the pdf, and you're set. Here is the R code:\\n\\n    x <- seq(-4, 4, len = 1000)\\n    f <- function(x, mu = 0, sigma = 1) {\\n      out <- 1 / sqrt(2*pi*sigma^2) * exp(-(x - mu)^2 / (2*sigma^2))\\n      out\\n    }\\n    \\n    x.ecdf <- cumsum(f(x)) / sum(f(x))\\n    \\n    out <- vector()\\n    y <- runif(100)\\n    for (i in 1:length(y)) {\\n      out[i] <- which((y[i] - x.ecdf)^2 == min((y[i] - x.ecdf)^2))\\n    }\\n    \\n    par(mfrow = c(1,2))\\n    plot(x, x.ecdf)\\n    hist(x[out], breaks = 20)\\n![alt text][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution#Generating_values_from_normal_distribution\\n  [2]: http://probabilitynotes.files.wordpress.com/2010/08/rnormish.png",added 23 characters in body,
3339,2,1468,045117f2-8a77-4123-88a9-288586a62bce,2010-08-09 21:18:01.0,223.0,"\\n **To fix ideas:** I will take the case when you obverse,  $n$ independent random variables $(X_i)_{i=1,\\dots,n}$ such that for $i=1,\\dots,n$ $X_i$ is drawn from $\\mathcal{N}(\\theta_i,1)$. I assume that you want to know which one have non zero mean, formally you want to test:\\n\\n$H_{0i} : \\theta_i=0$ Vs $H_{1i} : \\theta_i=0$\\n\\n **Definition of a threshold:** You have $n$ decisions to make and you may have different aim. For a given test $i$ you are certainly going to choose a threshold $\\tau_i$ and decide not to accept $H_{0i}$ if $|X_i|>\\tau_i$. \\n\\n**Different options:** You have to choose the thresholds $\\tau_i$ and for that you have two **options**:\\n\\n 1.  choose the **same threshold** for everyone\\n\\n 2. to choose  **a different threshold** for everyone (most often a datawise threshold, see below). \\n \\n**Different aims:** These options can be driven for different **aims** such as\\n\\n - Controling the probability to reject wrongly $H_{0i}$ for one or more than one $i$.\\n - Controlling the expectation of the false alarm ratio (or False Discovery Rate) \\n\\n What ever is your aim at the end, it is a good idea to use a datawise threshold. \\n\\n**My answer to your question:** your intuition is related to the main heuristic for choosing a datawise threshold. It is the following (at the origin of Holm's procedure which is more powerfull than Bonferoni): \\n\\n Imagine you have already taken a decision for the $p$ lowest $|X_{i}|$ and the decision is to accept $H_{0i}$  for all of them. Then you only have to make $n-p$ comparisons and you haven't taken any risk to reject $H_{0i}$ wrongly ! Since you haven't used your budget, you may take a little more risk for the remaining test and choose a larger threshold. \\n\\n**In the case of your judges:** I assume (and I guess you should do the same) that both judge have the same budgets of false accusation for their life.  The 60 years old judge may be less conservative if, in the past, he did not accuse anyone ! But if he already made a lot of accusation he will be more conservative and maybe even more than the youndest judge. ",,
3340,2,1469,04e153b5-a694-4a15-878e-e9c6df26621d,2010-08-09 21:55:08.0,511.0,Can someone recommend a text with derivations of classical estimator efficiency results? I'm particularly interested in likelihood and pseudo-likelihood estimators for multi-variate discrete models,,
3341,1,1469,04e153b5-a694-4a15-878e-e9c6df26621d,2010-08-09 21:55:08.0,511.0,A Primer on Estimator Efficiency?,,
3342,3,1469,04e153b5-a694-4a15-878e-e9c6df26621d,2010-08-09 21:55:08.0,511.0,<estimation><efficiency>,,
3343,4,1455,d438553d-a9fa-4985-abc5-419adff52fad,2010-08-09 22:15:29.0,159.0,What is the distribution of OR (odds ratio) ?,edited tags; edited title; edited tags,
3344,6,1455,d438553d-a9fa-4985-abc5-419adff52fad,2010-08-09 22:15:29.0,159.0,<distributions><odds-ratio>,edited tags; edited title; edited tags,
3345,2,1470,415c4c5f-0714-4e8a-a02c-33f4127a7bf1,2010-08-09 22:18:22.0,561.0,"Related to the comment earlier, what the fMRI researcher should remember is that clinically-important outcomes are what matter, not the density shift of a single pixel on a fMRI of the brain. If it doesn't result in a clinical improvement/detriment, it doesn't matter. That is one way of reducing the concern about multiple comparisons.\\n\\nSee also:\\n\\n 1. Bauer, P. (1991). Multiple testing in clinical trials. Stat Med, 10(6), 871-89; discussion 889-90.\\n 2. Proschan, M. A. & Waclawiw, M. A. (2000). Practical guidelines for multiplicity adjustment in clinical trials. Control Clin Trials, 21(6), 527-39.\\n 3. Rothman, K. J. (1990). No adjustments are needed for multiple comparisons. Epidemiology (Cambridge, Mass.), 1(1), 43-6.\\n 4. Perneger, T. V. (1998). What's wrong with bonferroni adjustments. BMJ (Clinical Research Ed.), 316(7139), 1236-8.\\n	",,
3346,2,1471,e23c37d9-0b86-4fc8-862c-5b3d253284c0,2010-08-09 22:25:11.0,840.0,"I need to analyze with R the data from a medical survey (with 100+ coded columns) that comes in a CSV. I will use [rattle][1] for some initial analysis but behind the scenes it's still R.\\n\\nIf I *read.csv()* the file, columns with numerical codes are treated as numerical data. I'm aware I could create categorical columns from them with *factor()* but doing it for 100+ columns is a pain. \\n\\nI hope there is a better way to tell R to import the columns directly as factors. Or to at least to convert them in place afterwards.\\n\\nThank you!\\n\\n\\n  [1]: http://rattle.togaware.com/",,
3347,1,1471,e23c37d9-0b86-4fc8-862c-5b3d253284c0,2010-08-09 22:25:11.0,840.0,Is it possible to directly read CSV columns as categorical data ?,,
3348,3,1471,e23c37d9-0b86-4fc8-862c-5b3d253284c0,2010-08-09 22:25:11.0,840.0,<r><categorical-data><data-transformation>,,
3349,2,1472,c72dd157-63cc-4e7e-a6e8-e6e8b53a4111,2010-08-09 22:27:53.0,159.0,"The classic approach, described in [Box, Jenkins & Reinsell (4th ed, 2008)][1] involves looking at the cross-correlation function and the various auto-correlation functions, and making a lot of subjective decisions about the orders and lags for the various terms. The approach works ok for a single predictor, but is not really suitable for multiple predictors.\\n\\nAn alternative approach, described in [Pankratz (1991)][2], involves fitting lagged regressions with AR errors and determining the appropriate rational lag structure from the fitted coefficients (also a relatively subjective process). Then refitting the entire model with the supposed lag structures and extracting the residuals. The order of the ARMA error process is determined from these residuals (using AIC for example). Then the final model is re-estimated. This approach works well for multiple predictors, and is considerably simpler to apply than the classic approach.\\n\\nI wish I could say there was this neat automated procedure that did it all for you, but I can't. At least not yet.\\n\\n\\n\\n  [1]: http://www.amazon.com/Time-Analysis-Forecasting-Probability-Statistics/dp/0470272848\\n  [2]: http://www.amazon.com/Forecasting-Dynamic-Regression-Models-Pankratz/dp/0471615285/",,
3350,2,1473,d0dc8f7d-1d8a-457c-b23f-e7445dfb2446,2010-08-09 22:31:23.0,251.0,"You can use the `colClasses` argument to specify the classes of your data columns.  For example:\\n\\n    data <- read.csv('foo.csv', colClasses=c('numeric', 'factor', 'factor'))\\n\\nwill assign numeric to the first column, factor to the second and third.",,
3351,5,1442,1347585a-f4de-47c2-8ed9-b32bceb02056,2010-08-09 22:33:02.0,159.0,"To assess the historical trend, I'd use a gam with trend and seasonal components. For example\\n\\n    require(mgcv)\\n    require(forecast)\\n    x <- ts(rpois(100,1+sin(seq(0,3*pi,l=100))),f=12)\\n    tt <- 1:100\\n    season <- seasonaldummy(x)\\n    fit <- gam(x ~ s(tt,k=5) + season, family=""poisson"")\\n    plot(fit)\\n\\nThen `summary(fit)` will give you a test of significance of the change in trend and the plot will give you some confidence intervals. The assumptions here are that the observations are independent and the conditional distribution is Poisson. Because the mean is allowed to change smoothly over time, these are not particularly strong assumptions.\\n\\nTo forecast is more difficult as you need to project the trend into the future. If you are willing to accept a linear extrapolation of the trend at the end of the data (which is certainly dodgy but probably ok for a few months), then use\\n\\n    fcast <- predict(fit,se.fit=TRUE,\\n                   newdata=list(tt=101:112,season=seasonaldummyf(x,h=12)))\\n\\nTo see the forecasts on the same graph:\\n\\n    plot(x,xlim=c(0,10.5))\\n    lines(ts(exp(fcast$fit),f=12,s=112/12),col=2)\\n    lines(ts(exp(fcast$fit-2*fcast$se),f=12,s=112/12),col=2,lty=2)\\n    lines(ts(exp(fcast$fit+2*fcast$se),f=12,s=112/12),col=2,lty=2)\\n\\n",added 1 characters in body,
3352,2,1474,ecb3f0a6-65e5-4ec7-94e2-aae2e620a5e7,2010-08-09 22:33:37.0,291.0,"or just do it after you read the data\\n\\n    dat <- read.csv(""kdfjdkf"")\\n    apply(dat, 2, factor)\\n\\nthough this type of Q is probably more fit for [Stack Overflow][1].\\n\\n\\n  [1]: http://stackoverflow.com/questions/tagged/r",,
3353,2,1475,0c057939-f0f4-4ecb-9b23-8bf7b544f0ef,2010-08-09 22:33:40.0,,"I want to cluster ~22000 points. Many clustering algorithms work better with higher quality initial guesses. What tools exist that can give me a good idea of the rough shape of the data?\\n\\nI do want to be able to choose my own distance metric, so a program I can feed a list of pairwise distances to would be just fine. I would like to be able to do something like highlight a region or cluster on the display and get a list of which data points are in that area.\\n\\nFree software preferred, but I do already have SAS and MATLAB. ",,anonymous
3354,1,1475,0c057939-f0f4-4ecb-9b23-8bf7b544f0ef,2010-08-09 22:33:40.0,,visualization software for clustering,,anonymous
3355,3,1475,0c057939-f0f4-4ecb-9b23-8bf7b544f0ef,2010-08-09 22:33:40.0,,<data-visualization><clustering><software>,,anonymous
3356,5,1450,a3452901-ce4c-4725-8bc9-cd716fbe227e,2010-08-09 22:36:46.0,485.0,"Start with the basic idea of variation.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variation that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variation in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variation is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another measure of variability after you have the regression line from the model.  This is the difference between each Y point and the regression line.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line) squared.  \\n\\nIf the regression line is anything but horizontal, we're going to get less total distance when we use that instead of the mean. The ratio between the variation explained and the original variation is your R^2.  It's the proportion of the original variation in your response that is explained by fitting that regression line.\\n\\nSee the (quick and dirty) image below...\\n\\n![http://img217.imageshack.us/img217/5536/rsquared.png][1]\\n\\nUploaded with [URL=http://imageshack.us]ImageShack.us[/URL]\\n\\nHere is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n\\n  [1]: http://img217.imageshack.us/img217/5536/rsquared.png",added 9 characters in body,
3357,5,1452,e52c995f-9ae7-465e-9f8d-eb5175411b0c,2010-08-09 23:35:40.0,251.0,"The log transforms with shifts are special cases of the [Box-Cox transformations][1]:\\n\\n$y(\\lambda_{1}, \\lambda_{2}) = \\n\\begin{cases} \\n \\frac {(y+\\lambda_{2})^{\\lambda_1} - 1} {\\lambda_{1}} & \\mbox{when } \\lambda_{1} \\neq 0 \\\\\\ \\log (y + \\lambda_{2}) & \\mbox{when } \\lambda_{1} = 0\\n\\end{cases}$\\n\\nThese are the extended form for negative values, but also applicable to data containing zeros.  Box and Cox (1964) presents an algorithm to find appropriate values for the $\\lambda$'s using maximum likelihood.  This gives you the ultimate transformation.  \\n\\nA reason to prefer Box-Cox transformations is that they're developed to ensure assumptions for the linear model.  There's some work done to show that even if your data cannot be transformed to normality, then the estimated $\\lambda$ still lead to a symmetric distribution.\\n\\nI'm not sure how well this addresses your data, since it could be that $\\lambda = (0, 1)$ which is just the log transform you mentioned, but it may be worth estimating the requried $\\lambda$'s to see if another transformation is appropriate.\\n\\nIn R, the [`boxcox`][2] function in package `MASS` will compute the parameters for you.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation\\n  [2]: http://stat.ethz.ch/R-manual/R-devel/library/MASS/html/boxcox.html\\n",added 172 characters in body,
3358,2,1476,c9aed120-b22e-4be3-b02b-c5c72f08d1f6,2010-08-09 23:39:55.0,319.0,"Well-respected statisticians have taken a wide variety of positions on multiple comparisons.  It's a subtle subject. If someone thinks it's simple, I'd wonder how much they've thought about it.\\n\\nHere's an interesting Bayesian perspective on multiple testing from Andrew Gelman: [Why we don't (usually) worry about multiple comparisons][1].  \\n  [1]: http://www.stat.columbia.edu/~cook/movabletype/archives/2008/03/why_i_dont_usua_1.html",,
3359,5,1474,b3f9c764-4eae-40ed-82cd-1c4c4aa13924,2010-08-09 23:48:44.0,291.0,"or just do it after you read the data\\n\\n    dat <- read.csv(""kdfjdkf"")\\n    apply(dat, 2, factor)\\n\\n<strike>though this type of Q is probably more fit for [Stack Overflow][1]</strike>.\\n\\n**edit**: see below.\\n\\n\\n  [1]: http://stackoverflow.com/questions/tagged/r",added 41 characters in body,
3360,5,1473,e3481977-1b6b-4172-9ce5-73fb3ae968a4,2010-08-10 00:25:34.0,251.0,"You can use the `colClasses` argument to specify the classes of your data columns.  For example:\\n\\n    data <- read.csv('foo.csv', colClasses=c('numeric', 'factor', 'factor'))\\n\\nwill assign numeric to the first column, factor to the second and third.  Since you have so many columns, a shortcut might be:\\n\\n    data <- read.csv('foo.csv', colClasses=c('numeric', rep('factor', 37), 'character'))\\n\\nor some such variation (i.e. assign numeric to first column, factor to next 37 columns, then character to the last one).",added 270 characters in body,
3361,2,1477,fe380c51-cc8f-4471-87c6-e7f812b752a6,2010-08-10 00:32:44.0,,If you have a reasonable hunch about the data generating process that is responsible for the data in question then you could use bayesian ideas to estimate the missing data. Under the bayesian approach you would simply assume that the missing data are also random variables and construct the posterior for the missing data conditional on the observed data. The posterior means would then be used as a substitute for the missing data. \\n\\nThe use of bayesian models may qualify as imputation under a broad sense of the term but I thought of mentioning it as it did not appear on your list.,,user28
3362,2,1478,42793a32-bcbd-4ad7-a3ad-79e43d181e01,2010-08-10 00:33:26.0,795.0,"\\nI am trying to write unit tests for a whole mess of statistics code. Some of the unit tests take the form: generate a sample following a null hypothesis, use code to get a p-value under that null, repeat hundreds of times, then look at all the p-values: if they are reasonably uniform, then the code passes. I usually check if the ratio of p-values < $\\alpha$ is near $\\alpha$ for $\\alpha = 0.01, 0.05, 0.1$. But I am usually also interested in whether the code deviates from uniformity. I usually test this with the Anderson-Darling test. \\n\\n\\nHere is where I have a circularity problem: how can I unit test my Anderson-Darling code? I can easily feed it uniformly generated variables, and get a p-value, repeat hundreds of times, but then I just have a bunch of p-values. I can q-q plot them, but I'm more interested in an automatic unit test I can run. What are some basic sanity checks I can implement automatically? there is the naive check of ratio of p-values < $\\alpha$ noted above. I can also implement a K-S test. What else can I easily check for?\\n\\n(I realize this question may seem hopelessly pedantic or naive...)",,
3363,1,1478,42793a32-bcbd-4ad7-a3ad-79e43d181e01,2010-08-10 00:33:26.0,795.0,testing implementation of Anderson-Darling test for uniform RV,,
3364,3,1478,42793a32-bcbd-4ad7-a3ad-79e43d181e01,2010-08-10 00:33:26.0,795.0,<hypothesis-testing><code><uniform>,,
3365,2,1479,8907ad8a-dd3b-4c08-a236-81f72eb094f8,2010-08-10 00:41:01.0,,"You could test your Anderson-Darling code using data that is generated from an external library. However, you then run into the issue of how to test/trust the external library. At some point you have to trust that well established libraries are error free and that their output can be relied on. \\n\\nOnce you have the Anderson-Dalring code tested against data generated from an external library the circularity will be broken and you can rely on your own code if it passes the Anderson-Darling tests. The same will hold for the K-S (I presume Kolmogorov–Smirnov) test.",,user28
3366,2,1480,282d4664-301c-44f1-84b5-87dded71dc07,2010-08-10 00:43:08.0,394.0,"I might be a little unorthodox here, but what the heck. ***Please note:* this line of thought comes from my own philosophy for classification, which is that I use it when my purpose is squarely on pure prediction -- not explanation, conceptual coherence, etc. Thus, what I'm saying here contradicts how I'd approach building a regression model.** \\n\\nDifferent classification approaches vary in their capability to handle missing data, and depending on some other factors^, I might just try #5: use a classifier that won't choke on those NAs. Part of the decision to go that route might also include thinking about how likely a similar proportion of NAs are to occur in the future data to which you'll be applying the model. If NAs for certain variables are going to be par for the course, then it might make sense to just roll with them (i.e., don't build a predictive model that assumes more informative data than what you'll actually have, or you'll be kidding yourself about how predictive it really is going to be). In fact, if I'm not convinced that NAs are missing at random, I'd be inclined to recode a new variable (or a new level if it's missing in a categorical variable) just to see if the missingness itself is predictive.\\n\\nIf I had a good reason to use a classifier that did not take missing data very well, then my approach would be #1 (multiple imputation), seeking to find a classification model that behaved similarly well across imputed data sets.\\n\\n^Including: how much missingness you have in your predictors, whether there are systematic patterns (if there are, it would be worth taking a closer look and thinking through the implications for your analysis), and how much data you have to work with overall.",,
3367,5,1435,2a9724b2-2e5f-4ff5-ad53-a626a6d775b5,2010-08-10 01:02:43.0,521.0,"The easiest residuals to understand are the deviance residuals as when squared these sum to -2 times the log-likelihood. In its simplest terms logistic regression can be understood in terms of fitting the function p = inv-logit(X*beta) for known X in such a way as to minimise the total deviance, which is the sum of squared deviance residuals of all the data points.\\n\\nThe (squared) deviance of each data point is equal to (-2 times) the logarithm of the difference between its predicted probability (inv-logit(X*beta))  and the complement of its actual value (1 for a control; a 0 for a case) in absolute terms. A perfect fit of a point (which never occurs) gives a deviance of zero as log(1) is zero. A poorly fitting point has a large residual deviance as -2 times the log of a very small value is a large number. \\n\\nDoing logistic regression is akin to finding a beta value such that the sum of squared deviance residuals is minimised. \\n\\nThis can be illustrated with a plot, but I don't know how to upload one.\\n\\n\\n",added 8 characters in body,
3368,2,1481,00fdaf04-58d1-4376-84dc-776d0fe1470c,2010-08-10 01:20:52.0,226.0,"If you have all the relevant data rather than just summary data such as mean, SD etc. you could create your own distribution model from the real life data you have. Sort the data (y values) from lowest to highest and equally space them between 0 and 1 (x values). Then solve to find the coefficients of an nth order polynomial curve fit to the data (or several in piecewise fashion over parts of the data if necessary). Once you have these, it would be a simple matter to use a uniform random number generator to generate an x value between 0 and 1, and to plug this value into the polynomial equation to get a random y value that would approximate a draw from your distribution.",,
3369,5,1425,d9f2eb97-1cc9-4d57-919e-27378d06746c,2010-08-10 01:46:18.0,805.0,"You should not do a calculation of probability for an event deemed surprising *post hoc* as if it were an event specified before it was rolled (observed).\\n\\nIt's very difficult to to do a proper calculation of *post hoc* probability, because what *other* events would have been deemed at least as surprising depends on what the context is. \\n\\nWould three ones twice in a row at an earlier or later stage of the game have been as surprising? Would *you* rolling three ones have been as surprising as him rolling them? Would three sixes be as surprising as three ones? and so on... What is the totality of all the events would have been surprising enough to generate a post like this one?\\n\\n(Even if it were legitimate to do the calculation *as if it were a pre-specified event*, it looks like you have that calculation incorrect. Specifically, the probability (*for an event specified before the roll*) of taking three dice and rolling (1,1,1) is (1/6)^3 = 1/216, because the three rolls are independent, not 1/56, and the probability of doing it twice *out of a total of two rolls* is the square of that - but neither the condition of being pre-specified nor the ""out of two rolls"" hold)\\n\\n\\n",(improved formatting),
3370,5,1452,31951460-efbd-423d-83e3-451520be2559,2010-08-10 01:59:44.0,251.0,"The log transforms with shifts are special cases of the [Box-Cox transformations][1]:\\n\\n$y(\\lambda_{1}, \\lambda_{2}) = \\n\\begin{cases} \\n \\frac {(y+\\lambda_{2})^{\\lambda_1} - 1} {\\lambda_{1}} & \\mbox{when } \\lambda_{1} \\neq 0 \\\\\\ \\log (y + \\lambda_{2}) & \\mbox{when } \\lambda_{1} = 0\\n\\end{cases}$\\n\\nThese are the extended form for negative values, but also applicable to data containing zeros.  Box and Cox (1964) presents an algorithm to find appropriate values for the $\\lambda$'s using maximum likelihood.  This gives you the ultimate transformation.  \\n\\nA reason to prefer Box-Cox transformations is that they're developed to ensure assumptions for the linear model.  There's some work done to show that even if your data cannot be transformed to normality, then the estimated $\\lambda$ still lead to a symmetric distribution.\\n\\nI'm not sure how well this addresses your data, since it could be that $\\lambda = (0, 1)$ which is just the log transform you mentioned, but it may be worth estimating the requried $\\lambda$'s to see if another transformation is appropriate.\\n\\nIn R, the [`boxcox.fit`][2] function in package `geoR` will compute the parameters for you.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation\\n  [2]: http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/geoR/html/boxcox.fit.html\\n",added 16 characters in body,
3371,2,1482,452b7cad-f6fe-4e09-9c04-e798e5b681a0,2010-08-10 02:54:09.0,251.0,"Take a look at [Cluster 3.0][1].  I'm not sure if it will do all you want, but it's pretty well documented and lets you choose from a few distance metrics.  The visualization piece is through a separate program called [Java TreeView][2] ([screenshot][3]).\\n\\n  [1]: http://bonsai.hgc.jp/~mdehoon/software/cluster/\\n  [2]: http://jtreeview.sourceforge.net/\\n  [3]: http://sourceforge.net/dbimage.php?id=20379\\n\\n\\n",,
3372,5,1450,49f6f02f-33a5-42fc-a3c3-6c1c52bf681f,2010-08-10 03:51:55.0,485.0,"Start with the basic idea of variation.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variation that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variation in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variation is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another measure of variability after you have the regression line from the model.  This is the difference between each Y point and the regression line.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line) squared.  \\n\\nIf the regression line is anything but horizontal, we're going to get less total distance when we use that instead of the mean--that is there is less unexplained variation.  The ratio between the extra variation explained and the original variation is your R^2.  It's the proportion of the original variation in your response that is explained by fitting that regression line.\\n\\nSee the (quick and dirty) image below...\\n\\n![http://img217.imageshack.us/img217/5536/rsquared.png][1]\\n\\nUploaded with [URL=http://imageshack.us]ImageShack.us[/URL]\\n\\nHere is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n\\n  [1]: http://img217.imageshack.us/img217/5536/rsquared.png",added 52 characters in body,
3373,5,1450,9200240b-c494-4b45-8ad6-2a449671adef,2010-08-10 04:00:36.0,159.0,"Start with the basic idea of variation.  Your beginning model is the sum of the squared deviations from the mean.  The R^2 value is the proportion of that variation that is accounted for by using an alternative model.  For example, R-squared tells you how much of the variation in Y you can get rid of by summing up the squared distances from a regression line, rather than the mean.\\n\\nI think this is made perfectly clear if we consider that regression problem and imagine that plotted out.  Imagine a typical scatterplot where you have a predictor X along the horizontal axis and a response Y along the vertical axis.\\n\\nThe mean is a horizontal line on the plot where Y is constant.  The Y variation is the sum of squared differences between the mean of Y and each individual data point.  It's the distance between the mean line and every individual point squared and added up.  \\n\\nYou can also calculate another measure of variability after you have the regression line from the model.  This is the difference between each Y point and the regression line.  Rather than each (Y - the mean) squared we get (Y - the point on the regression line) squared.  \\n\\nIf the regression line is anything but horizontal, we're going to get less total distance when we use that instead of the mean--that is there is less unexplained variation.  The ratio between the extra variation explained and the original variation is your R^2.  It's the proportion of the original variation in your response that is explained by fitting that regression line.\\n\\nSee the (quick and dirty) image below...\\n\\n![http://img217.imageshack.us/img217/5536/rsquared.png][1]\\n\\nUploaded with [ImageShack.us][2].\\n\\nHere is some R code for a basic graph with mean and the regression line plotted to help visualize, but without some helpful notes:\\n\\n    data(trees)\\n    plot((trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~trees$Girth))\\n    abline(lm(trees$Volume~1))\\n\\n\\n  [1]: http://img217.imageshack.us/img217/5536/rsquared.png\\n  [2]: http://imageshack.us",added 3 characters in body,
3374,2,1483,ffef3249-8293-471d-a941-52514b45dbdb,2010-08-10 04:19:05.0,251.0,"Assuming the odds ratios are independent, you can proceed as you would in general with any estimate, only you have to look at the log odds.\\n\\nTake the difference of the log odds, $\\delta$.  The standard error of $\\delta$ is $\\sqrt{SE_{1}^2 + SE_{2}^2}$.  Then you can obtain a p-value for the ratio $z = \\delta/SE(\\delta)$ from the standard normal.\\n\\n",,
3375,5,1468,0e51a066-60ea-4246-8064-a836f3ffadff,2010-08-10 05:10:38.0,223.0," **To fix ideas:** I will take the case when you obverse,  $n$ independent random variables $(X_i)_{i=1,\\dots,n}$ such that for $i=1,\\dots,n$ $X_i$ is drawn from $\\mathcal{N}(\\theta_i,1)$. I assume that you want to know which one have non zero mean, formally you want to test:\\n\\n$H_{0i} : \\theta_i=0$ Vs $H_{1i} : \\theta_i\\neq 0$\\n\\n **Definition of a threshold:** You have $n$ decisions to make and you may have different aim. For a given test $i$ you are certainly going to choose a threshold $\\tau_i$ and decide not to accept $H_{0i}$ if $|X_i|>\\tau_i$. \\n\\n**Different options:** You have to choose the thresholds $\\tau_i$ and for that you have two **options**:\\n\\n 1.  choose the **same threshold** for everyone\\n\\n 2. to choose  **a different threshold** for everyone (most often a datawise threshold, see below). \\n \\n**Different aims:** These options can be driven for different **aims** such as\\n\\n - Controling the probability to reject wrongly $H_{0i}$ for one or more than one $i$.\\n - Controlling the expectation of the false alarm ratio (or False Discovery Rate) \\n\\n What ever is your aim at the end, it is a good idea to use a datawise threshold. \\n\\n**My answer to your question:** your intuition is related to the main heuristic for choosing a datawise threshold. It is the following (at the origin of Holm's procedure which is more powerfull than Bonferoni): \\n\\n Imagine you have already taken a decision for the $p$ lowest $|X_{i}|$ and the decision is to accept $H_{0i}$  for all of them. Then you only have to make $n-p$ comparisons and you haven't taken any risk to reject $H_{0i}$ wrongly ! Since you haven't used your budget, you may take a little more risk for the remaining test and choose a larger threshold. \\n\\n**In the case of your judges:** I assume (and I guess you should do the same) that both judge have the same budgets of false accusation for their life.  The 60 years old judge may be less conservative if, in the past, he did not accuse anyone ! But if he already made a lot of accusation he will be more conservative and maybe even more than the youndest judge. ",added 2 characters in body,
3376,5,812,a9044480-8361-4f04-a0ad-2ce8fc620b11,2010-08-10 06:01:36.0,223.0,"There are a lot of references in the statistic literature to ""**functional data**"" (i.e. data that are curves), and in parallel, to ""**high dimensional data**"" (i.e. when data are high dimensional vectors). My question is about the difference between the two type of data. \\n\\nWhen talking about applied statistic methodologies that apply in case 1 can be understood as a rephrasing of methodologies from case 2 through a projection into a finite dimensional subspace of a space of functions, it can be polynomes, splines, wavelet, Fourier, .... and will translate the functional problem into a finite dimensional vectorial problem (since in applied mathematic everything comes to be finite at some point). \\n\\n**My question is:**  can we say that any statistical procedure that applies to functional data  can also be applied (almost directly) to high dimension data and that any procedure dedicated to high dimensional data can be (almost directly) applied to functional data ? \\n\\n\\nIf the answer is no, can you illustrate ?\\n\\n\\nEDIT/UPDATE with the help of Simon Byrne's answer:\\n\\n - sparsity (S-sparse assumption, $l^p$ ball and weak $l^p$ ball for $p<1$) is used as a structural assumption in high dimensional statistical analysis. \\n - ""smoothness"" is used as a structural assumption in functional data analysis. \\n\\nOn the other hand, inverse Fourier transform and inverse wavelet transform are transforming sparcity into smoothness, and smoothness is transformed into sparcity by wavelet and fourier transform. This make the critical difference mentionned by Simon not so critical?",added 86 characters in body,
3377,5,1150,91f77279-0e59-4824-b4b9-17de5c4116d4,2010-08-10 06:07:48.0,159.0,"Consider the simplest case where $Y$ is regressed against $X$ and $Z$ and where $X$ and $Z$ are highly positively correlated. Then the effect of $X$ on $Y$ is hard to distinguish from the effect of $Z$ on $Y$ because any increase in $X$ tends to be associated with an increase in $Z$. \\n\\nAnother way to look at this is to consider the equation. If we write $Y = b_0 + b_1X + b_2Z + e$, then the coefficient $b_1$ is the increase in $Y$ for every unit increase in $X$ while holding $Z$ constant. But in practice, it is often impossible to hold $Z$ constant and the positive correlation between $X$ and $Z$ mean that a unit increase in $X$ is usually accompanied by some increase in $Z$ at the same time.\\n\\nA similar but more complicated explanation holds for other forms of multicollinearity.",edited body,
3378,5,962,fb0055a1-cc74-4971-a60c-af2f7dbf3131,2010-08-10 06:08:14.0,223.0,"Wavelet is usefull to detect singularities in a signal (see for example the paper http://www.math.univ-toulouse.fr/~bigot/Site/Publications_files/Spectrometry.pdf (see figure 3 for an illustration) and the references mentionned in the paper.  I guess singularities are sometime anomaly? \\n\\nThe idea here is that the Continuous wavelet transform **(CWT)** has maxima lines that propagates along frequencies, the longuer the line is the higher is the singularity. See Figure 3 in the paper to see what I mean ! note that there is a free matlab code related to that paper... \\n\\n\\n----------\\n\\n Additionally, I can give you some heuristic about **why** the DISRCETE (preceding example is about the continuous one) wavelet transform (**DWT**) **is interesting for a statistician** (excuse non exhaustivity) :   \\n\\n-	There is a wide class of (realistic (Besov space)) signal that are transformed into a **sparse** sequence by the wavelet transform. (**compression property**)\\n-	A wide class of (quasi stationary) process that are transform into a sequence with almost uncorrelated features (**decorrelation property**)\\n-	 Wavelet coefficients contain are containing an information that is localized in **time and in frequency** (at different scales). (multiscale property)\\n-	Wavelet coefficients of a signal **concentrate on its singularities**. \\n\\n\\n",edited body,
3379,2,1484,56b7eee1-87b1-4849-a9e9-bf36eef071c7,2010-08-10 06:19:14.0,339.0,Exploring clustering results in high dimensions can be done in [R][1] using the packages [clusterfly][2] and [gcExplorer][3]. Look for more [here][4].\\n\\n\\n  [1]: http://www.r-project.org/\\n  [2]: http://had.co.nz/model-vis/\\n  [3]: http://cran.r-project.org/web/packages/gcExplorer/index.html\\n  [4]: http://cran.r-project.org/web/views/Cluster.html,,
3380,2,1485,74c268bd-520e-4685-ab38-a03a6fdc1534,2010-08-10 06:41:06.0,851.0,"In a particular application I was in need of machine learning (I know the things I studied in my undergraduate course). I used Support Vector Machines and got the problem solved. Its working fine.\\n\\nNow I need to improve the system. Problems here are\\n\\n 1. I get additional training examples every week. Right now the system starts training freshly with updated examples (old examples + new examples). I want to make it incremental learning. Using previous knowledge (instead of previous examples) with new examples to get new model (knowledge)\\n\\n 2. Right my training examples has 3 classes. So, every training example is fitted into one of these 3 classes. I want functionality of ""Unknown"" class. Anything that doesn't fit these 3 classes must be marked as ""unknown"". But I can't treat ""Unknown"" as a new class and provide examples for this too.\\n\\n 3. Assuming, the ""unknown"" class is implemented. When class is ""unknown"" the user of the application inputs the what he thinks the class might be. Now, I need to incorporate the user input into the learning. I've no idea about how to do this too. Would it make any difference if the user inputs a new class (i.e.. a class that is not already in the training set)?\\n\\n\\nDo I need to choose a new algorithm or Support Vector Machines can do this?\\n\\nPS: I'm using libsvm implementation for SVM.",,
3381,1,1485,74c268bd-520e-4685-ab38-a03a6fdc1534,2010-08-10 06:41:06.0,851.0,few problems in machine learning,,
3382,3,1485,74c268bd-520e-4685-ab38-a03a6fdc1534,2010-08-10 06:41:06.0,851.0,<machine-learning><svm>,,
3383,2,1486,684f7e26-c3c7-4010-b183-67f9374acb62,2010-08-10 06:42:10.0,438.0,"Conventional practice is to use the non-parametric statistics ***rank sum*** and ***mean rank*** to describe ordinal data.\\n\\nHere's how they work:\\n\\n**Rank Sum**\\n\\n - assign a rank to each member in each\\n   group;\\n\\n - e.g., suppose you are looking at goals for each\\n   player on two opposing football\\n   teams then rank each member on\\n   *both* teams from first to last;\\n\\n - calculate rank sum by adding the ranks *per group*;\\n\\n - the magnitude of the rank sum tells\\n   you how close together the ranks are\\n   for each group\\n<br><br>\\n\\n**Mean Rank**\\n\\nM/R is a more sophisticated statistic than R/S because it compensates for unequal sizes in the groups you are comparing. Hence, in addition to the steps above, you divide each sum by the number of members in the group.\\n\\nOnce you have these two statistics, you can, for instance, z-test the rank sum to see if the\\ndifference between the two groups is statistically significant (i believe that's known as the *Wilcixon rank sum test*, which is interchangeable, i.e., functionally equivalent to the Mann-Whitney U test).\\n\\nR Functions for these statistics (the ones i know about, anyway):\\n\\n**wilcox.test** in the standard R installation\\n\\n**meanranks** in the *cranks* Package\\n\\n",,
3384,5,1466,179e9ff3-a534-46b6-b597-bc8edc5d4824,2010-08-10 06:53:15.0,56.0,"If you want a realistic simulation you need to find a distribution that describes the real process good enough (a model).\\n\\nWhen a real player makes a move he will on average (e.g.) throw `X` yards, with a standard deviation of `Y`. This does however not mean that the distribution of throws is a normal distribution. You should plot the (histogrammed) `throw` distribution and plot your *fit* (a Normal distribution with mean `X` and &sigma;=`Y`) and determine if it fits good enough. If not find a distribution that describes the real data better.\\n\\nOnce you have that down you need to generate random numbers from the distribution you determined.\\n\\n**EDIT**\\n\\nIf you data is complete enough you could maybe avoid having to create a complete model. You would create a [frequency distribution][1] of your data (a histogram) and then do [rejection sampling][2] directly from it.\\n\\n[1]: http://en.wikipedia.org/wiki/Frequency_distribution\\n[2]: http://en.wikipedia.org/wiki/Rejection_sampling",Add rejection sampling,
3385,2,1487,c0b607d2-b0d6-4b06-a4a0-9996fe6c4df5,2010-08-10 06:55:21.0,213.0,"We have performed a microarray screening of about 200 samples. In each sample we measure about 100 different variables. For technical reasons the screening of these 200 samples was divided into two batches with a couple of weeks interval between them. When all the data has been collected, I have performed principle component analysis (PCA) on the 200 x 100 table. \\n\\nWhen we look on linear projection of first 4 components (responsible for ~70% of the variability), we see a clear division between the two experiment batches. An illustration to what I have can be seen here: http://img594.imageshack.us/img594/3687/pca.png\\n\\n\\nWhat are the accepted techniques to address this issue?\\n\\n\\n  [1]: ",,
3386,1,1487,c0b607d2-b0d6-4b06-a4a0-9996fe6c4df5,2010-08-10 06:55:21.0,213.0,Correcting experiment results,,
3387,3,1487,c0b607d2-b0d6-4b06-a4a0-9996fe6c4df5,2010-08-10 06:55:21.0,213.0,<statistical-analysis><correlation><data-transformation><pca><multiple-comparisons>,,
3388,2,1488,1ec79b56-163e-4c91-b4cf-7ac1e9284931,2010-08-10 06:59:13.0,213.0,"Weka is an open source program for data mining (wirtten and extensible in Java), [Orange][2] is an open source program and library for data mining and machine learning (written in Python). They both allow convenient and efficient visual exploration of multidimensional data\\n\\n\\n  [2]: http://www.ailab.si/orange/",,
3389,2,1489,7705460d-db8f-4b9e-ac0b-1f129431e5cd,2010-08-10 07:02:57.0,196.0,"I would think that the first step would be to examine the component loadings and the actual variables to see if you can identify why the two batches yielded discernible differences. Depending on the reasons for the differences you may or may not be able to use a statistical control to ""correct"" the results.  \\n\\nHowever, if you have every reason to believe samples were randomly assigned to batches, that the your testing methods have not reached floors or ceilings in their ability to assess the variables of interest, and the actual scores themselves are not of interest but only their relative position, perhaps you could standardize scores along each variable by batch.\\n",,
3390,2,1490,c830603f-9203-492a-bdcb-b3810beefe6a,2010-08-10 07:03:22.0,213.0,"Let me suggest you the [R time series tutorial][1]. It does not provide deep theoretical knowledge, but it does give you a nice introduction. Also, googling for ""r time series"" gives you a lot of very interesting links\\n\\n\\n  [1]: http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm",,
3391,5,1487,ed7b190f-f3a7-4228-a224-c363d3221023,2010-08-10 07:04:51.0,213.0,"We have performed a microarray screening of about 200 samples. In each sample we measure about 100 different variables. For technical reasons the screening of these 200 samples was divided into two batches with a couple of weeks interval between them. When all the data has been collected, I have performed principle component analysis (PCA) on the 200 x 100 table. \\n\\nWhen we look on linear projection of first 4 components (responsible for ~70% of the variability), we see a clear division between the two experiment batches. An illustration to what I have can be seen here: http://img594.imageshack.us/img594/3687/pca.png\\n\\n\\nWhat are the accepted techniques to address this issue?\\n\\n\\n",deleted 7 characters in body,
3392,2,1491,c0ea470c-1d30-42ad-b4a0-20c5286b775f,2010-08-10 07:15:35.0,5.0,"GGobi (http://www.ggobi.org/), along with the R package rggobi, is perfectly suited to this task. \\n\\nSee the related presentation for examples: http://www.ggobi.org/book/2007-infovis/05-clustering.pdf",,
3393,5,973,2cee88a5-9405-4e1d-901f-d1762c66f0c0,2010-08-10 07:48:26.0,223.0,"What are the **freely available data set for classification with more than 1000 features** (or sample points if it is about curves)? \\n\\nThere is already a community wiki about free data sets:\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nBut here, it would be nice to have a **more focused list that can be used more conveniently**, also I propose the following rules:\\n\\n 1. One post **per dataset** \\n - **No** link to set of dataset (however, many data set can be given in a post)\\n - each data set **must** be associated with\\n\\n    - a **name** (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    - the number of features (let say it is **p**) the size of the dataset (let say it is **n**) and the number of labels/class (let say it is **k**)    \\n\\n    - a typical **error rate** from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n",added 18 characters in body,
3394,2,1492,b5827f63-212d-4632-9f28-af52cdba2486,2010-08-10 08:04:22.0,167.0,"I was eating sushi once and thought that it might make a good intuitive demonstration of ill-conditioned problems. Suppose you wanted to show someone a plane using two sticks touching at their bases.  \\n\\nYou'd probably hold the sticks orthogonal to each other.  The effect of any kind of shakiness of your hands on the plane causes it to wobble a little around the what you were hoping to show people, but after watching you for a while they get a good idea of what plane you were intending to demonstrate.  \\n\\nBut let's say you bring the sticks' ends closer together and watch the effect of your hands shaking. The plane it forms will pitch far more wildly.  Your audience will have to watch longer to get a good idea of what plane the you are trying to demonstrate.\\n\\n",,
3395,2,1493,49d721e3-2cbc-4464-93c7-4e80bd2a5226,2010-08-10 08:45:44.0,852.0,$\\chi^n_k=\\sum_{i=1}^kx_i^n$ where $x_i$ are gaussian varable?\\nand n>2?,,
3396,1,1493,49d721e3-2cbc-4464-93c7-4e80bd2a5226,2010-08-10 08:45:44.0,852.0,what is the ditribution of $\\chi^n_k$?,,
3397,3,1493,49d721e3-2cbc-4464-93c7-4e80bd2a5226,2010-08-10 08:45:44.0,852.0,<probability><stochastic-processes>,,
3398,2,1494,ab27a715-7c90-4709-8af6-73935fe446e9,2010-08-10 08:50:49.0,8.0,How did you normalise your microarray data? Standard ways are:\\n\\n * Robust Multichip Average ([RMA][1])\\n * Genechip RMA - this can be a bit slow for lots of samples.\\n\\nThis [presentation][2] gives a good overview of the two techniques.\\n___\\n\\nThere are two R microarray tutorial papers which may also help:\\n\\n * A microarray analysis for differential gene expression in the soybean genome using Bioconductor and R ([link to paper][3])\\n * Analysing yeast time course microarray data using Bioconductor ([link to paper][4]).\\n\\nBoth these papers provide the data and R commands.\\n\\n*Competing interest: I'm a author on the second paper.*\\n\\n\\n  [1]: http://rss.acs.unt.edu/Rdoc/library/affy/html/rma.html\\n  [2]: http://www.ogic.ca/projects/SCNcourse/course_units/unit1/lecture/Introduction%20to%20Affymetrix%20Microarrays.ppt\\n  [3]: http://dx.doi.org/10.1093/bib/bbm043\\n  [4]: http://www.mas.ncl.ac.uk/~ncsg3/microarray/,,
3399,2,1495,f08d9870-43fd-48cc-81a2-622397fd4209,2010-08-10 09:02:02.0,665.0,"Does anyone know of research which investigates the effectiveness (understandability?) of different visualization techniques? \\n\\nFor example, how quickly do people understand one form of visualization over another? Does interactivity with the visualization help people recall the data? Anything along those lines. An example of visualizations might be: scatter plots, graphs, timelines, maps, interactive interfaces (like Parallel Coordinates) etc.\\n\\nI'm particularly interested in research within a lay-person population.",,
3400,1,1495,f08d9870-43fd-48cc-81a2-622397fd4209,2010-08-10 09:02:02.0,665.0,Cognitive processing/interpretation of visualizations techniques,,
3401,3,1495,f08d9870-43fd-48cc-81a2-622397fd4209,2010-08-10 09:02:02.0,665.0,<data-visualization>,,
3402,4,1495,15813d4a-4584-42f3-b58d-179d5d8ea0d2,2010-08-10 09:09:30.0,665.0,Cognitive processing/interpretation of data visualizations techniques,edited title,
3403,2,1496,66a51cb4-fc7c-4b78-baa9-3ff67ccddec4,2010-08-10 09:29:15.0,8.0,"I'm presuming that zero != missing data, as that's an entirely different question.\\n\\nWhen thinking about how to handle zero's in multiple linear regression, I tend to consider a number of points:\\n\\n**Only a couple of zeros**\\n\\nIf I have a single zero in a reasonably large data set, I tend to:\\n\\n1. Remove the point, take logs and fit the model\\n1. Add a small $c$ to the point, take logs and fit the model\\n\\nDoes the model fit change? What about the parameter values. If the model is fairly robust to the removal of the point, I'll go for quick and dirty approach of adding $c$.\\n\\nYou could make this procedure a bit less crude and use the boxcox method with shifts described in ars' answer.\\n\\n**Large number of zeros**\\n\\nIf my data set contains a large number of zeros, then this suggests that simple linear regression isn't the best tool for the job. Instead I would use something like mixture modelling (as suggested by Srikant and Robin).\\n\\n\\n\\n\\n",,
3404,5,1496,73d1df32-7a27-438d-9239-4fbd95b76aa2,2010-08-10 10:08:18.0,8.0,"I'm presuming that zero != missing data, as that's an entirely different question.\\n\\nWhen thinking about how to handle zero's in multiple linear regression, I tend to consider how many zeros do we actually have?\\n\\n**Only a couple of zeros**\\n\\nIf I have a single zero in a reasonably large data set, I tend to:\\n\\n1. Remove the point, take logs and fit the model\\n1. Add a small $c$ to the point, take logs and fit the model\\n\\nDoes the model fit change? What about the parameter values. If the model is fairly robust to the removal of the point, I'll go for quick and dirty approach of adding $c$.\\n\\nYou could make this procedure a bit less crude and use the boxcox method with shifts described in ars' answer.\\n\\n**Large number of zeros**\\n\\nIf my data set contains a large number of zeros, then this suggests that simple linear regression isn't the best tool for the job. Instead I would use something like mixture modelling (as suggested by Srikant and Robin).\\n\\n\\n\\n\\n",added 16 characters in body,
3405,5,1493,83a33e92-ea51-4b97-a36d-4791902f4dde,2010-08-10 10:21:46.0,159.0,$\\chi^n_k=\\sum_{i=1}^kx_i^n$ where $x_i$ are Gaussian variables and $n>2$?,added 2 characters in body; edited tags; edited title,
3406,4,1493,83a33e92-ea51-4b97-a36d-4791902f4dde,2010-08-10 10:21:46.0,159.0,What is the distribution of $\\chi^n_k$?,added 2 characters in body; edited tags; edited title,
3407,6,1493,83a33e92-ea51-4b97-a36d-4791902f4dde,2010-08-10 10:21:46.0,159.0,<distributions><probability><stochastic-processes>,added 2 characters in body; edited tags; edited title,
3408,2,1497,9d6cd7a8-aa2d-4221-8acd-299bf8f8655d,2010-08-10 10:24:28.0,159.0,Cleveland reports on a lot of this research in his 1994 book *[The Elements of Graphing Data][1]* (2nd ed). It is very readable and extremely useful.\\n\\n\\n  [1]: http://www.amazon.com/Elements-Graphing-Data-William-Cleveland/dp/0963488414,,
3409,2,1498,43059312-1374-4bba-b87f-ec221fcca358,2010-08-10 10:54:56.0,94.0,what I do is group the measurements by hour and day of week and compare standard deviations of that. Still doesn't correct for things like holidays and summer/winter seasonality but its correct most of the time.\\n\\nThe downside is that you really need to collect a year or so of data to have enough so that stddev starts making sense.,,
3410,2,1499,080c6347-0b9b-4a42-9020-ef9c3a7eb490,2010-08-10 10:58:52.0,22.0,"Couple of thoughts:\\n\\n 1. I think Bertin's [Semiology of Graphics][1] is a classic position in this area.\\n\\n 2. As [Rob][2] pointed out, Cleveland has done some interesting work in the area, example [here][3].\\n\\n 3. Some [examples][4] of poor design from Stephen Few. \\n\\n 4. Recently I stumbled upon interesting [and quite provocative, especially for Tufte / Cleveland fanatics as myself ;] piece of research from Human-Computer Interaction Lab at the University of Saskatchewan about [usefulness of junk][5].\\n\\n\\n  [1]: http://www.amazon.com/Semiology-Graphics-Diagrams-Networks-Maps/dp/1589482611/ref=sr_1_1?ie=UTF8&s=books&qid=1281437072&sr=8-1\\n  [2]: http://stats.stackexchange.com/users/159/rob-hyndman\\n  [3]: https://secure.cs.uvic.ca/twiki/pub/Research/Chisel/ComputationalAestheticsProject/cleveland.pdf\\n  [4]: http://www.perceptualedge.com/examples.php\\n  [5]: http://hci.usask.ca/uploads/173-pap0297-bateman.pdf",,
3411,2,1500,186e451f-169a-408b-b0dd-8c8cd978329b,2010-08-10 11:06:46.0,22.0,I've had good experience with [KNIME][1] during one of my project. It 's an excellent solution for quick exploratory mining and graphing. On top of that it provides R and Weka modules seamless integration.  \\n\\n\\n  [1]: http://www.knime.org/,,
3412,2,1501,3b6977e6-65f4-4690-b1fb-b9ed96b56e09,2010-08-10 13:47:10.0,114.0,"I'm interested in what datasets and published analysis exsist for various statistical methods that could be used to test or validate a particular implementation of a statistical method in practice.\\n\\nTo start off, the best one I know is the [NIST Statistical Reference Datasets][1] page that publishes a wide range of datasets and calculations that covers areas such as Analysis of Variance, Linaear Regression, Markov Chain and Monte Carlo simulation and Non-Linear regression.\\n\\nAre there any other good / notable ones out there.\\n\\n  [1]: http://www.itl.nist.gov/div898/strd/index.html",,
3413,1,1501,3b6977e6-65f4-4690-b1fb-b9ed96b56e09,2010-08-10 13:47:10.0,114.0,What open datasets exsit for testing/validation or evaluation of Statistical Methods,,
3414,3,1501,3b6977e6-65f4-4690-b1fb-b9ed96b56e09,2010-08-10 13:47:10.0,114.0,<dataset><validation><hypothesis-testing>,,
3415,16,1501,3b6977e6-65f4-4690-b1fb-b9ed96b56e09,2010-08-10 13:47:10.0,114.0,,,
3416,2,1502,bcb730e0-6bee-4b41-a9b8-134efd1cdaf1,2010-08-10 14:02:22.0,394.0,"GGobi does look interesting for this. Another approach could be to treat your similarity/inverse distance matrices as network adjacency matrices and feeding that into a network analysis routine (e.g., either igraph in R or perhaps Pajek). With this approach I would experiment with cutting the cutting the node distances into a binary tie at various cutpoints.",,
3417,2,1503,746d2b46-eeb1-4b20-8cc3-ce9360d9b776,2010-08-10 14:59:06.0,5.0,"See the stackoverflow question on this subject: [Datasets for Running Statistical Analysis on][1].\\n\\nI would reiterate [my answer][2], that R contains (in packages) many of the canonical datasets for specific statistical problems.\\n\\n\\n  [1]: http://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/\\n  [2]: http://stackoverflow.com/questions/2252144/datasets-for-running-statistical-analysis-on/2252450#2252450",,
3418,16,1503,746d2b46-eeb1-4b20-8cc3-ce9360d9b776,2010-08-10 14:59:06.0,-1.0,,,
3419,2,1504,02b19d70-288f-4a10-ab9e-4ad0522b3679,2010-08-10 15:11:18.0,5.0,"There is a lot of great work being done on this at Stanford under [Jeffrey Heer][1] (the creator of [protovis][2] and [prefuse][3] amongst other things) and in the Stanford [**Human-Computer Interaction Group**][4].  As an example, read the [""Sizing the Horizon: The Effects of Chart Size and Layering on the Graphical Perception of Time Series Visualizations""][5] paper.  The material on the [CS147: Introduction to Human-Computer Interaction Design][6] and [cs448b Data Visualization][7] homepages may also be of interest.\\n\\n\\n  [1]: http://hci.stanford.edu/jheer/\\n  [2]: http://vis.stanford.edu/protovis/\\n  [3]: http://prefuse.org/\\n  [4]: http://hci.stanford.edu/\\n  [5]: http://hci.stanford.edu/publications/2009/heer-horizon-chi09.pdf\\n  [6]: http://hci.stanford.edu/courses/cs147/2010/\\n  [7]: https://graphics.stanford.edu/wikis/cs448b-09-fall",,
3420,5,1504,a91d69db-d439-4b3c-ae7d-c265b29d4b4b,2010-08-10 15:25:14.0,5.0,"This subject matter is often discussed under the discipline of HCI ([human-computer interaction][1]) which has [it's own journal][2].\\n\\nThere is a lot of great work being done on this at Stanford under [Jeffrey Heer][3] (the creator of [protovis][4] and [prefuse][5] amongst other things) and in the Stanford [**Human-Computer Interaction Group**][6].  As an example, read the [""Sizing the Horizon: The Effects of Chart Size and Layering on the Graphical Perception of Time Series Visualizations""][7] paper.  The material on the [CS147: Introduction to Human-Computer Interaction Design][8] and [cs448b Data Visualization][9] homepages may also be of interest.\\n\\nYou can also look at the projects list in [the CMU HCI Institute][10].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\\n  [2]: http://www.informaworld.com/smpp/title~db=all~content=t775653648\\n  [3]: http://hci.stanford.edu/jheer/\\n  [4]: http://vis.stanford.edu/protovis/\\n  [5]: http://prefuse.org/\\n  [6]: http://hci.stanford.edu/\\n  [7]: http://hci.stanford.edu/publications/2009/heer-horizon-chi09.pdf\\n  [8]: http://hci.stanford.edu/courses/cs147/2010/\\n  [9]: https://graphics.stanford.edu/wikis/cs448b-09-fall\\n  [10]: http://www.hcii.cmu.edu/research/projects",added 407 characters in body,
3421,2,1505,bf67139b-6813-483c-9915-10b82bf6f252,2010-08-10 15:29:25.0,287.0,"Stephen Kosslyn studies human visual processing, and has written a book called [Graph Design for the Eye and Mind][1]. There's useful stuff in there, but he also suggests funny things sometimes. For example, he suggests truncating the y-axis on bar graphs at some point, so that they don't really start at 0, which seems a to be a deep sin to me.\\n\\n\\n  [1]: http://www.amazon.com/Graph-Design-Mind-Stephen-Kosslyn/dp/0195306627",,
3422,5,1489,4febe096-1749-47c1-9d55-4047603607da,2010-08-10 15:31:41.0,196.0,"I would think that the first step would be to examine the component loadings and the actual variables to see if you can identify why the two batches yielded discernible differences. Depending on the reasons for the differences you may or may not be able to use a statistical control to ""correct"" the results.  \\n\\nHowever, if you have every reason to believe samples were randomly assigned to batches, that the your testing methods have not reached floors or ceilings in their ability to assess the variables of interest, and the actual scores themselves are not of interest but only their relative position, perhaps you could standardize scores along each variable by batch.\\n\\nEdit:  It looks like csgillespie understands your research area and has provided you some good links.  All I was suggesting was that for each batch you could calculate a Z score for each observed variable.  This would have the effect of eliminating batch differences since for each variable in each batch the mean would be the same (0) and the standard deviation would be the same (1).",added 387 characters in body,
3423,2,1506,c2f63b3b-d893-427d-877a-4bf5ca9c07f6,2010-08-10 15:33:16.0,251.0,"Specifically on color and perception, I liked the papers below by Bergman, Rogowitz and Treinish.  \\n\\n\\n- [Why Should Engineers and Scientists Be Worried About Color?][1]\\n- [A Rule-based Tool for Assisting Colormap Selection][2]\\n- [How NOT to Lie with Visualization][3]\\n- [Lloyd Treinish's home page][4]  (for links to related work)\\n\\n\\n  [1]: http://www.research.ibm.com/people/l/lloydt/color/color.HTM\\n  [2]: http://www.research.ibm.com/dx/proceedings/pravda/index.htm\\n  [3]: http://www.research.ibm.com/dx/proceedings/pravda/truevis.htm\\n  [4]: http://www.research.ibm.com/people/l/lloydt/\\n\\n",,
3424,4,1501,e47fdfb7-1517-4349-a867-905cc0a5b108,2010-08-10 15:40:33.0,,What open datasets exist for testing/validation or evaluation of Statistical Methods,fixed typo,user28
3425,2,1507,48c6cd04-0356-4ec3-a9b0-d44cb0527ff4,2010-08-10 16:03:57.0,6967.0,"Q:  Is my approach correct?\\n\\nEvent:  You toss 5 coins at once. \\n\\nA student of mine claimed he got 4T & 1H in 39 out of 40 trials (!!)  \\n\\nI decided to calc the odds of this...\\n\\nFirst, P(4T & 1H) = 5C4 * (1/2)^4 * (1/2)^1 = .16\\n\\nI did this 2 ways:\\n\\n--------------------------------\\n\\n1) Binomial Probability\\n\\nn = 40\\n\\nr = 39\\n\\np = .16\\n\\nq = .84\\n\\nP(Exactly 39) = 40 C 39 * (.16)^39 * (.84)^1 = 0%\\n\\n-------------------------------\\n2) Binomial Distribution:\\n\\nn = 40\\n\\nr = 39 (or more)\\n\\np = .16\\n\\nq = .84\\n\\nE(X) = u = np = (.16)(40) = 6.4\\n\\nSD = SQRT(npq) = 3.16\\n\\nZ(39) = (observed - expected) / SD = (39 - 6.4) / 3.16 = 10.3\\n\\np = P( Z > 10.3) = 0%\\n\\n\\n--------------\\n\\nConclusion:  The odds of getting 4T & 1H in 39 out of 40 trials is negligible.\\n\\nStudent was on drugs at the time.",,
3426,1,1507,48c6cd04-0356-4ec3-a9b0-d44cb0527ff4,2010-08-10 16:03:57.0,6967.0,Example of using binomial distribution,,
3427,3,1507,48c6cd04-0356-4ec3-a9b0-d44cb0527ff4,2010-08-10 16:03:57.0,6967.0,<binomial>,,
3428,2,1508,4fdc9550-806e-4fa0-9bb8-db853b2dae01,2010-08-10 16:23:37.0,,"Technically, your case 1 and 2 calculations are not correct as they are not independent trials. You are tossing the same 5 coins 40 times. So, those events are dependent.\\n\\nIf you ignore the above issue then the above seems ok.",,user28
3429,5,1478,9d41354f-d155-4adf-b63a-0e39880c8b66,2010-08-10 16:42:56.0,795.0,"I am trying to write unit tests for a whole mess of statistics code. Some of the unit tests take the form: generate a sample following a null hypothesis, use code to get a p-value under that null, repeat hundreds of times, then look at all the p-values: if they are reasonably uniform, then the code passes. I usually check if the ratio of p-values < $\\alpha$ is near $\\alpha$ for $\\alpha = 0.01, 0.05, 0.1$. But I am usually also interested in whether the p-values output deviate from uniformity. I usually test this with the Anderson-Darling test. \\n\\n\\nHere is where I have a circularity problem: how can I unit test my Anderson-Darling code? I can easily feed it uniformly generated variables, and get a p-value, repeat hundreds of times, but then I just have a bunch of p-values. I can q-q plot them, but I'm more interested in an automatic unit test I can run. What are some basic sanity checks I can implement automatically? there is the naive check of ratio of p-values < $\\alpha$ noted above. I can also implement a Kolmogorov-Smirnov test. What else can I easily check for?\\n\\n(I realize this question may seem hopelessly pedantic or naive or subject to infinite regress...)\\n\\n**edit** some additional ideas: \\n\\n 1. test the code on $\\frac{i}{n}$ for $i = 1,2,...,n$, for different values of $n$. Presumably I can compute, by hand, the p-value for the A-D test in this case.\\n 2. compute $n$ p-values by feeding many uniform samples to the code $n$ times, then regress the order statistics of the p-values, $p_{(i)}$ vs $i/n$, to get $p_{(i)} = \\beta_1 i/n + \\beta_0$ and test the null $\\beta_1 = 1, \\beta_0 = 0$. Presumably I can simplify this test by hand in such a way that inspection reveals it to be correct.\\n 3. make sure the code is invariant with respect to permutation of the input. (duh)",add additional ideas to test,
3430,5,1508,379e8932-9264-4297-aea1-926ed2da8fda,2010-08-10 17:08:25.0,,"<strike>Technically, your case 1 and 2 calculations are not correct as they are not independent trials. You are tossing the same 5 coins 40 times. So, those events are dependent.\\n\\nIf you ignore the above issue then the above seems ok.</strike>\\n\\nOn some more reflection I think you can ignore the issue of dependency. Here is my reasoning: The probability of observing 4T & 1H is 0.16. In your case 1 and case 2 calculations you are using this probability across all 40 trials which implicitly accounts for the dependence in the trials. \\n\\nAnother way to think about the issue is: If you observe 4T and 1H in the first trial what can you say about the probability that you would observe 4T and 1H in the second. It clearly equals 0.16 and thus there is no dependency. Knowledge of the outcome of one trial does not give us any additional information about the events that are likely to happen in the subsequent trial. Thus, the trials are independent.\\n\\nI think the calculation is fine as it stands.",revised answer,user28
3431,2,1509,29ee74ee-b892-4e3b-a0c1-c29d27180ca8,2010-08-10 17:29:28.0,226.0,You could use the Hilbert Transformation from DSP theory to measure the instantaneous frequency of your data. The site http://ta-lib.org/ has open source code for measuring the dominant cycle period of financial data; the relevant function is called HT_DCPERIOD; you might be able to use this or adapt the code to your purposes. ,,
3432,2,1510,6ad513f8-e876-46ee-96ef-8eb1bc74c6f4,2010-08-10 17:51:38.0,251.0,"> I can q-q plot them, but I'm more interested in an automatic unit test I can run.\\n\\nIf a visual inspection of a q-q plot would suffice, then you could calculate an entropy measure such as the [Gini coefficient][1] and accept the test after allowing for some tolerance for deviation from the 45 degree line.  For more on comparing distributions in general, you could look to the [reldist][2] package in R for ideas.\\n\\nThough not directly applicable, I think you'll also find the method presented by Cook, Gelman & Rubin interesting:\\n\\n- [Validation of Software for Bayesian Models Using Posterior Quantiles][3]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Gini_coefficient\\n  [2]: http://cran.r-project.org/web/packages/reldist/index.html\\n  [3]: http://www.stat.columbia.edu/~gelman/research/published/Cook_Software_Validation.pdf\\n",,
3433,2,1511,d0100bb3-2521-4b9e-b734-524585696fe0,2010-08-10 18:25:52.0,601.0,The probability of observing 4 heads and 1 tail 39 times out of 40 after observing 4 heads and 1 tail 39 times out of 40 is 1.0.\\n\\n:),,
3434,2,1512,5dfab40e-71e3-4763-93d9-08301b210de6,2010-08-10 18:41:10.0,61.0,"If you expect the process to be stationary -- the periodicity/seasonality will not change over time -- then something like a Chi-square periodogram (see e.g. Sokolove and Bushell, 1978) might be a good choice.  It's commonly used in analysis of circadian data which can have extremely large amounts of noise in it, but is expected to have very stable periodicities.\\n\\nThis approach makes no assumption about the shape of the waveform (other than that it is consistent from cycle to cycle), but does require that any noise be of constant mean and uncorrelated to the signal.\\n\\n    chisq.pd <- function(x, min.period, max.period) {\\n	N <- length(x)\\n	variances = NULL\\n	periods = seq(min.pd, max.pd)\\n	rowlist = NULL\\n	for(lc in periods){\\n		ncol = lc\\n		nrow = floor(N/ncol)\\n		rowlist = c(rowlist, nrow)\\n		x.trunc = x[1:(ncol*nrow)]\\n		x.reshape = t(array(x.trunc, c(ncol, nrow)))\\n		variances = c(variances, var(colMeans(x.reshape)))\\n	}\\n	Qp = (rowlist * periods * variances) / var(x)\\n	df = periods - 1\\n	pvals = 1-pchisq(Qp, df)\\n	pass.periods = periods[pvals<alpha]\\n	pass.pvals = pvals[pvals<alpha]\\n	#return(cbind(pass.periods, pass.pvals))\\n	return(cbind(periods[pvals==min(pvals)], pvals[pvals==min(pvals)]))\\n    }\\n    \\n    x = cos( (2*pi/37) * (1:1000))+rnorm(1000)\\n    chisq.pd(x, 2, 72, .05)\\n\\nThe last two lines are just an example, showing that it can identify the period of a pure trigonometric function, even with lots of additive noise.\\n\\nAs written, the last argument (alpha) in the call is superfluous, the function simply returns the 'best' period it can find; uncomment the first 'return' statement and comment out the second to have it return a list of all periods significant at the level alpha.\\n\\nThis function doesn't do any sort of sanity checking to make sure that you've put in identifiable periods, nor does it (can it) work with fractional periods, nor is there any sort of multiple comparison control built in if you decide to look at multiple periods.  But other than that it should be reasonably robust.",,
3435,2,1513,0de964ca-a382-401a-a26b-5a6c712eeb1f,2010-08-10 20:20:04.0,125.0,"if you decide to generate your distribution from the data you have observed, your model will never spit out a ""tail value"", ie, something outside the range of what you have observed.  \\n\\nyour example data: \\naverage 5.3 yards per carry with a SD of 1.7 yards\\n\\nwill have a max and a min, say 10 and 2.  in that case, your calculated distribution will not have any weight in the tails, and will be unable to generate a value of 11 or 1.  maybe this is ok, but it prevents you from ever generating one of those super-human events that everyone loves to watch.\\n\\nthe functional form for the standard normal has tails that go to infinity, so if you assume the distribution is normal, you will be able to generate simulated values (at very small probabilities) that are higher or lower than your observed data.",,
3436,2,1514,c671e7b7-ec57-4130-9394-c4819ef18826,2010-08-10 20:48:11.0,856.0,If you want something quick and dirty why not use the square root? ,,
3437,2,1515,ad14335f-03a1-4823-a33d-03cce8873aaf,2010-08-10 21:34:55.0,795.0,"well, as a bound, if $n$ is even, $\\chi_k^n$ will be bounded from below by a Chi-square, and $(\\chi_k^n)^{1/n}$ should be bounded from above by the maximum of $k$ half-normals, or thereabouts. ",,
3438,2,1516,fbb30a21-3899-4c0d-97e1-ed68b521044c,2010-08-10 22:34:01.0,858.0,"I have to disagree with many of the answers.  The *reason* that we calculate standard deviation instead of absolute error is that we are **assuming error to be normally distributed**.  It's a part of the model.\\n\\nSuppose you were measuring very small lengths with a ruler, then standard deviation is a bad metric for error -- because you know you will never accidentally measure a negative length.  You might want to calculate a statistic that will help you fit a Gamma distribution to your error:\\n\\n$\\log(E(x)) - E(\\log(x))$\\n",,
3439,2,1517,11cd002c-a7f9-4e66-82b6-87e8dcab2b47,2010-08-10 22:39:06.0,857.0,"I am a database developer working for a sales and manufacturing business.  I am mostly ignorant about statistics.  We need useful metrics.  Our managers are tuned to accounting, and the vagaries of production tend to confound us.  We do very little measuring of our production, and what we have is poorly formed.  I should note we are a ""job shop"", not a ""flow shop"" -- we do a lot of engineer-to-order work, so the usual MRP standards are often hard to apply.  \\n\\nSome tradition business metrics are known to us; for example ""inventory turn-over rate.""  But we are unable to convert those to useful information.  I believe our inability to qualify data statistically is a big reason why.  \\n\\nOf course we perform averaging all the time.  Rolling averages (smoothing data over time using a 3-week rolling average, for example) is a helpful extension.  Recently I discovered how to apply standard deviation to labor costing with wonderful benefits.  \\n\\nNow that I understand (A) averaging, (B) rolling averages, and (C) standard deviation, what are the next useful functions or functions I should seek to learn?\\n\\nI would love to have your insights on ""business intelligence,"" I mean defining and using metrics.  But it's the use of stats to get from raw data to usable information that I'm really after.  No matter, give me whatever you've got.  ",,
3440,1,1517,11cd002c-a7f9-4e66-82b6-87e8dcab2b47,2010-08-10 22:39:06.0,857.0,Useful statistical functions for business -- for use by a newbie,,
3441,3,1517,11cd002c-a7f9-4e66-82b6-87e8dcab2b47,2010-08-10 22:39:06.0,857.0,<statistics>,,
3442,16,1517,11cd002c-a7f9-4e66-82b6-87e8dcab2b47,2010-08-10 22:39:06.0,857.0,,,
3443,5,1516,11c0d668-7cbf-4722-b8ae-9895c69758f9,2010-08-10 23:30:32.0,858.0,"The *reason* that we calculate standard deviation instead of absolute error is that we are **assuming error to be normally distributed**.  It's a part of the model.\\n\\nSuppose you were measuring very small lengths with a ruler, then standard deviation is a bad metric for error because you know you will never accidentally measure a negative length.  A better metric would be one to help fit a Gamma distribution to your error:\\n\\n$E(\\log(x)) - \\log(E(x))$\\n\\nLike st. dev., this is also non-negative and differentiable, but it is a better error statistic for this problem.",added 90 characters in body; deleted 46 characters in body,
3444,2,1518,610cbd6a-6c4f-4d0d-9a27-de9fb2b11e71,2010-08-10 23:36:25.0,,There are lots of generic tools. You should probably start from these:\\n\\n - Tests to [compare means][1]\\n - [Linear Regression][2]\\n\\nFoundational items to understand statistics:\\n\\n - [Probability Distribution][3] and [List of Probability Distributions][4]\\n\\nThis should probably keep you busy depending on how much background you already have.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Comparing_means\\n  [2]: http://en.wikipedia.org/wiki/Linear_regression\\n  [3]: http://en.wikipedia.org/wiki/Probability_distribution\\n  [4]: http://en.wikipedia.org/wiki/List_of_probability_distributions,,user28
3445,16,1518,610cbd6a-6c4f-4d0d-9a27-de9fb2b11e71,2010-08-10 23:36:25.0,-1.0,,,
3446,2,1519,755a59ea-530d-499c-80b3-23888c60357d,2010-08-11 02:33:03.0,830.0,"In (most of) the analytical chemistry literature, the standard test for detecting outliers in univariate data (e.g. a sequence of measurements of some parameter) is Dixon's Q test. Invariably, all the procedures listed in the textbooks have you compute some quantity from the data to be compared with a tabular value. By hand, this is not much of a concern; however I am planning to write a computer program for Dixon Q, and just caching values strikes me as inelegant. Which brings me to my first question:\\n\\n1. How are the tabular values for Dixon Q generated?\\n\\nNow, I have already looked into this [article][1], but I'm of the feeling that this is a bit of cheating, in that the author merely constructs a spline that passes through the tabular values generated by Dixon. I have the feeling that a special function (e.g. error function or incomplete beta/gamma) will be needed somewhere, but at least I have algorithms for those.\\n\\nNow for my second question: ISO seems to be slowly recommending Grubbs's test over Dixon Q nowadays, but judging from the textbooks it has yet to catch on. This on the other hand was relatively easy to implement since it only involves computing the inverse of the CDF of Student t. Now for my second question:\\n\\n2. Why would I want to use Grubbs's instead of Dixon's?\\n\\nOn the obvious front in my case, the algorithm is ""neater"", but I suspect there are deeper reasons. Can anyone care to enlighten me?\\n\\n\\n  [1]: http://pubs.acs.org/doi/pdf/10.1021/ac00002a010",,
3447,1,1519,755a59ea-530d-499c-80b3-23888c60357d,2010-08-11 02:33:03.0,830.0,On univariate outlier tests (or: Dixon Q versus Grubbs),,
3448,3,1519,755a59ea-530d-499c-80b3-23888c60357d,2010-08-11 02:33:03.0,830.0,<outliers>,,
3449,5,1402,2a8971ef-8297-4776-80aa-3773c46e3864,2010-08-11 04:32:31.0,438.0,"Well, i use **[graphviz][1]**, which has Java bindings [(Grappa][2]).\\n\\nAlthough the dot language (graphviz's syntax) is simple, i prefer to use graphviz as a library through the excellent and production-stable python bindings, [pygraphviz][3], and [networkx][4].\\n\\nHere's the code for a simple 'funnel diagram' using those tools; it's not the most elaborate diagram, but it is complete--it initializes the graph object, creates all of the necessary components, styles them, renders the graph, and writes it to file.\\n\\n    import networkx as NX\\n    import pygraphviz as PV\\n\\n    G = PV.AGraph(strict=False, directed=True)     # initialize graph object\\n\\n    # create graph components:\\n    node_list = [""Step1"", ""Step2"", ""Step3"", ""Step4""]\\n    edge_list = [(""Step1, Step2""), (""Step2"", ""Step3""), (""Step3"", ""Step4"")]\\n    G.add_nodes_from(node_list)\\n    G.add_edge(""Step1"", ""Step2"")\\n    G.add_edge(""Step2"", ""Step3"")\\n    G.add_edge(""Step3"", ""Step4"")\\n\\n    # style them:\\n    nak = ""fontname fontsize fontcolor shape style fill color size"".split()\\n    nav = ""Arial 11 white invtrapezium filled cornflowerblue cornflowerblue 1.4"".split()\\n    nas = dict(zip(nak, nav))\\n    for k, v in nas.iteritems() :\\n        G.node_attr[k] = v\\n\\n    eak = ""fontname fontsize fontcolor dir arrowhead arrowsize arrowtail"".split()\\n    eav = ""Arial 10 red4 forward normal 0.8 inv"".split()\\n    eas = dict(zip(eak, eav))\\n    for k, v in eas.iteritems() :\\n        G.edge_attr[k] = v\\n\\n    n1 = G.get_node(""Step1"")\\n    n1.attr['fontsize'] = '11'\\n    n1.attr['fontcolor'] = 'red4'\\n    n1.attr['label'] = '1411'\\n    n1.attr['shape'] = 'rectangle'\\n    n1.attr['width'] = '1.4'\\n    n1.attr['height'] = '0.05'\\n    n1.attr['color'] = 'firebrick4'\\n    n4 = G.get_node(""Step4"")\\n    n4.attr['shape'] = 'rectangle'\\n\\n    # it's simple to scale graph features to indicate 'flow' conditions, e.g., scale \\n    # each container size based on how many items each holds in a given time snapshot:\\n    # (instead of setting node attribute ('width') to a static quantity, you would\\n    # just bind 'n1.attr['width']' to a variable such as 'total_from_container_1'\\n\\n    n1 = G.get_node(""Step2"")\\n    n1.attr['width'] = '2.4'\\n    \\n    # likewise, you can do the same with edgewidth (i.e., make the arrow thicker\\n    # to indicate higher 'flow rate')\\n\\n    e1 = G.get_edge(""Step1"", ""Step2"")\\n    e1.attr['label'] = '  1411'\\n    e1.attr['penwidth'] = 2.6\\n\\n    # and you can easily add labels to the nodes and edges to indicate e.g., quantities: \\n    e1 = G.get_edge(""Step2"", ""Step3"")\\n    e1.attr['label'] = '  392'\\n\\n    G.write(""conv_fnl.dot"")      # save the dot file\\n    G.draw(""conv_fnl.png"")       # save the rendered diagram\\n\\n![alt text][5]\\n\\n\\n  [1]: http://www.graphviz.org\\n  [2]: http://www2.research.att.com/~john/Grappa/\\n  [3]: http://networkx.lanl.gov/pygraphviz/\\n  [4]: http://networkx.lanl.gov/\\n  [5]: http://a.imageshack.us/img148/390/convfunnel.png",added graphic (which is the output of my posted code),
3450,2,1520,79e5b20e-6894-454d-a789-50564e7052a8,2010-08-11 05:17:23.0,862.0,"\\n\\nCame across an interesting problem today. You are given a coin and x money, you double money if you get heads and lose half if tails on any toss.\\n\\n   1. What is the expected value of your money in n tries\\n   2. What is the probability of getting more than expected value in (1)\\n\\nThis is how I approached it. The probability of heads and tails is same (1/2). Expected value after first toss = 1/2(2*x) + 1/2(1/2*x) = 5x/4 So expected value is 5x/4 after first toss. Similarly repeating second toss expectation on 5x/4, Expected value after second toss = 1/2(2*5x/4) + 1/2(1/2*5x/4) = 25x/16\\n\\nSo you get a sequence of expected values: 5x/4, 25x/16, 125x/64, ...\\n\\nAfter n tries, your expected value should be 5^n/4^n * x.\\n\\nIf n is large enough, your expected value should approach the mean of the distribution. So probability that value is greater than expected value should be 0.5. I am not sure about this one.\\n",,
3451,1,1520,79e5b20e-6894-454d-a789-50564e7052a8,2010-08-11 05:17:23.0,862.0,expected value of random variable on tosses of a  dice,,
3452,3,1520,79e5b20e-6894-454d-a789-50564e7052a8,2010-08-11 05:17:23.0,862.0,<puzzle>,,
3453,2,1521,2d805f06-5e85-4592-a896-1b77d937247a,2010-08-11 05:31:50.0,485.0,"What is the difference between data mining and statistical analysis?\\n\\nFor some background, my statistical education has been, I think, rather traditional.  A specific question is posited, research is designed, and data are collected and analyzed to offer some insight on that question.  As a result, I've always been skeptical of what I considered ""data dredging""--looking for patterns in a large dataset and using these patterns to draw conclusions.  I tend to associate the latter with data-mining and have always considered this somewhat unprincipled--along with things like algorithmic variable selection routines.\\n\\nNonetheless, there is a large and growing literature on data mining.  Often I see this label referring to specific techniques--clustering, tree-based classification, etc.  Yet, at least from my perspective, these techniques can be ""set loose"" on a set of data or used in a structured way to address a question.  I'd call the former data mining and the latter statistical analysis.\\n\\nI work in academic administration and have been asked to do some ""data mining"" to identify issues and opportunities.  Consistent with my background, my first questions were--what do you want to learn and what are the things that you think contribute to issue.  From their response, it was clear that me and the person asking the question had different ideas on the nature and value of data mining.  Perhaps my take akin to the difference between supervised and unsupervised learning?",,
3454,1,1521,2d805f06-5e85-4592-a896-1b77d937247a,2010-08-11 05:31:50.0,485.0,Data Mining and Statistical Analysis,,
3455,3,1521,2d805f06-5e85-4592-a896-1b77d937247a,2010-08-11 05:31:50.0,485.0,<statistical-analysis><data-mining>,,
3456,2,1522,c667b0ea-da0d-40f1-9ed8-a695d4fe4555,2010-08-11 06:25:03.0,183.0,"I previously wrote a post where I made a few observations comparing data mining to psychology. I think these observations may capture some of the differences you are identifying:\\n\\n1. ""Data mining seems more concerned with prediction using observed variables than with understanding the causal system of latent variables; psychology is typically more concerned with the causal system of latent variables.\\n2. Data mining typically involves massive datasets (e.g. 10,000 + rows) collected for a purpose other than the purpose of the data mining. Psychological datasets are typically small (e.g., less than 1,000 or 100 rows) and collected explicitly to explore a research question.\\n3. Psychological analysis typically involves testing specific models. Automated model development approaches tend not to be theoretically interesting."" - [Data Mining and R][1]\\n\\n\\n  [1]: http://jeromyanglim.blogspot.com/2009/10/data-mining-and-r.html",,
3457,2,1523,d20a5771-7729-4c8c-ae12-be13c1331bc7,2010-08-11 06:29:22.0,74.0,"This is a duplicate, but I am going to answer it anyways.\\n\\nData mining is statistics, with some minor differences. You can think of it as re-branding statistics, because statisticians are kinda weird. \\n\\nIt is often associated with Computational Statistics, ie only stuff you can do with a computer.\\n\\nData miners *stole* a significant proportion of multivariate statistics and called it their own. Check the table of contents of any 1990s multivariate book and compare it to a new data mining book. Very similar.\\n\\nStatistics is associated with testing hypotheses and with model building, whereas Data Mining is more associated with prediction and classification, regardless of whether there is an understandable model.",,
3458,2,1524,9bc72ec1-3552-4952-80ce-9224d4bfee18,2010-08-11 06:42:21.0,339.0,"I would suggest, if you can afford the time, to follow two online (taped) courses, one in probability and another one in statistics. I think it is the best way to get some basic knowledge that will help you move forward.\\n\\n - [Probability course][1]\\n - [Statistics course][2] (This is lesson one. It goes up to [Lesson 64][3]. Unfortunately, I haven't found the links gathered together in one page. I think also that lessons 21,26 and 39 are missing but I don't remember having any problem whatsoever in following the course)\\n\\n\\n  [1]: http://academicearth.org/courses/math-and-proability-for-life-sciences\\n  [2]: http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#\\n  [3]: http://video.google.com/videoplay?docid=3835401745697888723&ei=coO0S9aOOJLF-Qb9kJnnBg&q=Statistics+401%3A+Lesson+64&hl=en&view=3#",,
3459,16,1524,9bc72ec1-3552-4952-80ce-9224d4bfee18,2010-08-11 06:42:21.0,-1.0,,,
3460,5,1524,bc1dd44a-3912-4013-b3f0-d1472a7af37b,2010-08-11 07:22:46.0,339.0,"I would suggest, if you can afford the time, to follow two online (taped) courses, one in probability and another one in statistics. I think it is the best way to get some basic knowledge that will help you move forward.\\n\\n - [Probability course][1]\\n - [Statistics course][2] (This is lesson one. It goes up to [Lesson 64][3]. Unfortunately, I haven't found the links gathered together in one page. I think also that lessons 21,26 and 39 are missing but I don't remember having any problem whatsoever in following the course). You can also find the handouts for this class posted [here][4].  \\n\\n\\n  [1]: http://academicearth.org/courses/math-and-proability-for-life-sciences\\n  [2]: http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#\\n  [3]: http://video.google.com/videoplay?docid=3835401745697888723&ei=coO0S9aOOJLF-Qb9kJnnBg&q=Statistics+401%3A+Lesson+64&hl=en&view=3#\\n  [4]: http://www.public.iastate.edu/~pcaragea/S401F07/Handouts07.html",added 139 characters in body,
3461,2,1525,34d3c260-bce4-485e-9354-f01a0a7f6392,2010-08-11 07:24:02.0,74.0,"Say I eat hamburgers every Tuesday. You could say that I eat hamburgers 14% of the time, or that the probability of me eating a hamburger in a given week is 14%.\\n\\nWhat are the main differences between probabilities and proportions?",,
3462,1,1525,34d3c260-bce4-485e-9354-f01a0a7f6392,2010-08-11 07:24:02.0,74.0,"Your blonde girlfriend asks, ""Hey, how's a probability different than a plain old proportion?"" Answer her in plain English. ",,
3463,3,1525,34d3c260-bce4-485e-9354-f01a0a7f6392,2010-08-11 07:24:02.0,74.0,<probability>,,
3464,2,1526,f797ca2c-b4c2-47b3-a232-eec52187a45f,2010-08-11 07:27:27.0,223.0,"I don't think the distinction you make is really related to the difference between data mining and statistical analysis. You are talking about the difference between exploratory analysis and modelling-prediction approach. \\n\\nI think the tradition of statisic is build with all steps : \\n exploratory analysis, then modeling, then estimation, then testing, then forecasting/infering. Statistician do exploratory analysis to figure out what the data looks like (function summary under R !) \\nI guess datamining is less structured and could be identified with exploratory analysis. However it uses techniques from statistics that are from estimation, forecasting, classification ....    ",,
3465,4,1525,189d5aa1-e897-495b-a8e5-23ac9dbd78ae,2010-08-11 07:43:43.0,74.0,"Your blonde girlfriend asks, ""Hey, how's a probability different than a plain old proportion?"" Answer her. ",edited title,
3466,2,1527,342eea59-1ba1-4ada-86f8-21cdc2a7db7d,2010-08-11 07:46:40.0,183.0,"If you flip a fair coin 10 times and it comes up heads 3 times, the **proportion** of heads is .30 but the **probability** of a head on any one flip is .50.",,
3467,2,1528,6426aa8c-b95a-4787-a7b9-6ee2e41744aa,2010-08-11 07:50:04.0,17.0,"A proportion implies it is a guaranteed event, whereas a probability is not.\\n\\nIf you eat hamburgers 14% of the time, in a given (4-week) month (or over whatever interval you based your proportion on), you must have eaten 4 hamburgers; whereas with probability there is a possibility of having eaten no hamburgers at all or perhaps eaten a hamburger everyday.\\n\\nProbability is a measure of uncertainty, whereas proportion is a measure of certainty.",,
3468,6,1520,c4c70f9c-f541-457a-8237-3a3aeb2d2e65,2010-08-11 08:15:36.0,8.0,<probability><games>,edited tags,
3469,4,1525,69f0d26e-d94e-42d6-af2f-bdc38656391b,2010-08-11 08:17:23.0,8.0,What's the difference between a probability and a proportion,edited title,
3470,4,1525,968b3392-de28-41f6-88d8-fc053eea5823,2010-08-11 08:26:11.0,74.0,What's the difference between a probability and a proportion?,edited title,
3471,5,421,d733de28-55d8-4b51-8726-b570a14f5734,2010-08-11 08:26:44.0,509.0,"What book would you recommend for scientists who are not statisticians?\\n\\nClear delivery is most appreciated. As well as the explanation of the appropriate techniques and methods for typical tasks: time series analysis, presentation and aggregation of large data sets.\\n",added 2 characters in body; edited title,
3472,4,421,d733de28-55d8-4b51-8726-b570a14f5734,2010-08-11 08:26:44.0,509.0,What book would you recommend for non-statisticians?,added 2 characters in body; edited title,
3473,5,1434,afd01f5c-b9de-4500-8da8-88f08d406f1d,2010-08-11 08:37:11.0,509.0,"""[How to Tell the Liars from the Statisticians][1]"" by Hooke. I am fond of its way of explaining the concepts of statistics to laypersons.\\n\\nAs for explaining the motivations of statisticians, ""The Lady Tasting Tea"" is good reading.\\n\\n  [1]: http://www.amazon.com/dp/0824718178\\n",Removed meta information.,
3474,5,592,23e70d21-3e52-49fd-9b2b-0a4723459302,2010-08-11 08:38:27.0,509.0,"It is a bit old, but I have found Chris Chatfield's book,\\n\\n[Statistics for Technology: A Course in Applied Technology][1]\\n\\nto be an excellent introduction.\\n\\nIt was how I first learned about statistics from a conceptual point of view.\\n\\n  [1]: http://www.amazon.com/Statistics-Technology-Applied-Chapman-Statistical/dp/0412253402/ref=ntt_at_ep_dpt_2\\n",added 1 characters in body,
3475,5,488,bd3a2954-ef49-4728-a871-c0e1e1a6dcc0,2010-08-11 08:42:35.0,509.0,A lot of Social Science / Psychology students with minimal mathematical background like Andy Field's book: [Discovering Statistics Using SPSS][1]. He also has a website that shares a [lot of material][2].\\n\\n  [1]: http://www.amazon.com/Discovering-Statistics-Introducing-Statistical-Methods/dp/0761944524\\n  [2]: http://www.statisticshell.com/woodofsuicides.html,Added link to the book.,
3476,5,436,46958ac5-b5be-4111-aea0-545c4e7dfe57,2010-08-11 08:46:15.0,509.0,"The answer would most definitely depend on their discipline, the methods/techniques that they would like to learn and their existing mathematical/statistical abilities.\\n\\nFor example, economists/social scientists who want to learn about cutting edge empirical econometrics could read Angrist and Pischke's [Mostly Harmless Econometrics][1]. This is a non-technical book covering the ""natural experimental revolution"" in economics. The book only presupposes that they know what regression is.\\n\\nBut I think the best book on applied regression is Gelman and Hill's [Data Analysis Using Regression and Multilevel/Hierarchical Models][2]. This covers basic regression, multilevel regression, and Bayesian methods in a clear and intuitive way. It would be good for any scientist with a basic background in statistics.\\n\\n  [1]: http://www.amazon.com/Mostly-Harmless-Econometrics-Empiricists-Companion/dp/0691120358\\n  [2]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X\\n",Added links to the books.,
3477,5,423,2e347005-52d7-4f9b-abd1-cb638923071e,2010-08-11 08:49:02.0,509.0,This is one of my favorites:\\n\\n![alt text][1]\\n\\nOne entry per answer. This is in the vein of the Stack Overflow question *[What’s your favorite “programmer” cartoon?][2]*.\\n\\nP.S. Do not hotlink the cartoon without the site's permission please.\\n\\n  [1]: http://imgs.xkcd.com/comics/correlation.png\\n  [2]: http://stackoverflow.com/questions/84556/whats-your-favorite-programmer-cartoon,Named the link.,
3478,5,694,a8fadd3a-e125-4ba2-baa2-4d26790bbfb5,2010-08-11 08:50:04.0,509.0,"This isn't technically a cartoon, but close enough:\\n\\n![alt text][1]\\n\\n  [1]: http://www.austinthirdgen.org/upload/piechart.jpg\\n",added 2 characters in body,
3479,5,656,63fa10ca-195f-4bd4-86f6-e0fc76994d9d,2010-08-11 08:50:54.0,509.0,![Image at bp1.blogger.com.][1]\\n\\n  [1]: http://bp1.blogger.com/_x7QjiZypFj0/Rp9dcGTsBKI/AAAAAAAAAA0/VWwfWDv6nzM/s400/Outlier.jpg\\n,Alt text.,
3480,5,546,2f31244e-cf91-4e58-8681-f5e7333567f3,2010-08-11 08:54:25.0,509.0,Another from [xkcd][1]:\\n\\n![Coconuts are so far down to the left they couldn't be fit on the chart.  Ever spent half an hour trying to open a coconut with a rock?  Down with coconuts.][2]\\n\\n  [1]: http://xkcd.com/388/\\n  [2]: http://imgs.xkcd.com/comics/fuck_grapefruit.png\\n,added 2 characters in body; added 5 characters in body,
3481,10,1521,5d88d474-6e0e-4d80-89f8-09e6e7f2a853,2010-08-11 09:09:43.0,-1.0,"{""OriginalQuestionIds"":[6],""Voters"":[{""Id"":74,""DisplayName"":""el chief""},{""Id"":5,""DisplayName"":""Shane""}]}",1,
3482,5,1521,7f2aa19d-60aa-4e93-a595-1e1c09acb10d,2010-08-11 09:09:43.0,-1.0,"> **Possible Duplicate:**  
> [The Two Cultures: statistics vs. machine learning?](http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning)  

<!-- End of automatically inserted text -->

What is the difference between data mining and statistical analysis?\\n\\nFor some background, my statistical education has been, I think, rather traditional.  A specific question is posited, research is designed, and data are collected and analyzed to offer some insight on that question.  As a result, I've always been skeptical of what I considered ""data dredging""--looking for patterns in a large dataset and using these patterns to draw conclusions.  I tend to associate the latter with data-mining and have always considered this somewhat unprincipled--along with things like algorithmic variable selection routines.\\n\\nNonetheless, there is a large and growing literature on data mining.  Often I see this label referring to specific techniques--clustering, tree-based classification, etc.  Yet, at least from my perspective, these techniques can be ""set loose"" on a set of data or used in a structured way to address a question.  I'd call the former data mining and the latter statistical analysis.\\n\\nI work in academic administration and have been asked to do some ""data mining"" to identify issues and opportunities.  Consistent with my background, my first questions were--what do you want to learn and what are the things that you think contribute to issue.  From their response, it was clear that me and the person asking the question had different ideas on the nature and value of data mining.  Perhaps my take akin to the difference between supervised and unsupervised learning?",insert duplicate link,
3483,11,1521,6b66861e-13af-40c4-98d1-7c54145b62d3,2010-08-11 09:13:41.0,5.0,"{""Voters"":[{""Id"":5,""DisplayName"":""Shane""}]}",,
3484,5,546,974ea4c0-e21a-4f6f-8136-22826d5fd2a7,2010-08-11 09:20:21.0,509.0,Another one from [xkcd][1]:\\n\\n![Coconuts are so far down to the left they couldn't be fit on the chart.  Ever spent half an hour trying to open a coconut with a rock?  Down with coconuts.][2]\\n\\n  [1]: http://xkcd.com/388/\\n  [2]: http://imgs.xkcd.com/comics/fuck_grapefruit.png\\n,added 4 characters in body,
3485,2,1529,ca396838-8afc-4b12-955a-5c38e1fde33d,2010-08-11 10:09:24.0,8.0,"When I carry out multiple linear regression, I use the the (internal) Studentized residuals. These are defined as\\n\\n\\begin{equation}\\ne^*_i = \\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\n\\end{equation}\\n\\nwhere $e_i$ is the residual and $h_{ii}$ are the diagonal elements of the hat matrix. To get these residuals in R, you can use `rstandard` command.\\n\\nWhat type of residuals to do people routinely use? \\n\\nNote: I'm not that interested in papers that define a new type of residual that no-one every uses.\\n",,
3486,1,1529,ca396838-8afc-4b12-955a-5c38e1fde33d,2010-08-11 10:09:24.0,8.0,What type of residuals do you use for regression,,
3487,3,1529,ca396838-8afc-4b12-955a-5c38e1fde33d,2010-08-11 10:09:24.0,8.0,<regression><residuals>,,
3488,6,1519,9e2b2e9e-84de-442a-8a8b-a2b3c3ad0000,2010-08-11 10:09:50.0,8.0,<outliers><hypothesis-testing>,edited tags,
3489,5,1520,cdc87998-1694-4994-bbfd-fbdadaf44439,2010-08-11 10:10:21.0,8.0,"\\nCame across an interesting problem today. You are given a coin and x money, you double money if you get heads and lose half if tails on any toss.\\n\\n   1. What is the expected value of your money in n tries\\n   2. What is the probability of getting more than expected value in (1)\\n\\nThis is how I approached it. The probability of heads and tails is same (1/2). Expected value after first toss = 1/2(2*x) + 1/2(1/2*x) = 5x/4 So expected value is 5x/4 after first toss. Similarly repeating second toss expectation on 5x/4, Expected value after second toss = 1/2(2*5x/4) + 1/2(1/2*5x/4) = 25x/16\\n\\nSo you get a sequence of expected values: 5x/4, 25x/16, 125x/64, ...\\n\\nAfter n tries, your expected value should be 5^n/4^n * x.\\n\\nIf n is large enough, your expected value should approach the mean of the distribution. So probability that value is greater than expected value should be 0.5. I am not sure about this one.\\n",deleted 2 characters in body; edited title,
3490,4,1520,cdc87998-1694-4994-bbfd-fbdadaf44439,2010-08-11 10:10:21.0,8.0,The expected value of random variable on tosses of a  dice,deleted 2 characters in body; edited title,
3491,5,1529,8e0b7b30-b600-41b5-9fa8-1602edf2dfd2,2010-08-11 10:23:44.0,159.0,"When I carry out multiple linear regression, I use the the (internal) Studentized residuals. These are defined as\\n\\n\\begin{equation}\\ne^*_i = \\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\n\\end{equation}\\n\\nwhere $e_i$ is the residual and $h_{ii}$ are the diagonal elements of the hat matrix. To get these residuals in R, you can use `rstandard` command.\\n\\nWhat type of residuals to do people routinely use? \\n\\nNote: I'm not that interested in papers that define a new type of residual that no-one ever uses.\\n",deleted 1 characters in body; edited title,
3492,4,1529,8e0b7b30-b600-41b5-9fa8-1602edf2dfd2,2010-08-11 10:23:44.0,159.0,What type of residuals do you use for regression?,deleted 1 characters in body; edited title,
3493,2,1530,dde0104a-4630-41a9-96b9-3d14bf0ef726,2010-08-11 10:29:48.0,609.0,"In addition to the rmeta package there is also the meta package in R, which produce publication quality plots.",,
3494,2,1531,f90af355-9a53-4ce7-aa7c-ce51fa10d289,2010-08-11 10:30:31.0,588.0,"I am working on some MRSA data and need to calculate the relative risk of a group of hospitals compared with the remaining hospital.\\n\\nMy colleagues throws me an excel with a formula inside to calculate the ""exact confidence interval of relative risk"", I can do the calculation without difficulties, but I have no idea on how and why this formula is used for do such calculation.\\n\\nI have attached the excel file [here][1] for your reference.\\n\\nCan anyone show me a reference on the rationale of the calculation? Article from textbooks will be fine to me. Thanks!\\n\\n\\n  [1]: http://www.speedfile.org/987618",,
3495,1,1531,f90af355-9a53-4ce7-aa7c-ce51fa10d289,2010-08-11 10:30:31.0,588.0,"calculate the ""exact confidence interval"" for relative risk",,
3496,3,1531,f90af355-9a53-4ce7-aa7c-ce51fa10d289,2010-08-11 10:30:31.0,588.0,<confidence-interval><epidemiology>,,
3497,5,1529,dda50b9a-bddd-4f66-941a-7b987ea1579e,2010-08-11 10:32:13.0,159.0,"When I carry out multiple linear regression, I use the (internal) Studentized residuals. These are defined as\\n\\n\\begin{equation}\\ne^*_i = \\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\n\\end{equation}\\n\\nwhere $e_i$ is the residual and $h_{ii}$ are the diagonal elements of the hat matrix. To get these residuals in R, you can use the `rstandard` command.\\n\\nWhat type of residuals to do people routinely use? \\n\\nNote: I'm not that interested in papers that define a new type of residual that no-one ever uses.\\n",edited body,
3498,2,1532,2468ea7e-bf1b-45d5-901c-ddbebabdff1b,2010-08-11 10:36:15.0,251.0,"Jerome Friedman wrote a paper a while back: [*Data Mining and Statistics: What's the Connection?*][1], which I think you'll find interesting.\\n\\nData mining was a largely commercial concern and driven by business needs (coupled with the ""need"" for vendors to sell software and hardware systems to businesses).  One thing Friedman noted was that all the ""features"" being hyped originated outside of statistics -- from algorithms and methods like neural nets to GUI driven data analysis -- and none of the traditional statistical offerings seemed to be a part of any of these systems (regression, hypothesis testing, etc).  ""Our core methodology has largely been ignored.""  It was also sold as user driven along the lines of what you noted: here's my data, here's my ""business question"", give me an answer.\\n\\nI think Friedman was trying to provoke.  He didn't think data mining had serious intellectual underpinnings where methodology was concerned, but that this would change and statisticians ought to play a part rather than ignoring it.  \\n\\nMy own impression is that this has more or less happened.  The lines have been blurred.   Statisticians now publish in data mining journals.  Data miners these days seem to have some sort of statistical training.  While data mining packages still don't hype generalized linear models, logistic regression is well known among the analysts -- in addition to clustering and neural nets.  Optimal experimental design may not be part of the data mining core, but the software can be coaxed to spit out p-values.  Progress!\\n\\n\\n  [1]: http://www-stat.stanford.edu/~jhf/ftp/dm-stat.pdf\\n\\n\\n\\n",,
3499,11,944,f28729a3-6ede-44fb-afdd-07bcbc165c5c,2010-08-11 10:41:22.0,8.0,"{""Voters"":[{""Id"":8,""DisplayName"":""csgillespie""}]}",,
3500,11,1160,96f007b4-207e-401d-a492-62246f1ab9ba,2010-08-11 10:43:10.0,8.0,"{""Voters"":[{""Id"":8,""DisplayName"":""csgillespie""}]}",,
3501,6,1395,17358232-7140-4003-8259-cb2891baf3ef,2010-08-11 10:53:09.0,8.0,<data-visualization><funnel-plot><java>,edited tags,
3502,6,1531,c4441288-679b-4436-bfa6-c1d346734dbc,2010-08-11 10:53:41.0,8.0,<confidence-interval><epidemiology><relative-risk>,edited tags,
3503,2,1533,a8112bf4-ea5f-4186-b7c4-b0ca5b11b734,2010-08-11 11:53:51.0,339.0,"This seems to be Fisher's Exact Test for Count Data.\\nYou can reproduce the results in R by giving:\\n\\n\\n    data <- matrix(c(678,4450547,63,2509451),2,2)\\n    fisher.test(data)\\n    \\n    data:  data \\n    p-value < 2.2e-16\\n    alternative hypothesis: true odds ratio is not equal to 1 \\n    95 percent confidence interval:\\n     4.682723 7.986867 \\n    sample estimates:\\n    odds ratio \\n      6.068817 ",,
3504,2,1534,ff6f093c-9024-43cd-af17-0d7fc66c7735,2010-08-11 12:12:35.0,867.0,"I've been asked to give some advice for some clinicians who are comparing two different methods of blood pressure measurement.  I suggested to them that we should proceed with a two-one-sided-test technique to determine equivalence of the two techniaues.\\n\\nUnfortunately I have now learned that the clinicians have multiple measurements of blood pressure by each of the two methods and that the blood pressure can be quite variable within each patient during the period of observation (they are theatre cases).\\n\\nIs it possible to use some multiple regression technique to perform an equivalence test?  Can I simply use confidence intervals to determine variability between the two techniques whilst accounting for inter-patient variability if I use the patients as factors in the regression model?\\n\\nSorry it's an amateur question, but despite the Masters degree, I still feel like quite the amateur!",,
3505,1,1534,ff6f093c-9024-43cd-af17-0d7fc66c7735,2010-08-11 12:12:35.0,867.0,Best method for comapring multiple ranging measures,,
3506,3,1534,ff6f093c-9024-43cd-af17-0d7fc66c7735,2010-08-11 12:12:35.0,867.0,<multiple-comparisons><equivalence>,,
3508,2,1536,0c020eb2-416f-4d1e-ac46-5eeda3e75797,2010-08-11 12:51:51.0,,Does mutual information discriminate against fold change differences?,,ambivalance
3509,1,1536,0c020eb2-416f-4d1e-ac46-5eeda3e75797,2010-08-11 12:51:51.0,,What are the advantages of using mutual information over Pearson correlation in network inference?,,ambivalance
3510,3,1536,0c020eb2-416f-4d1e-ac46-5eeda3e75797,2010-08-11 12:51:51.0,,<correlation><mutual-information>,,ambivalance
3511,2,1537,feb0f7ac-bc7e-4489-9ed5-4f736b6014f0,2010-08-11 12:56:09.0,8.0,"> If n is large enough, your expected\\n> value should approach the mean of the\\n> distribution.\\n\\nYes that's correct. \\n\\n> So probability that value is greater\\n> than expected value should be 0.5.\\n\\nThis would only be correct if the distribution is symmetric - which in your game isn't the case. You can see this easily if you think about what the median value of $x$ should be after $n$ throws.\\n\\n_____\\n\\nYou can think of your problem as a [random walk][1]. A basic one-dimensional random walk is a walk on the integer real line, where at each point we move $\\pm 1$ with probability $p$. This is exactly what you have **if** we ignore the doubling/halving of money and set $p=0.5$. All we have to do is remap your coordinate system to this example. So\\n\\n    x*2^{-2} = -2\\n    x*2^{-1} = -1 \\n      x = 0\\n     x*2 = 1  \\n\\ni.e. $2^k x=k$. Let $S_n$ denote how much money we have made from the game after $n$ turns, then\\n\\n\\begin{equation}\\nPr(S_n = 2^k x) = 2^{-n} \\binom{n}{(n+k)/2}\\n\\end{equation}\\n\\nThe above result is a standard result from Random walks. Google random walks for more info. Also from random walk theory, we can calculate the median return to be $x$, which is not the same as the expected value.\\n\\n\\n----\\n**For completeness**\\n\\n    Here's a quick simulation in R of your process:\\n    \\n    #Simulate 10 throws with a starting amount of 10\\n    simulate = function(){\\n      x = 10\\n      for(i in 1:10){\\n        if(runif(1) < 0.5)\\n          x = x/2\\n        else\\n          x = 2*x\\n      }\\n      return(x)\\n    }\\n    \\n    N = 1000\\n    money = numeric(N)\\n    for(i in 1:N)\\n      money[i]= simulate()\\n    \\n    mean(x);median(x)\\n    #Probabilities\\n    #Simulated\\n    table(x)/1000\\n    #Exact\\n    2^{-10}*choose(10,10/2)\\n\\n    #Plot the simulations\\n    plot(x)\\n\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Random_walk\\n",,
3512,6,846,1153bb8a-53dc-4269-b8e1-dc4a1a3240a7,2010-08-11 12:58:41.0,8.0,<regression>,edited tags,
3513,5,1529,143af5ed-bb99-4c69-8dd1-993c9283da18,2010-08-11 12:59:59.0,8.0,"When I carry out OLS multiple linear regression, I use (internal) Studentized residuals. These are defined as\\n\\n\\begin{equation}\\ne^*_i = \\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\n\\end{equation}\\n\\nwhere $e_i$ is the residual and $h_{ii}$ are the diagonal elements of the hat matrix. To get these residuals in R, you can use the `rstandard` command.\\n\\nWhat type of residuals to do people routinely use? \\n\\nNote: I'm not that interested in papers that define a new type of residual that no-one ever uses.\\n",Changed the title following Dirk's answer; deleted 4 characters in body,
3514,4,1529,143af5ed-bb99-4c69-8dd1-993c9283da18,2010-08-11 12:59:59.0,8.0,What type of post-fit analysis of residuals do you use,Changed the title following Dirk's answer; deleted 4 characters in body,
3515,5,1529,753c9b70-6cc6-489b-b1f7-e4354c4010d1,2010-08-11 13:06:46.0,8.0,"When carrying out OLS multiple linear regression, rather than plot the residuals against fitted values,  I plot the (internal) Studentized residuals against fitted values (ditto for covariates). These residuals are defined as:\\n\\n\\begin{equation}\\ne^*_i = \\frac{e_i}{\\sqrt{s^2 (1-h_{ii})}}\\n\\end{equation}\\n\\nwhere $e_i$ is the residual and $h_{ii}$ are the diagonal elements of the hat matrix. To get these studentized residuals in R, you can use the `rstandard` command. \\n\\nWhat type of residuals to do people routinely use in this context? For example, do you just stick with $e_i$ or do you use jackknife residuals, or something else entirely.\\n\\nNote: I'm not that interested in papers that define a new type of residual that no-one ever uses.\\n",added 250 characters in body,
3516,6,1520,ce62e6c4-e4f5-4a52-8481-957f136d4327,2010-08-11 13:09:08.0,8.0,<probability><games><stochastic-processes>,edited tags,
3517,5,1534,386a9ad7-3cfb-448a-a5e4-10819034dd62,2010-08-11 13:09:20.0,5.0,"I've been asked to give some advice for some clinicians who are comparing two different methods of blood pressure measurement.  I suggested to them that we should proceed with a two-one-sided-test technique to determine equivalence of the two techniques.\\n\\nUnfortunately I have now learned that the clinicians have multiple measurements of blood pressure by each of the two methods and that the blood pressure can be quite variable within each patient during the period of observation (they are theatre cases).\\n\\nIs it possible to use some multiple regression technique to perform an equivalence test?  Can I simply use confidence intervals to determine variability between the two techniques whilst accounting for inter-patient variability if I use the patients as factors in the regression model?\\n\\nSorry it's an amateur question, but despite the Masters degree, I still feel like quite the amateur!",Corrected spelling.,
3518,4,1534,386a9ad7-3cfb-448a-a5e4-10819034dd62,2010-08-11 13:09:20.0,5.0,Best method for comparing multiple ranging measures,Corrected spelling.,
3519,2,1538,11ffb4d0-8add-497a-8913-51671a765d3d,2010-08-11 13:15:24.0,650.0,"The Kolmogorov–Smirnov distribution is known from the [Kolmogorov–Smirnov test](http://en.wikipedia.org/wiki/Kolmogorov_Smirnov). However, it is also the distribution of the supremum of the Brownian bridge.\\n\\nSince this is far from obvious (to me), I would like to ask you for an intuitive explanation of this coincidence. References are also welcome.",,
3520,1,1538,11ffb4d0-8add-497a-8913-51671a765d3d,2010-08-11 13:15:24.0,650.0,Why does the supremum of the Brownian bridge have the Kolmogorov–Smirnov distribution?,,
3521,3,1538,11ffb4d0-8add-497a-8913-51671a765d3d,2010-08-11 13:15:24.0,650.0,<distributions><statistics><stochastic-processes>,,
3522,2,1539,826c1679-3454-4532-9991-de11b121e31e,2010-08-11 13:34:07.0,223.0,"\\n\\n$\\sqrt{n}\\sup_x|F_n-F|=\\sup_x|\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(t)| $ \\n\\nwhere $Z_i(x)=1_{X_i\\leq x}-E[1_{X_i\\leq x}]$\\n\\nby CLT you have \\n$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(x)\\rightarrow \\mathcal{N}(0,F(x)(1-F(x)))$\\n\\nhence the test ... ",,
3523,2,1540,75dbf729-67c8-4054-bde9-398dde624903,2010-08-11 13:38:19.0,870.0,"Assume you draw a uniformly distributed random number between 0 and 1 *n* times.  How would one go about calculating the expected *minimum* number drawn after n trials?  \\n\\nIn addition, how would one go about calculating a confidence interval to state that the minimum number drawn is in the interval [a,b] with m% confidence?",,
3524,1,1540,75dbf729-67c8-4054-bde9-398dde624903,2010-08-11 13:38:19.0,870.0,What is the expected MINIMUM value drawn from a uniform distribution between 0 and 1 after n trials?,,
3525,3,1540,75dbf729-67c8-4054-bde9-398dde624903,2010-08-11 13:38:19.0,870.0,<confidence-interval><random-variable><minimum>,,
3526,2,1541,59855399-bb08-4d05-ad22-d319d2176518,2010-08-11 13:47:37.0,869.0,"> It must be indicative of something\\n> besides the redistribution of wealth.  \\n\\n> Heads.  \\n\\n> A weaker man might be moved to\\n> re-examine his faith, for nothing else\\n> at least in the law of probability...  \\n\\n> Heads.   \\n\\n> Consider. One, probability is a\\n> factor which operates within natural\\n> forces. Two, probability is not\\n> operating as a factor. Three, we are\\n> now held within um... sub or\\n> supernatural forces. Discuss!  \\n\\n> What?  \\n\\n> Look at it this way. If six monkeys...\\n> If six monkeys... The law of averages,\\n> if I have got this right means... that\\n> if six monkeys were thrown up in the\\n> air long enough... they would land on\\n> their tails about as often as they\\n> would land on their...  \\n\\n> Heads, getting\\n> a bit of a bore, isn't it?\\n\\n> &ndash; Tom Stoppard *Rosencrantz and Guildenstern are Dead* (1966)\\n\\nAs John Christie pointed out, no matter how unlikely the student's result was, you can't infer anything from a single trial. A clever student might well have tried this gambit knowing it could not be refuted, in which case I might be inclined to commend her. \\n\\nIncidentally, Rosencrantz (or Guildenstern) tossed at least 157 consecutive heads and it was nothing to write home about.",,
3527,2,1542,a4537239-fcda-4e45-acca-7830fcf58b53,2010-08-11 13:48:20.0,125.0,"i'm not sure how to google for this as i am not very familiar with time series analysis.\\n\\ni have 500 websites, and i am measuring the number of visitors to each website each day.  at some point, i turn on SEO (search engine optimization) for each of the websites.  this happens on different days for different websites.  the distribution of visitors by account for any given day has a long tail to the right.  SEO may not have an immediate effect; it might take a few days/weeks to really start to see some results.\\n\\ni want to measure something like the ""average"" lift in the number of visitors, but an average is probably not going to do the trick because of the mix of websites.  (a daily/weekly/monthly trend curve would be really cool, but the average problem will exist there, too)\\n\\ni can probably average the number of visitors per day for any given account, but i can't do it across accounts.\\n\\ndo i simply need to segment the websites into ""number of visitor"" groups?  what other kinds of approaches should i read about?",,
3528,1,1542,a4537239-fcda-4e45-acca-7830fcf58b53,2010-08-11 13:48:20.0,125.0,analysis of multiple time series,,
3529,3,1542,a4537239-fcda-4e45-acca-7830fcf58b53,2010-08-11 13:48:20.0,125.0,<time-series>,,
3530,2,1543,e582116e-e0e4-4027-b1c8-360e9b1c1885,2010-08-11 13:53:56.0,,"You are looking for [order statistics][1]. The wiki indicates that the distribution of the minimum draw from a uniform distribution between 0 and 1 after $n$ trials is a beta distribution (I have not checked it for correctness which you should probably do.). Specifically, let $U_{(1)}$ be the minimum order statistic. Then:\\n\\n$U_{(1)} \\sim B(1,n)$\\n\\nTherefore, the mean is $\\frac{1}{1+n}$. You can use the beta distribution to identify $a$ and $b$ such that \\n\\n$Prob(a \\le U_{(1)} \\le b) = 0.95$. \\n\\nBy the way, the use of the term confidence interval is not appropriate in this context as you are not performing inference.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Order_statistic",,user28
3531,2,1544,3dd6f32b-439c-4852-a0e0-4bc7f4081bb4,2010-08-11 14:05:25.0,601.0,It looks to me like [mixed effects or multi-level modelling][1] is what you want here.\\n\\n\\n  [1]: http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-mixed-models.pdf,,
3532,5,1501,a2cf6672-01fc-4b16-92c4-7345f505c7f4,2010-08-11 14:06:20.0,114.0,"I'm interested in the process of testing or validating a particular implementation of a statistical method, and what datasets and/or published analysis exist that could be used to do this in practice.\\n\\nFor instance, if I write an al-gore-rythm to implement a simple linear regression, I might feed in soem numbers and check the result looks good, or I might feed numbers into my code and some other system and compare.  In some cases, people seem to have already done this and then publish the numbers and results which could be defined as reference data.\\n\\nTo start off, the best one I know is the [NIST Statistical Reference Datasets][1] page that publishes a wide range of datasets and calculations that covers areas such as Analysis of Variance, Linaear Regression, Markov Chain and Monte Carlo simulation and Non-Linear regression.\\n\\nAre there any other good / notable ones out there.\\n\\n  [1]: http://www.itl.nist.gov/div898/strd/index.html\\n\\nEdit: I reworded to make clear that I'm not just looking for open datasets, but I'm interested in datasets and solutions to specific statistical problems that could be used to test an implementation of a technique.",Reworded to better explain,
3533,4,1501,a2cf6672-01fc-4b16-92c4-7345f505c7f4,2010-08-11 14:06:20.0,114.0,What resources/methods exist for testing/validation or evaluation of Statistical Methods,Reworded to better explain,
3534,2,1545,c58b20d8-a02c-45ed-a4dc-46b8e001da8d,2010-08-11 14:06:35.0,702.0,"How about a simulation based approach?  Here's some R code to generate 100000 students each trying the 40 tosses.\\n    \\n    theSum = c()\\n    for (i in 1:100000) {\\n      coin1 = rbinom(40,1,.5)\\n      coin2 = rbinom(40,1,.5)\\n      coin3 = rbinom(40,1,.5)\\n      coin4 = rbinom(40,1,.5)\\n      coin5 = rbinom(40,1,.5)\\n      theSum[i] = sum(coin1+coin2+coin3+coin4+coin5 == 1)\\n    }\\n    \\n    summary(theSum)\\n    hist(theSum, xlim = c(0,40), freq = F, main = """", xlab = """")\\n\\nThe range of times the HTTTT combination occurred (in any order): 0-18 (out of 40), with a mean of around 6.  \\n\\nBelow: a histogram of the 100000 attempts and how many times the magical combination occurred.  You'd have to be very lucky indeed to get it 39 times out of 40 with fair coins.  But stranger things have happened by chance (e.g., our evolution).\\n\\n![alt text][1]\\n\\n\\n  [1]: http://img80.imageshack.us/img80/9268/coinflips.png",,
3535,2,1546,770846a8-ad29-4d63-925d-93b31f1ae184,2010-08-11 14:10:21.0,5.0,"Any study should start out with some conception of a goal.  Are you interested in measuring the impact of your SEO?  Or are you trying to model the visitor behavior?  \\n\\nIt isn't clear to me that you're making use of the ""time series"" aspect of this data.  Are you also interested in the time of day or day of week of visits, for instance?  Or visits around specific events?  You could just as easily divide your # of visits/day into two groups -- with/without SEO -- and eliminate time.  This would then be a categorical variable in your data.  \\n\\nA basic next step could be to run a logistic regression to see the impact of SEO on your site traffic.  In R, this could look something like this (where ""seo"" is a `factor`):\\n\\n    site.traffic.lg <- glm(num.visits ~ seo, family=binomial, data=your.data)\\n    summary(site.traffic.lg)",,
3536,5,1546,e6b03564-2a09-47a5-92b2-6c7101afc4b3,2010-08-11 14:16:42.0,5.0,"Any study should start out with some conception of a goal.  Are you interested in measuring the impact of your SEO?  Or are you trying to model the visitor behavior?  \\n\\nIt isn't clear to me that you're making use of the ""time series"" aspect of this data.  Are you also interested in the time of day or day of week of visits, for instance?  Or visits around specific events?  You could just as easily divide your # of visits/day into two groups -- with/without SEO -- and eliminate time.  This would then be a categorical variable in your data.  \\n\\nA basic next step could be to run a logistic regression to see the impact of SEO on your site traffic.  In R, this could look something like this (where ""seo"" is a `factor`):\\n\\n    site.traffic.lg <- glm(num.visits ~ seo, family=binomial, data=your.data)\\n    summary(site.traffic.lg)\\n\\nIf you want to use the fact that this is across a number of different kinds of sites, you could include this by adding it into the formula as another variable.",added 163 characters in body,
3537,6,1540,9d4120b1-00a5-4b5d-b25e-8154059b8db4,2010-08-11 14:23:13.0,,<minimum><uniform><order-statistics>,edited tags; edited tags,user28
3538,2,1547,2f253d9d-223c-467e-8da2-336ede0534cb,2010-08-11 14:37:06.0,339.0,"Data mining is categorized as either Descriptive or Predictive. Descriptive data mining is to search massive data sets and discover the locations of unexpected structures or relationships, patterns, trends, clusters, and outliers in the data. On the other hand, Predictive is to build models and procedures for regression, classification, pattern recognition, or machine learning tasks, and assess the predictive accuracy of those models and procedures when applied to fresh data.\\n\\nThe mechanism used to search for patterns or structure in high-dimensional data might be manual or automated; searching might require interactively querying a database management system, or it might entail using visualization software to spot anomalies in the data. In machine-learning terms, descriptive data mining is known as unsupervised learning, whereas predictive data mining is known as supervised learning.\\n\\nMost of the methods used in data mining are related to methods developed in statistics and machine learning. Foremost among those methods are the general topics of regression, classification, clustering, and visualization. Because of the enormous sizes of the data sets, many applications of data mining focus on dimensionality-reduction techniques (e.g., variable selection) and situations in which high-dimensional data are suspected of lying\\non lower-dimensional hyperplanes. Recent attention has been directed to methods of identifying high-dimensional data lying on nonlinear surfaces or manifolds.\\n\\nThere are also situations in data mining when *statistical inference* — in its classical sense — either has no meaning or is of dubious validity: the former occurs when we have the *entire population* to search for answers, and the latter occurs when a data set is a “convenience” sample rather than being a random sample drawn from some large population. When data are collected through time (e.g., retail transactions, stock-market transactions, patient records, weather records), sampling also may not make sense; the time-ordering of the observations is crucial to understanding the phenomenon generating the data, and to treat the observations as independent when they may be highly correlated will provide biased results.\\n\\nThe central components of data mining are — in addition to statistical theory and methods\\n— computing and computational efficiency, automatic data processing, dynamic and interactive data visualization techniques, and algorithm development.\\n\\nOne of the most important issues in data mining is the computational problem of *scalability*. Algorithms developed for computing standard exploratory and confirmatory statistical methods were designed to be fast and computationally efficient when applied to small and medium-sized data sets; yet, it has been shown that most of these algorithms are not up to the challenge of handling huge data sets. As data sets grow, many existing\\nalgorithms demonstrate a tendency to slow down dramatically (or even grind to a halt).\\n",,
3539,6,1538,612489bc-225d-4e8a-87d3-e1979e1c8c7b,2010-08-11 14:50:40.0,650.0,<distributions><statistics><stochastic-processes><intuition>,edited tags,
3540,5,1521,b78715dc-3a34-431b-9762-a223e7f9e77a,2010-08-11 14:51:01.0,485.0,"> **Possible Duplicate:**  \\n> [The Two Cultures: statistics vs. machine learning?](http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning)  \\n\\n<!-- End of automatically inserted text -->\\n\\nWhat is the difference between data mining and statistical analysis?\\n\\nFor some background, my statistical education has been, I think, rather traditional.  A specific question is posited, research is designed, and data are collected and analyzed to offer some insight on that question.  As a result, I've always been skeptical of what I considered ""data dredging""--looking for patterns in a large dataset and using these patterns to draw conclusions.  I tend to associate the latter with data-mining and have always considered this somewhat unprincipled--along with things like algorithmic variable selection routines.\\n\\nNonetheless, there is a large and growing literature on data mining.  Often I see this label referring to specific techniques--clustering, tree-based classification, etc.  Yet, at least from my perspective, these techniques can be ""set loose"" on a set of data or used in a structured way to address a question.  I'd call the former data mining and the latter statistical analysis.\\n\\nI work in academic administration and have been asked to do some ""data mining"" to identify issues and opportunities.  Consistent with my background, my first questions were--what do you want to learn and what are the things that you think contribute to issue.  From their response, it was clear that me and the person asking the question had different ideas on the nature and value of data mining.  Perhaps my take akin to the difference between supervised and unsupervised learning?",added 5 characters in body; edited tags,
3541,6,1521,b78715dc-3a34-431b-9762-a223e7f9e77a,2010-08-11 14:51:01.0,485.0,<statistical-analysis><machine-learning><data-mining>,added 5 characters in body; edited tags,
3542,2,1548,25e726e4-9837-459c-a1e5-70c9c3008f9e,2010-08-11 15:02:36.0,873.0,"For Kolmogorov-Smirnov, consider the null hypothesis.  It says that a sample is drawn from a particular distribution.  So if you construct the empirical distribution function for $n$ samples $f(x) = \\frac{1}{n} \\sum_i \\chi_{(-\\infty, X_i]}(x)$, in the limit of infinite data, it will converge to the underlying distribution.\\n\\nFor finite information, it will be off.  If one of the measurements is $q$, then at $x=q$ the empirical distribution function takes a step up.  We can look at it as a random walk which is constrained to begin and end on the true distribution function.  Once you know that, you go ransack the literature for the huge amount of information known about random walks to find out what the largest expected deviation of such a walk is.\\n\\nYou can do the same trick with any $p$-norm of the difference between the empirical and underlying distribution functions.  For $p=2$, it's called the Cramer-von Mises test.  I don't know the set of all such tests for arbitrary real, positive $p$ form a complete class of any kind, but it might be an interesting thing to look at.",,
3543,5,1543,d5f31d42-2fc1-402d-8c02-bf4c91aeba35,2010-08-11 15:10:55.0,,"You are looking for [order statistics][1]. The wiki indicates that the distribution of the minimum draw from a uniform distribution between 0 and 1 after $n$ trials is a beta distribution (I have not checked it for correctness which you should probably do.). Specifically, let $U_{(1)}$ be the minimum order statistic. Then:\\n\\n$U_{(1)} \\sim B(1,n)$\\n\\nTherefore, the mean is $\\frac{1}{1+n}$. You can use the beta distribution to identify $a$ and $b$ such that \\n\\n$Prob(a \\le U_{(1)} \\le b) = 0.95$. \\n\\nBy the way, the use of the term confidence interval is not appropriate in this context as you are not performing inference.\\n\\n**Update**\\n\\nCalculating $a$ and $b$ such that $Prob(a \\le U_{(1)} \\le b) = 0.95$ is not straightforward. There are several possible ways in which you can calculate $a$ and $b$. One approach is to center the interval around the mean. In this approach, you would set:\\n\\n$a = \\mu - \\delta$ and \\n\\n$b = \\mu + \\delta$\\n\\nwhere\\n\\n$\\mu = \\frac{1}{1+n}$.\\n\\nYou would then calculate $\\delta$ such that the required probability is 0.95. Do note that under this approach you may not be able to identify a symmetric interval around the mean for high $n$ but this is just my hunch.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Order_statistic",update to calculate a and b,user28
3544,5,1539,fadd24dd-175e-4e1e-8b8e-eb7e6ba81fb5,2010-08-11 15:13:41.0,223.0,"\\n$\\sqrt{n}\\sup_x|F_n-F|=\\sup_x|\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(t)| $ \\n\\nwhere $Z_i(x)=1_{X_i\\leq x}-E[1_{X_i\\leq x}]$\\n\\nby CLT you have \\n$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(x)\\rightarrow \\mathcal{N}(0,F(x)(1-F(x)))$\\n\\n\\n\\nhence the test ... \\n\\n(brownian bridge $B(t)$ has variance $t(1-t)$ http://en.wikipedia.org/wiki/Brownian_bridge replace $t$ by $F(x)$ and you are done)",added 136 characters in body,
3545,2,1549,1ae2abfb-8497-4b5b-ad43-5a2151dfc10a,2010-08-11 15:16:55.0,8.0,"As Srikant suggests, you need to look at [order statistics][1].\\n\\nTo add to Srikant's answer, you can simulate this process easily in R:\\n\\n    n = 10\\n    N = 1000;sims = numeric(N)\\n    for(i in 1:N)\\n      sims[i] = min(runif(n))\\n    \\n    hist(sims, freq=FALSE)\\n    x = seq(0,1,0.01)\\n    lines(x, dbeta(x, 1, n), col=2)\\n\\nTo get\\n\\n![alt text][2]\\n\\n_____\\n**Slight digression** \\n\\n\\nThis question is related to one of my favourite statistics problems, the [German tank problem][3]. This problem is about the maximum of uniform distributions, and can be summarised as:\\n\\n> Suppose one is an Allied intelligence\\n> analyst during World War II, and one\\n> has some serial numbers of captured\\n> German tanks. Further, assume that the\\n> tanks are numbered sequentially from 1\\n> to N. How does one estimate the total\\n> number of tanks?\\n\\n> Taken from wikipedia\\n\\nCheck out the wikipedia page for more details.\\n\\n  [1]: http://en.wikipedia.org/wiki/Order_statistic\\n  [2]: http://img441.imageshack.us/img441/6826/tmpe.jpg\\n  [3]: http://en.wikipedia.org/wiki/German_tank_problem",,
3546,2,1550,57a3673d-03e0-499c-95d1-2c6edf70eafa,2010-08-11 15:20:19.0,873.0,"""Efficient"" usually just means that in the class of estimators considered and for a given loss function, you choose one which is optimal.  If you look up ""admissible"" instead, you'll find a huge amount of information, though it's a slightly weaker criterion.",,
3547,2,1551,38cc2443-6c7d-45be-a93d-9cff56711962,2010-08-11 15:20:38.0,5.0,"The difference between statistics and data mining is largely a historical one, since they came from different traditions: statistics and computer science.  Data mining grew in parallel out of work in the area of artificial intelligence and statistics.\\n\\nSection 1.4 from [Witten & Frank][2] summarizes my viewpoint so I'm going to quote it at length:\\n\\n> What's the difference between machine\\n> learning and statistics?  Cynics,\\n> looking wryly at the explosion of\\n> commercial interest (and hype) in this\\n> area, equate data mining to statistics\\n> plus marketing.  In truth, you should\\n> not look for a dividing line between\\n> machine learning and statistics\\n> because there is a continuum--and a\\n> multidimensional one at that--of data\\n> analysis techniques.  Some derive from\\n> the skills taught in standard\\n> statistics courses, and others are\\n> more closely associated with the kind\\n> of machine learning that has arisen\\n> out of computer science. \\n> Historically, the two sides have had\\n> rather different traditions.  If\\n> forced to point to a single difference\\n> of emphasis, it might be that\\n> statistics has been more concerned\\n> with testing hypotheses, whereas\\n> machine learning has been more\\n> concerned with formulating the process\\n> of generalization as a search through\\n> possible hypotheses...\\n>\\n> In the past,\\n> very similar methods have developed in\\n> parallel in machine learning and\\n> statistics...\\n> \\n> But now the two\\n> perspectives have converged.\\n\\n[IMO, data mining and machine learning are *very* closely related terms.  In one sense, machine learning techniques are used in data mining.  I regularly see these terms as interchangeable, and in so far as they are different, they usually go together.  I would suggest looking through [""The Two Cultures"" paper][1] as well as the other threads from my original question.]\\n\\n  [1]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning\\n  [2]: http://www.cs.waikato.ac.nz/~ml/weka/book.html",,
3548,5,1551,cd87b158-c0b3-406e-b98b-4f311e6a4ea0,2010-08-11 15:42:21.0,5.0,"The difference between statistics and data mining is largely a historical one, since they came from different traditions: statistics and computer science.  Data mining grew in parallel out of work in the area of artificial intelligence and statistics.\\n\\nSection 1.4 from [Witten & Frank][2] summarizes my viewpoint so I'm going to quote it at length:\\n\\n> What's the difference between machine\\n> learning and statistics?  Cynics,\\n> looking wryly at the explosion of\\n> commercial interest (and hype) in this\\n> area, equate data mining to statistics\\n> plus marketing.  In truth, you should\\n> not look for a dividing line between\\n> machine learning and statistics\\n> because there is a continuum--and a\\n> multidimensional one at that--of data\\n> analysis techniques.  Some derive from\\n> the skills taught in standard\\n> statistics courses, and others are\\n> more closely associated with the kind\\n> of machine learning that has arisen\\n> out of computer science. \\n> Historically, the two sides have had\\n> rather different traditions.  If\\n> forced to point to a single difference\\n> of emphasis, it might be that\\n> statistics has been more concerned\\n> with testing hypotheses, whereas\\n> machine learning has been more\\n> concerned with formulating the process\\n> of generalization as a search through\\n> possible hypotheses...\\n>\\n> In the past,\\n> very similar methods have developed in\\n> parallel in machine learning and\\n> statistics...\\n> \\n> But now the two\\n> perspectives have converged.\\n\\nN.B.1 IMO, data mining and machine learning are *very* closely related terms.  In one sense, machine learning techniques are used in data mining.  I regularly see these terms as interchangeable, and in so far as they are different, they usually go together.  I would suggest looking through [""The Two Cultures"" paper][1] as well as the other threads from my original question.\\n\\nN.B.2 The term ""data mining"" can have a negative connotation when used colloquially to mean letting some algorithm loose on the data without any conceptual understanding.  The sense is that data mining will lead to spurious results and over-fitting.  I typically avoid using the term when talking to non-experts as a result, and instead use machine learning or statistical learning as a synonym.\\n\\n  [1]: http://stats.stackexchange.com/questions/6/the-two-cultures-statistics-vs-machine-learning\\n  [2]: http://www.cs.waikato.ac.nz/~ml/weka/book.html",added 403 characters in body,
3549,2,1552,961dce01-7f9f-4037-9876-ebfd59a94092,2010-08-11 15:56:33.0,,A statistician confidently tried to cross a river that was 1 meter deep on average. He drowned.,,Reinout Roels
3550,16,1552,961dce01-7f9f-4037-9876-ebfd59a94092,2010-08-11 15:56:33.0,-1.0,,,
3551,5,1539,808c1db8-27fe-43db-a9db-1eb3aa5b64e2,2010-08-11 16:33:10.0,223.0,"$\\sqrt{n}\\sup_x|F_n-F|=\\sup_x|\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(t)| $ \\n\\nwhere $Z_i(x)=1_{X_i\\leq x}-E[1_{X_i\\leq x}]$\\n\\nby CLT you have \\n$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(x)\\rightarrow \\mathcal{N}(0,F(x)(1-F(x)))$\\n\\n\\n\\nhence the test ... \\n\\n(brownian bridge $B(t)$ has variance $t(1-t)$ http://en.wikipedia.org/wiki/Brownian_bridge replace $t$ by $F(x)$. The **difficult** part is to show that the limit is kept by passing to the supremum)",added 66 characters in body,
3552,5,1539,51795777-bff4-4d67-b4fe-fff3513367ff,2010-08-11 16:58:05.0,223.0,"$\\sqrt{n}\\sup_x|F_n-F|=\\sup_x|\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(t)| $ \\n\\nwhere $Z_i(x)=1_{X_i\\leq x}-E[1_{X_i\\leq x}]$\\n\\nby CLT you have \\n$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(x)\\rightarrow \\mathcal{N}(0,F(x)(1-F(x)))$\\n\\nthis is the intuition... \\n\\nbrownian bridge $B(t)$ has variance $t(1-t)$ http://en.wikipedia.org/wiki/Brownian_bridge replace $t$ by $F(x)$. \\n\\nThe **difficult** part is to show that the limit is kept by passing to the supremum, understanding why this happens requires some empirical process theory... such as van der Waart and Welner. The name of the Theorem is Donsker Theorem http://en.wikipedia.org/wiki/Donsker%27s_theorem ... )",added 208 characters in body; added 2 characters in body,
3553,5,1539,c97915a4-b479-48be-b33a-095c36a04ef3,2010-08-11 17:10:44.0,223.0,"$\\sqrt{n}\\sup_x|F_n-F|=\\sup_x|\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(x)| $ \\n\\nwhere $Z_i(x)=1_{X_i\\leq x}-E[1_{X_i\\leq x}]$\\n\\nby CLT you have \\n$G_n=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^nZ_i(x)\\rightarrow \\mathcal{N}(0,F(x)(1-F(x)))$\\n\\nthis is the intuition... \\n\\nbrownian bridge $B(t)$ has variance $t(1-t)$ http://en.wikipedia.org/wiki/Brownian_bridge replace $t$ by $F(x)$. This is for one $x$...\\n\\nYou also need to check the covariance and hence it still is easy to show (CLT) that \\nfor ($x_1,\\dots,x_k$)\\n$(G_n(x_1),\\dots,G_n(x_k))\\rightarrow (B_1,\\dots,B_k)$ where $(B_1,\\dots,B_k)$ is $\\mathcal{N}(0,\\Sigma)$ with $\\Sigma=(\\sigma_{ij})$, $\\sigma_{ij}=\\min(F(x_i),F(x_j))-F(x_i)F(x_j)$.  \\n\\n\\n\\nThe **difficult** part is to show that the distribution of the suppremum of the limit is the supremum of the distribution of the limit...  Understanding why this happens requires some empirical process theory, reading books such as van der Waart and Welner (not easy). The name of the Theorem is Donsker Theorem http://en.wikipedia.org/wiki/Donsker%27s_theorem ... ",added 327 characters in body; added 76 characters in body,
3554,5,1523,94022037-05bb-4dee-ad25-cb52c5161a97,2010-08-11 17:23:20.0,74.0,"This might be a duplicate, but I am going to answer it anyways.\\n\\nData mining is statistics, with some minor differences. You can think of it as re-branding statistics, because statisticians are kinda weird. \\n\\nIt is often associated with Computational Statistics, ie only stuff you can do with a computer.\\n\\nData miners *stole* a significant proportion of multivariate statistics and called it their own. Check the table of contents of any 1990s multivariate book and compare it to a new data mining book. Very similar.\\n\\nStatistics is associated with testing hypotheses and with model building, whereas Data Mining is more associated with prediction and classification, regardless of whether there is an understandable model.",added 6 characters in body,
3555,2,1553,82bb3618-5d6c-4edd-ae8d-03ab3812b1bc,2010-08-11 18:55:03.0,251.0,"See this question for the differences/advantages of using mutual information versus Pearson correlation or Spearman's rank:\\n\\n- [What is the major difference between correlation and mutual information?][1]\\n\\n> Does mutual information discriminate against fold change differences?\\n\\nIf that's the variation measure you use for your correlation values, then that is assumed for the hypothesized network.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1052/what-is-the-major-difference-between-correlation-and-mutual-information\\n\\n",,
3556,2,1554,0b9c05b0-b21d-4330-89bd-1393c16e37d0,2010-08-11 19:52:29.0,795.0,"following @Srikant, one can compute the CDF of the beta distribution, and find conditions on $a, b$ such that the interval $[a,b]$ contains the minimum of $n$ draws of a uniform with 95% probability. The condition is: $(1-a)^n - (1-b)^n = 0.95$. One attractive choice would then be the interval $[0,1 - 0.05^{1/n}]$. This is also the smallest interval with the desired property.",,
3557,2,1555,2ee639b3-a39a-4620-89ff-22a12982a844,2010-08-11 20:31:25.0,559.0,"I have the following frequency table:\\n\\n    35	0	4	3	7	6	5	4\\n    39	1	9	6	7	7	6	8\\n    36	0	7	10	11	11	10	16\\n    41	0	9	8	8	7	6	7\\n    41	0	8	9	10	9	12	11\\n    55	2	12	9	11	12	11	13\\n    55	1	10	10	11	10	12	11\\n    47	1	14	8	12	15	12	12\\n    45	1	10	11	10	10	9	18\\n    56	0	13	16	12	12	12	11\\n\\n\\nThe Kruskal-Wallis ANOVA test returns:\\n\\n    Source      SS      df     MS     Chi-sq   Prob>Chi-sq  \\n    Columns  25306.8    7   3615.26   47.16    5.18783e-008  \\n    Error     17083.2   72   237.27\\n    Total     42390     79\\n\\n                                \\nAccording to a multiple comparison of mean ranks:\\n\\n - Six groups of mean significantly different from group 1 (column 1)\\n \\n - Six groups of mean significantly different from group 2 (column 2)\\n\\n\\n----------\\n\\n\\nNow the Kruskal-Wallis and multiple comparison tests make sense, however the Chi Square Test returns a chi square value of 31.377 and a p-value of 0.9997, which leads us to accept the null hypothesis that the frequencies are independent. I understand that an assumption of ANOVA is independence, but...\\n\\nI want to see test if the frequencies are statistically independent, was the Kruskal-Wallis and multiple comparison tests the correct methodology? Note: I am not trying to be subjective, but for a given set of frequencies, how do you test that the differences between groups are significant?",,
3558,1,1555,2ee639b3-a39a-4620-89ff-22a12982a844,2010-08-11 20:31:25.0,559.0,Test if differences between frequencies is significant ,,
3559,3,1555,2ee639b3-a39a-4620-89ff-22a12982a844,2010-08-11 20:31:25.0,559.0,<multiple-comparisons><statistical-significance><anova><chi-squared>,,
3560,2,1556,880952b6-f637-4068-b63f-94a01cac02f2,2010-08-11 20:55:41.0,559.0,What is the difference between having something statistically significant (such as a difference between two numbers) and stating if a group of numbers are independent or dependent. ,,
3561,1,1556,880952b6-f637-4068-b63f-94a01cac02f2,2010-08-11 20:55:41.0,559.0,Statistically Significant vs independence/Dependent,,
3562,3,1556,880952b6-f637-4068-b63f-94a01cac02f2,2010-08-11 20:55:41.0,559.0,<statistical-significance>,,
3563,2,1557,f3f721df-87fd-4020-940d-73ed433b800d,2010-08-11 20:57:31.0,795.0,"I am taken by the idea of James-Stein shrinkage (i.e. that a nonlinear function of a single observation of a vector of possibly independent normals can be a better estimator of the means of the random variables, where 'better' is measured by squared error). However, I have never seen it in applied work. Clearly I am not well enough read. Are there any classic examples of where James-Stein has improved estimation in an applied setting? If not, is this kind of shrinkage just an intellectual curiosity?",,
3564,1,1557,f3f721df-87fd-4020-940d-73ed433b800d,2010-08-11 20:57:31.0,795.0,James-Stein shrinkage 'in the wild'?,,
3565,3,1557,f3f721df-87fd-4020-940d-73ed433b800d,2010-08-11 20:57:31.0,795.0,<estimation><error><application>,,
3566,2,1558,c351e6c9-63f4-4cb1-a7ba-a3cb6e1d5467,2010-08-11 21:12:16.0,880.0,"Significance in a independent samples t test just means that the probability of sampling a mean difference as extreme as the mean mean difference you sampled is less than .05 if the null were true.  This is totally unrelated to dependent/independent.  dependent means the observations in the groups are connected somehow A) they are the same person taking the same test a second time, B) people in each group are matched on some pre-test variable, C) people in the two groups are related (i.e. family).  Independent means the observations in the two groups are independent (i.e. the observations in group 1 are independent of the observations in group 2).. ",,
3567,2,1559,05ba7b50-7cd5-43a8-8ecb-00138df3902d,2010-08-11 21:19:03.0,880.0,"I don't know if there is a difference, but probabilities are not % they range from 0 to 1. I mean if you multiply a probability by 100 you get %.  If your question is what's the difference between probability and % then this would be my answer, but this is not your question.  The definition of probability assumes an infinite number of sampling experiments, so then we can never truly get probability because we can never truly conduct an infinite number of sampling experiments.  ",,
3568,2,1560,f6663387-089c-4c5b-a8d2-64c2fee23722,2010-08-11 21:20:47.0,223.0,"James-Stein estimator is not widely used but it has inspired soft thresholding, hard thresholding which is really widely used \\nsee the section of candes's review about shrinkage estimation (p20):\\n\\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.8881&rep=rep1&type=pdf\\n\\n",,
3569,2,1561,257369f9-94bb-4ed0-a8f3-b36ce49cab0f,2010-08-11 21:28:10.0,876.0,"i have a different sample size `n` for every month\\n\\nfor example i have `13890` one month then `17756`, then `21425`\\n\\nthe data every month for example `13890` is broken down into:\\n\\n    48 chairs, 12 tables, 2 couches etc...\\n\\nnext month we have similar metrics like\\n\\n    3 chairs, 23 tables, 4 couches etc..\\n\\ni would like to know how i am doing with the furniture in relation to the total per month\\n",,
3570,1,1561,257369f9-94bb-4ed0-a8f3-b36ce49cab0f,2010-08-11 21:28:10.0,876.0,simple question on normalizing data,,
3571,3,1561,257369f9-94bb-4ed0-a8f3-b36ce49cab0f,2010-08-11 21:28:10.0,876.0,<normality>,,
3572,5,1560,fe85d6a4-cec6-4ccc-8d47-b3e34b6a78d0,2010-08-11 21:28:13.0,223.0,"James-Stein estimator is not widely used but it has inspired soft thresholding, hard thresholding which is really widely used. \\n\\nWavelet shrinkage estimation (see R package wavethresh), shrunken centroid (package pamr under R) for classication, there are a lot of examples of practical efficiency of shrinkage...\\n \\nFor theoretical purpose, see the section of candes's review about shrinkage estimation (p20):\\n\\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.8881&rep=rep1&type=pdf\\n\\n",added 216 characters in body,
3573,5,1560,f5604e81-adf2-4e27-94d5-9880281ffd33,2010-08-11 21:48:32.0,223.0,"James-Stein estimator is not widely used but it has inspired soft thresholding, hard thresholding which is really widely used. \\n\\nWavelet shrinkage estimation (see R package wavethresh) is used a lot in signal processing, shrunken centroid (package pamr under R) for classication is used for DNA micro array, there are a lot of examples of practical efficiency of shrinkage...\\n \\nFor theoretical purpose, see the section of candes's review about shrinkage estimation (p20-> James stein and after that soft and hard threshold):\\n\\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.8881&rep=rep1&type=pdf\\n\\n",added 116 characters in body,
3574,2,1562,f4544a92-befe-46bf-bbea-207c45ac7106,2010-08-11 21:49:08.0,,"I have a question on subject chi-square test for independence.\\n\\nI have, for example, two events A and B. If chi square test is not passed: is A dependent on B (A|B) or B on A (B|A)? Or does be valid both? (A|B and B|A).\\n\\nThank you in advance.\\n",,sewa373
3575,1,1562,f4544a92-befe-46bf-bbea-207c45ac7106,2010-08-11 21:49:08.0,,chi square test for independence,,sewa373
3576,3,1562,f4544a92-befe-46bf-bbea-207c45ac7106,2010-08-11 21:49:08.0,,<test>,,sewa373
3577,2,1563,8487283e-f370-4f7a-b638-44f42165152e,2010-08-11 22:10:15.0,511.0,"Naive Bayes is usually the starting point for text classification, here's an <a href=""http://www.drdobbs.com/184406064;jsessionid=A35OB1KFVVLTTQE1GHPCKH4ATMY32JVN"">article</a> from Dr. Dobbs on how to implement one",,
3578,2,1564,43c2b836-dc28-4558-b8d2-7264322449b9,2010-08-11 22:14:39.0,,"Could you inform me please, how can I calculate conditioned probability of several events?\\n\\nfor example:\\n\\nP (A | B, C, D) - ?\\n\\nI know, that:\\n\\nP (A | B) = P (A intersection B) / P (B)\\n\\nBut, unfortunately, I can't find any formula if an event A depends on several variables. Thanks in advance.\\n",,sewa373
3579,1,1564,43c2b836-dc28-4558-b8d2-7264322449b9,2010-08-11 22:14:39.0,,conditional probability,,sewa373
3580,3,1564,43c2b836-dc28-4558-b8d2-7264322449b9,2010-08-11 22:14:39.0,,<probability>,,sewa373
3581,2,1565,76c70a55-66ac-482b-b217-290bcf217ca9,2010-08-11 22:17:04.0,601.0,A chi-square test is not motivated by your description thus far.  Give many more details.  \\n\\nChi-square tests for independence are used to see if the probability or counts of each kind of event in a given variable are independent of other variables.,,
3582,5,1563,8ad2820e-e728-48aa-918b-f4eaa0fd958c,2010-08-11 22:17:23.0,511.0,"Naive Bayes is usually the starting point for text classification, here's an <a href=""http://www.drdobbs.com/184406064;jsessionid=A35OB1KFVVLTTQE1GHPCKH4ATMY32JVN"">article</a> from Dr. Dobbs on how to implement one. It's also often the ending point for text classification because it's so efficient and parallelizes well, SpamAssassin and POPFile use it.",added 140 characters in body,
3583,2,1566,ee414d09-6a76-4b1c-89cc-efa50f9601de,2010-08-11 22:48:19.0,881.0,"I'm not really sure if an implementation exists to address all your needs.\\n\\nFor (1), you can use any of the online implementations of SVM such as Pegasos or LASVM. If you want something simpler, you may use Perceptron or Kernel Perceptron. Basically, in all these algorithms, given an already learned weight vector (say w0), you can update w0 incrementally, given a fresh set of new examples.\\n\\nFor (2) and (3), I'm not sure if the above approaches would straightaway allow but you can probably borrow some ideas from the literature dealing with unknown classes. I'd suggest taking a look at [this][1].\\n\\n  [1]: http://homepage.tudelft.nl/a9p19/papers/prl_08_reject.pdf",,
3584,2,1567,5390f630-bd51-4ee9-a946-1ff685effc6a,2010-08-11 22:49:25.0,196.0,"Do you mean you want the percentage of the n in that month that belongs to each furniture category?  If so, can't you take the N for each month and divide all of the values from that month by N?\\n\\nFor example, your first case would be:\\n0.0034557235 0.0008639309 0.0001439885\\nand your second case (where N = 17756) would be \\n0.0001689570 0.0012953368 0.0002252760.\\n\\nOr do you want a comparison of your observed values to your expected values?  If that is the case you can construct a table with furniture type as columns and months as rows.  For each cell you can take the (sum of values in the row to which it belongs) * (sum of values in the column to which it belongs) and divide by the total number of values you have.  That will give you the expected value for the cell.  If you subtract that from your initial value it will tell you how many more or less of each type of furniture you had than expected given the type of furniture it is and the month in which you made your observation. \\n",,
3585,2,1568,e0ae7a78-c42d-4a7b-bf60-31a6ce9cd9ef,2010-08-11 22:49:28.0,572.0,"Take the intersection of B,C and D call it U. Then perform P(A|U).",,
3586,6,1564,e71e23a4-cca1-4e81-b4e9-ef821f10c3a5,2010-08-11 22:56:10.0,,<conditional-probability>,changed tags to appropriate one,user28
3587,5,1567,189987a9-bfd1-495c-a61f-872353a31c1e,2010-08-11 23:00:26.0,196.0,"Do you mean you want the percentage of the n in that month that belongs to each furniture category?  If so, can't you take the N for each month and divide all of the values from that month by N?\\n\\nFor example, your first case would be:\\n0.0034557235 0.0008639309 0.0001439885\\nand your second case (where N = 17756) would be \\n0.0001689570 0.0012953368 0.0002252760.\\n\\nOr do you want a comparison of your observed values to your expected values?  If that is the case you can construct a table with furniture type as columns and months as rows.  For each cell you can take the (sum of values in the row to which it belongs) * (sum of values in the column to which it belongs) and divide by the total number of values you have.  That will give you the expected value for the cell.  If you subtract that from your initial value it will tell you how many more or less of each type of furniture you had than expected given the type of furniture it is and the month in which you made your observation. \\n\\nFor example, consider this source data\\n<pre>\\nMonth  Chairs  Tables  Couches  Other   Total\\n1      48      12      2        13828   13890\\n2      3       24      4        17725   17756\\nTotal  51      36      6        31553   31646\\n48 chairs, 12 tables, 2 couches\\n</pre>\\nIt would be calculated like so...\\n<pre>\\nMonth  Chairs                 Tables  Couches  Other   Total\\n1      (51*13890)/31646=22.38     12        2  13828   13890\\n2      (51*17756)/31646=28.61     24        4  17725   17756\\nTotal  51                         36        6  31553   31646\\n</pre>\\nLetting you know that in Month 1 there were 48-22.38=25.62 more chairs than expected and that in Month 2 there were 3-28.61=-25.61 more chairs than expected - but to make more sense we can flip the sign and the terminology and say there were 25.61 fewer chairs than expected.",added 851 characters in body,
3588,5,1567,67ce390b-26c9-46ac-8645-cb4f90016b13,2010-08-11 23:05:30.0,196.0,"Do you mean you want the percentage of the n in that month that belongs to each furniture category?  If so, can't you take the N for each month and divide all of the values from that month by N?\\n\\nFor example, your first case would be:\\n0.0034557235 0.0008639309 0.0001439885\\nand your second case (where N = 17756) would be \\n0.0001689570 0.0012953368 0.0002252760.\\n\\nOr do you want a comparison of your observed values to your expected values?  If that is the case you can construct a table with furniture type as columns and months as rows.  For each cell you can take the (sum of values in the row to which it belongs) * (sum of values in the column to which it belongs) and divide by the total number of values you have.  That will give you the expected value for the cell.  If you subtract that from your initial value it will tell you how many more or less of each type of furniture you had than expected given the type of furniture it is and the month in which you made your observation. \\n\\nFor example, consider this source data\\n<pre>\\nMonth  Chairs  Tables  Couches  Other   Total\\n1      48      12      2        13828   13890\\n2      3       24      4        17725   17756\\nTotal  51      36      6        31553   31646\\n48 chairs, 12 tables, 2 couches\\n</pre>\\nIt would be calculated like so...\\n<pre>\\nMonth  Chairs                 Tables  Couches  Other   Total\\n1      (51*13890)/31646=22.38     12        2  13828   13890\\n2      (51*17756)/31646=28.61     24        4  17725   17756\\nTotal  51                         36        6  31553   31646\\n</pre>\\nLetting you know that in Month 1 there were 48-22.38=25.62 more chairs than expected and that in Month 2 there were 3-28.61=-25.61 more chairs than expected - but to make more sense we can flip the sign and the terminology and say there were 25.61 fewer chairs than expected.\\n\\nFor more details consider looking [here][1].\\n\\n\\n  [1]: http://davidmlane.com/hyperstat/B143466.html",added 105 characters in body,
3589,2,1569,fee5ae4f-b268-464f-80e3-4d2574b16f9e,2010-08-11 23:09:01.0,511.0,"Suppose you are given n IID samples either from p or from q. You want to identify which distribution they came from, with the probability of mistakenly outputting q when true distribution was p below e. Then, for large n, small e and optimal decision procedure, the probability of making the opposite mistake is\\n\\nexp(-n KL(p,q))\\n\\nIn other words, the probability of mistaking q for p falls by a factor of exp(D(p,q)) with each datapoint.\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>",,
3590,5,1569,c627eab4-76ae-4a05-a993-5ee24dccbc33,2010-08-11 23:17:38.0,511.0,"Suppose you are given n IID samples either from p or from q. You want to identify which distribution they came from, with the probability of mistakenly outputting q when true distribution was p below e. Then, for large n, small e and optimal decision procedure, the probability of making the opposite mistake is\\n\\nexp(-n KL(p,q))\\n\\nIn other words, the probability of mistaking q for p falls by a factor of exp(KL(p,q)) with each datapoint.\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>",added 1 characters in body,
3591,2,1570,0137d57e-379e-4133-ac55-37e4c1d7e83c,2010-08-11 23:33:59.0,159.0,"I assume A and B are both random variables taking discrete values and you are thinking of a chi-squared test on the 2x2 table formed by the counts of observations on the two variables.\\n\\nIn that case, a significant result indicates both directions of dependence: A|B and B|A.\\n\\nIf you think about Bayes' theorem, it is clear that one always implies the other:\\n\\nP(A|B) = P(B|A) P(A) / P(B)\\n\\nSo P(A|B) = P(A) if and only if P(B|A)=P(B).",,
3592,4,1564,599f80af-8937-45d4-9e5a-a630d3e11163,2010-08-11 23:35:32.0,159.0,How can I calculate the conditional probability of several events?,edited title,
3593,4,1562,51ac451c-40ec-4088-a929-b9c94b821a0a,2010-08-11 23:36:13.0,159.0,What dependence is implied by a chi square test for independence?,edited title; edited tags,
3594,6,1562,51ac451c-40ec-4088-a929-b9c94b821a0a,2010-08-11 23:36:13.0,159.0,<statistical-significance><chi-squared>,edited title; edited tags,
3595,2,1571,cc1e39f8-8450-43c2-ab44-051a0b279e7d,2010-08-11 23:47:41.0,291.0,"I am trying to recreate (in R) a frequentist hypothesis testing in Bayesian from, by calculating Bayes factors of the null (H0) and alternative (H1) models.\\n\\nThe model is simply a simple linear regression that tries to detect a trend in global temp. data from 1995 to 2009 ([here][1]). Therefore, H0 is no trend (i.e. slope = 0), or similary, the H0 model is a linear model with only the intercept. \\n\\nSo I calculated the `lm()` of both models to arrive at negative log likelihood values that are significantly different. The p-value for the H1 lm() model is 0.0877.\\n\\nI also calculated this in a Bayesian way by using [MCMCpack][2], and I get negative log likelihood values that are **super duper uber** different. Log likelihood values of 13.7 and 4.3 are about a 10000 fold difference in their likelihood ratios (where [>100 is considered to be ""decisive""][3]).\\n\\nThe means and sds of the estimates are very similar, so why am I getting such different likelihood values? (particularly for the Bayesian H0 model) I feel like there is a gap in my understanding on marginal likelihoods, but I can't pinpoint the problem.\\n\\nThanks\\n\\n    library(MCMCpack)\\n    \\n    ## data: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt\\n\\n    head(hadcru, 2)\\n    ##  Year      1      2      3      4      5      6      7      8      9     10\\n    ## 1 1850 -0.691 -0.357 -0.816 -0.586 -0.385 -0.311 -0.237 -0.340 -0.510 -0.504\\n    ## 2 1851 -0.345 -0.394 -0.503 -0.480 -0.391 -0.264 -0.279 -0.175 -0.211 -0.123\\n    ##       11     12    Avg\\n    ## 1 -0.259 -0.318 -0.443\\n    ## 2 -0.141 -0.151 -0.288\\n    \\n    hadcru.lm <- lm(Avg ~ 1 + Year, data = subset(hadcru, (Year <= 2009 & Year >= 1995)))\\n    hadcru.lm.zero <- lm(Avg ~ 1, data = subset(hadcru, (Year <= 2009 & Year >= 1995)))\\n    \\n    hadcru.mcmc <- MCMCregress(Avg ~ 1 + Year, data = subset(hadcru, (Year <= 2009 & Year >= 1995)), thin = 100, mcmc = 100000, b0 = c(-20, 0), B0 = c(.00001, .00001), marginal = ""Laplace"")\\n    hadcru.mcmc.zero <- MCMCregress(Avg ~ 1, data = subset(hadcru, (Year <= 2009 & Year >= 1995)), thin = 100, mcmc = 100000, b0 = c(0), B0 = c(.00001), marginal = ""Laplace"")\\n\\n    -logLik(hadcru.lm)\\n    ## 'log Lik.' -14.55338 (df=3)\\n    -logLik(hadcru.lm.zero)\\n    ## 'log Lik.' -12.80723 (df=2)\\n    \\n    attr(hadcru.mcmc, ""logmarglike"")\\n    ##           [,1]\\n    ## [1,] -13.65188\\n    attr(hadcru.mcmc.zero, ""logmarglike"")\\n    ##           [,1]\\n    ## [1,] -4.310564\\n\\n\\n  [1]: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt\\n  [2]: http://cran.r-project.org/web/packages/MCMCpack/index.html\\n  [3]: http://en.wikipedia.org/wiki/Bayes_factor",,
3596,1,1571,cc1e39f8-8450-43c2-ab44-051a0b279e7d,2010-08-11 23:47:41.0,291.0,recreating traditional null hypothesis testing with Bayesian methods,,
3597,3,1571,cc1e39f8-8450-43c2-ab44-051a0b279e7d,2010-08-11 23:47:41.0,291.0,<r><bayesian><maximum-likelihood><mcmc>,,
3598,5,1569,0f9af383-361d-4e8d-aa1c-c7e677a3c955,2010-08-11 23:53:31.0,511.0,"Suppose you are given n IID samples either from p or from q. You want to identify which distribution they came from, with the probability of mistakenly outputting q when true distribution was p below e. Then, for large n, small e and optimal decision procedure, the probability of making the opposite mistake is\\n\\nexp(-n KL(p,q))\\n\\nIn other words, the probability of mistaking q for p falls by a factor of exp(KL(p,q)) with each datapoint.\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>, also, this is Theorem 3.2 in Kullback's ""Information Theory and Statistics"" (1978)",added 78 characters in body; added 6 characters in body,
3599,5,1571,b7a84a6f-f2bb-4460-9539-3c13dc5c98e0,2010-08-11 23:54:05.0,291.0,"I am trying to recreate (in R) a frequentist hypothesis testing in Bayesian from, by calculating Bayes factors of the null (H0) and alternative (H1) models.\\n\\nThe model is simply a simple linear regression that tries to detect a trend in global temp. data from 1995 to 2009 ([here][1]). Therefore, H0 is no trend (i.e. slope = 0), or similary, the H0 model is a linear model with only the intercept. \\n\\nSo I calculated the `lm()` of both models to arrive at negative log likelihood values that are significantly different. The p-value for the H1 lm() model is 0.0877.\\n\\nI also calculated this in a Bayesian way by using [MCMCpack][2], and I get negative log likelihood values that are **super duper uber** different. Log likelihood values of 13.7 and 4.3 are about a 10000 fold difference in their likelihood ratios (where [>100 is considered to be ""decisive""][3]).\\n\\nThe means and sds of the estimates are very similar, so why am I getting such different likelihood values? (particularly for the Bayesian H0 model) I feel like there is a gap in my understanding on marginal likelihoods, but I can't pinpoint the problem.\\n\\nThanks\\n\\n    library(MCMCpack)\\n    \\n    ## data: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt\\n\\n    head(hadcru, 2)\\n    ##  Year      1      2      3      4      5      6      7      8      9     10\\n    ## 1 1850 -0.691 -0.357 -0.816 -0.586 -0.385 -0.311 -0.237 -0.340 -0.510 -0.504\\n    ## 2 1851 -0.345 -0.394 -0.503 -0.480 -0.391 -0.264 -0.279 -0.175 -0.211 -0.123\\n    ##       11     12    Avg\\n    ## 1 -0.259 -0.318 -0.443\\n    ## 2 -0.141 -0.151 -0.288\\n    \\n    hadcru.lm <- lm(Avg ~ 1 + Year, data = subset(hadcru, (Year <= 2009 & Year >= 1995)))\\n    hadcru.lm.zero <- lm(Avg ~ 1, data = subset(hadcru, (Year <= 2009 & Year >= 1995)))\\n    \\n    hadcru.mcmc <- MCMCregress(Avg ~ 1 + Year, data = subset(hadcru, (Year <= 2009 & Year >= 1995)), thin = 100, mcmc = 100000, b0 = c(-20, 0), B0 = c(.00001, .00001), marginal = ""Laplace"")\\n    hadcru.mcmc.zero <- MCMCregress(Avg ~ 1, data = subset(hadcru, (Year <= 2009 & Year >= 1995)), thin = 100, mcmc = 100000, b0 = c(0), B0 = c(.00001), marginal = ""Laplace"")\\n\\n    -logLik(hadcru.lm)\\n    ## 'log Lik.' -14.55338 (df=3)\\n    -logLik(hadcru.lm.zero)\\n    ## 'log Lik.' -12.80723 (df=2)\\n    \\n    attr(hadcru.mcmc, ""logmarglike"")\\n    ##           [,1]\\n    ## [1,] -13.65188\\n    attr(hadcru.mcmc.zero, ""logmarglike"")\\n    ##           [,1]\\n    ## [1,] -4.310564\\n\\n![alt text][4]\\n\\n\\n  [1]: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt\\n  [2]: http://cran.r-project.org/web/packages/MCMCpack/index.html\\n  [3]: http://en.wikipedia.org/wiki/Bayes_factor\\n  [4]: http://www.skepticalscience.com/images/HadCRUT_1995_2009.gif",added 87 characters in body,
3600,2,1572,06e9040b-1822-4d30-b596-4353f029e63c,2010-08-12 00:43:27.0,,"I do know the packages you are using or their internal working but perhaps the choice of priors matter? Perhaps, you should consider using different prior structures to see how sensitive the mcmc marginal likelihoods are to your choice of priors.",,user28
3601,5,1572,49a57272-7182-4fee-8ee0-ac913467ab4e,2010-08-12 00:48:32.0,,"I do not know the packages you are using or their internal working but perhaps the choice of priors matter? Perhaps, you should consider using different prior structures to see how sensitive the mcmc marginal likelihoods are to your choice of priors.",fixed typo,user28
3602,5,1572,a70031ca-ea98-48e5-b0e6-92eb27fed499,2010-08-12 00:54:34.0,,"I do not know the packages you are using or their internal working but perhaps the choice of priors matter? Perhaps, you should consider using different prior structures to see how sensitive the mcmc marginal likelihoods are to your choice of priors.\\n\\nIn particular, I suspect that the mcmc and the traditional likelihoods are likely to converge better as the priors become more diffuse. Note that in mcmc the marginal likelihoods are computed by integrating out the likelihood function with respect to the priors. Thus, I have a feeling that the 'diffuseness' of the priors may matter (could be wrong on this issue but worth checking out).",added additional info,user28
3603,2,1573,85ed1137-413a-41ce-a087-2329ed78921d,2010-08-12 01:11:05.0,319.0,"When you're computing Bayes factors, the priors matter. The influence of the priors can persist even if you have a large amount of data.  When you're doing posterior inference, the effect of the prior goes away as you collect more data, but not so with Bayes factors.\\n\\nAlso, you'll get faster convergence if your null and alternative priors have disjoint support.  Details [here][1].\\n\\n\\n  [1]: http://www.bepress.com/mdandersonbiostat/paper47/",,
3606,2,1576,b1f0a083-c568-41e1-9c5f-aaf23b9631cd,2010-08-12 03:46:05.0,776.0,"It seems that a number of the statistical packages that I use seem to wrap these two concepts together. However, I'm wondering if there are different assumptions or data 'formalities' that must be true to use one over the other. A real example would be incredibly useful. ",,
3607,1,1576,b1f0a083-c568-41e1-9c5f-aaf23b9631cd,2010-08-12 03:46:05.0,776.0,What are the differences between Factor Analysis and Principal Component Analysis,,
3608,3,1576,b1f0a083-c568-41e1-9c5f-aaf23b9631cd,2010-08-12 03:46:05.0,776.0,<pca><factor-analysis>,,
3609,2,1577,97d27818-e0c6-4664-a147-dfab883ba1f4,2010-08-12 03:55:36.0,334.0,"\\nThere are numerous suggested definitions on the web. Here is one from a [on-line glossary on statistical learning](http://alumni.media.mit.edu/~tpminka/statlearn/glossary/glossary.html):\\n\\n> Principal Component Analysis\\n> \\n> Constructing new features which are\\n> the principal components of a data\\n> set. The principal components are\\n> random variables of maximal variance\\n> constructed from linear combinations\\n> of the input features. Equivalently,\\n> they are the projections onto the\\n> principal component axes, which are\\n> lines that minimize the average\\n> squared distance to each point in the\\n> data set. To ensure uniqueness, all of\\n> the principal component axes must be\\n> orthogonal. PCA is a\\n> maximum-likelihood technique for\\n> linear regression in the presence of\\n> Gaussian noise on both inputs and\\n> outputs. In some cases, PCA\\n> corresponds to a Fourier transform,\\n> such as the DCT used in JPEG image\\n> compression. See ""Eigenfaces for\\n> recognition"" (Turk&Pentland, J\\n> Cognitive Neuroscience 3(1), 1991),\\n> Bishop, ""Probabilistic Principal\\n> Component Analysis"", and ""Automatic\\n> choice of dimensionality for PCA"".choice of dimensionality for PCA"".\\n>\\n> Factor analysis\\n> \\n> A generalization of PCA which is based\\n> explicitly on maximum-likelihood. Like\\n> PCA, each data point is assumed to\\n> arise from sampling a point in a\\n> subspace and then perturbing it with\\n> full-dimensional Gaussian noise. The\\n> difference is that factor analysis\\n> allows the noise to have an arbitrary\\n> diagonal covariance matrix, while PCA\\n> assumes the noise is spherical. In\\n> addition to estimating the subspace,\\n> factor analysis estimates the noise\\n> covariance matrix. See ""The EM\\n> Algorithm for Mixtures of Factor\\n> Analyzers"".choice of dimensionality\\n> for PCA"".",,
3612,2,1579,301e3092-da47-44de-999d-96e06410737a,2010-08-12 04:44:12.0,183.0,"**Principal components analysis** involves extracting linear composites of observed variables.\\n\\n**Factor analysis** is based on a formal model predicting observed variables from theoretical latent factors.\\n\\nIn psychology these two techniques are often applied in the construction of multi-scale tests\\n to determine which items load on which scales.\\nThey typically yield similar substantive conclusions (for a discussion see Comrey (1988) Factor-Analytic Methods of Scale Development in Personality and Clinical Psychology).\\nThis helps to explain why some statistics packages seem to bundle them together.\\nI have also seen situations where ""principal component analysis"" is incorrectly labelled ""factor analysis"".\\n\\nIn terms of a **simple rule of thumb**, I'd suggest that you:\\n\\n1. Run factor analysis if you  assume or wish to test a theoretical model of latent factors causing observed variables.\\n\\n2. Run principal components analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.\\n\\n\\n\\n",,
3613,2,1580,87f6e715-7a34-4c7e-a013-58c61e06a18f,2010-08-12 05:03:22.0,183.0,"Imagine you,\\n\\n* Run a linear regression with four numeric predictors (IV1, ..., IV4)\\n* When only IV1 is included as a predictor the standardised beta is `+.20`\\n* When you also include IV2 to IV4 the sign of the standardised regression coefficient of IV1 flips to `-.25` (i.e., it's become negative).\\n\\nThis gives rise to a few questions:\\n\\n* With regards to terminology, do you call this a ""supressor effect""?\\n* What strategies would you use to explain and understand this effect?\\n* Do you have any examples of such effects in practice and how did you explain and understand these effects?\\n\\n\\n",,
3614,1,1580,87f6e715-7a34-4c7e-a013-58c61e06a18f,2010-08-12 05:03:22.0,183.0,Regression coefficients that flip sign after including other predictors,,
3615,3,1580,87f6e715-7a34-4c7e-a013-58c61e06a18f,2010-08-12 05:03:22.0,183.0,<regression><predictor><suppresor>,,
3616,2,1581,99f70261-2def-4ec0-a1c2-c80a8df83a4e,2010-08-12 05:31:08.0,287.0,"I believe effects like these are frequently caused by colinearity (see [this question][1]). I think the book on multilevel modeling by Gelman and Hill talks about it. The problem is that `IV1` is correlated with one or more of the other predictors, and when they are all included in the model, their estimation becomes erratic. \\n\\nIf the coefficient flipping is due to colinearity, then it's not really interesting to report, because it's not due to the relationship between your predictors to the outcome, but really due to the relationship between predictors.\\n\\nWhat I've seen suggested to resolve this problem is residualization. First, you fit a model for `IV2 ~ IV1`, then take the residuals of that model as `rIV2`. If all of your variables are correlated, you should really residualize all of them. You may choose do to so like this\\n\\n    rIV2 <- resid(IV2 ~ IV1)\\n    rIV3 <- resid(IV3 ~ IV1 + rIV2)\\n    rIV4 <- resid(IV4 ~ IV1 + rIV2 + rIV3)\\n\\nNow, fit the final model with\\n    \\n    DV ~ IV1 + rIV2 + rIV3 + rIV4\\n\\nNow, the coefficient for `rIV2` represents the independent effect of `IV2` given its correlation with `IV1`. I've heard you won't get the same result if you residualized in a different order, and that choosing the residualization order is really a judgment call within your research. \\n\\n  [1]: http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-re",,
3617,2,1582,408c6b36-85b7-47d9-bc8c-2e54aaabaa5f,2010-08-12 06:39:13.0,251.0,"Multicollinearity is the usual suspect as JoFrhwld mentioned.  Basically, if your variables are positively correlated, then the coefficients will be negatively correlated leading to sign reversal.  \\n\\nOne check would be to perform a principal components regression or ridge regression.  This reduces the dimensionality of the regression space, handling the multicollinearity.  You end up with biased estimates but a possibly lower MSE and corrected signs.  Whether you go with those particular results or not, it's a good diagnostic check.  If you still get sign changes, it may be theoretically interesting.  \\n",,
3618,2,1583,260a37c6-c147-4fcf-b318-4bcbed534481,2010-08-12 06:46:44.0,196.0,"Contrary to [some here][1], others (e.g. [Brian Ripley][2], the authors of [sensR][3], and the authors of [psyphy][4]) appear to think that using a standard binomial link function when analyzing two alternative forced choice data in which the minimum expected proportion correct is .5 is incorrect.  However, their approach as to what the link function should be varies.  [Gabriel Baud-Bovy][5] implicitly recommends (1+exp(x)/(1+exp(x)))/2.  The sensR library uses:\\n\\n    function (mu) \\n    {\\n        tres <- mu\\n        for (i in 1:length(mu)) {\\n            if (mu[i] > 0.5) \\n                tres[i] <- sqrt(2) * qnorm(mu[i])\\n            if (mu[i] <= 0.5) \\n                tres[i] <- 0\\n        }\\n        tres\\n    }\\n\\n... and the psyphy library uses:\\n\\n    function (mu) \\n    {\\n        m <- 2\\n        mu <- pmax(mu, 1/m + .Machine$double.eps)\\n        qlogis((m * mu - 1)/(m - 1))\\n    }\\n\\nBoth approaches of course have consequences for how other values are calculated.  Is there a ""right"" link function to be using with these sorts of problems, or so long as the link, inverse link, mu.eta, and variance functions all agree is everything going to be all right?  Is there a single source material that provides any authoritative guidance on this issue?\\n\\n  [1]: http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur\\n  [2]: https://stat.ethz.ch/pipermail/r-help/2006-December/122353.html\\n  [3]: http://cran.r-project.org/web/packages/sensR/\\n  [4]: http://cran.r-project.org/web/packages/psyphy/index.html\\n  [5]: https://stat.ethz.ch/pipermail/r-help/2006-December/122351.html\\n",,
3619,1,1583,260a37c6-c147-4fcf-b318-4bcbed534481,2010-08-12 06:46:44.0,196.0,Appropriate link function for 2AFC data?,,
3620,3,1583,260a37c6-c147-4fcf-b318-4bcbed534481,2010-08-12 06:46:44.0,196.0,<logistic><logit><link-function>,,
3621,2,1584,eabbfd90-27d2-4888-9541-a3f1038df045,2010-08-12 06:49:58.0,339.0,"Differences between factor analysis and principal components analysis are:\\n\\n• In factor analysis there is a structured model and some assumptions. In this respect it is a statistical technique which does not apply to principal components analysis which is a purely mathematical transformation.\\n\\n• The aim of principal components analysis is to explain the variance while factor analysis explains the covariance between the variables. \\n\\nOne of the biggest reasons for the confusion between the two, has to do with the fact that one of the factor extraction methods in Factor Analysis is called ""method of principal components"". However, it's one thing to use PCA and another thing to use the *method of principal components* in FA. The names maybe similar, but there are significant differences. The former is an independent analytical method while the latter is merely a tool for factor extraction.",,
3622,5,1583,b7cec487-234c-42d9-a2ef-f8005e82a527,2010-08-12 07:03:38.0,196.0,"Contrary to [some here][1], others (e.g. [Brian Ripley][2], the authors of [sensR][3], and the authors of [psyphy][4]) appear to think that using a standard binomial link function when analyzing two alternative forced choice data in which the minimum expected proportion correct is .5 is incorrect.  However, their approach as to what the link function should be varies.  [Gabriel Baud-Bovy][5] implicitly recommends (1+exp(x)/(1+exp(x)))/2.  The sensR library uses:\\n\\n    function (mu) \\n    {\\n        tres <- mu\\n        for (i in 1:length(mu)) {\\n            if (mu[i] > 0.5) \\n                tres[i] <- sqrt(2) * qnorm(mu[i])\\n            if (mu[i] <= 0.5) \\n                tres[i] <- 0\\n        }\\n        tres\\n    }\\n\\n... and the psyphy library uses:\\n\\n    function (mu) \\n    {\\n        m <- 2\\n        mu <- pmax(mu, 1/m + .Machine$double.eps)\\n        qlogis((m * mu - 1)/(m - 1))\\n    }\\n\\nThe approach selected seems like it may have some consequences for the result.  Is there a ""correct"" link function to be using with these sorts of problems, or so long as the link, inverse link, mu.eta, and variance functions all agree is everything going to be all right?  Is there a single source material that provides any authoritative guidance on this issue?\\n\\n  [1]: http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur\\n  [2]: https://stat.ethz.ch/pipermail/r-help/2006-December/122353.html\\n  [3]: http://cran.r-project.org/web/packages/sensR/\\n  [4]: http://cran.r-project.org/web/packages/psyphy/index.html\\n  [5]: https://stat.ethz.ch/pipermail/r-help/2006-December/122351.html\\n",edited body,
3623,5,1560,ab841d2d-010a-4136-9729-328c4ba96529,2010-08-12 07:04:11.0,223.0,"James-Stein estimator is not widely used but it has inspired soft thresholding, hard thresholding which is really widely used. \\n\\nWavelet shrinkage estimation (see R package wavethresh) is used a lot in signal processing, shrunken centroid (package pamr under R) for classication is used for DNA micro array, there are a lot of examples of practical efficiency of shrinkage...\\n \\nFor theoretical purpose, see the section of candes's review about shrinkage estimation (p20-> James stein and the section after after that one deals with soft and hard thresholding):\\n\\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.8881&rep=rep1&type=pdf\\n\\n",added 33 characters in body; added 3 characters in body,
3624,5,1012,dfb3ff8b-27ce-43b9-9ade-9170a7ac080c,2010-08-12 07:19:30.0,196.0,"A colleague wants to compare models that use either a Gaussian distribution or a uniform distribution and for other reasons needs the standard devation of these two distributions to be equal.  In R I can do a simulation...\\n\\n    sd(runif(100000000))\\n    sd(runif(100000000,min=0,max=2))\\n\\nand see that the calculated standard deviation is likely to be ~.2887 * the range of the uniform distribution.  However, I was wondering if there was an equation that could yield the exact value, and if so, what that formula was.",Dropped the statements that were giving people hives in the comments and focused on the content.,
3625,5,1582,61f55c5a-866d-42c9-8d84-b82842fe12a0,2010-08-12 07:22:09.0,251.0,"Multicollinearity is the usual suspect as JoFrhwld mentioned.  Basically, if your variables are positively correlated, then the coefficients will be negatively correlated, which can lead to a wrong sign on one of the coefficients.\\n\\nOne check would be to perform a principal components regression or ridge regression.  This reduces the dimensionality of the regression space, handling the multicollinearity.  You end up with biased estimates but a possibly lower MSE and corrected signs.  Whether you go with those particular results or not, it's a good diagnostic check.  If you still get sign changes, it may be theoretically interesting.  \\n",added 32 characters in body,
3626,2,1585,982b896c-5ea3-4dc7-b4b2-80d1c652f4f1,2010-08-12 07:27:47.0,159.0,"Ridge regression is a form of shrinkage. See [Draper & Van Nostrand (1979)][1].\\n\\nShrinkage has also proved useful in estimating seasonal factors for time series. See [Miller and Williams (IJF, 2003)][2].\\n\\n\\n  [1]: http://www.jstor.org/pss/1268284\\n  [2]: http://www.forecasters.org/ijf/journal-issue/273/article/5847",,
3627,2,1586,e6859300-a0d2-44c1-8084-739fda38a50e,2010-08-12 07:30:47.0,601.0,See [Simpson's Paradox][1].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Simpson's_paradox,,
3628,5,1421,3fec9a11-79ed-453c-8435-95b3bb3b7763,2010-08-12 07:32:48.0,601.0,"I don't see how the question in your example is sensible.  The slope of the values is the slope of the values. Using a logistic link function then you get the slope of the logit of the values.  There's no under or overestimating.  \\n\\nThe more interesting case in your (our) field is that of interactions in accuracy.  You might want to read [Dixon (2008)][1] as one of the more recent papers on this problem.  It also addresses many of your fundamental concerns.\\n\\nIn general, in cognitive and perceptual psychology a logit link function is better than any other standard link.  If you want to know the true effects of your independent variables, (i.e. whether they interact or are additive, whether they are linear or curvilinear) then you would need to know better the true underlying model.  Since you probably don't know that logistic regression is probably better than almost anything else and vastly better than just analyzing meaned accuracy scores.\\n\\nThe primary consequence of doing this is contradicting other findings where mean accuracy scores were put into an ANOVA or regression.\\n\\n*** EDIT***\\n\\nNow that you've added some data it looks like you're trying to model a floor effect that you shouldn't be.  At some point the task becomes impossible.  It looks like that already happened at your level 4 difficulty.  Modelling level 5 is useless.  What if you had a level 6 or 7 difficulty?\\n\\nIt looks like a logistic will fit points 1-4 pretty well.\\n\\nAnd, you should be looking at residuals to assess fit, not just the curves overlaid.\\n\\n  [1]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6WK4-4RH8SFS-1&_user=10&_coverDate=11%2F30%2F2008&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1424726112&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=47db7bad266e5fff07e6fc8c0ceeac42",edited body,
3629,2,1587,5325b43d-909d-479e-8ec4-21cfb2df48b4,2010-08-12 07:45:05.0,887.0,"I'm note sure I follow the R-code as I have only used R once or twice, but it looks to me as if you are comparing the marginal likelihood of a model with only an intercept and no slope (hadcru.mcmc.zero) and the marginal likelihood of a model with a slope and an intercept (hadcru.mcmc).  However, while hadcru.mcmc.zero seems to be the correct model for H0, hadcru.mcmc does not seem to me to correctly represent H1 as there is nothing as far as I can see that constrains the slope to be positive.  Is the something in the prior for the slope that makes it strictly positive (I don't know enough about MCMC in R to know)?  If not, that may be where your problem lies as the marginal likelihood would then have a component representing the likelihood of the data for all of then egative values of the slop permitted under the prior (and 0) as well as the positive.\\n\\nIt is debatable whether the H0 for this question should be that the slope is exactly zero, nobody would believe that to be plausible a-priori.  Perhaps a test using the Bayes factor for a model where the slope is strictly positive (H1) against a model where it is zero or negative (H0).\\n\\nHTH (and I am not just confusing things)",,
3630,2,1588,bc2f9d44-bbff-4f60-ad9e-fb76cab098a2,2010-08-12 08:11:37.0,8.0,[Korbinian Strimmer][1] uses the James-Stein estimator for [infering gene networks][2]. I've used his R packages a few times and it seems to provide a very good and quick answer.\\n\\n\\n  [1]: http://strimmerlab.org/index.html\\n  [2]: http://jmlr.csail.mit.edu/papers/v10/hausser09a.html,,
3632,2,1589,cec27325-cada-4f14-95c8-c8aa9655d533,2010-08-12 08:37:45.0,601.0,"It doesn't just seem like it will have consequences, it will have large consequences.  Fit that second function to the data you put in your last question on this.  It goes dramatically negative as it approaches 0.5.\\n\\nPerhaps more importantly, you also need to consider what the different equations mean for how one interprets the functioning of the mind.\\n\\nThere is no known function that is just best for all 2AFC*.  Such a function would be tantamount to proving a universal law of the operations of the mind.  You have to model your data if you want the very best fit.\\n\\n*OK, some models like splines will just fit most anything but you'd have to justify why you have all the extra parameters theoretically.\\n\\n(ASIDE: you were opposed to clipping when difficulty achieved maximum (or minimum).  Consider, if you were modelling a robotic arm at the maximum point of travel you would just clip the results at the maximum point of travel (something your first equation does).  Just because you didn't know what that point was before you found it doesn't mean anything.  You found it when performance reached chance.)",,
3633,2,1590,40f8e1fb-8a3e-470f-b873-4b5c5b314d6b,2010-08-12 08:50:26.0,888.0,"If one has an r value of 0.60, can one state that an increase in one variable is 60% likely to mean an increase in the other variable?",,
3634,1,1590,40f8e1fb-8a3e-470f-b873-4b5c5b314d6b,2010-08-12 08:50:26.0,888.0,Specific question about interpretation of r,,
3635,3,1590,40f8e1fb-8a3e-470f-b873-4b5c5b314d6b,2010-08-12 08:50:26.0,888.0,<r>,,
3636,6,1590,1daa4f8a-3eb2-4848-b00e-fc41168801bf,2010-08-12 08:56:54.0,8.0,<correlation>,edited tags,
3641,2,1593,599e021a-5e6c-419b-b150-91dfd42a8180,2010-08-12 10:19:39.0,521.0,"Another approach would be:\\n\\n    P(A| B, C, D) = P(A, B, C, D)/P(B, C, D)\\n                  = P(B| A, C, D).P(A, C, D)/P(B, C, D)\\n                  = P(B| A, C, D).P(C| A, D).P(A, D)/{P(C| B, D).P(B, D)}\\n                  = P(B| A, C, D).P(C| A, D).P(D| A).P(A)/{P(C| B, D).P(D| B).P(B)}\\n\\nAnd there are many equivalent forms.\\n\\nTaking U = (B, C, D) gives:\\n    P(A| B, C, D) = P(A, U)/P(U)\\n\\n    P(A| B, C, D) = P(A, U)/P(U)\\n                  = P(U| A).P(A)/P(U)\\n                  = P(B, C, D| A).P(A)/P(B, C, D)\\n\\nI'm sure they're equivalent, but do you want the joint probability of B, C & D given A?\\n                  ",,
3642,2,1594,06cfd76d-2c95-4e19-aa60-fffdd263167b,2010-08-12 10:20:31.0,702.0,"Interesting question!  All statistical models can be viewed as performing lossy data compression.  For instance simple linear regression with one predictor replaces $N$ points (where $N$ can be massive, e.g., in the 1000s) with two parameters: a slope and intercept.  The parameters may then be used to reconstruct the data, with degree of success depending on how good the original fit was.\\n\\nYour specific example concerns predicting binary time series data (Bernoulli distributed data, which is a specific case of the binomial distribution).  Binary data can encode a lot: coin flips, pictures, sounds, the digits of $\\pi$, statistical programming languages...  \\n\\nAs you can imagine, and as a quick search around Google will confirm, there are a lot of statistical models which could apply.   One is logistic regression, or (to express the same model in a more general framework) a Generalized Linear Model with a binomial distribution and a logit link function.  The function fit is of the following form:\\n$\\mbox{logit}[P(Y)] = \\beta X + \\epsilon$, where $X$ (predictors), $Y$ (probability of a 1), and $\\epsilon$ (residuals) are vectors.\\n\\nOkay.  Now a little demonstration. Suppose data are generated so that the probability of a 1 correlates with the sine of time (represented as black points in the graph below).  You don't know this, however.  You get data for time points from 0 to 359 (blue points).\\n\\n![alt text][1]\\n\\nWith the available data points, I fitted the function $\\mbox{logit}[P(Y)] = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\beta_3 t^3$, which popped out as $\\mbox{logit}[P(Y)] = -0.2  -30.9 t  -3.1 t^2 + 22.2 t^3$.  It's a good fit to the data (between 0 and 359).  However as you can see, when extrapolating, it does a rather poor job: beyond a certain point it says ""just guess 1!""\\n\\nTake-home message: to do the analysis correctly, you need to have a some idea of the likely processes generating the data.  If I knew a sine process were doing the job, then I'd be able to do a wonderful job predicting.  Thinking about this is where a statistician would start.  The appropriate model is always going to be domain specific, which is why, for example, compression techniques working well for images don't automatically apply to sounds.\\n                               \\n\\n  [1]: http://img196.imageshack.us/img196/589/cointimepredict2.png",,
3644,2,1595,76d61c9f-1f60-43f9-9a7e-3005c2aaf9df,2010-08-12 10:46:45.0,890.0,"Lots of people use a main tool like Excel or another spreadsheet, SPSS, STATA, or R for their statistics needs. They might turn to some specific package for very special needs, but a lot of things can be done with a simple spreadsheet or a general stats package or stats programming environment.\\n\\nI've always liked Python as a programming language, and for simple needs, it's easy to write a short program that calculates what I need. Matplotlib allows me to plot it.\\n\\nHas anyone switched completely from, say R, to Python? R (or any other statistics package) has a lot of functionality specific to statistics, and it has data structures that allow you to think about the statistics you want to perform and less about the internal representation of your data. Python (or some other dynamic language) has the benefit of allowing me to program in a familiar, high-level language, and it lets me programmatically interact with real-world systems in which the data resides or from which I can take measurements. But I haven't found any Python package that would allow me to express things with ""statistical terminology"" – from simple descriptive statistics to more complicated multivariate methods.\\n\\nWhat can you recommend if I wanted to use Python as a ""statistics workbench"" to replace R, SPSS, etc.?\\n\\nWhat would I gain and lose, based on your experience?",,
3645,1,1595,76d61c9f-1f60-43f9-9a7e-3005c2aaf9df,2010-08-12 10:46:45.0,890.0,Python as a statistics workbench,,
3646,3,1595,76d61c9f-1f60-43f9-9a7e-3005c2aaf9df,2010-08-12 10:46:45.0,890.0,<r><spss><stata>,,
3647,6,1595,dfae3bf5-bdde-4a35-875a-8404f41bf065,2010-08-12 10:47:58.0,8.0,<r><spss><stata><python>,edited tags,
3648,2,1596,f2b5f9a3-c52f-4bf9-ae44-10aad194a61b,2010-08-12 10:53:26.0,183.0,"No, your interpretation is incorrect.\\n\\nCommon interpretations of a correlation between X and Y equal to .60 include:\\n\\n1. X explains the following proportion of variance in Y: $0.60^2=.36$ . I.e., 36% of variance. \\n2. A value one standard deviation larger on X is associated with a value .60 of a standard deviation larger on Y.\\n\\nBoth these statements are bidirectional. I.e., you could switch the words X and Y in the above two statements and the statements would still be true.",,
3649,2,1597,bc7d7372-6b8d-48db-b512-9b9705282fbc,2010-08-12 10:58:37.0,183.0,"The following StackOverflow discussions might be useful\\n\\n* [R versus Python][1]\\n* [SciPy versus R][2]\\n* [Psychology research choosing between R, Python, and Matlab][3]\\n\\n\\n\\n\\n  [1]: http://stackoverflow.com/questions/1177019/what-can-be-done-in-r-that-cant-be-done-with-python-numpy-scipy\\n  [2]: http://stackoverflow.com/questions/1944261/when-to-choose-r-vs-scipy\\n  [3]: http://stackoverflow.com/questions/2264974/psychology-researcher-wants-to-learn-new-language",,
3650,2,1598,576d6a15-1b55-4e58-8a15-5be9ad69d1cd,2010-08-12 10:59:17.0,8.0,"I don't think there's any argument that the range of statistical packages in [cran][1] and [Bioconductor][2] far exceed anything on offer from other languages, however, that isn't the only think to consider.\\n\\nIn my research, I use R when I can but sometimes R is just to slow. For example, a large MCMC run. \\n\\nRecently, I combined python and C to tackle this problem. Brief summary: fitting a large stochastic population model with ~60 parameters and inferring around 150 latent states using MCMC. \\n\\n1. Read in the data in python\\n1. Construct the data C data structures in python using [ctypes][3].\\n1. Using a python `for` loop, call C functions that updated parameters, calculated likelihoods.\\n\\nA quick calculation showed that the programme spent 95% in C functions. However, I didn't have to write painful C code to read in data, construct C structs.\\n\\n____\\n\\nI know there's also [rpy][4], where python can call R functions. This can be useful, but if you're ""just"" doing statistics then I would use R.\\n\\n\\n  [1]: http://cran.r-project.org/\\n  [2]: http://bioconductor.org/\\n  [3]: http://python.net/crew/theller/ctypes/\\n  [4]: http://rpy.sourceforge.net/",,
3651,5,1594,0db35fa2-d67d-40ab-89ad-e78a527cc0a7,2010-08-12 11:03:36.0,702.0,"Interesting question!  All statistical models can be viewed as performing lossy data compression.  For instance simple linear regression with one predictor replaces $N$ points (where $N$ can be massive, e.g., in the 1000s) with two parameters: a slope and intercept.  The parameters may then be used to reconstruct the data, with degree of success depending on how good the original fit was.\\n\\nYour specific example concerns predicting binary time series data (Bernoulli distributed data, which is a specific case of the binomial distribution).  Binary data can encode a lot: coin flips, pictures, sounds, the digits of $\\pi$, statistical programming languages...  \\n\\nAs you can imagine, and as a quick search around Google will confirm, there are a lot of statistical models which could apply.   One is logistic regression, or (to express the same model in a more general framework) a Generalized Linear Model with a binomial distribution and a logit link function.  The function fit is of the following form:\\n$\\mbox{logit}[P(Y)] = \\beta X + \\epsilon$, where $X$ (predictors), $Y$ (probability of a 1), and $\\epsilon$ (residuals) are vectors.\\n\\nOkay.  Now a little demonstration. Suppose data are generated so that the probability of a 1 correlates with the sine of time (represented as black points in the graph below).  You don't know this, however.  You get data for time points from 0 to 359 (blue points).\\n\\n![alt text][1]\\n\\nWith the available data points, I fitted the function $\\mbox{logit}[P(Y)] = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\beta_3 t^3$, which popped out as $\\mbox{logit}[P(Y)] = -0.2  -30.9 t  -3.1 t^2 + 22.2 t^3$.  (The probability predictions are plotted in red.)  It's a good fit to the data (between 0 and 359).  However as you can see, when extrapolating, it does a rather poor job: beyond a certain point it says ""just guess 1!""\\n\\nTake-home message: to do the analysis correctly, you need to have a some idea of the likely processes generating the data.  If I knew a sine process were doing the job, then I'd be able to do a wonderful job predicting.  Thinking about this is where a statistician would start.  The appropriate model is always going to be domain specific, which is why, for example, compression techniques working well for images don't automatically apply to sounds.\\n                               \\n\\n  [1]: http://img196.imageshack.us/img196/589/cointimepredict2.png",added 51 characters in body,
3652,2,1599,dd523ddd-338e-4200-9c77-70f738dc06ad,2010-08-12 12:00:55.0,449.0,"I think the contrast between using absolute deviations and squared deviations becomes clearer once you move beyond a single variable and think about linear regression. There's a nice discussion at http://en.wikipedia.org/wiki/Least_absolute_deviations, particularly the section ""Contrasting Least Squares with Least Absolute Deviations"" , which links to some student exercises with a neat set of applets at http://www.math.wpi.edu/Course_Materials/SAS/lablets/7.3/73_choices.html .\\n\\nTo summarise, least absolute deviations is more robust to outliers than ordinary least squares, but it can be unstable (small change in even a single datum can give big change in fitted line) and doesn't always have a unique solution - there can be a whole range of fitted lines. Also least absolute deviations requires iterative methods, while ordinary least squares has a simple closed-form solution, though that's not such a big deal now as it was in the days of Gauss and Legendre, of course.",,
3653,2,1600,2ab6184e-dcd1-4bbb-86b7-b051eca7b07d,2010-08-12 12:43:41.0,319.0,"One benefit of moving to Python is the possibility to do more work in one language.  Python is a reasonable choice for number crunching, writing web sites, administrative scripting, etc. So if you do your statistics in Python, you wouldn't have to switch languages to do other programming tasks.",,
3654,5,1598,b33231aa-5c8c-4134-ad32-456e47127eb3,2010-08-12 13:06:09.0,8.0,"I don't think there's any argument that the range of statistical packages in [cran][1] and [Bioconductor][2] far exceed anything on offer from other languages, however, that isn't the only think to consider.\\n\\nIn my research, I use R when I can but sometimes R is just to slow. For example, a large MCMC run. \\n\\nRecently, I combined python and C to tackle this problem. Brief summary: fitting a large stochastic population model with ~60 parameters and inferring around 150 latent states using MCMC. \\n\\n1. Read in the data in python\\n1. Construct the C data structures in python using [ctypes][3].\\n1. Using a python `for` loop, call the C functions that updated parameters and calculated the likelihood.\\n\\nA quick calculation showed that the programme spent 95% in C functions. However, I didn't have to write painful C code to read in data or construct C data structures.\\n\\n____\\n\\nI know there's also [rpy][4], where python can call R functions. This can be useful, but if you're ""just"" doing statistics then I would use R.\\n\\n\\n  [1]: http://cran.r-project.org/\\n  [2]: http://bioconductor.org/\\n  [3]: http://python.net/crew/theller/ctypes/\\n  [4]: http://rpy.sourceforge.net/",added 15 characters in body,
3655,2,1601,d3bd2e6d-f6cd-4266-8ecd-60910887683e,2010-08-12 13:20:13.0,445.0,"In the analysis of test scores (e.g., in Education or Psychology), common analysis techniques often assume that data are normally distributed. However, perhaps more often than not, scores tend to deviate sometimes wildly from normal.\\n\\nI am familiar with some basic normalizing transformations, like:\\nsquare roots, logarithms, reciprocal transformations for reducing positive skew,\\nreflected versions of the above for reducing negative skew,\\nsquaring for leptokurtic distributions. I have heard of arcsine transformations and power transformations, though I am not really knowledgeable about them.\\n\\nSo, I am curious as to what other transformations are commonly used by analysts?",,
3656,1,1601,d3bd2e6d-f6cd-4266-8ecd-60910887683e,2010-08-12 13:20:13.0,445.0,Normalizing Transformation Options,,
3657,3,1601,d3bd2e6d-f6cd-4266-8ecd-60910887683e,2010-08-12 13:20:13.0,445.0,<normality><data-transformation>,,
3658,5,372,408434e2-f432-45a8-9862-31bafdfccad2,2010-08-12 13:24:12.0,509.0,What topics in statistics are most useful/relevant to data mining?\\n,added 1 characters in body; edited tags; edited title,
3659,4,372,408434e2-f432-45a8-9862-31bafdfccad2,2010-08-12 13:24:12.0,509.0,What are the key statistical concepts that relate to data mining?,added 1 characters in body; edited tags; edited title,
3660,6,372,408434e2-f432-45a8-9862-31bafdfccad2,2010-08-12 13:24:12.0,509.0,<probability><data-mining><cart>,added 1 characters in body; edited tags; edited title,
3661,16,381,a3811c53-16ef-4bc8-b34a-073896f9fc32,2010-08-12 13:30:08.0,5.0,,,
3662,6,1383,8baf7e31-8898-4c7f-94ca-9cca6f6a6e9d,2010-08-12 13:39:10.0,8.0,<modeling><compression>,edited tags,
3663,6,1028,ac6b6501-35d2-4b24-9298-22775a7e0367,2010-08-12 13:40:14.0,8.0,<distributions><kullback-leibler>,edited tags,
3668,6,1054,02b3942b-95d3-42f8-b3ce-6244e10b2ee6,2010-08-12 13:42:44.0,8.0,<r><survival-analysis><stata><rpkg-survival>,edited tags,
3669,2,1602,dc230acf-8ee9-41c7-b197-1ca60b62c717,2010-08-12 14:53:36.0,887.0,"Tongue firmly in cheek:\\n\\nA Bayesian defines a ""probability"" in exactly the same way that most non-statisticians do - namely an indication of the plausibility of a proposition or a situation.  If you ask him a question, he will give you a direct answer assigning probabilities describing the plausibilities of the possible outcomes for the particular situation (and state his prior assumptions).\\n\\nA Frequentist is someone that believes probabilies represent long run frequencies with which events ocurr; if needs be, he will invent a fictitious population from which your particular situation could be considered a random sample so that he can meaningfully talk about long run frequencies.  If you ask him a question about a particular situation, he will not give a direct answer, but instead make a statement about this (possibly imaginary) population.  Many non-frequentists statisticians will be easily confused by the answer and interpret it as Bayesian probability about the particular situation.\\n\\nHowever, it is important to note that most Frequentist methods have a Bayesian equivalent that in most circumstances will give essentially the same result, the difference is largely a matter of philosophy, and in practice it is a matter of ""horses for courses"".\\n\\nAs you may have guessed, I am a Bayesian and an engineer. ;o)\\n\\n",,
3670,5,1583,9860ed3a-c093-4f33-8e7e-1eaeb7710f01,2010-08-12 14:53:53.0,196.0,"Contrary to [some here][1], others (e.g. [Brian Ripley][2], the authors of [sensR][3], and the authors of [psyphy][4]) appear to think that using a standard binomial link function when analyzing two alternative forced choice data in which the minimum expected proportion correct is .5 is incorrect.  However, their approach as to what the link function should be varies.  \\n\\n1.The sensR library uses:\\n\\n    function (mu) \\n    {\\n        tres <- mu\\n        for (i in 1:length(mu)) {\\n            if (mu[i] > 0.5) \\n                tres[i] <- sqrt(2) * qnorm(mu[i])\\n            if (mu[i] <= 0.5) \\n                tres[i] <- 0\\n        }\\n        tres\\n    }\\n\\n2.The psyphy library uses:\\n\\n    function (mu) \\n    {\\n        m <- 2\\n        mu <- pmax(mu, 1/m + .Machine$double.eps)\\n        qlogis((m * mu - 1)/(m - 1))\\n    }\\n\\n3.[Gabriel Baud-Bovy][5] implicitly recommends (1+exp(x)/(1+exp(x)))/2.  \\n\\nThe approach selected seems like it may have some consequences for the result.  Is there a ""correct"" link function to be using with these sorts of problems, or so long as the link, inverse link, mu.eta, and variance functions all agree is everything going to be all right?  Is there a single source material that provides any authoritative guidance on this issue?\\n\\n  [1]: http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur\\n  [2]: https://stat.ethz.ch/pipermail/r-help/2006-December/122353.html\\n  [3]: http://cran.r-project.org/web/packages/sensR/\\n  [4]: http://cran.r-project.org/web/packages/psyphy/index.html\\n  [5]: https://stat.ethz.ch/pipermail/r-help/2006-December/122351.html\\n",Renumbered the equations...,
3671,2,1603,a518e629-d883-4624-8831-40fdc092d201,2010-08-12 15:11:07.0,,"I am not sure to what extent this is a better bound then the trivial bound but here is one approach. Take the taylor series expansion of $exp(z^2)$ and ignoring terms higher than the second term, you get:\\n\\n$\\int e^{z^2} f(z) dz < \\int (1 + z^2) f(z) dz$\\n\\nNow,\\n\\n$\\int (1 + z^2) f(z) dz = 1 + \\int z^2 f(z) dz$\\n\\nBut, we know that:\\n\\n$\\int z^2 f(z) dz = Var(z) + E(z)^2$\\n\\nSubstituting for the variance and mean of the [binomial distribution][1] and simplifying, we get:\\n\\n$\\int e^{z^2} f(z) dz < 1 +  n^{1-\\beta} (2-n^{-\\beta})$\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Binomial_distribution",,user28
3672,2,1604,ff4d55e6-9df0-4796-b32c-33385eed3a58,2010-08-12 15:16:55.0,847.0,Can someone provide me with a book or online reference on how to construct a smoothing splines with cross-validation? I have a programming and undergraduate level mathematics background.  I would also appreciate an overview of whether this is smoothing technique is a good one for smoothing data and whether there are any disadvantages of which a non-statistician needs to be aware.,,
3673,1,1604,ff4d55e6-9df0-4796-b32c-33385eed3a58,2010-08-12 15:16:55.0,847.0,Constructing smoothing splines with cross-validation,,
3674,3,1604,ff4d55e6-9df0-4796-b32c-33385eed3a58,2010-08-12 15:16:55.0,847.0,<cross-validation>,,
3675,5,1603,045c5c21-51e6-4b44-b4b5-2b41d8fe311a,2010-08-12 15:17:28.0,,"I am not sure to what extent this is a better bound then the trivial bound but here is one approach. Take the [taylor series expansion][1] of $exp(z^2)$ and ignoring terms higher than the second term, you get:\\n\\n$\\int e^{z^2} f(z) dz < \\int (1 + z^2) f(z) dz$\\n\\nNow,\\n\\n$\\int (1 + z^2) f(z) dz = 1 + \\int z^2 f(z) dz$\\n\\nBut, we know that:\\n\\n$\\int z^2 f(z) dz = Var(z) + E(z)^2$\\n\\nSubstituting for the variance and mean of the [binomial distribution][2] and simplifying, we get:\\n\\n$\\int e^{z^2} f(z) dz < 1 +  n^{1-\\beta} (2-n^{-\\beta})$\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Taylor_series\\n  [2]: http://en.wikipedia.org/wiki/Binomial_distribution",added link to taylor expansion,user28
3676,5,1583,f3767bfa-6cc8-41e3-b236-483c9a67014c,2010-08-12 15:18:27.0,196.0,"Contrary to [some here][1], others (e.g. [Brian Ripley][2], the authors of [sensR][3], and the authors of [psyphy][4]) appear to think that using a standard binomial link function when analyzing two alternative forced choice data in which the minimum expected proportion correct is .5 is incorrect.  However, their approach as to what the link function should be varies.  \\n\\n1.The sensR library uses:\\n\\n    function (mu) \\n    {\\n        tres <- mu\\n        for (i in 1:length(mu)) {\\n            if (mu[i] > 0.5) \\n                tres[i] <- sqrt(2) * qnorm(mu[i])\\n            if (mu[i] <= 0.5) \\n                tres[i] <- 0\\n        }\\n        tres\\n    }\\n\\n2.The psyphy library uses:\\n\\n    function (mu) \\n    {\\n        m <- 2\\n        mu <- pmax(mu, 1/m + .Machine$double.eps)\\n        qlogis((m * mu - 1)/(m - 1))\\n    }\\n\\n3.[Gabriel Baud-Bovy][5] implicitly recommends (1+exp(x)/(1+exp(x)))/2.  \\n\\nThe approach selected seems like it may have some consequences for the result.  Is there a ""correct"" link function to be using with these sorts of problems, or so long as the link, inverse link, mu.eta, and variance functions all agree is everything going to be all right?  Is there a single source material that provides any authoritative guidance on this issue?\\n\\nFollowing John's advice I plotted these functions...\\n\\n![alt text][6]\\n\\nThe black line is a standard logistic function.  The red line is the function from sensR.  The blue line is from psyphy and the cyan line is from Gabriel Baud-Bovy, but given the oddness of the shape it provides, perhaps I misinterpreted him.  The psyphy function line looks like what I'd expect a logistic function to look like in a psychophysics 2AFC experiment.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur\\n  [2]: https://stat.ethz.ch/pipermail/r-help/2006-December/122353.html\\n  [3]: http://cran.r-project.org/web/packages/sensR/\\n  [4]: http://cran.r-project.org/web/packages/psyphy/index.html\\n  [5]: https://stat.ethz.ch/pipermail/r-help/2006-December/122351.html\\n  [6]: http://psychlab2.ucr.edu/~russell/ForInternet/2AFCFunctionPlots.jpg",added 518 characters in body,
3677,5,1567,13b17166-f4e5-4a07-b834-8d99edc56318,2010-08-12 15:21:12.0,196.0,"Do you mean you want the percentage of the n in that month that belongs to each furniture category?  If so, can't you take the N for each month and divide all of the values from that month by N?\\n\\nFor example, your first case would be:\\n0.0034557235 0.0008639309 0.0001439885\\nand your second case (where N = 17756) would be \\n0.0001689570 0.0012953368 0.0002252760.\\n\\nOr do you want a comparison of your observed values to your expected values?  If that is the case you can construct a table with furniture type as columns and months as rows.  For each cell you can take the (sum of values in the row to which it belongs) * (sum of values in the column to which it belongs) and divide by the total number of values you have.  That will give you the expected value for the cell.  If you subtract that from your initial value it will tell you how many more or less of each type of furniture you had than expected given the type of furniture it is and the month in which you made your observation. \\n\\nFor example, consider this source data\\n<pre>\\nMonth  Chairs  Tables  Couches  Other   Total\\n1      48      12      2        13828   13890\\n2      3       24      4        17725   17756\\nTotal  51      36      6        31553   31646\\n48 chairs, 12 tables, 2 couches\\n</pre>\\nIt would be calculated like so (with the ?? marking values that still need to be calculated)...\\n<pre>\\nMonth  Chairs                 Tables  Couches  Other   Total\\n1      (51*13890)/31646=22.38     ??       ??     ??   13890\\n2      (51*17756)/31646=28.61     ??       ??     ??   17756\\nTotal  51                         ??       ??     ??   31646\\n</pre>\\nLetting you know that in Month 1 there were 48-22.38=25.62 more chairs than expected and that in Month 2 there were 3-28.61=-25.61 more chairs than expected - but to make more sense we can flip the sign and the terminology and say there were 25.61 fewer chairs than expected.\\n\\nFor more details consider looking [here][1].\\n\\n\\n  [1]: http://davidmlane.com/hyperstat/B143466.html",added 62 characters in body,
3678,5,1603,744264a8-310e-4139-b058-e3d9b52c947e,2010-08-12 15:22:35.0,,"I am not sure to what extent this is a better bound then the trivial bound but here is one approach. Take the [taylor series expansion][1] of $exp(z^2)$ and ignoring terms higher than the second term, you get:\\n\\n$\\int e^{z^2} f(z) dz < \\int (1 + z^2) f(z) dz$\\n\\nNow,\\n\\n$\\int (1 + z^2) f(z) dz = 1 + \\int z^2 f(z) dz$\\n\\nBut, we know that:\\n\\n$\\int z^2 f(z) dz = Var(z) + E(z)^2$\\n\\nSubstituting for the variance and mean of the [binomial distribution][2] and simplifying, we get:\\n\\n$\\int e^{z^2} f(z) dz < 1 +  n^{1-\\beta} (1-n^{-\\beta} + n^{1-\\beta})$\\n\\nPS: Please check the math as I corrected one error.\\n\\n  [1]: http://en.wikipedia.org/wiki/Taylor_series\\n  [2]: http://en.wikipedia.org/wiki/Binomial_distribution",fixed an error in the bound; added 29 characters in body,user28
3679,12,1603,a44117c0-a247-4a4f-9185-2429d7f419a9,2010-08-12 15:27:58.0,,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""}]}",,user28
3680,5,1603,6a466115-3bd3-45f1-8fc2-283b4a76702c,2010-08-12 15:30:56.0,,"This approach is technically not ok as $Z$ is a discrete variable but can be justified if we take the normal approximation to the binomial.\\n\\nI am not sure to what extent this is a better bound then the trivial bound but here is one approach. Take the [taylor series expansion][1] of $exp(z^2)$ and ignoring terms higher than the second term, you get:\\n\\n$\\int e^{z^2} f(z) dz < \\int (1 + z^2) f(z) dz$\\n\\nNow,\\n\\n$\\int (1 + z^2) f(z) dz = 1 + \\int z^2 f(z) dz$\\n\\nBut, we know that:\\n\\n$\\int z^2 f(z) dz = Var(z) + E(z)^2$\\n\\nSubstituting for the variance and mean of the [binomial distribution][2] and simplifying, we get:\\n\\n$\\int e^{z^2} f(z) dz < 1 +  n^{1-\\beta} (1-n^{-\\beta} + n^{1-\\beta})$\\n\\nPS: Please check the math as I corrected one error.\\n\\n  [1]: http://en.wikipedia.org/wiki/Taylor_series\\n  [2]: http://en.wikipedia.org/wiki/Binomial_distribution",added 143 characters in body,user28
3681,13,1603,17ffbfa5-75ec-4dd2-b8ef-635d38eb1762,2010-08-12 15:31:20.0,,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""}]}",,user28
3682,2,1605,12bb015e-f9c7-40c8-b644-3ac8ccb57bff,2010-08-12 16:03:50.0,666.0,"A simple option is to use sums of scores instead of the scores themselves. The sum of distributions tends to normality. For example, in Education you could add a student's scores over a series of tests.\\n\\nAnother option, of course, is to use techniques that do not assume normality, which are underestimated and underused.",,
3683,5,870,55f664a7-9278-426f-9173-ae191f240285,2010-08-12 16:07:55.0,520.0,"Given a list of p-values generated from independent tests, sorted in ascending order, one can use the [Benjamini-Hochberg procedure][1] for [multiple testing correction][2]. For each p-value, the Benjamini-Hochberg procedure allows you to calculate the False Discovery Rate (FDR) for each of the p-values. That is, at each ""position"" in the sorted list of p-values, it will tell you what proportion of those are likely to be false rejections of the null hypothesis.\\n\\nMy question is, are these FDR values to be referred to as ""**[q-values][3]**"", or as ""**corrected p-values**"", or as something else entirely?\\n\\n**EDIT 2010-07-12:** I would like to more fully describe the correction procedure we are using. First, we sort the test results in increasing order by their un-corrected original p-value. Then, we iterate over the list, calculating what I have been interpreting as ""the FDR expected if we were to reject the null hypothesis for this and all tests prior in the list,"" using the B-H correction, with an alpha equal to the observed, un-corrected p-value for the respective iteration. We then take, as what we've been calling our ""q-value"", the maximum of the previously corrected value (FDR at iteration i - 1) or the current value (at i), to preserve monotonicity.\\n\\nBelow is some Python code which represents this procedure:\\n\\n    def calc_benjamini_hochberg_corrections(p_values, num_total_tests):\\n        """"""\\n        Calculates the Benjamini-Hochberg correction for multiple hypothesis\\n        testing from a list of p-values *sorted in ascending order*.\\n\\n        See\\n        http://en.wikipedia.org/wiki/False_discovery_rate#Independent_tests\\n        for more detail on the theory behind the correction.\\n\\n        **NOTE:** This is a generator, not a function. It will yield values\\n        until all calculations have completed.\\n\\n        :Parameters:\\n        - `p_values`: a list or iterable of p-values sorted in ascending\\n          order\\n        - `num_total_tests`: the total number of tests (p-values)\\n\\n        """"""\\n        prev_bh_value = 0\\n        for i, p_value in enumerate(p_values):\\n            bh_value = p_value * num_total_tests / (i + 1)\\n            # Sometimes this correction can give values greater than 1,\\n            # so we set those values at 1\\n            bh_value = min(bh_value, 1)\\n\\n            # To preserve monotonicity in the values, we take the\\n            # maximum of the previous value or this one, so that we\\n            # don't yield a value less than the previous.\\n            bh_value = max(bh_value, prev_bh_value)\\n            prev_bh_value = bh_value\\n            yield bh_value\\n\\n\\n\\n\\n\\n  [1]: http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf\\n  [2]: http://en.wikipedia.org/wiki/False_discovery_rate#Independent_tests\\n  [3]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1074290335",I more fully described our correcting procedure and provided Python code which performs the correction.,
3684,4,870,55f664a7-9278-426f-9173-ae191f240285,2010-08-12 16:07:55.0,520.0,"Multiple hypothesis testing correction with Benjamini-Hochberg, p-values or q-values?",I more fully described our correcting procedure and provided Python code which performs the correction.,
3686,2,1607,a6fb685a-4c81-40e6-ac39-fe49b83db34b,2010-08-12 16:54:18.0,251.0,The [Box-Cox][1] transformation includes many of the ones you cited.  See this answer for some details:\\n\\n- [How should I transform non-negative data including zeros?][2]\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation\\n  [2]: http://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1452#1452\\n\\n\\n,,
3687,5,1603,113e611b-4793-460b-b357-9023f3513aea,2010-08-12 17:08:46.0,,"This answer is inspired by shabbychef's answer using the median. By definition:\\n\\n$E[exp(Z^2)] = \\sum_{z=1}^{z=n} exp(z^2) P(z;n,n^{-\\beta})$ \\n\\nwhere,\\n\\n$P(z;n,n^{-\\beta})$ is the binomial probability.\\n\\nDenote the [mode of this binomial distribution][1] by: $m(n,n^{-\\beta})$. Thus, by definition we have:\\n\\n$P(z;n,n^{-\\beta}) \\le P(m(n,n^{-\\beta});n,n^{-\\beta}) \\ \\ \\forall z$\\n\\nLet,\\n\\n$\\bar{P} = P(m(n,n^{-\\beta});n,n^{-\\beta})$\\n\\nThus,\\n\\n$E[exp(Z^2] \\le \\sum_{z=1}^{z=n} exp(z^2) \\bar{P}$\\n\\nThis upper bound is a function of $n$ and $\\beta$ as desired. \\n\\nHopefully, this in the right track unlike my previous attempt.\\n\\n<strike>This approach is technically not ok as $Z$ is a discrete variable but can be justified if we take the normal approximation to the binomial.\\n\\nI am not sure to what extent this is a better bound then the trivial bound but here is one approach. Take the [taylor series expansion][2] of $exp(z^2)$ and ignoring terms higher than the second term, you get:\\n\\n$\\int e^{z^2} f(z) dz < \\int (1 + z^2) f(z) dz$\\n\\nNow,\\n\\n$\\int (1 + z^2) f(z) dz = 1 + \\int z^2 f(z) dz$\\n\\nBut, we know that:\\n\\n$\\int z^2 f(z) dz = Var(z) + E(z)^2$\\n\\nSubstituting for the variance and mean of the [binomial distribution][3] and simplifying, we get:\\n\\n$\\int e^{z^2} f(z) dz < 1 +  n^{1-\\beta} (1-n^{-\\beta} + n^{1-\\beta})$\\n\\nPS: Please check the math as I corrected one error.\\n</strike>\\n\\n  [1]: http://en.wikipedia.org/wiki/Binomial_distribution#Mode_and_median\\n  [2]: http://en.wikipedia.org/wiki/Taylor_series\\n  [3]: http://en.wikipedia.org/wiki/Binomial_distribution",provided a upper bound using the mode,user28
3689,5,1582,5c898edf-39d6-435f-a6e6-994d8073046f,2010-08-12 17:31:16.0,251.0,"Multicollinearity is the usual suspect as JoFrhwld mentioned.  Basically, if your variables are positively correlated, then the coefficients will be negatively correlated, which can lead to a wrong sign on one of the coefficients.\\n\\nOne check would be to perform a principal components regression or ridge regression.  This reduces the dimensionality of the regression space, handling the multicollinearity.  You end up with biased estimates but a possibly lower MSE and corrected signs.  Whether you go with those particular results or not, it's a good diagnostic check.  If you still get sign changes, it may be theoretically interesting.  \\n\\nUPDATE\\n\\nFollowing from the comment in John Christie's answer, this might be interesting.  Reversal in association (magnitude or direction) are examples of Simpson's Paradox, Lord's Paradox and Suppression Effects.  The differences essentially relate to the type of variable.  It's more useful to understand the underlying phenomenon rather than think in terms of a particular ""paradox"" or effect.  For a causal perspective, the paper below does a good job of explaining why and I'll quote at length their introduction and conclusion to whet your appetite.\\n\\n- [The role of causal reasoning in understanding Simpson's paradox, Lord's paradox, and the suppression effect: covariate selection in the analysis of observational studies][1]\\n\\n> Tu et al present an analysis of the equivalence of three paradoxes, concluding that all three simply reiterate the unsurprising change in the association of any two variables when a third variable is statistically controlled for. I call this unsurprising because reversal or change in magnitude is common in conditional analysis. To avoid either, we must avoid conditional analysis altogether. What is it about Simpson's and Lord's paradoxes or the suppression effect, beyond their pointing out the obvious, that attracts the intermittent and sometimes alarmist interests seen in the literature?\\n\\n> [...]\\n\\n> In conclusion, it cannot be overemphasized that although Simpson's and related paradoxes reveal the perils of using statistical criteria to guide causal analysis, they hold neither the explanations of the phenomenon they purport to depict nor the pointers on how to avoid them. The explanations and solutions lie in causal reasoning which relies on background knowledge, not statistical criteria. It is high time we stopped treating misinterpreted signs and symptoms ('paradoxes'), and got on with the business of handling the disease ('causality'). We should rightly turn our attention to the perennial problem of covariate selection for causal analysis using non-experimental data.\\n\\n  [1]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2266743/\\n\\n\\n\\n\\n\\n",added 2117 characters in body; deleted 2 characters in body,
3690,2,1608,9b167d89-d1d8-4253-9186-78408997e432,2010-08-12 17:54:22.0,279.0,"Based on your additional explanation in the comments, it appears that you have 8 groups (each corresponding to a column) and a continuous outcome variable that you grouped into 10 bins (each bin corresponding to a row). Note that it also implies that the rows are ordered with later rows implying larger values.\\n\\nFirst of all, if you do have the underlying continuous variable, then do not bin it - just use Kruskall-Wallis or ANOVA to compare the groups.\\n\\nAssuming that the binning is unavoidable, you can still use a Kruskall-Wallis test, but not on the frequencies as you have apparently done it. Your current KW inference just tells you that you have *more data* in some groups as compared to others. The actual observations in this case are the row numbers (1 through 10), and the values in the table are just the frequencies of occurrences. Most statistical software has an option of specifying these as ""weights"" or ""frequencies"".\\n\\nThe chi-square test can be used on the frequencies, however if the rows are ordered it might have much lower power compared to the Kruskall-Wallis test to actually detect differences, since it completely ignores the ordering of the rows. Thus even though its results are valid, I would not recommend using these due to the loss of power.",,
3691,2,1609,da2f0ee9-1614-41c7-919b-fdeca1ff3f17,2010-08-12 19:52:35.0,251.0,"[*Nonparametric Regression and Spline Smoothing*][1] by Eubank is a good book.  You probably want to start with Chapters 2 and 5 which cover goodness of fit and the theory and construction of smoothing splines.  I've heard good things about [*Generalized Additive Models: An Introduction with R*][2], which might be better if you're looking for examples in R.  For a quick introduction, a google search turns up a course on [Nonparametric function estimation][3] where you can peruse the slides and see examples in R.\\n\\nThe general problem with splines is overfitting your data, but this is where cross validation comes in.  Of course, if you think your data arise from a parametric distribution, then you should be using different methods.\\n\\n\\n  [1]: http://www.amazon.com/Nonparametric-Regression-Spline-Smoothing-Statistics/dp/0824793374/\\n  [2]: http://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1584884746/\\n  [3]: http://www.stat.osu.edu/~yklee/763/note.html\\n  \\n\\n",,
3692,2,1610,0e3af40b-77fe-4dc5-b1d0-72c3c6f7136b,2010-08-12 19:55:02.0,110.0,"I'm not a statistician by education, I'm a software engineer. Yet statistics comes up a lot. In fact, questions specifically about Type I and Type II error are coming up a lot in the course of my studying for the Certified Software Development Associate exam (mathematics and statistics are 10% of the exam). I'm having trouble always coming up with the right definitions for Type I and Type II error - although I'm memorizing them now (and can remember them most of the time), I really don't want to freeze up on this exam trying to remember what the difference is.\\n\\nI know that Type I Error is a false positive, or when you reject the null hypothesis and it's actually true and a Type II error is a false negative, or when you accept the null hypothesis and it's actually false.\\n\\nIs there an easy way to remember what the difference is, such as a mnemonic? How do professional statisticians do it - is it just something that they know from using or discussing it often?\\n\\n(Side Note: This question can probably use some better tags. One that I wanted to create was ""terminology"", but I don't have enough reputation to do it. If someone could add that, it would be great. Thanks.)",,
3693,1,1610,0e3af40b-77fe-4dc5-b1d0-72c3c6f7136b,2010-08-12 19:55:02.0,110.0,Is there a way to remember the definitions of Type I and Type II Errors?,,
3694,3,1610,0e3af40b-77fe-4dc5-b1d0-72c3c6f7136b,2010-08-12 19:55:02.0,110.0,<error>,,
3695,6,1610,14f44def-eaea-440a-8e9f-33856e7f49d8,2010-08-12 20:01:33.0,,<typei-errors><typeii-errors>,added tags,user28
3696,2,1611,23f73346-52da-4db3-a4dc-d630a2d865ac,2010-08-12 20:09:33.0,572.0,"As an outsider, it appears that there are two competing views on how one should perform statistical inference. \\n\\nAre the two different methods both considered valid by working statisticians? \\n\\nIs choosing one considered more of a philosophical question? Or is the current situation considered problematic and attempts are being made to somehow unify the different approaches?\\n",,
3697,1,1611,23f73346-52da-4db3-a4dc-d630a2d865ac,2010-08-12 20:09:33.0,572.0,Do working statistians care about the difference between frequentist and bayesian inference? ,,
3698,3,1611,23f73346-52da-4db3-a4dc-d630a2d865ac,2010-08-12 20:09:33.0,572.0,<bayesian><frequentist>,,
3699,2,1612,05867b52-cbb1-4cd9-9561-1415eb68cd2b,2010-08-12 20:10:13.0,251.0,"I used to think of it in terms of the usual [picture][1] of two Normal distributions (or bell curves).  Going left to right, distribution 1 is the Null, and the distribution 2 is the Alternative.  Type I (erroneously) rejects the first (Null) and Type II ""rejects"" the second (Alternative).  \\n\\n(Now you just need to remember that you're not actually rejecting the alternative, but erroneously accepting (or failing to reject) the Null  --  i.e. restate everything in the form of the Null.  Hey, it worked for me!)\\n\\n\\n  [1]: http://intuitor.com/statistics/T1T2Errors.html\\n\\n",,
3700,6,1610,5ea745a6-4c57-4201-9fb6-135aa6d80e62,2010-08-12 20:10:28.0,,<type-i-errors><type-ii-errors><terminology>,edited tags,user28
3701,16,1611,83bbd686-08c9-4df7-9c16-ffaf16411261,2010-08-12 20:20:11.0,572.0,,,
3702,2,1613,c27148b5-afd9-4bdb-a730-5415482f61bc,2010-08-12 20:25:12.0,5.0,"While this is subjective, I would say:\\n\\nIt is called the bayesian/frequentist ""**debate**"" for a reason.  There is a clear philosophical difference between the two approaches.  \\n\\nBut as with most things, it's a spectrum.  Some people are very much in one camp or the other and completely reject the alternative.  Most people probably fall somewhere in the middle.  I myself would use either method depending on the circumstances.",,
3703,16,1613,c27148b5-afd9-4bdb-a730-5415482f61bc,2010-08-12 20:25:12.0,-1.0,,,
3704,2,1614,3075276f-928b-4bef-938e-9b05db56aee6,2010-08-12 20:27:49.0,601.0,Here's a handy way that happens to have some truth to it.\\n\\nYoung scientists commit Type-I because they want to find effects and jump the gun while old scientist commit Type-II because they refuse to change their beliefs.\\n\\n(someone comment in a funnier version of that :) ),,
3705,6,1604,3497872f-1ef3-465e-980f-df68f1593517,2010-08-12 20:50:11.0,71.0,<cross-validation><smoothing><splines>,edited tags,
3706,2,1615,edbd65db-1869-4e52-811f-d4d53a273541,2010-08-12 20:51:36.0,394.0,"Adding to what Shane says, I think the continuum comprises:\\n\\n 1. Firm philosophical standing in the Bayes camp\\n 2. Both considered valid, with one approach more or less preferable for a given problem\\n 3. I'd use a Bayesian approach (at all or more often) but I don't have the time.\\n 4. Firm philosophical standing in the frequentist camp\\n 5. I do it like I learned in class. What's Bayes?\\n\\nAnd yes, I know working statisticians and analysts at all of these points. Most of the time I'm living at #3, striving to spend more time at #2.",,
3707,16,1615,edbd65db-1869-4e52-811f-d4d53a273541,2010-08-12 20:51:36.0,-1.0,,,
3708,2,1616,fbc3e892-02e2-4422-b846-dda1ddb329df,2010-08-12 20:52:04.0,900.0,"Since type two means ""False negative"" or sort of ""false false"", I remember it as the number of falses. \\n\\n - Type I: ""I **falsely** think hypothesis is true"" (one false)\\n - Type II: ""I **falsely** think hypothesis is **false**"" (two falses)\\n",,
3709,5,1604,4929bd28-7ca7-4b2c-bb6b-287e2cd7bdd9,2010-08-12 20:57:22.0,847.0,Can someone provide me with a book or online reference on how to construct smoothing splines with cross-validation? I have a programming and undergraduate level mathematics background.  I would also appreciate an overview of whether this is smoothing technique is a good one for smoothing data and whether there are any disadvantages of which a non-statistician needs to be aware.,deleted 2 characters in body,
3710,2,1617,f5de35e9-513b-4a08-bb2d-ce3c2b02b1af,2010-08-12 21:21:19.0,364.0,"I remember it by thinking: What's the first thing I do when I do a null-hypothesis significance test? I set the criterion for the probability that I will make a false rejection. Thus, type 1 is this criterion and type 2 is the other probability of interest: the probability that I will fail to reject the null when the null is false. So, 1=first probability I set, 2=the other one.",,
3711,5,1569,71688489-035c-4f2b-92c6-ed30c8508ebb,2010-08-12 21:23:24.0,511.0,"Suppose you are given n IID samples generated by either p or by q. You want to identify which distribution generated them. Take as null hypothesis that they were generated by q. Let a indicate probability of Type I error, mistakenly rejecting the null hypothesis, and b indicate probability of Type II error.\\n\\nThen for large n, probability of Type I error is at least\\n\\nexp(-n KL(p,q))\\n\\nIn other words, for any decision procedure, probability of Type I falls at most by a factor of exp(KL(p,q)) with each datapoint. Type II error falls by factor of exp(KL(q,p)) at most.\\n\\nFor small n, a and b are related as follows\\n\\n$b \\log \\frac{b}{1-a}+(1-b)\\log \\frac{1-b}{a}  \\le n \\text{KL}(p,q)$\\n\\nand\\n\\n$a \\log \\frac{a}{1-b}+(1-a)\\log \\frac{1-a}{b}  \\le n \\text{KL}(q,p)$\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>, and pages 74-77 of Kullback's ""Information Theory and Statistics"" (1978).\\n\\nAs a side note, this interpretation can be used to <a href=""http://www.pnas.org/content/97/21/11170.abstract"">motivate</a> Fisher Information metric, since for any pair of distributions p,q with Fisher's distance k (small k) you need the same number of observations to to tell them apart",added 631 characters in body; deleted 17 characters in body,
3712,2,1618,ed418a24-50a4-4964-9cd7-6aa890dfe347,2010-08-12 21:48:24.0,,"I would imagine that in applied fields the divide is not paid that much attention as researchers/practitioners tend to be pragmatic in applied works. You choose the tool that works given the context. \\n\\nHowever, the debate is alive and well among those who care about the philosophical issues underlying these two approaches. See for example the following blog posts of [Andrew Gelman][1]:\\n\\n - [Ya don't know Bayes, Jack][2]\\n - [Philosophy and the practice of Bayesian statistics][3]\\n\\n\\n  [1]: http://stat.columbia.edu/~gelman\\n  [2]: http://www.stat.columbia.edu/~cook/movabletype/archives/2010/06/hey_dude_ya_don.html\\n  [3]: http://www.stat.columbia.edu/~cook/movabletype/archives/2010/06/philosophy_and.html",,user28
3713,16,1618,ed418a24-50a4-4964-9cd7-6aa890dfe347,2010-08-12 21:48:24.0,-1.0,,,
3714,5,1569,7e81e2f5-cfb9-4873-b26f-5e22c5b769f8,2010-08-12 22:08:43.0,511.0,"Suppose you are given n IID samples generated by either p or by q. You want to identify which distribution generated them. Take as null hypothesis that they were generated by q. Let a indicate probability of Type I error, mistakenly rejecting the null hypothesis, and b indicate probability of Type II error.\\n\\nThen for large n, probability of Type I error is at least\\n\\nexp(-n KL(p,q))\\n\\nIn other words, for any decision procedure, probability of Type I falls at most by a factor of exp(KL(p,q)) with each datapoint. Type II error falls by factor of exp(KL(q,p)) at most.\\n\\nFor small n, a and b are related as follows\\n\\n$b \\log \\frac{b}{1-a}+(1-b)\\log \\frac{1-b}{a}  \\le n \\text{KL}(p,q)$\\n\\nand\\n\\n$a \\log \\frac{a}{1-b}+(1-a)\\log \\frac{1-a}{b}  \\le n \\text{KL}(q,p)$\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>, and pages 74-77 of Kullback's ""Information Theory and Statistics"" (1978).\\n\\nAs a side note, this interpretation can be used to <a href=""http://www.pnas.org/content/97/21/11170.abstract"">motivate</a> Fisher Information metric, since for any pair of distributions p,q at Fisher's distance k from each other (small k) you need the same number of observations to to tell them apart",added 14 characters in body,
3715,5,1569,26c026c4-3eb4-4030-8716-456454d1e948,2010-08-12 22:59:35.0,511.0,"Suppose you are given n IID samples generated by either p or by q. You want to identify which distribution generated them. Take as null hypothesis that they were generated by q. Let a indicate probability of Type I error, mistakenly rejecting the null hypothesis, and b indicate probability of Type II error.\\n\\nThen for large n, probability of Type I error is at least\\n\\nexp(-n KL(p,q))\\n\\nIn other words, for any decision procedure, probability of Type I falls at most by a factor of exp(KL(p,q)) with each datapoint. Type II error falls by factor of exp(KL(q,p)) at most.\\n\\nFor arbitrary n, a and b are related as follows\\n\\n$b \\log \\frac{b}{1-a}+(1-b)\\log \\frac{1-b}{a}  \\le n \\text{KL}(p,q)$\\n\\nand\\n\\n$a \\log \\frac{a}{1-b}+(1-a)\\log \\frac{1-a}{b}  \\le n \\text{KL}(q,p)$\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>, and pages 74-77 of Kullback's ""Information Theory and Statistics"" (1978).\\n\\nAs a side note, this interpretation can be used to <a href=""http://www.pnas.org/content/97/21/11170.abstract"">motivate</a> Fisher Information metric, since for any pair of distributions p,q at Fisher's distance k from each other (small k) you need the same number of observations to to tell them apart",added 4 characters in body,
3716,2,1619,999f5f14-ebe5-4726-9c65-0f6cbe1f84c5,2010-08-12 23:02:11.0,830.0,"I use the ""judicial"" approach for remembering the difference between type I and type II: a judge committing a type I error sends an innocent man to jail, while a judge committing a type II error lets a guilty man walk free.",,
3717,2,1620,9e1ea0a3-bde7-4de1-9da1-d19f863ec771,2010-08-12 23:38:47.0,110.0,"I was talking to a friend of mine about this and he kicked me a link to [the Wikipedia article on type I and type II errors][1], where they apparently now provide a (somewhat unhelpful, in my opinion) mnemonic. I did, however, want to add it here just for the sake of completion. Although I didn't think it helped me, it might help someone else:\\n\\n> For those experiencing difficulty\\n> correctly identifying the two error\\n> types, the following mnemonic is based\\n> on the fact that (a) an ""error"" is\\n> false, and (b) the Initial letters of\\n> ""Positive"" and ""Negative"" are written\\n> with a different number of vertical\\n> lines:\\n>\\n> * A Type I error is a false POSITIVE; and P has a single vertical line.\\n> * A Type II error is a false NEGATIVE; and N has two vertical lines.\\n\\nWith this, you need to remember that a false positive means rejecting a true null hypothesis and a false negative is failing to reject a false null hypothesis.\\n\\nThis is by no means the best answer here, but I did want to throw it out there in the event someone finds this question and this can help them.\\n\\n  [1]: http://en.wikipedia.org/wiki/Type_I_and_type_II_errors",,
3718,2,1621,da2132e0-1139-4d77-8e96-a4d35c85a939,2010-08-12 23:53:12.0,511.0,"I came across a <a href=""http://www.citeulike.org/user/yaroslavvb/tag/information-geometry"">large</a> body of literature which advocates using Fisher's Information metric as a natural metric in the space of probability distributions and then integrating over this local metric to define distances and volumes.\\n\\nBut are these ""integrated"" quantities actually useful for anything? I found no theoretical justifications and very few practical applications. One is Guy Lebanon's <a href=""http://www.cs.cmu.edu/~lafferty/pub/hyperplane.pdf""> work</a> where he uses ""Fisher's distance"" to classify documents and another one is Rodriguez' <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.9259&rep=rep1&type=pdf"">ABC of Model Selection…</a> where ""Fisher's volume"" is used for model selection. \\n\\nA theoretical justification might be to have a generalization bound which uses this measure of distance or volume and is better than bounds derived from MDL or asymptotic arguments, or a method relying on one of these quantities that's provably better in some reasonably practical situation, are there any results of this kind?",,
3719,1,1621,da2132e0-1139-4d77-8e96-a4d35c85a939,2010-08-12 23:53:12.0,511.0,Using information geometry to define distances and volumes…useful?,,
3720,3,1621,da2132e0-1139-4d77-8e96-a4d35c85a939,2010-08-12 23:53:12.0,511.0,<statistics>,,
3721,2,1622,f12424e2-344b-4782-a788-f3a390bad2bd,2010-08-13 00:18:34.0,795.0,"One estimate of the 'quality' of a portfolio is its _Sharpe ratio_, which is defined as the mean of the returns divided by the standard deviation of the returns (modulo adjustments for risk free rate, etc). The sample Sharpe ratio is the sample mean divided by the sample standard deviation. Up to a constant factor ($\\sqrt{n}$, where $n$ is the number of observations), this is distributed as a (possibly non-central) $t$-statistic. \\n\\nAre there known techniques for comparing the mean of independent variables distributed as non-central $t$-statistics? Of course, there are non-parametric tests of mean, but is there something specific to the case of noncentral $t$?",,
3722,1,1622,f12424e2-344b-4782-a788-f3a390bad2bd,2010-08-13 00:18:34.0,795.0,comparing 2 independent non-central t statistics,,
3723,3,1622,f12424e2-344b-4782-a788-f3a390bad2bd,2010-08-13 00:18:34.0,795.0,<hypothesis-testing><mean><parametric>,,
3724,11,134,24eae004-c4b8-4ea9-b154-03167c95c3d1,2010-08-13 00:30:00.0,-1.0,"{""Voters"":[{""Id"":251,""DisplayName"":""ars""},{""Id"":159,""DisplayName"":""Rob Hyndman""}]}",,
3725,6,1621,83175e79-7a04-44e4-beaf-e84cd4960f36,2010-08-13 00:34:07.0,159.0,,edited tags,
3726,2,1623,b96e29b1-13ee-44fc-8b3d-1f64fe9e3925,2010-08-13 00:37:26.0,,"Rather than using the Gelman-Rubin statistic, which is a nice aid but not perfect (as with all convergence diagnostics), I simply use the same idea and plot the results for a visual graphical assessment. In almost all cases I have considered (which is a very large number), graphing the trace plots of multiple MCMC chains started from widely varied starting positions is sufficient to show or assess whether the same posterior is being converged to or not, in each case. I use this method to: 1. Whether the MCMC chain (ever) converges; 2.Assess how long I should set the burn-in period; and 3. to calculate Gelman's R statistic (see Gelman, Carlin, Stern and Rubin, Bayesian Data Analysis) to measure the efficiency and speed of mixing in the MCMC sampler. Efficiency and convergence are slightly different issues: e.g. you can have convergence with very low efficiency (i.e. thus requiring long chains to converge). I have used this graphical method to successfully diagnose (and later correct) lack of convergence problems in specific and general situations.\\n",,Richard Gerlach
3727,2,1624,f1c199d5-45b7-4a1d-b784-e608c3bb4c41,2010-08-13 00:48:15.0,850.0,How are the values in a z-table computed? Is it possible to compute confidence without looking up a z-table?,,
3728,1,1624,f1c199d5-45b7-4a1d-b784-e608c3bb4c41,2010-08-13 00:48:15.0,850.0,"Based on z-score, is it possible to compute confidence without looking at a z-table?",,
3729,3,1624,f1c199d5-45b7-4a1d-b784-e608c3bb4c41,2010-08-13 00:48:15.0,850.0,<confidence-interval>,,
3730,6,1169,7bc6998c-3b73-44ba-8a06-b967ab60f3a7,2010-08-13 00:56:41.0,364.0,<confidence-interval>,"oops, my previous attempt to remove ""credible-interval"" tag erroneously removed the ""confidence-interval"" tag. Fixed.",
3731,6,1399,cb526f2c-9269-4d0a-9599-b5bad0751719,2010-08-13 00:57:22.0,364.0,<confidence-interval><bootstrap>,"removed the ""credible interval"" tag (I erroneously thought this was simply a synonym for confidence interval!)",
3732,2,1625,84fcf994-adde-4a84-9620-5534a81272a5,2010-08-13 01:08:39.0,,"See also Jennrich, RJ, Oman, SD ""How much does Stein estimation help in multiple linear regression?"" Technometrics, 28, 113-121, 1986.",,Robert Jernigan
3733,2,1626,4d8579bc-4591-4272-970e-c6eab57e960e,2010-08-13 01:10:05.0,394.0,"I don't know how to post formulas here, so search google on ""normal distribution function"" or see, e.g., [here][1].\\n\\n\\n  [1]: http://www.rapidtables.com/math/probability/normal_distribution.htm#normal",,
3734,5,1625,b10d9fde-5a88-4372-9419-1fd5fae665cd,2010-08-13 01:26:17.0,159.0,"See also [Jennrich, RJ, Oman, SD ""How much does Stein estimation help in multiple linear regression?"" *Technometrics*, **28**, 113-121, 1986.][1]\\n\\n\\n  [1]: http://www.jstor.org/pss/1270447",added 56 characters in body,
3735,2,1627,a0630a11-cfbc-43ae-aac3-dfa0816790cf,2010-08-13 01:30:32.0,,"Take a look at the R graphics library, ggplot2. Details are at the web page http://had.co.nz/ggplot2/ This package generates very good default plots, that follow the Tufte principles, Cleveland's guidelines and Ihaka's color package.",,visnut
3736,2,1628,56f1da6a-7560-403e-99c1-e27ef91ac1d4,2010-08-13 01:38:42.0,183.0,"Based on the principle of Occam's razor, Type I errors (rejecting the null hypothesis when it is true) are **""arguably""** worse than Type II errors (not rejecting the null hypothesis it is false).\\n\\nIf you believe such an argument:\\n\\n - Type I errors are of primary concern\\n - Type II errors are of secondary concern\\n\\nNote: I'm not endorsing this value judgement, but it does help me remember Type I from Type II.",,
3737,5,1569,13626069-0054-4aa9-9fc3-ec230511bcdc,2010-08-13 01:42:47.0,511.0,"Suppose you are given n IID samples generated by either p or by q. You want to identify which distribution generated them. Take as null hypothesis that they were generated by q. Let a indicate probability of Type I error, mistakenly rejecting the null hypothesis, and b indicate probability of Type II error.\\n\\nThen for large n, probability of Type I error is at least\\n\\nexp(-n KL(p,q))\\n\\nIn other words, for an ""optimal"" decision procedure, probability of Type I falls at most by a factor of exp(KL(p,q)) with each datapoint. Type II error falls by factor of exp(KL(q,p)) at most.\\n\\nFor arbitrary n, a and b are related as follows\\n\\n$b \\log \\frac{b}{1-a}+(1-b)\\log \\frac{1-b}{a}  \\le n \\text{KL}(p,q)$\\n\\nand\\n\\n$a \\log \\frac{a}{1-b}+(1-a)\\log \\frac{1-a}{b}  \\le n \\text{KL}(q,p)$\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>, and pages 74-77 of Kullback's ""Information Theory and Statistics"" (1978).\\n\\nAs a side note, this interpretation can be used to <a href=""http://www.pnas.org/content/97/21/11170.abstract"">motivate</a> Fisher Information metric, since for any pair of distributions p,q at Fisher's distance k from each other (small k) you need the same number of observations to to tell them apart",added 9 characters in body,
3738,5,1628,52168a03-372a-4150-9419-b8cabbb612b3,2010-08-13 01:48:32.0,183.0,"Based on the principle of [Occam's razor][1], Type I errors (rejecting the null hypothesis when it is true) are **""arguably""** worse than Type II errors (not rejecting the null hypothesis when it is false).\\n\\nIf you believe such an argument:\\n\\n - Type I errors are of primary concern\\n - Type II errors are of secondary concern\\n\\nNote: I'm not endorsing this value judgement, but it does help me remember Type I from Type II.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Occam's_razor",added 60 characters in body; added 5 characters in body,
3739,5,1569,7d4052cb-16f5-444a-a1ff-3f6f8cc7a6c5,2010-08-13 01:59:22.0,511.0,"Suppose you are given n IID samples generated by either p or by q. You want to identify which distribution generated them. Take as null hypothesis that they were generated by q. Let a indicate probability of Type I error, mistakenly rejecting the null hypothesis, and b indicate probability of Type II error.\\n\\nThen for large n, probability of Type I error is at least\\n\\nexp(-n KL(p,q))\\n\\nIn other words, for an ""optimal"" decision procedure, probability of Type I falls at most by a factor of exp(KL(p,q)) with each datapoint. Type II error falls by factor of exp(KL(q,p)) at most.\\n\\nFor arbitrary n, a and b are related as follows\\n\\n$b \\log \\frac{b}{1-a}+(1-b)\\log \\frac{1-b}{a}  \\le n \\text{KL}(p,q)$\\n\\nand\\n\\n$a \\log \\frac{a}{1-b}+(1-a)\\log \\frac{1-a}{b}  \\le n \\text{KL}(q,p)$\\n\\nIf we express the bound above as the lower bound on a in terms of b and KL and decrease b to 0, result <a href=""http://yaroslavvb.com/upload/lower-bounds.png"">seems</a> to approach the ""exp(-n KL(q,p))"" bound even for small n\\n\\nMore details on page 10 <a href=""http://arxiv.org/abs/adap-org/9601001"">here</a>, and pages 74-77 of Kullback's ""Information Theory and Statistics"" (1978).\\n\\nAs a side note, this interpretation can be used to <a href=""http://www.pnas.org/content/97/21/11170.abstract"">motivate</a> Fisher Information metric, since for any pair of distributions p,q at Fisher's distance k from each other (small k) you need the same number of observations to to tell them apart",added 209 characters in body; added 20 characters in body,
3740,2,1629,c24fc243-3e3b-461e-aac9-0d8813c47a81,2010-08-13 02:18:09.0,183.0,"The **first step** should be to **ask why** your variables are non-normally distributed. This can be illuminating. \\nCommon findings from my experience:\\n\\n - Ability tests (e.g., exams, intelligence tests, admission tests) tend to be negatively skewed when there are ceiling effects and positively skewed when there are floor effects. Both findings suggest that the difficulty level of the test is not optimised for the sample, either being too easy or too difficult to optimally differentiate ability. It also implies that the latent variable of interest could still be normally distributed, but that the structure of the test is inducing a skew in the measured variable.\\n - Ability tests often have outliers in terms of low scorers. In short there are many ways to do poorly on a test. In particular this can sometimes be seen on exams where there are a small percentage of students where some combination of lack of aptitude and lack of effort have combined to create very low test scores. This implies that the latent variable of interest probably has a few outliers.\\n - In relation to self-report tests (e.g., personality, attitude tests, etc.) skew often occurs when the sample is inherently high on the scale (e.g., distributions of life satisfaction are negatively skewed because most people are satisfied) or when the scale has been optimised for a sample different to the one the test is being applied to (e.g., applying a clinical measure of depression to a non-clinical sample).\\n\\nThis first step may suggest design modifications to the test. If you are aware of these issues ahead of time, you can even design your test to avoid them, if you see them as problematic.\\n\\nThe **second step** is to **decide what to do** in the situation where you have non-normal data.\\nNote transformations are but one possible strategy.\\nI'd reiterate the general advice from a [previous answer regarding non-normality][1]:\\n\\n - Many procedures that assume normality of residuals are **robust** to modest violations of normality of residuals\\n - **Bootstrapping** is generally a good strategy\\n - **Transformations** are another good strategy. Note that from my experience the kinds of mild skew that commonly occur with ability and self-report psychological tests can usually be fairly readily transformed to a  distribution approximating normality using a log, sqrt, or inverse transformation (or the reversed equivalent).\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1386/a-robust-t-test-for-the-mean/1391#1391",,
3741,5,1622,8b2a1517-1616-47b2-9d14-0a36adb81440,2010-08-13 02:28:57.0,795.0,"One estimate of the 'quality' of a portfolio of stocks is the _Sharpe ratio_, which is defined as the mean of the returns divided by the standard deviation of the returns (modulo adjustments for risk free rate, etc). The sample Sharpe ratio is the sample mean divided by the sample standard deviation. Up to a constant factor ($\\sqrt{n}$, where $n$ is the number of observations), this is distributed as a (possibly non-central) $t$-statistic. \\n\\nAre there known techniques for comparing <strike>the mean of</strike> independent variables distributed as non-central $t$-statistics? <strike>Of course, there are non-parametric tests of mean, but is there something specific to the case of noncentral $t$?</strike> (I'm not sure what I meant by that.)\\n\\n**edit**: the original question is somewhat ambiguous (well, it's actually not what I want). Is there a way to test the null hypothesis: population Sharpe ratio of $X$ equals population Sharpe ratio of $Y$, given independent collections of observations drawn from $X$ and $Y$? Here Sharpe ratio is mean divided by standard deviation.",added 418 characters in body; edited tags,
3742,6,1622,8b2a1517-1616-47b2-9d14-0a36adb81440,2010-08-13 02:28:57.0,795.0,<hypothesis-testing><finance><mean>,added 418 characters in body; edited tags,
3743,5,1609,50b06179-b0a0-4f6e-9750-8e58e1e9178b,2010-08-13 02:46:22.0,251.0,"[*Nonparametric Regression and Spline Smoothing*][1] by Eubank is a good book.  You probably want to start with Chapters 2 and 5 which cover goodness of fit and the theory and construction of smoothing splines.  I've heard good things about [*Generalized Additive Models: An Introduction with R*][2], which might be better if you're looking for examples in R.  For a quick introduction, a google search turns up a course on [Nonparametric function estimation][3] where you can peruse the slides and see examples in R.\\n\\nThe general problem with splines is overfitting your data, but this is where cross validation comes in.  \\n\\n  [1]: http://www.amazon.com/Nonparametric-Regression-Spline-Smoothing-Statistics/dp/0824793374/\\n  [2]: http://www.amazon.com/Generalized-Additive-Models-Introduction-Statistical/dp/1584884746/\\n  [3]: http://www.stat.osu.edu/~yklee/763/note.html\\n  \\n\\n",deleted 117 characters in body,
3744,2,1630,af63f181-54b1-43ca-97a3-11bf26c44267,2010-08-13 04:27:15.0,159.0,"No-one mentioned the inverse hyperbolic sine transformation. So for completeness I'm adding it here.\\n\\nThis is an alternative to the Box-Cox transformations and is defined by\\n\\begin{equation}\\nf(y,\\theta) = \\text{sinh}^{-1}(\\theta y)/\\theta = \\log[\\theta y + (\\theta^2y^2+1)^{1/2}]/\\theta,\\n\\end{equation}\\nwhere $\\theta>0$. For any value of $\\theta$, zero maps to zero. There is also a two parameter version allowing a shift, just as with the two-parameter BC transformation. [Burbidge, Magee and Robb (1988)][1] discuss the IHS transformation including estimation of $\\theta$.\\n\\nThe IHS transformation works with data defined on the whole real line including negative values and zeros. For large values of $y$ it behaves like a log transformation, regardless of the value of $\\theta$ (except 0). The limiting case as $\\theta\\rightarrow0$ gives $f(y,\\theta)\\rightarrow y$.\\n\\nIt looks to me like the IHS transformation should be a lot better known than it is.\\n\\n\\n  [1]: http://www.jstor.org/pss/2288929",,
3745,2,1631,101dce20-779b-4f95-94f4-866e791da492,2010-08-13 05:17:37.0,183.0,"A z-table gives you values of the cumulative distribution function for the standard (i.e., mean = 0, standard deviation = 1) normal distribution. I believe that the integral needs to be estimated numerically. See [here for a discussion][1].\\n\\nOf course, to answer your question literally, you could use some other lookup system other than a table, such as the `pnorm` [function][2] in R.\\nExample:\\n\\n    > pnorm(2)\\n    [1] 0.9772499\\n\\nHelp for `pnorm` also provides this reference for how the values are calculated:\\n\\n* Cody, W. D. (1993) Algorithm 715: SPECFUN – A portable FORTRAN package of special function routines and test drivers. ACM Transactions on Mathematical Software 19, 22–32.\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function\\n  [2]: http://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html",,
3746,2,1632,af54dbd1-f09b-48c4-bd7a-950f53906cb4,2010-08-13 05:30:09.0,251.0,"It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a *lot* of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  \\n\\n\\n- [NumPy/Scipy][1] You probably know about these already.  But let me point out the [Cookbook][2] where you can read about many statistical facilities already available and the [Example List][3] which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's [Distributions in Scipy][4].\\n\\n- [pandas][5] This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.\\n\\n- [larry][6]  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.\\n\\n- [python-statlib][7] A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.\\n\\n- [statsmodels][8] Statistical modeling: Linear models, GLMs, among others.  \\n\\n- [scikits][9]  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.\\n\\n- [PyMC][10] For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.\\n\\n- [PyMix][11] Mixture models.\\n\\nThere's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned.\\n\\n\\n  [1]: http://scipy.org\\n  [2]: http://scipy.org/Cookbook\\n  [3]: http://www.scipy.org/Numpy_Example_List\\n  [4]: http://www.johndcook.com/distributions_scipy.html\\n  [5]: http://code.google.com/p/pandas/\\n  [6]: http://pypi.python.org/pypi/la/0.1.0\\n  [7]: http://code.google.com/p/python-statlib/\\n  [8]: http://statsmodels.sourceforge.net/\\n  [9]: http://scikits.appspot.com/scikits\\n  [10]: http://code.google.com/p/pymc/\\n  [11]: http://www.pymix.org/pymix/index.php?n=PyMix.Home\\n",,
3747,5,1632,5edf1c0a-f6aa-431f-b6ee-73369e4c716e,2010-08-13 05:37:13.0,251.0,"It's hard to ignore the wealth of statistical packages available in R/CRAN.  That said, I spend a *lot* of time in Python land and would never dissuade anyone from having as much fun as I do.  :)  Here are some libraries/links you might find useful for statistical work.  \\n\\n\\n- [NumPy/Scipy][1] You probably know about these already.  But let me point out the [Cookbook][2] where you can read about many statistical facilities already available and the [Example List][3] which is a great reference for functions (including data manipulation and other operations).  Another handy reference is John Cook's [Distributions in Scipy][4].\\n\\n- [pandas][5] This is a really nice library for working with statistical data -- tabular data, time series, panel data.  Includes many builtin functions for data summaries, grouping/aggregation, pivoting.  Also has a statistics/econometrics library.\\n\\n- [larry][6]  Labeled array that plays nice with NumPy.  Provides statistical functions not present in NumPy and good for data manipulation.\\n\\n- [python-statlib][7] A fairly recent effort which combined a number of scattered statistics libraries.  Useful for basic and descriptive statistics if you're not using NumPy or pandas.\\n\\n- [statsmodels][8] Statistical modeling: Linear models, GLMs, among others.  \\n\\n- [scikits][9]  Statistical and scientific computing packages -- notably smoothing, optimization and machine learning.\\n\\n- [PyMC][10] For your Bayesian/MCMC/hierarchical modeling needs. Highly recommended.\\n\\n- [PyMix][11] Mixture models.\\n\\nIf speed becomes a problem, consider [Theano][12] -- used with good success by the deep learning people.\\n\\nThere's plenty of other stuff out there, but this is what I find the most useful along the lines you mentioned.\\n\\n\\n  [1]: http://scipy.org\\n  [2]: http://scipy.org/Cookbook\\n  [3]: http://www.scipy.org/Numpy_Example_List\\n  [4]: http://www.johndcook.com/distributions_scipy.html\\n  [5]: http://code.google.com/p/pandas/\\n  [6]: http://pypi.python.org/pypi/la/0.1.0\\n  [7]: http://code.google.com/p/python-statlib/\\n  [8]: http://statsmodels.sourceforge.net/\\n  [9]: http://scikits.appspot.com/scikits\\n  [10]: http://code.google.com/p/pymc/\\n  [11]: http://www.pymix.org/pymix/index.php?n=PyMix.Home\\n  [12]: http://deeplearning.net/software/theano/\\n",added 158 characters in body,
3748,2,1633,7f236c96-3594-41ce-b5a3-e28c5eb5ea06,2010-08-13 05:43:29.0,830.0,"The simplest answer is that one either uses numerical quadrature techniques (Simpson's, for instance) on the PDF, or approximations such as those found in [Abramowitz and Stegun][1]. Personally however, since one merely needs a few digits of accuracy for the ""error function"" that is involved in computing z-scores, you might be interested the simple approximations given [here][2] by S. Winitzki.\\n\\n\\n  [1]: http://www.math.ucla.edu/~cbm/aands//page_932.htm\\n  [2]: http://homepages.physik.uni-muenchen.de/~Winitzki/erf-approx.pdf",,
3749,5,1607,42183d4b-546a-4641-b76e-288ffe681f1d,2010-08-13 05:49:53.0,251.0,The [Box-Cox][1] transformation includes many of the ones you cited.  See this answer for some details:\\n\\n- [How should I transform non-negative data including zeros?][2]\\n\\nUPDATE: These [slides][3] provide a pretty good overview of Box-Cox transformations.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Box-Cox_transformation\\n  [2]: http://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1452#1452\\n  [3]: http://www.stat.uconn.edu/~studentjournal/index_files/pengfi_s05.pdf\\n\\n,added 165 characters in body,
3750,2,1634,46d49efd-d040-4275-8f7b-e7edeeb4b0ce,2010-08-13 05:51:00.0,419.0,"There are two definitions of statistical independence:\\n1) P(A,B)=P(A)*P(B) <=> 2) P(A|B)=P(A) <=> 2a) P(B|A)=P(B).\\n\\n(<=> means if and only if)\\n\\nSo to answer your question: both are valid.\\n\\nPearson Chi-square test of independence is motivated by definition 1), logistic regression and multinomial regression are motivated by definition 2) of independence.\\n",,
3751,2,1635,18848a22-2a1d-46a5-92b8-6a2adfb552c8,2010-08-13 05:51:02.0,196.0,"I'm probably missing something important, but why does the fact that your observed variable is a Sharpe ratio change the statistic you would use to test the difference in Sharpe ratios? I understand that they are already distributed like 2 independent non-central t statistics, but what forces you to treat them that way?\\n\\nPresumably the central limit theorem would hold even for Sharpe ratios and as such you should be able to apply a parametric test of mean differences, e.g. independent-samples Z.  \\n\\nMore importantly, if your data is financial data wouldn't it be better to treat these as paired samples paired by the times at which they were observed?",,
3752,2,1636,6b4bc713-bc73-44f3-ad91-d798aea11cc5,2010-08-13 06:09:24.0,196.0,"If your contingency tables are like a binomial effect size display (BESD), with clear YES/NO predictions being provided by each of your K methods you'll have a number of tables like this...\\n\\n<pre>\\n                 Reality\\n	         +       -	\\nPred  +     70      30    100\\nPred  -     30	  70	100\\n            100	100	   200\\n</pre>\\n\\nI believe you can find the difference between the success rates, e.g. 70/100 – 30/100 = 40/100 = .40, this value can be considered as being equivalent to an effect size r for each of your K methods.\\n",,
3753,5,1636,83f0d423-919d-49bb-ab2f-a9c22b374e6a,2010-08-13 06:24:50.0,196.0,"I may be a little unclear about the question.  But here would be my solution computing some ""statistic on each of the tables"" and comparing those values.\\n\\nIf your contingency tables are like a binomial effect size display (BESD), with clear YES/NO predictions being provided by each of your K methods you'll have a number of tables like this...\\n\\n<pre>\\n                 Reality\\n             +    -	\\nPred  +     70   30 100\\nPred  -     30   70 100\\n            100 100 200\\n</pre>\\n\\nI believe you can find the difference between the success rates, e.g. 70/100 – 30/100 = 40/100 = .40, this value can be considered as being equivalent to an effect size r for each of your K methods.  As a proof of concept I've included equivalent R code...\\n\\n    x <- rep(c(1,0),each=100)\\n    y <- c(rep(1,70),rep(0,30),rep(1,30),rep(0,70))\\n    cor(x,y)\\n\\nYou can then compare them using Fisher's Z' transformation for r in the standard way, e.g. [here][1].  To deal with situations where K is greater than 2 one may want to apply some familywise error correction to the Z' tests, but the exact one selected I leave open for another debate.  P.S. I might be remembering incorrectly, but I think you can find more details in Essentials of Behavioral Research: Methods and Data Analysis by Rosenthal & Rosnow, 2007, Ch 11\\n\\n  [1]: http://www.fon.hum.uva.nl/Service/Statistics/Two_Correlations.html",added reference,
3754,5,1570,d5f1a4be-87a9-4db6-b88e-b0be395f1136,2010-08-13 06:32:21.0,159.0,"I assume A and B are both random variables taking discrete values and you are thinking of a chi-squared test on the two-way frequency table formed by the counts of observations on the two variables.\\n\\nIn that case, a significant result indicates both directions of dependence: A|B and B|A.\\n\\nIf you think about Bayes' theorem, it is clear that one always implies the other:\\n\\nP(A|B) = P(B|A) P(A) / P(B)\\n\\nSo P(A|B) = P(A) if and only if P(B|A)=P(B).",added 14 characters in body,
3755,5,1593,b035db0c-8e99-4962-9a99-baa1fd0fda12,2010-08-13 06:44:05.0,521.0,"Another approach would be:\\n\\n    P(A| B, C, D) = P(A, B, C, D)/P(B, C, D)\\n                  = P(B| A, C, D).P(A, C, D)/P(B, C, D)\\n                  = P(B| A, C, D).P(C| A, D).P(A, D)/{P(C| B, D).P(B, D)}\\n                  = P(B| A, C, D).P(C| A, D).P(D| A).P(A)/{P(C| B, D).P(D| B).P(B)}\\n\\nNote the similarity to:\\n\\n          P(A| B) = P(A, B)/P(B)\\n                  = P(B| A).P(A)/P(B)\\n\\nAnd there are many equivalent forms.\\n\\nTaking U = (B, C, D) gives:\\n    P(A| B, C, D) = P(A, U)/P(U)\\n\\n    P(A| B, C, D) = P(A, U)/P(U)\\n                  = P(U| A).P(A)/P(U)\\n                  = P(B, C, D| A).P(A)/P(B, C, D)\\n\\nI'm sure they're equivalent, but do you want the joint probability of B, C & D given A?\\n                  ",added 102 characters in body,
3756,5,1456,bbc95ad2-bcd0-4b51-a160-36aa04785777,2010-08-13 06:44:20.0,251.0,"The log odds ratio has a Normal asymptotic distribution :\\n\\n$\\log(\\hat{OR}) \\sim N(\\log(OR), \\sigma_{\\log(OR)}^2)$\\n\\nwith $\\sigma$ estimated from the contingency table.  See, for example, page 6 of the notes:\\n\\n- [Asymptotic Theory for Parametric Models][1]\\n\\n  [1]: http://faculty.business.utsa.edu/rtripath/7853/Chapter14/Lecture14_2.pdf\\n  \\n\\n\\n",added 6 characters in body,
3758,5,1631,35aac3f2-f04c-43e0-8508-a0cbeb53edd4,2010-08-13 08:22:26.0,183.0,"A z-table gives you values of the cumulative distribution function for the standard (i.e., mean = 0, standard deviation = 1) normal distribution. The integral needs to be estimated numerically. See [here for a discussion][1].\\n\\nOf course, to answer your question literally, you could use some other lookup system other than a table, such as the `pnorm` [function][2] in R.\\nExample:\\n\\n    > pnorm(2)\\n    [1] 0.9772499\\n\\nHelp for `pnorm` also provides the following reference on how to calculate the cdf of a normal distribution:\\n\\n* Cody, W. D. (1993) Algorithm 715: SPECFUN – A portable FORTRAN package of special function routines and test drivers. ACM Transactions on Mathematical Software 19, 22–32.\\n\\n  [1]: http://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function\\n  [2]: http://stat.ethz.ch/R-manual/R-devel/library/stats/html/Normal.html",added 13 characters in body,
3759,6,1624,6044dfe8-2c07-4b61-83b2-e9dd6e0d4106,2010-08-13 08:35:39.0,8.0,<confidence-interval><z-statistic>,edited tags,
3760,2,1637,773226cb-e919-421c-9f77-3c6acff186ab,2010-08-13 09:41:13.0,199.0,"I'm sure I've got this completely wrapped round my head, but I just can't figure it out.\\n\\nThe t-test compares two normal distributions using the Z distribution. That's why there's an assumption of normality in the DATA.\\n\\nANOVA is equivalent to linear regression with dummy variables, and uses sums of squares, just like OLS. That's why there's an assumption of normality of RESIDUALS.\\n\\nIt's taken me several years, but I think I've finally grasped those basic facts. So why is it that the t-test is equivalent to ANOVA with two groups? How can they be equivalent if they don't even assume the same things about the data?",,
3761,1,1637,773226cb-e919-421c-9f77-3c6acff186ab,2010-08-13 09:41:13.0,199.0,"If the t-test and the ANOVA for two groups are equivalent, why aren't their assumptions equivalent?",,
3762,3,1637,773226cb-e919-421c-9f77-3c6acff186ab,2010-08-13 09:41:13.0,199.0,<distributions><normality><anova><t-test>,,
3763,2,1638,b3d7cca1-6125-4da2-b4ab-0120a478a1b3,2010-08-13 09:50:51.0,199.0,"Hurrah, a question non-technical enough so as I can answer it!\\n\\n""Type one is a con"" [rhyming]- i.e. fools you into thinking that a difference exists when it doesn't. Always works for me.",,
3764,2,1639,37c361db-7a30-4e92-b905-80e946322455,2010-08-13 09:52:44.0,159.0,"The t-test with two groups assumes that each group is normally distributed with the same variance (although the means may differ under the alternative hypothesis). That is equivalent to a regression with a dummy variable as the regression allows the mean of each group to differ but not the variance. Hence the residuals (equal to the data with the group means subtracted) have the same distribution --- that is, they are normally distributed with zero mean.\\n\\nA t-test with unequal variances is not equivalent to a one-way ANOVA.",,
3765,6,1637,7099acac-4bb0-4a14-b816-2971beba2cfb,2010-08-13 10:00:01.0,8.0,<distributions><regression><normality><t-test><anova>,edited tags,
3766,6,1084,9ed51534-92bf-48fd-9f97-9d10db5ab17a,2010-08-13 11:29:34.0,159.0,<data-visualization><smoothing>,edited tags,
3770,2,1641,89c356e5-2202-4ed0-bb6c-c7c026228b49,2010-08-13 11:42:08.0,702.0,"Bayes' theorem is a way to rotate a conditional probability $P(A|B)$ to another conditional probability $P(B|A)$.\\n\\nA stumbling block for some is the meaning of $P(B|A)$.  This is a way to reduce the space of possible events by considering only those events where $A$ definitely happens (or is true).  So for instance the probability that a thrown, fair, dice lands showing six, $P(\\mbox{dice lands six})$, is 1/6, however the probability that a dice lands six given that it landed an even number, $P(\\mbox{dice lands six}|\\mbox{dice lands even})$, is 1/3.\\n\\nYou can derive Bayes' theorem yourself as follows.  Start with the ratio definition of a conditional probability:\\n\\n$P(B|A) = \\frac{P(AB)}{P(A)}$\\n\\nwhere $P(AB)$ is the joint probability of $A$ and $B$ and $P(A)$ is the marginal probability of $A$.\\n\\nCurrently the formula makes no reference to $P(A|B)$, so let's write down the definition of this too:\\n\\n$P(A|B) = \\frac{P(BA)}{P(B)}$\\n\\nThe little trick for making this work is seeing that $P(AB) = P(BA)$ (since a Boolean algebra is underneath all of this, you can easily prove this with a truth table by showing $AB = BA$), so we can write:\\n\\n$P(A|B) = \\frac{P(AB)}{P(B)}$\\n\\nNow to slot this into the formula for $P(B|A)$, just rewrite the formula above so $P(AB)$ is on the left:\\n\\n$P(AB) = P(A|B)P(B)$\\n\\nand hey presto:\\n\\n$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$\\n\\nAs for what the point is to rotating a conditional probability in this way, consider the common example of trying to infer the probability that someone has a disease given that they have a symptom, i.e., we *know* that they have a symptom - we can just see it - but we cannot be certain whether they have a disease and have to infer it.  I'll start with the formula and work back.\\n\\n$P(\\mbox{disease}|\\mbox{symptom}) = \\frac{P(\\mbox{symptom}|\\mbox{disease})P(\\mbox{disease})}{P(\\mbox{symptom})}$\\n\\nSo to work it out, you need to know the prior probability of the symptom, the prior probability of the disease (i.e., how common or rare are the symptom and disease) and also the probability that someone has a symptom given we know someone has a disease (e.g., via expensive time consuming lab tests).\\n\\nIt can get a lot more complicated than this, e.g., if you have multiple diseases and symptoms, but the idea is the same.  Even more generally, Bayes' theorem often makes an appearance if you have a probability theory of relationships between causes (e.g., diseases) and effects (e.g., symptoms) and you need to reason backwards (e.g., you see some symptoms from which you want to infer the underlying disease).",,
3771,5,492,f950b42f-b814-477a-9a3e-195dacc0dff1,2010-08-13 11:45:07.0,159.0,"I am proposing to try and find a trend in some very noisy long term data. The data is basically weekly measurements of something which moved about 5mm over a period of about 8 months. The data is to 1mm accuracey and is very noisy regularly changing +/-1 or 2mm in a week. We only have the data to the nearest mm. \\n\\nWe plan to use some basic signal processing with a fast fourier transform to separate out the noise from the raw data. The basic assumption is if we mirror our data set and add it to the end of our existing data set we can create a full wavelength of the data and therefore our data will show up in a fast fourier transform and we can hopefully then separate it out. \\n\\nGiven that this sounds a little dubious to me, is this a method worth purusing or is the method of mirroring and appending our data set somehow fundamentally flawed? We are looking at other approaches such as using a low pass filter as well. ",Fixed typos,
3772,6,168,76705f57-94d0-4980-9631-92e06eded9d3,2010-08-13 11:46:06.0,159.0,<smoothing><kde><kernel>,edited tags; edited title,
3773,4,168,76705f57-94d0-4980-9631-92e06eded9d3,2010-08-13 11:46:06.0,159.0,Choosing a bandwidth for kernel density estimators,edited tags; edited title,
3774,2,1642,8f35fbc0-3851-44f6-ab59-bd0ac34c3243,2010-08-13 12:22:25.0,702.0,"You could reject the idea entirely.\\n\\nSome authors (Andrew Gelman is one) are shifting to discussing Type S (sign) and Type M (magnitude) errors.  You can infer the wrong effect direction (e.g., you believe the treatment group does better but actually does worse) or the wrong magnitude (e.g., you find a massive effect where there is only a tiny, or essentially no effect, or vice versa).\\n\\nSee more at [Gelman's blog][1].\\n\\n\\n  [1]: http://www.stat.columbia.edu/~cook/movabletype/archives/2004/12/type_1_type_2_t.html",,
3775,2,1643,85c6102d-60d6-4d6f-ae9c-22b53f714e9e,2010-08-13 12:24:11.0,442.0,"I totally agree with Rob's answer, but let me put it another way (using wikipedia):\\n\\n[Assumptions ANOVA][1]:\\n\\n - Independence of cases – this is an assumption of the model that simplifies the statistical analysis.\\n - Normality – the distributions of the residuals are normal.\\n - Equality (or ""homogeneity"") of variances, called homoscedasticity \\n\\n\\n[Assumptions t-test][2]:\\n\\n - Each of the two populations being compared should follow a normal distribution ...\\n - ... the two populations being compared should have the same variance ...\\n - The data used to carry out the test should be sampled independently from the two populations being compared.\\n\\nHence, I would refute the question, as they obviously have the same assumptions (although in a different order :-) ).\\n\\n  [1]: http://en.wikipedia.org/wiki/Analysis_of_variance#Assumptions_of_ANOVA\\n  [2]: http://en.wikipedia.org/wiki/Student%27s_t-test#Assumptions",,
3776,2,1644,5179917c-8117-4073-881e-35f5439a47d0,2010-08-13 12:31:09.0,887.0,"I don't think it matters very much, as long as the interpretation of the results is performed within the same framework as the analysis.  The main problem with frequentist statistics is that there is a natural tendency to treat the p-value of a frequentist significance test as if it was a Bayesian a-posteriori probability that the null hypothesis is true (and hence 1-p is the probability that the alternative hypothesis is true), or treating a frequentist confidence interval as a Bayesian credible interval (and hence assuming there is a 95% probability that the true value lies within a 95% confidence interval for the particular sample of data we have).  These sorts of interpretation are natural as it would be the direct answer to the question we would naturally want to ask.  It is a trade-off between whether the subjective element of the Bayesian approach (which is itself debatable, see e.g. Jaynes book) is sufficiently abhorrent that it is worth making do with an indirect answer to the key question (and vice versa).\\n\\nAs long as the form of the answer is acceptable, and we can agree on the assumptions made, then there is no reason to prefer one over the other - it is a matter of horses for courses.\\n\\nI'm still a Bayesian though ;o)",,
3777,16,1644,5179917c-8117-4073-881e-35f5439a47d0,2010-08-13 12:31:09.0,-1.0,,,
3778,2,1645,3d699c27-6823-4e31-8258-9777fc59b841,2010-08-13 12:42:30.0,1356.0,"Title says enough... so far, I've been using Shapiro-Wilk's statistic in order to test normality assumptions in small samples. Could you, please, recommend another technique?",,
3779,1,1645,3d699c27-6823-4e31-8258-9777fc59b841,2010-08-13 12:42:30.0,1356.0,Appropriate normality tests for small samples,,
3780,3,1645,3d699c27-6823-4e31-8258-9777fc59b841,2010-08-13 12:42:30.0,1356.0,<normality><hypothesis-testing><small-sample>,,
3781,2,1646,ca8eb797-a322-4225-8fa9-e4137a3596a6,2010-08-13 12:44:23.0,183.0,"Imagine that:\\n\\n- You have a sample of 1000 teams each with 10 members.\\n- You measured team functioning by asking each team member how well they think their team is functioning using a reliable multi-item numeric scale.\\n- You want to describe the extent to which the measure of team effectiveness is a property of the team member's idiosyncratic belief or a property of a shared belief about the team.\\n\\nIn this and related situations (e.g., aggregating to organisations), many researchers report the intraclass correlation.\\nThus, my questions are:\\n\\n 1. What descriptive labels would you attach to different values of the intra-class correlation? I.e., the aim is to actually relate the values of the intra-class correlation to qualitative language such as: ""When the intraclass correlation is greater than x, it suggests that the attitudes are modestly/moderately/strongly shared across team members.""\\n 2. Do you think the intraclass correlation is the appropriate statistic or would you use a different strategy?\\n\\n",,
3782,1,1646,ca8eb797-a322-4225-8fa9-e4137a3596a6,2010-08-13 12:44:23.0,183.0,Intraclass correlation and aggregation,,
3783,3,1646,ca8eb797-a322-4225-8fa9-e4137a3596a6,2010-08-13 12:44:23.0,183.0,<correlation><intraclass-correlation><aggregation><interpretation><effect-size>,,
3784,2,1647,d64502fe-122a-4bc7-b2b3-7147fbdba04d,2010-08-13 12:47:16.0,159.0,"There is a whole [Wikipedia category on normality tests][1] including:\\n\\n  - the [Anderson-Darling test][2], popular amongst statisticians; and\\n  - the [Jarque-Bera test][3], popular amongst econometricians.\\n\\nI think A-D is probably the best of them.\\n\\n  [1]: http://en.wikipedia.org/wiki/Category:Normality_tests\\n  [2]: http://en.wikipedia.org/wiki/Anderson%E2%80%93Darling_test\\n  [3]: http://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test",,
3785,5,1646,36be8b17-532f-4615-a551-ce0bcb83a13e,2010-08-13 12:50:59.0,183.0,"Imagine that:\\n\\n- You have a sample of 1000 teams each with 10 members.\\n- You measured team functioning by asking each team member how well they think their team is functioning using a reliable multi-item numeric scale.\\n- You want to describe the extent to which the measure of team effectiveness is a property of the team member's idiosyncratic belief or a property of a shared belief about the team.\\n\\nIn this and related situations (e.g., aggregating to organisations), many researchers report the intraclass correlation (e.g., Table 1 in [Campion &  Medsker, 1993][1]).\\nThus, my questions are:\\n\\n 1. What descriptive labels would you attach to different values of the intra-class correlation? I.e., the aim is to actually relate the values of the intra-class correlation to qualitative language such as: ""When the intraclass correlation is greater than x, it suggests that the attitudes are modestly/moderately/strongly shared across team members.""\\n 2. Do you think the intraclass correlation is the appropriate statistic or would you use a different strategy?\\n\\n\\n  [1]: http://www.krannert.purdue.edu/faculty/campionm/Relations_Between_Work.pdf",added 132 characters in body,
3786,5,175,fb5861a5-6970-4a8d-b0c4-42c622386802,2010-08-13 12:59:06.0,159.0,"Often times a statistical analyst is handed a set dataset and asked to fit a model using a technique such as linear regression.  Very frequently the dataset is accompanied with a disclaimer similar to ""Oh yeah, we messed up collecting some of these data points -- do what you can"".\\n\\nThis situation leads to regression fits that are heavily impacted by the presence of outliers that may be erroneous data. Given the following:\\n\\n  - It is dangerous from both a scientific and moral standpoint to throw out data for no reason other than it ""makes the fit look bad"".\\n\\n  - In real life, the people who collected the data are frequently not available to answer questions such as ""when generating this data set, which of the points did you mess up, exactly?""\\n\\nWhat statistical tests or rules of thumb can be used as a basis for excluding outliers in linear regression analysis?\\n\\nAre there any special considerations for multilinear regression?",Fixed a typo,
3787,6,175,fb5861a5-6970-4a8d-b0c4-42c622386802,2010-08-13 12:59:06.0,159.0,<regression><outliers>,Fixed a typo,
3788,5,1442,e86651dc-df43-4742-bac4-7da58ade20d0,2010-08-13 13:01:56.0,159.0,"To assess the historical trend, I'd use a gam with trend and seasonal components. For example\\n\\n    require(mgcv)\\n    require(forecast)\\n    x <- ts(rpois(100,1+sin(seq(0,3*pi,l=100))),f=12)\\n    tt <- 1:100\\n    season <- seasonaldummy(x)\\n    fit <- gam(x ~ s(tt,k=5) + season, family=""poisson"")\\n    plot(fit)\\n\\nThen `summary(fit)` will give you a test of significance of the change in trend and the plot will give you some confidence intervals. The assumptions here are that the observations are independent and the conditional distribution is Poisson. Because the mean is allowed to change smoothly over time, these are not particularly strong assumptions.\\n\\nTo forecast is more difficult as you need to project the trend into the future. If you are willing to accept a linear extrapolation of the trend at the end of the data (which is certainly dodgy but probably ok for a few months), then use\\n\\n    fcast <- predict(fit,se.fit=TRUE,\\n                   newdata=list(tt=101:112,season=seasonaldummyf(x,h=12)))\\n\\nTo see the forecasts on the same graph:\\n\\n    plot(x,xlim=c(0,10.5))\\n    lines(ts(exp(fcast$fit),f=12,s=112/12),col=2)\\n    lines(ts(exp(fcast$fit-2*fcast$se),f=12,s=112/12),col=2,lty=2)\\n    lines(ts(exp(fcast$fit+2*fcast$se),f=12,s=112/12),col=2,lty=2)\\n\\nYou can spot the unusual months by looking for outliers in the (deviance) residuals of the fit.\\n",added 97 characters in body,
3789,4,1459,e132d71f-7cc0-48b0-b3fc-3aa260b79b32,2010-08-13 13:04:38.0,159.0,How to identify transfer functions in a time series regression forecasting model?,Made title clearer and added a tag; edited title,
3790,6,1459,e132d71f-7cc0-48b0-b3fc-3aa260b79b32,2010-08-13 13:04:38.0,159.0,<time-series><forecasting><dynamic-regression>,Made title clearer and added a tag; edited title,
3791,2,1648,a2c6049c-456d-46dd-8e2d-545a8dc7bfeb,2010-08-13 13:32:27.0,5.0,"The [**fBasics**][1] package in R (part of [Rmetrics][2]) includes [several normality tests][3], covering many of the popular [frequentist tests][4] -- Kolmogorov-Smirnov, Shapiro-Wilk's, Jarque–Bera, and D'Agostino -- along with a wrapper for the normality tests in the [**nortest**][5] package -- Anderson–Darling, Cramer–von Mises, Lilliefors (Kolmogorov-Smirnov), Pearson chi–square, and Shapiro–Francia.  The package documentation also provides all the important references.  Here is a demo that shows how to use the [tests from nortest][6].\\n\\nOne approach, if you have the time, is to use more than one test and check for agreement.  I mostly use the Jarque-Bera test.  The tests vary in a number of ways, so it isn't entirely straightforward to choose ""the best"".  What do other researchers in your field use?  This can vary and it may be best to stick with the accepted methods so that others will accept your work.  You can look at [""Comparison of Tests for Univariate Normality""][7] (Seier 2002) and [""A comparison of various tests of normality""][8] (Yazici; Yolacan 2007) for a comparison and discussion of the issues.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/fBasics/index.html\\n  [2]: https://www.rmetrics.org/\\n  [3]: http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fBasics/html/NormalityTests.html\\n  [4]: http://en.wikipedia.org/wiki/Normality_test#Frequentist_tests\\n  [5]: http://cran.r-project.org/web/packages/nortest/index.html\\n  [6]: http://duncanjg.files.wordpress.com/2008/12/ksdemo3.pdf\\n  [7]: http://interstat.statjournals.net/YEAR/2002/articles/0201001.pdf\\n  [8]: http://www.informaworld.com/smpp/content~db=all~content=a759350109",,
3792,6,805,2cfb610a-96cb-46d5-84e0-4eb7680d21ff,2010-08-13 13:35:40.0,8.0,<error><standard-error>,edited tags,
3793,6,452,389ece7a-0dca-42de-9a9f-d6d9a819731c,2010-08-13 13:35:57.0,8.0,<regression><error><standard-error>,Retaged and reformated the list,
3794,5,452,389ece7a-0dca-42de-9a9f-d6d9a819731c,2010-08-13 13:35:57.0,8.0,It has been suggested by Angrist and Pischke that Robust (i.e. robust to heteroskedasticity or unequal variances) Standard Errors are reported as a matter of course rather than testing for it. Two questions:\\n\\n1. What is impact on the standard errors of doing so when there is homoskedasticity?\\n1. Does anybody actually do this in their work?,Retaged and reformated the list,
3795,5,1648,21052879-ac42-4e98-b801-4dd7b09aa00a,2010-08-13 13:38:15.0,5.0,"The [**fBasics**][1] package in R (part of [Rmetrics][2]) includes [several normality tests][3], covering many of the popular [frequentist tests][4] -- Kolmogorov-Smirnov, Shapiro-Wilk's, Jarque–Bera, and D'Agostino -- along with a wrapper for the normality tests in the [**nortest**][5] package -- Anderson–Darling, Cramer–von Mises, Lilliefors (Kolmogorov-Smirnov), Pearson chi–square, and Shapiro–Francia.  The package documentation also provides all the important references.  Here is a demo that shows how to use the [tests from nortest][6].\\n\\nOne approach, if you have the time, is to use more than one test and check for agreement.  The tests vary in a number of ways, so it isn't entirely straightforward to choose ""the best"".  What do other researchers in your field use?  This can vary and it may be best to stick with the accepted methods so that others will accept your work.  I mostly use the Jarque-Bera test, partly for that reason.\\n\\nYou can look at [""Comparison of Tests for Univariate Normality""][7] (Seier 2002) and [""A comparison of various tests of normality""][8] (Yazici; Yolacan 2007) for a comparison and discussion of the issues.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/fBasics/index.html\\n  [2]: https://www.rmetrics.org/\\n  [3]: http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fBasics/html/NormalityTests.html\\n  [4]: http://en.wikipedia.org/wiki/Normality_test#Frequentist_tests\\n  [5]: http://cran.r-project.org/web/packages/nortest/index.html\\n  [6]: http://duncanjg.files.wordpress.com/2008/12/ksdemo3.pdf\\n  [7]: http://interstat.statjournals.net/YEAR/2002/articles/0201001.pdf\\n  [8]: http://www.informaworld.com/smpp/content~db=all~content=a759350109",added 26 characters in body,
3796,6,1580,72c5ad31-97d5-4d14-a7e1-90bd66f836e6,2010-08-13 13:40:33.0,8.0,<regression><predictor>,edited tags,
3797,2,1649,c2536d11-faa6-4fcc-a494-33614b1fd4cf,2010-08-13 13:58:38.0,364.0,"Say I observe two groups of 10 people, measuring some quantity 100 times in each person. There will presumably be some variability across these 100 measures in each person. Can I use mixed effects analysis to assess whether this within-person variability is, on average, different between the two groups? For example, using traditional statistics, I could compute the standard deviation (SD) within each person then submit these SDs to an anova comparing the groups, but I wonder if this two-stage process can be replaced by a single mixed effects model, consequently obtaining the various advantages of mixed effects modelling (shrinkage, accounting for different numbers of observations per person, etc) as well.\\n\\nTo be clear, here is R code depicting the scenario described above and the SD/anova-based approach:\\n\\n\\n	set.seed(1)\\n\\n	group_A_base_sd = 1\\n	group_B_base_sd = 2\\n	within_group_sd_of_sds = .1\\n\\n	n_per_group = 10\\n	obs_per_id = 100\\n\\n	temp = data.frame(\\n		id = 1:(n_per_group*2)\\n		, group = rep(c('A','B'))\\n	)\\n\\n	#generate example data\\n	library(plyr) #to avoid loops (for coding convenience only)\\n	obs_data = ddply(\\n		.data = temp\\n		, .variables = .(id,group)\\n		, .fun = function(x){\\n			 #generate a unique sd for this individual\\n			 # based on their group's sd plus some\\n			 # within-group variability\\n			 id_sd = ifelse(\\n				  x$group=='A'\\n				  , rnorm(\\n					   1\\n					   , group_A_base_sd\\n					   , within_group_sd_of_sds\\n				  )\\n				  , rnorm(\\n					   1\\n					   , group_B_base_sd\\n					   , within_group_sd_of_sds\\n				  )\\n			 )\\n			 #generate data points with the above generated\\n			 # variability\\n			 to_return = data.frame(\\n				  obs_num = 1:obs_per_id\\n				  , measurement = rnorm(obs_per_id,0,id_sd)\\n			 )\\n			 return(to_return)\\n		}\\n	)\\n\\n	#first step of an anova-based approach:\\n	# compute SDs within each Ss\\n	obs_sds = ddply(\\n		.data = obs_data\\n		, .variables = .(id,group)\\n		, .fun = function(x){\\n			 to_return = data.frame(\\n				  obs_sd = sd(x$measurement)\\n			 )\\n		}\\n	)\\n\\n	#second step of an anova-based approach:\\n	# compute the anova on the SDs\\n	summary(\\n		aov(\\n			 formula = obs_sd~group\\n			 , data = obs_sds\\n		)\\n	)\\n\\n\\n",,
3798,1,1649,c2536d11-faa6-4fcc-a494-33614b1fd4cf,2010-08-13 13:58:38.0,364.0,Using mixed effects modelling to estimate and compare variability,,
3799,3,1649,c2536d11-faa6-4fcc-a494-33614b1fd4cf,2010-08-13 13:58:38.0,364.0,<variance><mixed-model>,,
3800,5,1648,3890e1a0-376b-4a27-819f-687590bf5bf4,2010-08-13 14:16:43.0,5.0,"The [**fBasics**][1] package in R (part of [Rmetrics][2]) includes [several normality tests][3], covering many of the popular [frequentist tests][4] -- Kolmogorov-Smirnov, Shapiro-Wilk's, Jarque–Bera, and D'Agostino -- along with a wrapper for the normality tests in the [**nortest**][5] package -- Anderson–Darling, Cramer–von Mises, Lilliefors (Kolmogorov-Smirnov), Pearson chi–square, and Shapiro–Francia.  The package documentation also provides all the important references.  Here is a demo that shows how to use the [tests from nortest][6].\\n\\nOne approach, if you have the time, is to use more than one test and check for agreement.  The tests vary in a number of ways, so it isn't entirely straightforward to choose ""the best"".  What do other researchers in your field use?  This can vary and it may be best to stick with the accepted methods so that others will accept your work.  I mostly use the Jarque-Bera test, partly for that reason.\\n\\nYou can look at [""Comparison of Tests for Univariate Normality""][7] (Seier 2002) and [""A comparison of various tests of normality""][8] (Yazici; Yolacan 2007) for a comparison and discussion of the issues.\\n\\nIt's also trivial to test these methods for comparison in R, thanks to all the [distribution functions][9].  Here's an example with simulated data (I won't print out the results to save space):\\n\\n    library(fBasics); library(ggplot2)\\n    set.seed(1)\\n\\n    # normal distribution\\n    x1 <- rnorm(1e+06)   \\n    x1.samp <- sample(x1, 200)\\n    qplot(x1.samp, geom=""histogram"")\\n    jbTest(x1.samp)\\n    adTest(x1.samp)\\n\\n    # cauchy distribution\\n    x2 <- rcauchy(1e+06)\\n    x2.samp <- sample(x2, 200)\\n    qplot(x2.samp, geom=""histogram"")\\n    jbTest(x2.samp)\\n    adTest(x2.samp)\\n\\nOnce you have the results from the various tests over different distributions, you can compare which were the most effective.  For instance, the p-value for the Jarque-Bera test above returned 0.276 for the normal distribution (accepting) and < 2.2e-16 for the cauchy (rejecting the null hypothesis).\\n\\n  [1]: http://cran.r-project.org/web/packages/fBasics/index.html\\n  [2]: https://www.rmetrics.org/\\n  [3]: http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fBasics/html/NormalityTests.html\\n  [4]: http://en.wikipedia.org/wiki/Normality_test#Frequentist_tests\\n  [5]: http://cran.r-project.org/web/packages/nortest/index.html\\n  [6]: http://duncanjg.files.wordpress.com/2008/12/ksdemo3.pdf\\n  [7]: http://interstat.statjournals.net/YEAR/2002/articles/0201001.pdf\\n  [8]: http://www.informaworld.com/smpp/content~db=all~content=a759350109\\n  [9]: http://cran.r-project.org/doc/manuals/R-intro.html#Probability-distributions",added 803 characters in body; added 175 characters in body,
3801,2,1650,a9bd1275-f528-4e0a-8b7e-7f6dcccfc846,2010-08-13 14:17:15.0,,"You can structure the model along the following lines. Let,\\n\\n$j = 1, 2$ be the two groups and\\n\\n$i$ index the individuals in the two groups. \\n\\nThen your model is:\\n\\n$y_{ij} \\sim N(\\mu_j,\\sigma_j^2)$  $\\forall \\ i, j$ \\n\\n$\\sigma_j^2 \\sim IG(v,1)$  $\\forall \\ j$\\n\\n(Note: $IG(.)$ is the inverse gamma distribution.)\\n\\n**Priors**\\n\\n$\\mu_j \\sim N(\\bar{\\mu},\\sigma_{\\mu}^2)$\\n\\n$v \\sim IG(\\bar{v},1)$ is the prior for $v$.\\n\\n\\nThe above structure will let you shrink the error variances ($\\sigma_j^2$) appropriately. You can then evaluate whether the within group variability is different by looking at the credible intervals associated with the group variabilities.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Wishart_distribution",,user28
3802,5,1648,1084f41e-2645-4a09-b8ac-78c9b173453c,2010-08-13 14:22:09.0,5.0,"The [**fBasics**][1] package in R (part of [Rmetrics][2]) includes [several normality tests][3], covering many of the popular [frequentist tests][4] -- Kolmogorov-Smirnov, Shapiro-Wilk's, Jarque–Bera, and D'Agostino -- along with a wrapper for the normality tests in the [**nortest**][5] package -- Anderson–Darling, Cramer–von Mises, Lilliefors (Kolmogorov-Smirnov), Pearson chi–square, and Shapiro–Francia.  The package documentation also provides all the important references.  Here is a demo that shows how to use the [tests from nortest][6].\\n\\nOne approach, if you have the time, is to use more than one test and check for agreement.  The tests vary in a number of ways, so it isn't entirely straightforward to choose ""the best"".  What do other researchers in your field use?  This can vary and it may be best to stick with the accepted methods so that others will accept your work.  I mostly use the Jarque-Bera test, partly for that reason.\\n\\nYou can look at [""Comparison of Tests for Univariate Normality""][7] (Seier 2002) and [""A comparison of various tests of normality""][8] (Yazici; Yolacan 2007) for a comparison and discussion of the issues.\\n\\nIt's also trivial to test these methods for comparison in R, thanks to all the [distribution functions][9].  Here's a simple example with simulated data (I won't print out the results to save space), although a more full exposition would be required:\\n\\n    library(fBasics); library(ggplot2)\\n    set.seed(1)\\n\\n    # normal distribution\\n    x1 <- rnorm(1e+06)   \\n    x1.samp <- sample(x1, 200)\\n    qplot(x1.samp, geom=""histogram"")\\n    jbTest(x1.samp)\\n    adTest(x1.samp)\\n\\n    # cauchy distribution\\n    x2 <- rcauchy(1e+06)\\n    x2.samp <- sample(x2, 200)\\n    qplot(x2.samp, geom=""histogram"")\\n    jbTest(x2.samp)\\n    adTest(x2.samp)\\n\\nOnce you have the results from the various tests over different distributions, you can compare which were the most effective.  For instance, the p-value for the Jarque-Bera test above returned 0.276 for the normal distribution (accepting) and < 2.2e-16 for the cauchy (rejecting the null hypothesis).\\n\\n  [1]: http://cran.r-project.org/web/packages/fBasics/index.html\\n  [2]: https://www.rmetrics.org/\\n  [3]: http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fBasics/html/NormalityTests.html\\n  [4]: http://en.wikipedia.org/wiki/Normality_test#Frequentist_tests\\n  [5]: http://cran.r-project.org/web/packages/nortest/index.html\\n  [6]: http://duncanjg.files.wordpress.com/2008/12/ksdemo3.pdf\\n  [7]: http://interstat.statjournals.net/YEAR/2002/articles/0201001.pdf\\n  [8]: http://www.informaworld.com/smpp/content~db=all~content=a759350109\\n  [9]: http://cran.r-project.org/doc/manuals/R-intro.html#Probability-distributions",added 57 characters in body,
3803,2,1651,05c7130d-577f-4a3a-8acc-a7f753cc35d2,2010-08-13 14:28:02.0,," 0  down vote  favorite\\n1\\n \\n\\nI need to fit Y_ij ~ NegBin(m_ij,k), hence a negative binomial distribution to a count. However, the data I have observed are censored, I know the value of y_ij, but it could be more than that value. Writting down the loglikelihood going with this problem is:\\n\\nll = \\sum_{i=1}^n w_i (c_i log(P(Y_ij=y_ij|X_ij)) + (1- c_i) log(1- \\sum_{k=1}^32 P(Y_ij = k|X_ij)))\\n\\nWhere X_ij represent the design matrix (with the covariates of interest), w_i is the weight for each observation, y_ij is the response variable and P(Y_ij=y_ij|Xij) is the negative binomial distribution where the m_ij=exp(X_ij \\beta) and \\alpha is the overdispersion parameter.\\n\\nDoes someone knows if there exist a build-in code in R that could be used to obtain this?\\n",,Kim
3804,1,1651,05c7130d-577f-4a3a-8acc-a7f753cc35d2,2010-08-13 14:28:02.0,,How to fit a negative binomial distribution in R while incorporating censoring,,Kim
3805,3,1651,05c7130d-577f-4a3a-8acc-a7f753cc35d2,2010-08-13 14:28:02.0,,<r>,,Kim
3806,5,1651,cf26fdee-7cc6-437f-ae4b-520bfcb7fd80,2010-08-13 14:31:33.0,," 0  down vote  favorite\\n1\\n \\n\\nI need to fit $Y_{ij} \\sim NegBin(m_{ij},k)$, hence a negative binomial distribution to a count. However, the data I have observed are censored, I know the value of $y_{ij}$, but it could be more than that value. Writting down the loglikelihood going with this problem is:\\n\\n$ll = \\sum_{i=1}^n w_i (c_i log(P(Y_{ij}=y_{ij}|X_{ij})) + (1- c_i) log(1- \\sum_{k=1}^32 P(Y_ij = k|X_{ij})))$\\n\\nWhere $X_{ij}$ represent the design matrix (with the covariates of interest), $w_i$ is the weight for each observation, $y_{ij}$ is the response variable and $P(Y_{ij}=y_{ij}|X_{ij})$ is the negative binomial distribution where the $m_{ij}=exp(X_{ij} \\beta)$ and $\\alpha$ is the overdispersion parameter.\\n\\nDoes someone knows if there exist a build-in code in R that could be used to obtain this?\\n",fixed eqns and tags,user28
3807,6,1651,cf26fdee-7cc6-437f-ae4b-520bfcb7fd80,2010-08-13 14:31:33.0,,<r><censoring><negative-binomial>,fixed eqns and tags,user28
3808,5,1651,57ac04cd-c2ec-4a16-90ce-793b56088c84,2010-08-13 14:35:18.0,," 0  down vote  favorite\\n1\\n \\n\\nI need to fit $Y_{ij} \\sim NegBin(m_{ij},k)$, hence a negative binomial distribution to a count. However, the data I have observed are censored, I know the value of $y_{ij}$, but it could be more than that value. Writting down the loglikelihood going with this problem is:\\n\\n$ll = \\sum_{i=1}^n w_i (c_i log(P(Y_{ij}=y_{ij}|X_{ij})) + (1- c_i) log(1- \\sum_{k=1}^32 P(Y_{ij} = k|X_{ij})))$\\n\\nWhere $X_{ij}$ represent the design matrix (with the covariates of interest), $w_i$ is the weight for each observation, $y_{ij}$ is the response variable and $P(Y_{ij}=y_{ij}|X_{ij})$ is the negative binomial distribution where the $m_{ij}=exp(X_{ij} \\beta)$ and $\\alpha$ is the overdispersion parameter.\\n\\nDoes someone knows if there exist a build-in code in R that could be used to obtain this?\\n",added 2 characters in body,Kim
3809,5,1621,3d3a691d-dda0-4f8b-a692-c11f746fa13c,2010-08-13 14:58:33.0,511.0,"I came across a <a href=""http://www.citeulike.org/user/yaroslavvb/tag/information-geometry"">large</a> body of literature which advocates using Fisher's Information metric as a natural local metric in the space of probability distributions and then integrating over it to define distances and volumes.\\n\\nBut are these ""integrated"" quantities actually useful for anything? I found no theoretical justifications and very few practical applications. One is Guy Lebanon's <a href=""http://www.cs.cmu.edu/~lafferty/pub/hyperplane.pdf""> work</a> where he uses ""Fisher's distance"" to classify documents and another one is Rodriguez' <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.9259&rep=rep1&type=pdf"">ABC of Model Selection…</a> where ""Fisher's volume"" is used for model selection. \\n\\nA theoretical justification might be to have a generalization bound which uses this measure of distance or volume and is better than bounds derived from MDL or asymptotic arguments, or a method relying on one of these quantities that's provably better in some reasonably practical situation, are there any results of this kind?",deleted 9 characters in body,
3810,2,1652,58ca34b9-8f86-4b96-8ee9-b2a83d0f2abc,2010-08-13 15:21:55.0,702.0,"Correlation alone never implies causation.  It's that simple.\\n\\nBut it's very rare to have only a correlation between two variables.  Often you also know something about what those variables are and a theory, or theories, suggesting why there might be a causal relationship between the variables.  If not, then we bother checking for a correlation?  (However people mining massive correlation matrices for significant results often have no casual theory - otherwise, why bother mining.  A counterargument to that is that often some exploration is needed to get ideas for casual theories. And so on and so on...)\\n\\nA response to the common criticism ""Yeah, but that's just a correlation: it doesn't imply causation"":\\n\\n 1. For a casual relationship, correlation is necessary.  A repeated failure to find a correlation would be bad news indeed.\\n 2. I didn't just give you a correlation.\\n 3. Then go on to explain possible causal mechanisms explaining the correlation...\\n",,
3811,2,1653,497a3de7-ae51-4289-98d7-cd4c461acf69,2010-08-13 15:24:20.0,485.0,"The t-test simply a special case of the F-test where only two groups are being compared.  The result of either will be exactly the same in terms of the p-value and there is a simple relationship between the F and t statistics as well.  F = t^2.  The two tests are algebraically equivalent and their assumptions are the same.\\n\\nIn fact, these equivalences extend to the whole class of ANOVAs, t-tests, and linear regression models.  The t-test is a special case of ANOVA.  ANOVA is a special case of regression.  All of these procedures are subsumed under the General Linear Model and share the same assumptions.\\n\\n1. Independence of observations.\\n2. Normality of residuals = normality in each group in the special case.\\n3. Equal of variances of resisuals = equal variances across groups in the special case.\\n \\nYou might think of it as normality in the data, but you are checking for normality in each group--which is actually the same as checking for normality in the residuals when the only predictor in the model is the group.  Likewise with equal variances.",,
3812,2,1654,f7621bc6-e943-45c0-ac63-64e0b03ccf37,2010-08-13 15:31:27.0,,"You may try to model your data using a Dynamic Generalized Linear Model (DGLM). In R, you can fit this kind of models using packages sspir and KFAS. In a sense, this is similar to the gam approach suggested by Rob, except that instead of assuming that the log mean of the Poisson observations be a smooth function of time, it assumes that it follows a stochastic dynamics.",,Giovanni Petris
3813,5,1653,ffba9f53-1f39-4d1d-9700-a4932ccb140d,2010-08-13 15:33:16.0,485.0,"The t-test simply a special case of the F-test where only two groups are being compared.  The result of either will be exactly the same in terms of the p-value and there is a simple relationship between the F and t statistics as well.  F = t^2.  The two tests are algebraically equivalent and their assumptions are the same.\\n\\nIn fact, these equivalences extend to the whole class of ANOVAs, t-tests, and linear regression models.  The t-test is a special case of ANOVA.  ANOVA is a special case of regression.  All of these procedures are subsumed under the General Linear Model and share the same assumptions.\\n\\n1. Independence of observations.\\n2. Normality of residuals = normality in each group in the special case.\\n3. Equal of variances of residuals = equal variances across groups in the special case.\\n \\nYou might think of it as normality in the data, but you are checking for normality in each group--which is actually the same as checking for normality in the residuals when the only predictor that represents the groups is in the model is the group.  Likewise with equal variances.\\n\\nJust as an aside, R does not have seperate routines for ANOVA.  The anova functions in R are just wrappers to the lm() function--the same thing that is used to fit linear regression models--packaged a little differently to provide what is typically found in an ANOVA summary rather than a regression summary.",edited body; added 30 characters in body; added 312 characters in body,
3814,5,1653,4f1e5e3b-ad26-4827-9691-371266f9ebaa,2010-08-13 15:42:56.0,485.0,"The t-test simply a special case of the F-test where only two groups are being compared.  The result of either will be exactly the same in terms of the p-value and there is a simple relationship between the F and t statistics as well.  F = t^2.  The two tests are algebraically equivalent and their assumptions are the same.\\n\\nIn fact, these equivalences extend to the whole class of ANOVAs, t-tests, and linear regression models.  The t-test is a special case of ANOVA.  ANOVA is a special case of regression.  All of these procedures are subsumed under the General Linear Model and share the same assumptions.\\n\\n1. Independence of observations.\\n2. Normality of residuals = normality in each group in the special case.\\n3. Equal of variances of residuals = equal variances across groups in the special case.\\n \\nYou might think of it as normality in the data, but you are checking for normality in each group--which is actually the same as checking for normality in the residuals when the only predictor in the model is an indicator of group.  Likewise with equal variances.\\n\\nJust as an aside, R does not have seperate routines for ANOVA.  The anova functions in R are just wrappers to the lm() function--the same thing that is used to fit linear regression models--packaged a little differently to provide what is typically found in an ANOVA summary rather than a regression summary.",deleted 17 characters in body; deleted 1 characters in body,
3815,2,1655,693a966a-02fd-4164-8ede-30bc3610a0b9,2010-08-13 15:48:29.0,919.0,"John Tukey systematically discusses transformations in his book on EDA.  In addition to the Box-Cox family (affinely scaled power transformations) he defines a family of ""folded"" transformations for proportions (essentially powers of x/(1-x)) and ""started"" counts (adding a positive offset to counted data before transforming them).  The folded transformations, which essentially generalize the logit, are especially useful for test scores.\\n\\nIn a completely different vein, Johnson & Kotz in their books on distributions offer many transformations intended to convert test statistics to approximate normality (or to some other target distribution), such as the cube-root transformation for chi-square.  This material is a great source of ideas for useful transformations when you anticipate your data will follow some specific distribution.",,
3816,2,1656,e9450b97-e9b8-4e75-ab54-d9e3036de2cd,2010-08-13 15:54:37.0,334.0,"For completeness, econometricians also likes the Kiefer and Salmon test from their 1983 paper in Economics Letters -- it sums 'normalized' expressions of skewness and kurtosis which is then chi-square distributed.  I have an old C++ version I wrote during grad school I could translate into R.\\n\\n\\n\\n",,
3817,2,1657,170b0ff2-535e-4552-9d2c-b8d223bfb006,2010-08-13 16:04:08.0,919.0,"CauseWeb ([link text][1]) has a collection of statistics quotations.  Many have already been repeated here, but it has plenty that haven't yet been quoted, such as\\n\\n> ""The only statistics you can trust are those you falsified yourself.""\\n\\n(Falsely attributed to Sir Winston Churchill.)  For the rest, follow the CauseWeb links to Resources->Fun->Quote.\\n\\n  [1]: http://www.causeweb.org/",,
3818,16,1657,170b0ff2-535e-4552-9d2c-b8d223bfb006,2010-08-13 16:04:08.0,-1.0,,,
3819,2,1658,d143da1d-5dc6-4092-9f7c-143f39cdc448,2010-08-13 16:11:20.0,887.0,"""I cannot conceal the fact here that in the [application of probability theory], I foresee many things happening which can cause one to be badly mistaken if he does not proceed cautiously."",\\n\\nBernoulli (1713) (via ET Jaynes)\\n\\n""A statistician is someone who knows what to assume to be Gaussian""\\n\\nDikran Marsupial (2009) (not famous yet ;o).",,
3820,16,1658,d143da1d-5dc6-4092-9f7c-143f39cdc448,2010-08-13 16:11:20.0,-1.0,,,
3821,2,1659,143d82e0-aba0-4ba5-8cba-e482253db0e2,2010-08-13 16:15:41.0,919.0,"A player's yardage is unlikely to be anywhere near normally distributed.  If it were, your guy averaging 5.3 give or take 1.7 yards would almost never lose yards or gain more than 11 yards on any play in the entire season.  Gone is the excitement of the game, to be replaced by some statistical mediocrity.  If football were played like this, a team's chances of making a set of downs would be almost certain; there would almost never be a loss of downs; and the game would simply be determined by who won the initial coin flip and got on the field first.\\n\\nWhy not just draw a value at random from a list of the player's recent gains (and losses)?  It's fairly easy to program: you just have to generate a uniformly distributed integer to index into an array of the gains.  It doesn't require any kind of statistical model--no need to fit anything.  It can account for change in the player's ability over time (just by selecting which time period you will use to draw the data from).  And it's obviously driven by ""real-live data.""",,
3822,5,1656,8d5d2585-2518-4102-bb38-ed977343099b,2010-08-13 16:29:02.0,334.0,"For completeness, econometricians also like the Kiefer and Salmon test from their 1983 paper in Economics Letters -- it sums 'normalized' expressions of skewness and kurtosis which is then chi-square distributed.  I have an old C++ version I wrote during grad school I could translate into R.\\n\\n*Edit:* And [here](http://econ.la.psu.edu/~hbierens/NORMTEST.PDF) is recent paper by Bierens (re-)deriving Jarque-Bera and Kiefer-Salmon.\\n",added 134 characters in body,
3823,2,1660,03cc79cd-a230-401f-b4d1-84aa2398cf4c,2010-08-13 16:43:45.0,559.0,"I need to define what a test of independence is, without the use of heavily statistic terms.",,
3824,1,1660,03cc79cd-a230-401f-b4d1-84aa2398cf4c,2010-08-13 16:43:45.0,559.0,What is a test of independence,,
3825,3,1660,03cc79cd-a230-401f-b4d1-84aa2398cf4c,2010-08-13 16:43:45.0,559.0,<statistics><definition>,,
3826,5,1656,5a7b24c6-cad6-4c39-a1bc-e4380745e0e2,2010-08-13 16:54:43.0,334.0,"For completeness, econometricians also like the Kiefer and Salmon test from their 1983 paper in Economics Letters -- it sums 'normalized' expressions of skewness and kurtosis which is then chi-square distributed.  I have an old C++ version I wrote during grad school I could translate into R.\\n\\n*Edit:* And [here](http://econ.la.psu.edu/~hbierens/NORMTEST.PDF) is recent paper by Bierens (re-)deriving Jarque-Bera and Kiefer-Salmon.\\n\\n*Edit 2:* I looked over the old code, and it seems that it really is the same test between Jarque-Bera and Kiefer-Salmon. \\n",added 126 characters in body,
3827,5,1622,9954aa69-837e-4298-af91-045a097e098f,2010-08-13 17:02:26.0,795.0,"One estimate of the 'quality' of a portfolio of stocks is the _Sharpe ratio_, which is defined as the mean of the returns divided by the standard deviation of the returns (modulo adjustments for risk free rate, etc). The sample Sharpe ratio is the sample mean divided by the sample standard deviation. Up to a constant factor ($\\sqrt{n}$, where $n$ is the number of observations), this is distributed as a (possibly non-central) $t$-statistic. \\n\\nAre there known techniques for comparing <strike>the mean of</strike> independent variables distributed as non-central $t$-statistics? <strike>Of course, there are non-parametric tests of mean, but is there something specific to the case of noncentral $t$?</strike> (I'm not sure what I meant by that.)\\n\\n**edit**: the original question is somewhat ambiguous (well, it's actually not what I want). Is there a way to test the null hypothesis: population Sharpe ratio of $X$ equals population Sharpe ratio of $Y$, given independent collections of observations drawn from $X$ and $Y$? Here Sharpe ratio is mean divided by standard deviation.\\n\\n**edit**: given $n_x, n_y$ observations of $X, Y$, construct sample means, standard deviations, to get sample Sharpe ratios: $\\hat{S}_x = \\frac{\\hat{\\mu}_x}{\\hat{\\sigma}_x}, \\hat{S}_y = \\frac{\\hat{\\mu}_y}{\\hat{\\sigma}_y}$. Then $t_x = \\sqrt{n_x}\\hat{S}_x$, and $t_y = \\sqrt{n_y}\\hat{S}_y$ are distributed as non-central t-statistics with noncentrality parameters $\\sqrt{n_x}S_x$ and $\\sqrt{n_y}S_y$, where $S_x, S_y$ are the population Sharpe ratios of $X, Y$. Given these independent observations, I wish to test the null hypothesis $H_0: S_x = S_y$. In one form of the problem, one only has the summary statistics $n_x, n_y, \\hat{\\mu}_x, \\hat{\\mu}_y, \\hat{\\sigma}_x, \\hat{\\sigma}_y$. \\n\\nFor large sample sizes, $t_x, t_y$ are approximately normal, I believe but the small sample size case is also of interest (funds often quote performance based on monthly returns).",more background material. actual description of problem at hand.,
3828,5,414,8acb1b58-8c03-48a9-9db3-e898f59ff66e,2010-08-13 17:20:39.0,509.0,"What is a good introduction to statistics for a mathematician who is already well-versed in probability?  I have two distinct motivations for asking, which may well lead to different suggestions:\\n\\n1. I'd like to better understand the statistics motivation behind many problems considered by probabilists.\\n\\n2. I'd like to know how to better interpret the results of Monte Carlo simulations which I sometimes do to form mathematical conjectures.\\n\\nI'm open to the possibility that the best way to go is not to look for something like ""Statistics for Probabilists"" and just go to a more introductory source.\\n",deleted 11 characters in body; edited title,
3829,4,414,8acb1b58-8c03-48a9-9db3-e898f59ff66e,2010-08-13 17:20:39.0,509.0,Introduction to statistics for mathematicians,deleted 11 characters in body; edited title,
3830,2,1661,4e323744-51fe-41b1-99c2-2c314f4172dd,2010-08-13 17:21:37.0,,"X1 is wing length, X2 is tail length for 45 male and 45 female bugs.\\n\\nWhich 2-sample univariate t-test should I use?\\n\\nMy thought was to use Hoetelling's T^2 ?\\n\\nBut Hoetelling's is multi-variate not univariate. Now, I'm not sure...\\n\\nAny ideas?",,Matt
3831,1,1661,4e323744-51fe-41b1-99c2-2c314f4172dd,2010-08-13 17:21:37.0,,Which 2-sample univariate t-test to use?,,Matt
3832,3,1661,4e323744-51fe-41b1-99c2-2c314f4172dd,2010-08-13 17:21:37.0,,<t-test>,,Matt
3833,2,1662,f8eac497-7b6c-474c-9d87-0cf4ad4f9664,2010-08-13 17:39:02.0,,While your questions is not clear (which means do you want to compare?) you can consult the wiki: [Comparing Means][1] to decide what to do.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Comparing_means,,user28
3834,5,77,e62b293f-910e-40fb-93e2-69c976402750,2010-08-13 17:51:01.0,74.0,"1. Sometimes correlation is enough. For example, in car insurance, male drivers are correlated with more accidents, so insurance companies charge them more. There is no way you could actually test this for causation. You cannot change the genders of the drivers experimentally.\\n\\n2. To find causation, you generally need experimental data, not observational data [1]\\n\\n*addition:*  \\n\\n3. Correlation is a necessary but not sufficient condition for causation. To show causation requires a counter-factual.   \\n\\n    \\n[1] Though, in economics, they often use observed ""shocks"" to the system to test for causation, like if a CEO dies suddenly and the stock price goes up, you can assume causation. ",added 147 characters in body,
3835,5,149,e7ca6623-a02b-456c-9634-437e4e2733d7,2010-08-13 17:57:24.0,74.0,"A principal component is a weighted linear combination of all your factors (X's).\\n\\nexample: PCA1 = 0.1X1 + 0.3X2\\n\\nThere will be one principal component for each factor (though in general a small number are selected). \\n\\nThe principal components are created such that they have zero correlation (are orthogonal), by design.\\n\\nTherefore, principal component PCA1 should not explain any variation in principal component PCA2.\\n\\nFor a book, I recommend Multivariate Data Analysis by Hair\\n\\nThis is also good: http://www.statsoft.com/textbook/principal-components-factor-analysis/",added 20 characters in body,
3836,5,523,13dc5d8a-14bd-4716-8c61-f02eae9044ec,2010-08-13 17:59:07.0,74.0,"I don't think that supervised/unsupervised is the best way to think about it. For basic data mining, it's better to think about what you are trying to do. There are four main tasks:\\n\\n1. prediction. if you are predicting a real number, it is called regression. if you are predicting a whole number or class, it is called classification.\\n\\n2. modeling. modeling is the same as prediction, but the model is comprehensible by humans. Neural networks and support vector machines work great, but do not produce comprehensible models [1]. decision trees and classic linear regression are examples of easy-to-understand models. \\n\\n3. similarity. if you are trying to find natural groups of attributes, it is called factor analysis. if you are trying to find natural groups of observations, it is called clustering.\\n\\n4. association. it's much like correlation, but for enormous binary datasets. \\n\\n\\n[1] Apparently Goldman Sachs created tons of great neural networks for prediction, but then no one understood them, so they had to write other programs to try to explain the neural networks. ",edited body,
3837,5,333,9051e5d5-65eb-4991-8607-5c52bcfe8d91,2010-08-13 18:01:09.0,74.0,"Ed Thorpe started the whole statistical arbitrage thing. He has a website, and some good articles.\\n\\nhttp://edwardothorp.com/\\n\\nYou should also read Nassim Taleb's ""Fooled By Randomness"".\\n\\nAlso, go on Google Scholar and read the top articles by Markowitz, Sharpe, Fama, Modigliani. If you don't have full access,  go to the nearest college and get a community library card. ",added 90 characters in body,
3838,2,1663,5d858fa4-7264-41d3-b607-d4366793478c,2010-08-13 18:27:56.0,927.0,"Why don't you take the definition of wikipedia. It's quite short und doesn't use heavily statistic terms.\\n> A test of independence assesses whether paired observations on two variables, expressed in a contingency table, are independent of each other – for example, whether people from different regions differ in the frequency with which they report that they support a political candidate.",,
3839,2,1664,6763f41b-a797-41f1-b00e-dc818d34f946,2010-08-13 19:29:26.0,8.0,"As others have said, you need to clarify your question. \\n\\nHowever, I'm guessing that you want to determine if wing length or tail length differ between male and female bugs. In this case I would just do a couple of [two sample t-tests][1]. So for wing length you would have the following hypothesis:\\n\\n    H_0: wing length does not depend on gender\\n    H_1: wing length differs by gender.\\n\\nYou would have something similar for tail length.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test\\n",,
3840,2,1665,3c853aaa-3715-4c51-a4cc-8b65823d5a81,2010-08-13 19:39:43.0,8.0,"I would start by defining what you mean by independence. For example, \\n\\n> If two variables are independent this\\n> means that knowing the value of one\\n> variable does not tell you anything\\n> about the value of the other variable.\\n\\nThen I would describe the test:\\n\\n> To test for independence we construct\\n> a table of values that we would expect\\n> to see if the variables were\\n> independent. If we observed something\\n> ""very"" different from these expected\\n> values, we would conclude that the\\n> variables are unlikely to be independent.\\n\\n",,
3841,2,1666,36ddbd24-60fb-49b9-81b8-5139da576932,2010-08-13 21:22:56.0,919.0,"I loved the Freedman, Pisani, Purves *Statistics* text because it is extremely *non*-mathematical.  As a mathematician you will find it to be such a clear guide to the statistical concepts that you will be able to develop all the mathematical theory as an exercise: that's a rewarding thing to do.  (The first edition of this text was my initiation to statistics after I completed a PhD in pure mathematics and I still enjoy re-reading it.)",,
3842,16,1666,36ddbd24-60fb-49b9-81b8-5139da576932,2010-08-13 21:22:56.0,-1.0,,,
3843,2,1667,fed7f441-0af6-418b-8b0b-869d41a56625,2010-08-13 22:29:08.0,110.0,"As an engineer, I'm interested in topics such as designing experiments that are statistically valid, quality control, process control, reliability, and cost control. I took a course in engineering statistics, but unfortunately neither the book nor the professor were that good. I did OK in the course, but I'm interested in learning more about these topics and how to apply them to engineering problems. I would prefer a general book that covered as many of these topics as possible - great depth is not needed.\\n\\nI think that I can learn a lot about improving my abilities by looking at how all engineering disciplines use statistics, so I'm not looking for any particular engineering field.\\n\\nCan the Statistical Analysis community recommend books that I can use to learn more about applying statistics to engineering problems?\\n\\n(Also, can someone add the ""engineering-statistics"" tag? Thanks.)",,
3844,1,1667,fed7f441-0af6-418b-8b0b-869d41a56625,2010-08-13 22:29:08.0,110.0,What books provide an overview of engineering statistics?,,
3845,3,1667,fed7f441-0af6-418b-8b0b-869d41a56625,2010-08-13 22:29:08.0,110.0,<books>,,
3846,16,1667,fed7f441-0af6-418b-8b0b-869d41a56625,2010-08-13 22:29:08.0,110.0,,,
3847,2,1668,2f43da97-77e8-42c7-901a-16d641062c52,2010-08-13 22:35:50.0,110.0,"As a software engineer, I'm interested in topics such as statistical algorithms, data mining, machine learning, Bayesian networks, classification algorithms, neural networks, Markov chains, Monte Carlo methods, and random number generation.\\n\\nI personally haven't had the pleasure of working hands-on with any of these techniques, but I have had to work with software that, under the hood, employed them and would like to know more about them, at a high level. I'm looking for books that cover a great breadth - great depth is not necessary at this point. I think that I can learn a lot about software development if I can understand the mathematical foundations behind the algorithms and techniques that are employed.\\n\\nCan the Statistical Analysis community recommend books that I can use to learn more about implementing various statistical elements in software?\\n\\n(Also, could someone add the tags computational-statistics and computing to this question? Thanks.)",,
3848,1,1668,2f43da97-77e8-42c7-901a-16d641062c52,2010-08-13 22:35:50.0,110.0,What books provide an overview of computational statistics as it applies to computer science?,,
3849,3,1668,2f43da97-77e8-42c7-901a-16d641062c52,2010-08-13 22:35:50.0,110.0,<books>,,
3850,16,1668,2f43da97-77e8-42c7-901a-16d641062c52,2010-08-13 22:35:50.0,110.0,,,
3851,6,1668,c6bcf1c4-0430-45e5-93d5-f6c2cd9a30a4,2010-08-13 23:29:20.0,174.0,<books><computational-statistics><computing>,added requested tags,
3852,6,1667,8c2ef50d-b478-4d42-91dc-57a054d9bcff,2010-08-13 23:32:54.0,174.0,<books><engineering-statistics>,"added requested tag -- I'm a bit less convinced by this one, but then I'm not an engineer",
3853,2,1669,a8767f20-093c-469d-b1fb-4ccddf45ea08,2010-08-13 23:58:11.0,74.0,NIST/SEMATECH e-Handbook of Statistical Methods is a good start. Free and online:\\n\\nhttp://www.itl.nist.gov/div898/handbook/,,
3854,16,1669,a8767f20-093c-469d-b1fb-4ccddf45ea08,2010-08-13 23:58:11.0,-1.0,,,
3855,2,1670,93605acf-0c0e-45dc-9919-1c4dfd6f42db,2010-08-14 00:58:21.0,110.0,"When I took the Engineering Statistics course I mentioned in the question, the assigned textbook wasn't very helpful. Instead, I used [Probability and Statistics for Engineers and Scientists - Anthony Hayter][1] to get through the course. It didn't cover everything in the same order and depth of the course, but it was sufficient to get me through the material and get a passing grade.\\n\\nTopics covered include probability theory, random variables, discrete and continuous probability distributions, normal distributions, descriptive statistics, statistical estimation and sampling distributions, population means, discrete data analysis, ANOVA, linear regression, nonlinear regression, multifactor experimental design and analysis, nonparametric statistical analysis, quality control methods, and reliability analysis. Unfortunately, the course only covered the first 11 chapters and occasionally in more depth then this book went into.\\n\\n\\n  [1]: http://www.amazon.com/Probability-Statistics-Engineers-Scientists-CD-ROM/dp/0495107573/ref=sr_1_1?s=books&ie=UTF8&qid=1281747216&sr=1-1",,
3856,16,1670,93605acf-0c0e-45dc-9919-1c4dfd6f42db,2010-08-14 00:58:21.0,-1.0,,,
3857,2,1671,a6b7d080-0de5-4302-a3c6-2dddad72780c,2010-08-14 01:02:31.0,110.0,"I picked up a copy of [Probability and Statistics for Computer Scientists - Michael Baron][1] on sale with another statistics book (I honestly bought it because of the name - I wanted a book that would take some kind of look at statistics from a computer science perspective, even if it wasn't perfect). I haven't had a chance to read it or work any problems in it yet, but it seems like a solid book.\\n\\nThe preface of the book says that it's for upper level undergraduate students and beginning graduate students, and I would agree with this. Some understanding of probability and statistics are necessary to grasp the contents of this book.\\n\\nTopics include probability, discrete random variables, continuous distributions, Monte Carlo methods, stochastic processes, queuing systems, statistical inference, and regression.\\n\\n\\n  [1]: http://www.amazon.com/Probability-Statistics-Computer-Scientists-Michael/dp/1584886412/ref=sr_1_1?s=books&ie=UTF8&qid=1281747555&sr=1-1",,
3858,16,1671,a6b7d080-0de5-4302-a3c6-2dddad72780c,2010-08-14 01:02:31.0,-1.0,,,
3859,2,1672,a4211327-ffb3-417a-a482-6963cff248a8,2010-08-14 01:09:33.0,110.0,"Although it's not specifically computational statistics, [A Handbook of Statistical Analyses Using R - Brian S. Everitt and Torsten Hothorn][1] covers a lot of topics that I've seen covered in basic and intermediate statistics books - inference, ANOVA, linear regression, logistic regression, density estimation, recursive partitioning, principal component analysis, and cluster analysis - using the R language. This might be of interest to those interested in programming.\\n\\nHowever, unlike other books, the emphasis is on using the R language to carry out these statistical functions. Other books I've seen use combinations of algebra and calculus to demonstrate statistics. This book actually focuses on how to analyze data using the R language. And to make it even more useful, the data sets the authors use are in CRAN - the R Repository.\\n\\n\\n  [1]: http://www.amazon.com/Handbook-Statistical-Analyses-Using/dp/1584885394",,
3860,16,1672,a4211327-ffb3-417a-a482-6963cff248a8,2010-08-14 01:09:33.0,-1.0,,,
3861,2,1673,4ea1cf14-266b-446b-bf6f-9d8f2e43010d,2010-08-14 01:16:16.0,110.0,"[Statistical Computing with R - Maria L. Rizzo][1] covers a lot of the topics in Probability and Statistics for Computer Scientists - basic probability and statistics, random variables, Bayesian statistics, Markov chains, visualization of multivariate data, Monte Carlo methods, Permutation tests, probability density estimation, and numerical methods.\\n\\nThe equations and formulas used are presented both as mathematical formulas as well as in R code. I would say that a basic knowledge of probability, statistics, calculus, and maybe discrete mathematics would be advisable for anyone who wants to read this book. A programming background would also be helpful, but there are some references for the R language, operators, and syntax.\\n\\n\\n  [1]: http://www.amazon.com/Statistical-Computing-Chapman-Computer-Analysis/dp/1584885459/ref=sr_1_1?s=books&ie=UTF8&qid=1281748268&sr=1-1",,
3862,16,1673,4ea1cf14-266b-446b-bf6f-9d8f2e43010d,2010-08-14 01:16:16.0,-1.0,,,
3863,5,1668,af020d8d-f344-48b9-89bb-8dcc58f00800,2010-08-14 01:21:07.0,110.0,"As a software engineer, I'm interested in topics such as statistical algorithms, data mining, machine learning, Bayesian networks, classification algorithms, neural networks, Markov chains, Monte Carlo methods, and random number generation.\\n\\nI personally haven't had the pleasure of working hands-on with any of these techniques, but I have had to work with software that, under the hood, employed them and would like to know more about them, at a high level. I'm looking for books that cover a great breadth - great depth is not necessary at this point. I think that I can learn a lot about software development if I can understand the mathematical foundations behind the algorithms and techniques that are employed.\\n\\nCan the Statistical Analysis community recommend books that I can use to learn more about implementing various statistical elements in software?",deleted 103 characters in body,
3864,5,1667,07dd1013-e052-4b88-bd48-c1b08f8bf85a,2010-08-14 01:21:36.0,110.0,"As an engineer, I'm interested in topics such as designing experiments that are statistically valid, quality control, process control, reliability, and cost control. I took a course in engineering statistics, but unfortunately neither the book nor the professor were that good. I did OK in the course, but I'm interested in learning more about these topics and how to apply them to engineering problems. I would prefer a general book that covered as many of these topics as possible - great depth is not needed.\\n\\nI think that I can learn a lot about improving my abilities by looking at how all engineering disciplines use statistics, so I'm not looking for any particular engineering field.\\n\\nCan the Statistical Analysis community recommend books that I can use to learn more about applying statistics to engineering problems?",deleted 69 characters in body,
3865,2,1674,cbc182f0-ef04-4775-aae4-7d6330936571,2010-08-14 01:31:43.0,511.0,"On page 331 of ""Elements of Information Theory"" (1991), author says that while entropy is related to the volume of the typical set, Fisher information is related to the surface area of the typical set, but I can't find anything more on this...can anyone explain this connection?",,
3866,1,1674,cbc182f0-ef04-4775-aae4-7d6330936571,2010-08-14 01:31:43.0,511.0,"Fisher information and the ""surface area of the typical set""",,
3867,3,1674,cbc182f0-ef04-4775-aae4-7d6330936571,2010-08-14 01:31:43.0,511.0,<statistics>,,
3868,2,1675,ce2d8d4d-b897-432f-850c-2490832d878f,2010-08-14 01:32:29.0,110.0,"The book [Statistical Methods for Engineers - Geoffrey Vining][1] is used in my university's Engineering Statistics course. However, I do not recommend this book. When I took the course, I ended up not being able to learn from the professor, so I was using this book to teach myself the material. It went along with the course in terms of content and depth, but I found the examples presented to be confusing and not as clear as they could have been. If you have a strong statistics background to begin with, it might be a suitable book, but this was the first statistics course that I had taken (and the only one required). There were no errors with the book - the examples and solutions were all correct.\\n\\nIf you are an engineer with a preexisting background in statistics, perhaps it might be worth it to visit your local library and check it out first before you buy it - it might work better for you than it did for me.\\n\\n\\n  [1]: http://www.amazon.com/Statistical-Methods-Engineers-Geoffrey-Vining/dp/053873518X/ref=sr_1_1?ie=UTF8&s=books&qid=1281749129&sr=8-1",,
3869,16,1675,ce2d8d4d-b897-432f-850c-2490832d878f,2010-08-14 01:32:29.0,-1.0,,,
3870,2,1676,316eee87-5440-464d-9351-7e2f4048fbf4,2010-08-14 01:39:23.0,110.0,"Are there any R packages that are just plain good to have, regardless of the type of work you are doing? If so, what are these packages? If not, what packages do you find the most useful?",,
3871,1,1676,316eee87-5440-464d-9351-7e2f4048fbf4,2010-08-14 01:39:23.0,110.0,I just installed the latest version of R. What packages should I obtain?,,
3872,3,1676,316eee87-5440-464d-9351-7e2f4048fbf4,2010-08-14 01:39:23.0,110.0,<r><statistical-packages>,,
3873,2,1677,fffe0b4a-6e80-4c96-8e90-e3b255f88b4e,2010-08-14 01:41:18.0,183.0,You might want to read the extremely popular question on Stack Overflow on \\n[what statistics a programmer or computer scientist should know][1].\\n\\n\\n  [1]: http://stackoverflow.com/questions/2039904/what-statistics-should-a-programmer-or-computer-scientist-know,,
3874,16,1677,fffe0b4a-6e80-4c96-8e90-e3b255f88b4e,2010-08-14 01:41:18.0,-1.0,,,
3875,2,1678,495de2a2-6f3c-46bb-b52b-5c74b98c6895,2010-08-14 01:46:57.0,183.0,"I imagine graphics and data manipulation are two things that are useful no matter what you are doing. Thus, I'd recommend:\\n\\n- ggplot2 (great graphics)\\n- lattice (great graphics)\\n- plyr (useful for data manipulation)\\n- Hmisc (good for descriptive statistics and much more)\\n\\n\\n",,
3876,2,1679,359b16cf-d04b-41f2-8225-655e4b287257,2010-08-14 03:56:28.0,,"Excel has pretty good functions for the normal distribution, so long as you aren't way out in the tail (where they fall apart). ",,ZBicyclist
3877,2,1680,e126de59-d199-484a-b606-d92d95cb5a10,2010-08-14 04:58:27.0,485.0,"This is definitely a question that doesn't have ""an answer"".  It is completely dependent on what you want to do.  That aside, I'll share the packages that I install as a standard with an R update...\\n\\n    install.packages(c(""car"",""gregmisc"",""xtable"",""Design"",""Hmisc"",""psych"",\\n                            ""CCA"", ""fda"", ""zoo"", ""fields"",\\n                          ""catspec"",""sem"",""multilevel"",""Deducer"",""RQDA""))\\n\\nand leave it to you to investigate those packages and see if they are valuable to you.\\n\\n",,
3878,2,1681,a47d7648-0a99-4ce7-8a24-45c04fcdfec8,2010-08-14 07:16:00.0,511.0,"Chernoff bound (for absolute error) gives a bound on probability of large deviation in terms of sample size and amount of deviation, but it doesn't seem possible to rewrite it to give an explicit bound on the amount of deviation. So, what is good way to bound the largest deviation from the mean in terms of sample size and probability of that deviation?",,
3879,1,1681,a47d7648-0a99-4ce7-8a24-45c04fcdfec8,2010-08-14 07:16:00.0,511.0,Chernoff-like bound for largest allowed deviation? ,,
3880,3,1681,a47d7648-0a99-4ce7-8a24-45c04fcdfec8,2010-08-14 07:16:00.0,511.0,<statistics>,,
3881,2,1682,635330f6-8a14-4716-9284-06cd97cae19e,2010-08-14 07:52:35.0,887.0,"What the p-value doesn't tell you is how likely it is that the null hypothesis is true.  Under the conventional (Fisher) significance testing framework we first compute the likelihood of observing the data assuming the null hypothesis is true, this is the p-value.  It seems intuitively reasonable then to assume the null hypothesis is probably false if the data are sufficiently unlikely to be observed under the null hypothesis.  This is entirely reasonable.  Statisticians tranditionally use a threshold and ""reject the null hypothesis at the 95% significance level"" if (1 - p) > 0.95; however this is just a convention that has proven reasonable in practice - it doesn't mean that there is less than 5% probability that the null hypothesis is false (and therefore a 95% probability that the alternative hypothesis is true).  One reason that we can't say this is that we have not looked at the alternative hypothesis yet.\\n\\nImaging a function f() that maps the p-value onto the probability that the alternative hypothesis is true.  It would be reasonable to assert that this function is strictly decreasing (such that the more likely the observations under the null hypothesis, the less likely the alternative hypothesis is true), and that it gives values between 0 and 1 (as it gives an estimate of probability).  However, that is all that we know about f(), so while there is a relationship between p and the probability that the alternative hypothesis is true, it is uncalibrated.  This means we cannot use the p-value to make quantitative statements about the plausibility of the nulll and alternatve hypotheses.\\n\\nCaveat lector: It isn't really within the frequentist framework to speak of the probability that a hypothesis is true, as it isn't a random variable - it is either true or it isn't.  So where I have talked of the probability of the truth of a hypothesis I have implicitly moved to a Bayesian interpretation.  It is incorrect to mix Bayesian and frequentist, however there is always a temptation to do so as what we really want is an quantative indication of the relative plausibility/probability of the hypotheses.  But this is not what the p-value provides.",,
3882,6,1681,cb0ab8a9-e84b-4520-8cd7-6dfef399ab1d,2010-08-14 08:02:27.0,183.0,<probability><statistics><chernoff-bound>,edited tags,
3883,2,1683,d3e4b85d-b503-431b-8cad-b97d5107685c,2010-08-14 08:12:41.0,887.0,"I think semi-supervised methods may be what you are looking for, there is quite a lot of litterature on this in Machine learning.  There is a good <a href=""http://www.amazon.co.uk/Semi-Supervised-Learning-Adaptive-Computation-Machine/dp/0262514125"">book</a> on this topic, which gives a good idea of recent developments in this area.\\n\\nAn E.M. algorithm for logistic regression (a discriminative model) is easily implemented as follows:  \\n\\n(i) Train a LR model using only the labelled data.  \\n\\n(ii) Use the LR model to assign labels to the unlabelled data.  \\n\\n(iii) Train a new LR model using both the labelled and unlabelled data (using the predicted labels).\\n\\n(iv) repeat (ii) and (iii) until convergence is reached (i.e. none of the predicted labels for the unlabelled examples change).\\n\\nThis works quite well for some problems (e.g. text classification), not so well in others.  EM also works well with naieve Bayes.  McLachlan's excellent <a href=""http://www.amazon.co.uk/Discriminant-Statistical-Recognition-Probability-Statistics/dp/0471691151"">book</a> on discriminant analysis also has some material on basic algorithms.\\n\\nHTH",,
3884,2,1684,d38e6f55-25fa-4f77-8185-96dc54ae0c68,2010-08-14 08:51:32.0,939.0,"If you are working with Latex, I recommend **TikZ Device** for outputting nice, Latex-formatted (like PSTricks) graphics. The output you get is text-based Latex code, which can be embedded with include(filename) into any figure environment. \\n\\nPros: \\n\\n - Same font in graphics as in your text\\n - Professional look\\n\\nCons:\\n\\n - Takes longer to compile than PNG or PDF\\n - for very complex R graphics, there are could be some display errors\\n\\nhttps://r-forge.r-project.org/projects/tikzdevice/ - Project, Download",,
3885,16,1676,98870954-54b3-4318-9a88-57467c68dd92,2010-08-14 09:41:44.0,110.0,,,
3886,5,1594,e533e2a3-0319-4c21-bf37-6ccd291fe6d1,2010-08-14 11:17:52.0,702.0,"Interesting question!  All statistical models can be viewed as performing lossy data compression.  For instance simple linear regression with one predictor replaces $N$ points (where $N$ can be massive, e.g., in the 1000s) with two parameters: a slope and intercept.  The parameters may then be used to reconstruct the data, with degree of success depending on how good the original fit was.\\n\\nYour specific example concerns predicting binary time series data (Bernoulli distributed data, which is a specific case of the binomial distribution).  Binary data can encode a lot: coin flips, pictures, sounds, the digits of $\\pi$, statistical programming languages...  \\n\\nAs you can imagine, and as a quick search around Google will confirm, there are a lot of statistical models which could apply to binary data.   One is logistic regression, or (to express the same model in a more general framework) a Generalized Linear Model with a binomial distribution and a logit link function.  The function fit is of the following form:\\n$\\mbox{logit}[P(Y)] = \\beta X + \\epsilon$, where $X$ (predictors), $Y$ (probability of a 1), and $\\epsilon$ (residuals) are vectors.\\n\\nOkay.  Now a little demonstration. Suppose data are generated so that the probability of a 1 correlates with the sine of time (represented as black points in the graph below).  You don't know this, however.  You get data for time points from 0 to 359 (blue points).\\n\\n![alt text][1]\\n\\nWith the available data points, I fitted the function $\\mbox{logit}[P(Y)] = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\beta_3 t^3$, which popped out as $\\mbox{logit}[P(Y)] = -0.2  -30.9 t  -3.1 t^2 + 22.2 t^3$.  (The probability predictions are plotted in red.)  It's a good fit to the data (between 0 and 359).  However as you can see, when extrapolating, it does a rather poor job: beyond a certain point it says ""just guess 1!""\\n\\nTake-home message: to do the analysis correctly, you need to have a some idea of the likely processes generating the data.  If I knew a sine process were doing the job, then I'd be able to do a wonderful job predicting.  Thinking about this is where a statistician would start.  The appropriate model is always going to be domain specific, which is why, for example, compression techniques working well for images don't automatically apply to sounds.\\n                               \\n\\n  [1]: http://img196.imageshack.us/img196/589/cointimepredict2.png",added 15 characters in body,
3887,2,1685,0883ad9a-d040-4c82-b397-6945615d6e81,2010-08-14 12:10:58.0,799.0,"So, here's the example: http://nishi.dreamhosters.com/u/book1bwt_de.txt\\n\\nIts a list of choices between 'd' and 'e' in coding of BWT output of a plaintext book.\\n(This is a practical task, think bzip2).\\nFor the reference, current result is (reasonably good, but not really the best possible) \\n6087 bytes = 48686 bits (ie log-likelihood) for that string of 99054 bits",,
3888,5,196,36cfa9d0-1d01-45f5-b95c-24a52d0d81fd,2010-08-14 12:21:50.0,509.0,"Besides [gnuplot][1] and [ggobi][2], what open source tools are people using for visualizing multi-dimensional data?\\n\\nGnuplot is more or less a basic plotting package. \\n\\nGgobi can do a number of nifty things, such as:\\n\\n -  animate data along a dimension or among discrete collections\\n -  animate linear combinations varying the coefficients\\n -  compute principal components and other transformations\\n -  visualize and rotate 3 dimensional data clusters\\n -  use colors to represent a different dimension\\n\\nWhat other useful approaches are based in open source and thus freely reusable or customizable?\\n\\n  [1]: http://en.wikipedia.org/wiki/Gnuplot\\n  [2]: http://www.ggobi.org/\\n",added 11 characters in body; edited title,
3889,4,196,36cfa9d0-1d01-45f5-b95c-24a52d0d81fd,2010-08-14 12:21:50.0,509.0,Open source tools for visualizing multi-dimensional data?,added 11 characters in body; edited title,
3890,2,1686,6bf8e7e9-7c3b-4f6d-9bfb-231da834d528,2010-08-14 13:06:45.0,334.0,"In a narrow sense, R Core has a recommendation: the ""recommended"" packages.\\n\\nEverything else depends on your data analysis tasks at hand, and I'd recommend the [Task Views](http://cran.r-project.org/web/views) at CRAN.",,
3891,16,1686,6bf8e7e9-7c3b-4f6d-9bfb-231da834d528,2010-08-14 13:06:45.0,-1.0,,,
3892,2,1687,54900a5d-3716-4f28-94b1-b3eb18272af0,2010-08-14 14:00:16.0,937.0,"The problem I’m trying to solve is “How do I figure out how much gunpowder should I put into a cartridge so that I can give myself a good probability of making the minimum power factor?”\\n\\nI compete in USPSA/IPSC which requires that a competitors rounds make a minimum power factor. Power Factor is computed to be the FLOOR(average bullet velocity * bullet weight) / 1000) where velocity is in feet per second, and bullet weight is in grains. Note the use of FLOOR. No rounding is done. Only the integral part of the computation is used. The higher the power factor, the higher the felt recoil and harder it is to quickly do follow up shots. Since the sport is about firing shots as quickly and as accurately as possible, the lower the recoil the better.\\n\\nDifferent divisions within the sport have different power factor floors, but the particular division I compete in has a minimum of 165 Power Factor. The bullets I use are 180 gn bullets, and vary by about +/- 0.2 gn and is normally distributed. \\n\\nWhat makes this an interesting problem (and move it out of my meager stats and probability skills) is the testing procedure during a major match. Random sample of 8 rounds are collected. Of the 8 rounds, one is taken apart and the bullet is weighed for use in the formula above. Next, 3 rounds are fired and the average velocity is used. If the resulting power factor is below the minimum, then another 3 rounds are fired. The average of the 3 fastest velocities from the 6 rounds fired is now used to compute the power factor. If the resulting power factor is still below the floor, then the shooter has the option of having the last round taken apart and weighed or the last round fired. If the bullet is taken apart and it is heavier than the first bullet, the heavier weight is used to compute the power factor. If the last round is fired, the average of the 3 fastest velocities from the 7 rounds fired is used to compute the power factor.\\n\\nTo add spice to this problem, not all chronographs used to measure bullet velocities are created equal. The chronograph industry acknowledges that there can be as much as +/- 4% variance between chronographs of different brands. Even more interesting is that the rules allow for the same chronograph used for a particular match to have +/- 4% variance over the duration of a match. I don’t know if either of these 4% variances are normally distributed or not.\\n\\nWith my own chronograph, I test batches of a particular gunpowder load to get the average velocity and standard deviation. After statistical analyses of many different batches, I’ve confirmed that this data is normally distributed.\\n\\nThe way I’m currently determining my minimum load is by finding the load the gives me 165 < FLOOR( target * 179.9 / 1000) where target =  (average velocity - standard deviation) * fudge factor. For fudge factor, I’ve unscientifically chosen 1.04. The 0.04 is the 4% variance between chronographs, but ignores the day-to-day allowable variance. I chose to just subtract just 1 standard deviation because it’ll only be 16% of the time that one bullet will be below the floor. In my mind, the probability of all the first 3 bullets all going below the floor is 0.16^3 which less than half a percent.\\n\\nMy specific questions are: Am I going about computing the target the right way? Should my fudge factor include another 4% for the day-to-day variance allowed? Is the 1 standard deviation too much or too little? How should I write the formula for my target? \\n",,
3893,1,1687,54900a5d-3716-4f28-94b1-b3eb18272af0,2010-08-14 14:00:16.0,937.0,Applied statistics to find the minimum load for power factor floor,,
3894,3,1687,54900a5d-3716-4f28-94b1-b3eb18272af0,2010-08-14 14:00:16.0,937.0,<probability><standard-deviation>,,
3895,2,1688,4c1737b4-049b-4f7a-ab3b-ecd062cf09bc,2010-08-14 14:06:52.0,573.0,You can get user reviews of packages on [crantastic][1]\\n\\n\\n  [1]: http://crantastic.org/reviews,,
3896,16,1688,4c1737b4-049b-4f7a-ab3b-ecd062cf09bc,2010-08-14 14:06:52.0,-1.0,,,
3897,2,1689,9eb1c7e1-434f-4515-bb3e-53776c298efc,2010-08-14 17:42:41.0,144.0,Sperm count in males in Slovene villages and the number of bears (also in Slovenia) show a negative correlation. Some people find this very worrying. I'll try and get the study that did this.,,
3898,2,1690,2e0a5023-934e-43bf-bdd8-8307e6466b8e,2010-08-14 17:59:20.0,942.0,"The standard citation pointing out the correlation between the number of newborn babies and breeding-pairs of Stalks in West Germany is [*A new parameter for sex education*, Nature 332, 495 (07 April 1988); doi:10.1038/332495a0][1]\\n\\n\\n  [1]: http://nature.com/nature/journal/v332/n6164/abs/332495a0.html",,
3899,2,1691,c0f948ad-b527-4bab-9b70-372091027daa,2010-08-14 18:32:47.0,,"You have a complex statistical problem and a complete analysis would be too long. However, I will suggest one idea that may perhaps help you to some extent. \\n\\nYou have already performed some calibration tests to assess the mean and standard deviation of the velocity of a bullet. However, I suspect that either your testing or your calculations or both are sub-optimal for the reasons I mention below.\\n\\nThe crucial point is this: The measured velocity of a bullet is a random variable that depends on three factors: (a) amount of gunpowder in the bullet, (b) random factors due to imperfections of the gun, and (c) errors because of chronograph inaccuracy. In other words, you can model the measured velocity of a bullet as follows:\\n\\n$v_m = v(gp) + \\epsilon_g + \\epsilon_c$\\n\\nwhere,\\n\\n$v_m$ is the measured velocity,\\n\\n$v(gp)$ is the true velocity of the bullet given that the bullet has $gp$ amount of gunpowder,\\n\\n$\\epsilon_g \\sim N(0,\\sigma_g^2)$ is the random error that arises due to imperfections of the gun and\\n\\n$\\epsilon_c \\sim N(0,\\sigma_c^2)$ is the error induced by the chronograph.\\n\\nI think assuming that the errors are normally distributed with a mean zero and a finite variance is reasonable. Thus, we have:\\n\\n$E(v_m) = v(gp)$\\n\\nand\\n\\n$Var(v_m) = \\sigma_g^2 + \\sigma_c^2$\\n\\nThe goal of testing is to get a sense of the average velocity and the standard deviation of the velocity of a bullet. You could simply compute the mean of the observed velocities to get a sense of the mean as $E(v_m) = v(g)$ but you cannot use the standard deviation of the observed velocities for setting your target as the the errors induced by the chronograph and the errors due to the imperfections of the gun are confounded.\\n\\nThus, you need to design your test to disentangle the variation induced by the chronograph and the intrinsic variation because of other factors. One way to achieve the above disentanglement is to perform a test as follows:\\n\\n 1. Start test 1 and choose a certain amount of gunpowder that you will fill up each bullet for this test.\\n 2. Load the bullet with the amount of gunpowder you chose for this test in step 1.\\n 3. Measure the velocity of the bullet.\\n 4. Repeat steps 2 and 3 for a certain number of bullets.\\n\\n 5. Perform several tests along the above lines ensuring that you choose a different amount of gunpowder for each test.\\n\\nLet:\\n\\n$i$ index the bullets,\\n\\n$j$ index the test.\\n\\nThen you have a set of velocities indexed $v_{ij}$. Then, you can calculate the variance of the velocity (and hence the standard deviation) as follows:\\n\\n 1. Calculate the variance of **all** velocity measurements. Denote this number by $S_v^2$.\\n 2. Calculate the variance of velocities for each test separately. Denote each test specific variance by $S_j^2$. Note that each of these variance measures is a measure of how much variation is induced because of the chronograph as the amount of gunpowder is the same for each test.\\n\\nThen a measure of the variance of the velocity that you can attribute to the gun itself would be:\\n\\n$S_g^2 = S_v^2 - \\frac {\\sum_j S_j^2}{J}$\\n\\nOnce you know $S_g$ you can use your existing formula and there is no need to worry about the additional variation induced in your measurements because of your own chronograph as that has been 'taken care of' in our analysis.\\n\\nHope that helps to some extent.\\n\\n",,user28
3900,2,1692,b01aefe9-d077-43f4-a957-c84586810c1e,2010-08-14 19:50:51.0,495.0,"Although it's more of an illustration of the problem of multiple comparisons, it is also a good example of misattributed causation:\\n\\n[Rugby (the religion of Wales) and its influence on the Catholic church: should Pope Benedict XVI be worried?][1]\\n\\n> ""every time Wales win the rugby grand slam, a Pope dies, except for 1978 when Wales were really good, and two Popes died.""\\n\\n  [1]: http://www.bmj.com/cgi/content/abstract/337/dec17_2/a2768",,
3901,2,1693,5a209761-a9a2-447b-b1ba-cde027a2812f,2010-08-14 21:05:33.0,251.0,The basic intuition is through the isoperimetric inequality for the surface area of a sphere maximizing the volume.  We can arrive at a similar relationship concerning the trace of the Fisher information matrix and the entropy w.r.t the Gaussian.  The following may be helpful.\\n\\n- [Information Theoretic Inequalities for Contoured Probability Distributions][1]\\n- [On Isoperimetric Inequalities in Minkowski Spaces][2]\\n\\n  [1]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7743\\n  [2]: http://www.hindawi.com/journals/jia/2010/697954.html\\n\\n,,
3902,2,1694,cfbc7b51-4920-4e55-a540-420ed0cc9bb3,2010-08-14 21:19:58.0,942.0,"Sir Austin Bradford Hill's President's Address to the Royal Society of Medicine ([The Environment and Disease: Association or Causation?][1]) explains nine criteria which help to judge whether there is a causal relationship between two correlated or associated variables.\\n\\nThey are:\\n\\n 1. Strength of the association\\n 2. Consistency: ""has it been repeatedly\\n    observed by different persons, in\\n    different places, cirumstances and\\n    times?"" \\n 3. Specificity  \\n 4. Temporality: ""which is the cart and\\n    which is the horse?"" - the cause\\n    must precede the effect \\n 5. Biological gradient (dose-response\\n    curve) - in what way does the\\n    magnitude of the effect depended\\n    upon the magnitude of the (suspected) causal variable? \\n 6. Plausibility - is there a likely\\n    explanation for causation?  \\n 7. Coherance - would causation\\n    contradict other established facts?\\n 8. Experiment - does experimental\\n    manipulation of the (suspected)\\n    causal variable affect the\\n    (suspected) dependent variable \\n 9. Analogy - have we encountered\\n    similar causal relationships in the\\n    past?\\n\\n  [1]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1898525/?tool=pubmed",,
3903,6,1674,8edb0cc1-c1b7-42fc-a853-6348e45d861d,2010-08-14 21:20:27.0,88.0,<statistics><fisher-information>,edited tags,
3904,2,1695,36aa36f9-d9df-4740-a07e-92a8babf3cf7,2010-08-14 21:52:11.0,8.0,"I would suggest using some of the packages provided by [revolution R][1]. In particular, I quite like the:\\n\\n  * [multicore][2] package for parallel computing using shared memory processors\\n  * there optimized packages for [matrices][3]\\n\\n\\n  [1]: http://www.revolutionanalytics.com/\\n  [2]: http://cran.r-project.org/web/packages/multicore/index.html\\n  [3]: http://www.revolutionanalytics.com/why-revolution-r/benchmarks.php",,
3905,16,1695,36aa36f9-d9df-4740-a07e-92a8babf3cf7,2010-08-14 21:52:11.0,-1.0,,,
3906,2,1696,bb69bebc-fba4-4f53-b22c-c5e8dcd145d0,2010-08-14 21:55:32.0,942.0,"I would show them the raw data of [Anscombe's Quartet][1] ([JSTOR link to the paper][2]) in a big table, alongside another table showing the Mean & Variance of x and y, the correlation coefficient, and the equation of the linear regression line. Ask them to explain the differences between each of the 4 datasets. They will be confused.\\n\\nThen show them 4 graphs. They will be enlightened.  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Anscombe%27s_quartet\\n  [2]: http://jstor.org/stable/pdfplus/2682899.pdf",,
3907,2,1697,e404c043-720e-4673-8abe-be02a8b4f525,2010-08-15 00:31:42.0,389.0,"Jeromy mentioned my first pick: Lattice.\\n\\nI also have found the `doBy` package and its `summaryBy` function to be insanely useful.  They extend `aggregate` with a formula syntax that lets you aggregate multiple funtions simulatenously in non-trivial ways.  Great if you want, say, mean, std. dev., and length.",,
3908,16,1697,e404c043-720e-4673-8abe-be02a8b4f525,2010-08-15 00:31:42.0,-1.0,,,
3909,2,1698,47a7f70d-f6b4-43bd-9f9a-9359ef032b49,2010-08-15 00:35:01.0,511.0,"A related question might be -- under what conditions can you reliably extract causal relations from data?\\n\\nA 2008 NIPS <a href=""http://jmlr.csail.mit.edu/proceedings/papers/v6/"">workshop</a> addressed that. One of the tasks was to infer the direction of causality from observations of pairs of variables where one variable was known to cause another, and the best method was able to correctly extract causal direction 80% of the time.",,
3910,5,1698,acc7afa6-01dc-4568-aa6d-b5c996e224e8,2010-08-15 00:40:38.0,511.0,"A related question might be -- under what conditions can you reliably extract causal relations from data?\\n\\nA 2008 NIPS <a href=""http://jmlr.csail.mit.edu/proceedings/papers/v6/"">workshop</a> try to address that question empirically. One of the tasks was to infer the direction of causality from observations of pairs of variables where one variable was known to cause another, and the best method was able to correctly extract causal direction 80% of the time.",added 26 characters in body,
3911,2,1699,fd477111-bd91-4468-8884-44d992927821,2010-08-15 03:10:05.0,862.0,"I am trying to solve for an efficient portfolio in R. How do I translate my constraints for a tangency point for 2 risky asset portfolio, and a given risk free rate to R solve.QP function? So basically I have the following equations:\\n\\n    w = weight of the first risky asset\\n    R1 = mean return of the first risky asset\\n    R2 = mean return of the second risky asset\\n    sd1 = sdev of first risky asset\\n    sd2 = sdev of second risky asset\\n    corr = correlation between two risky assets\\n    rf = risk free rate\\n    Return of portfolio, R = R2*(1-w)+R1*w\\n    Standard Dev of portfolio, SD = sqrt((sd1*w)^2+(sd2*(1-w))^2+2*w*(1-w)*corr*sd1*sd2)\\n    Now I need to maximize R-rf while minimizing SD (that is maximize my sharpe).\\n\\nI am confused how to express these constraints as expected by http://127.0.0.1:12359/library/quadprog/html/solve.QP.html. If you could also point me to a solved derivation of the final formula, that would be helpful as well as I am unable to get to the final formula.\\n",,
3912,1,1699,fd477111-bd91-4468-8884-44d992927821,2010-08-15 03:10:05.0,862.0,tangency portfolio in R,,
3913,3,1699,fd477111-bd91-4468-8884-44d992927821,2010-08-15 03:10:05.0,862.0,<r><finance><maximum>,,
3914,5,1699,afc16f00-407b-4bc3-88f0-defaeb8acfc2,2010-08-15 03:28:04.0,862.0,"I am trying to solve for an efficient portfolio in R. How do I translate my constraints for a tangency point for 2 risky asset portfolio, and a given risk free rate to R solve.QP function? So basically I have the following equations:\\n\\n    w = weight of the first risky asset\\n    R1 = mean return of the first risky asset\\n    R2 = mean return of the second risky asset\\n    sd1 = sdev of first risky asset\\n    sd2 = sdev of second risky asset\\n    corr = correlation between two risky assets\\n    rf = risk free rate\\n    Return of portfolio, R = R2*(1-w)+R1*w\\n    Standard Dev of portfolio, SD = sqrt((sd1*w)^2+(sd2*(1-w))^2+2*w*(1-w)*corr*sd1*sd2)\\n\\n    Now I need to maximize R-rf while minimizing SD (that is maximize my sharpe). \\n    Let sigma be covariance matrix. So my function to minimize is W^T*sigma*W where W is\\n    the weights vector. Now simulataneously I need to maximize the excess return (R-rf)\\n    and W^T*1=1. I don't know how to express that in the constraints function.\\n\\nI am confused how to express these constraints as expected by http://127.0.0.1:12359/library/quadprog/html/solve.QP.html. If you could also point me to a solved derivation of the final formula, that would be helpful as well as I am unable to get to the final formula.\\n",added 247 characters in body; added 15 characters in body,
3915,2,1700,a7cb44d9-42f2-4c3a-b890-eef0c32a3ab6,2010-08-15 05:29:00.0,139.0,"What you're discovering is a degree of instability in either the algorithm or the data itself. The approach termed 'consensus' or 'ensemble' clustering is a way of dealing with the problem. The problem there is: given a collection of clusterings, find a ""consensus"" clustering that is in some sense the ""average"" of the clusterings. \\n\\nThere's a fair bit of work on this topic, and a good place to start is the [clustering ensembles paper by Strehl and Ghosh][1]. \\n\\n\\n  [1]: http://strehl.com/download/strehl-jmlr02.pdf ",,
3916,2,1701,6c53d0a7-c481-440a-912a-49554a07b5db,2010-08-15 06:13:47.0,881.0,"Which flat-clustering algorithm are you using? It might also be the case that the different results are because maybe it's not your data but your algorithm itself is non-deterministic (e.g., using K-means with random initialization, or using a model-based clustering with EM or MCMC for inference with random initialization)?",,
3917,5,1520,501fa4b9-2d9e-4ca6-b515-c7cc45f5b7d7,2010-08-15 06:51:45.0,8.0,"Came across an interesting problem today. You are given a coin and x money, you double money if you get heads and lose half if tails on any toss.\\n\\n   1. What is the expected value of your money in n tries\\n   2. What is the probability of getting more than expected value in (1)\\n\\nThis is how I approached it. The probability of heads and tails is same (1/2). Expected value after first toss = 1/2(2*x) + 1/2(1/2*x) = 5x/4 So expected value is 5x/4 after first toss. Similarly repeating second toss expectation on 5x/4, Expected value after second toss = 1/2(2*5x/4) + 1/2(1/2*5x/4) = 25x/16\\n\\nSo you get a sequence of expected values: 5x/4, 25x/16, 125x/64, ...\\n\\nAfter n tries, your expected value should be 5^n/4^n * x.\\n\\nIf n is large enough, your expected value should approach the mean of the distribution. So probability that value is greater than expected value should be 0.5. I am not sure about this one.\\n",Changed dice to coin in the title,
3918,4,1520,501fa4b9-2d9e-4ca6-b515-c7cc45f5b7d7,2010-08-15 06:51:45.0,8.0,The expected value of random variable on tosses of a  coin,Changed dice to coin in the title,
3919,6,1520,501fa4b9-2d9e-4ca6-b515-c7cc45f5b7d7,2010-08-15 06:51:45.0,8.0,<probability><stochastic-processes><games>,Changed dice to coin in the title,
3920,5,1699,4d305f07-f5dc-4308-b08e-92620f227617,2010-08-15 08:41:18.0,88.0,"I am trying to solve for an efficient portfolio in R. How do I translate my constraints for a tangency point for 2 risky asset portfolio, and a given risk free rate to R solve.QP function? So basically I have the following equations:\\n\\n    w = weight of the first risky asset\\n    R1 = mean return of the first risky asset\\n    R2 = mean return of the second risky asset\\n    sd1 = sdev of first risky asset\\n    sd2 = sdev of second risky asset\\n    corr = correlation between two risky assets\\n    rf = risk free rate\\n    Return of portfolio, R = R2*(1-w)+R1*w\\n    Standard Dev of portfolio, SD = sqrt((sd1*w)^2+(sd2*(1-w))^2+2*w*(1-w)*corr*sd1*sd2)\\n\\n    Now I need to maximize R-rf while minimizing SD (that is maximize my sharpe). \\n    Let sigma be covariance matrix. So my function to minimize is W^T*sigma*W where W is\\n    the weights vector. Now simulataneously I need to maximize the excess return (R-rf)\\n    and W^T*1=1. I don't know how to express that in the constraints function.\\n\\nI am confused how to express these constraints as expected by http://pbil.univ-lyon1.fr/library/quadprog/html/solve.QP.html . If you could also point me to a solved derivation of the final formula, that would be helpful as well as I am unable to get to the final formula.\\n",Link now works.,
3921,5,1661,e8fdd0aa-de11-4686-aaeb-64259f8a792d,2010-08-15 08:47:32.0,88.0,"X1 is wing length, X2 is tail length for 45 male and 45 female bugs.  \\nWhich 2-sample univariate t-test should I use? \\nMy thought was to use Hotelling's T-square?  \\nBut Hotelling's is multi-variate not univariate. Now, I'm not sure...  \\n\\nAny ideas?",Fixed spelling.,
3922,2,1702,14b62e34-c50b-406e-a031-dfa4401d358a,2010-08-15 08:59:45.0,88.0,"You've mentioned some ML techniques, so two quite nice books (quite because unfortunately my favorite is in Polish):  \\nhttp://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1420067184  \\nhttp://ai.stanford.edu/~nilsson/mlbook.html\\n\\nFor numeric stuff like random number generation:  \\nhttp://www.nr.com/\\n\\n",,
3923,16,1702,14b62e34-c50b-406e-a031-dfa4401d358a,2010-08-15 08:59:45.0,-1.0,,,
3927,5,114,811a7cab-2a59-41e9-931d-f3032eb2685f,2010-08-15 11:35:01.0,509.0,I currently subscribe to \\n\\n1. Andrew Gelman's [blog][1]\\n2. Christian Roberts' [blog][2]\\n3. Darren Wilkinson's [blog][3]\\n\\nWhat other research-level statistical blogs are there?\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/blog/\\n  [2]: http://xianblog.wordpress.com/\\n  [3]: http://darrenjw.wordpress.com/\\n,deleted 1 characters in body,
3934,2,1706,0bb36ee7-a4aa-4130-9a15-691858da7dd3,2010-08-15 12:30:19.0,5.0,"I haven't looked at your code yet, but here are two pointers:\\n\\n - Rmetrics has the tangencyPortfolio function in the fPortfolio package: http://help.rmetrics.org/fPortfolio/html/class-fPORTFOLIO.html\\n - Here is a solution from David Ruppert's ""Statistics and Finance"" book: http://www.stat.tamu.edu/~ljin/Finance/chapter5/Fig5_9.txt",,
3936,2,1708,ff305186-0bd0-4bdb-8488-e20aee06a3df,2010-08-15 14:10:53.0,952.0,"I have weights of SNP variation (output through Eigenstrat program) for each SNP for the three main PCs. I wish to reduce my list of SNPs to those that show maximum differentiation between the three PCs. Can anyone help me with which statistical method to use to do this. \\n\\nsay, if each PC describes the magnitude of variation, how can I find mutually exclusive rows or rows that define the strongest differenciator of each PC.\\ne.g:\\n\\n\\n>SNPNam PC1-wt PC 2-wt PC3-wt  \\nSNP_1 -1.489 -0.029 -0.507  \\nSNP_2 -1.446 -0.816 0.661  \\nSNP_3 -1.416 0.338 1.631  \\nSNP_4 -1.392 -1.452 0.062  \\nSNP_5 -1.278 0.362 -1.006  \\nSNP_6 -1.21 0.514 0.144  \\nSNP_7 -1.119 -0.633 0.163  \\nSNP_8 -1.112 -0.193 -0.256  \\nSNP_9 -1.054 1.081 -1.519  \\nSNP_10 -0.936 -1.052 -0.419  \\nSNP_11 -0.861 0.381 -0.207  \\nSNP_12 -0.662 0.852 -0.211    \\nSNP_13 -0.503 -1.602 0.585  \\nSNP_14 -0.417 0.529 -1.003  \\nSNP_15 0.101 -0.85 -0.258  \\nSNP_16 0.198 -0.435 -1.599  \\nSNP_17 0.588 -1.292 -1.257  \\nSNP_18 1.167 0.891 1.106  \\nSNP_19 1.35 0.036 0.729  \\nSNP_20 1.532 1.599 0.499  \\n\\nAny help regarding which test to perform and how to (which package) is very much appreciated.\\n",,
3937,1,1708,ff305186-0bd0-4bdb-8488-e20aee06a3df,2010-08-15 14:10:53.0,952.0,Variation in PCA weights,,
3938,3,1708,ff305186-0bd0-4bdb-8488-e20aee06a3df,2010-08-15 14:10:53.0,952.0,<pca>,,
3939,2,1709,284463bf-bb5f-4d07-b4a1-6d9da727982d,2010-08-15 14:30:23.0,198.0,"I have collected positional data. To visualize the data, I'd like to draw a 'typical' distribution of the data. For example, if I get on average 5 hits per experiment, my typical distribution would contain 5 points. But how should they be distributed?\\n\\nTo simplify the problem, assume that the data follows a 2D normal distribution. If I were just to randomly draw 5 points from the distribution, I might get one point at [3,3], which would be a very rare outcome, and would thus not reflect the 'typical', or 'average' outcome. However, just drawing 5 points at [0,0] would also not make sense - even though [0,0] is the average position of the objects, 5 overlapping points are not an 'average' outcome of the process, either.\\n\\nIn other words, how can I get a 'likely' draw from a distribution?",,
3940,1,1709,284463bf-bb5f-4d07-b4a1-6d9da727982d,2010-08-15 14:30:23.0,198.0,How to draw a probable outcome from a distribution?,,
3941,3,1709,284463bf-bb5f-4d07-b4a1-6d9da727982d,2010-08-15 14:30:23.0,198.0,<distributions><data-visualization>,,
3942,5,1709,82275d3c-8505-45b3-afab-b82ddaca9c75,2010-08-15 15:26:31.0,198.0,"I have collected positional data. To visualize the data, I'd like to draw a 'typical' outcome of an experiment. \\n\\nThe data comes from a few hundred experiments, where I identify a variable number of objects at different positions relative to the origin in 2D. Thus, I can calculate the average number of objects, as well as estimate the empirical distribution of the objects. A plot of the 'typical' outcome would then have the average (or possibly mode) number of objects, say, 5. What I'm not sure about is where to position these 5 objects.\\n\\nTo simplify the problem, assume that the data follows a 2D normal distribution. If I were just to randomly draw 5 points from the distribution, I might get one point at [3,3], which would be a very rare outcome, and would thus not reflect the 'typical', or 'average' outcome. However, just drawing 5 points at [0,0] would also not make sense - even though [0,0] is the average position of the objects, 5 overlapping points are not an 'average' outcome of the process, either.\\n\\nIn other words, how can I get a 'likely' draw from a distribution?\\n",clarification,
3943,2,1710,4efc9c40-fd30-4c14-a821-955cfc3accd4,2010-08-15 15:33:31.0,582.0,"I've recently been to a conference and one of the speakers gave this very interesting example (although the point was to illustrate something else):\\n\\n- Americans and English eat a lot of fat food. There is a high rate of cardiovascular diseases in US and UK.\\n\\n- French eat a lot of fat food, but they have a low(er) rate of cardiovascular diseases.\\n\\n- Americans and English drink a lot of alcohol. There is a high rate of cardiovascular diseases in US and UK.\\n\\n- Italians drink a lot of alcohol but, again, they have a low(er) rate of cardiovascular diseases.\\n\\nThe conclusion? Eat and drink what you want.\\nAnd you have a higher chance of getting a heart attack if you speak English!",,
3944,2,1711,516d75a8-b34c-4096-85df-4db357e1fbdf,2010-08-15 15:47:36.0,582.0,"One thing that you could do is to plot the position of all your experiments in the 2D plane, one point for each object, maybe colored by experiment (if you have a lot of experiments you may just plot a random subset of them).\\n\\nIf there is a pattern in the position of the objects it should emerge when doing this.\\n\\nAlso, depending on what you are measuring, maybe is not the absolute position that counts but the relative position of the objects. In that case you could rotate the positions around the origin so that for each experiment the first point always lies, for instance, on the x axis.",,
3945,2,1712,81e9b661-261c-45d6-a276-1a9e4b387c02,2010-08-15 16:05:02.0,88.0,"Maybe you could use a [smoothed scatterplot][1]? It is an analogy to kernel density approximation, but in 2D.\\n\\n\\n  [1]: http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=139",,
3946,2,1713,641907d0-2f54-4fe3-ab4c-9c2908c24425,2010-08-15 17:01:49.0,339.0,"For some measurements, the results of an analysis are appropriately presented on the transformed scale. In most of the cases, however, it's desirable to present the results on the original scale of measurement (otherwise your work is more or less worthless). \\n\\nFor example, in case of log-transformed data, a problem with interpretation on the original scale arises because the mean of the logged values is not the log of the mean. Taking the antilogarithm of the estimate of the mean on the log scale does not give an estimate of the mean on the original scale.\\n\\nIf, however, the log-transformed data have symmetric distributions, the following\\nrelationships hold (since the log preserves ordering):\\n\\n    Mean[log (Y)] = Median[log (Y)] = log[Median (Y)]\\n\\n(the antilogarithm of the mean of the log values is the median on the original scale of measurements).\\n\\nSo I only can make inferences about the difference (or the ratio) of the medians on the original scale of measurement.\\n\\nTwo-sample t-tests and confidence intervals are most reliable if the populations are roughly normal with approximately standard deviations, so we may be tempted to use the `Box-Cox` transformation for the normality assumption to hold (I also think that it is a variance stabilizing transformation too). \\n\\nHowever, if we apply t-tools to `Box-Cox` transformed data , we will get inferences about the difference in means of the transformed data. How can we interpret those on the original scale of measurement? (The mean of the transformed values is not the transformed mean). In other words, taking the inverse transform of the estimate of the mean, on the transformed scale, does not give an estimate of the mean on the original scale.\\n\\nCan I also make inferences only about the medians in this case? Is there a transformation that will allow me to go back to the means (on the original scale) ?\\n\\n*This question was initially posted as a comment [here][1]* \\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1601/normalizing-transformation-options/1607#1607",,
3947,1,1713,641907d0-2f54-4fe3-ab4c-9c2908c24425,2010-08-15 17:01:49.0,339.0,"Express answers in terms of original units, in Box-Cox transformed data.",,
3948,3,1713,641907d0-2f54-4fe3-ab4c-9c2908c24425,2010-08-15 17:01:49.0,339.0,<data-transformation><confidence-interval><t-test><interpretation>,,
3949,2,1714,d93f72a4-646b-443f-8ee6-e5a384300434,2010-08-15 17:26:03.0,957.0,[Viewpoints][1] is useful for multi-variate data sets.\\n\\n\\n  [1]: http://astrophysics.arc.nasa.gov/~pgazis/viewpoints.htm,,
3950,16,1714,d93f72a4-646b-443f-8ee6-e5a384300434,2010-08-15 17:26:03.0,-1.0,,,
3951,2,1715,27795561-4977-405b-aff3-098ccd4241d6,2010-08-15 18:36:58.0,,To understand God's Thoughts\\nwe must study statistics\\nfor these are the measure\\nof His purpose.\\n\\nFlorence Nightingale\\n\\n,,Charles Berry
3952,16,1715,27795561-4977-405b-aff3-098ccd4241d6,2010-08-15 18:36:58.0,-1.0,,,
3953,2,1716,382ef1c6-e658-463e-915a-9ca5a80d61ba,2010-08-15 18:48:27.0,8.0,"You're question seems confused, however I think you just want to explore multivariate data. Here my suggestions:\\n\\n1. If you sample from the distribution (in your example a 2d Normal), then choosing the point (3, 3) will be very unlikely. By sample, I mean generating random numbers following that distribution. It doesn't make sense taking about ""typical"" data points when your data is continuous.\\n1. You mention that you want to summarise the data using a ""few points"". If this is the case, why not just calculate the mean and variances. You could plot them using [geom_errorbar][1] in ggplot2.\\n1. Alternatively, you could plot the data using [boxplots][2].\\n1. Another suggestion is just to [bootstrap][3] your data.\\n\\n\\n  [1]: http://had.co.nz/ggplot2/geom_errorbar.html\\n  [2]: http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=6\\n  [3]: http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29",,
3954,2,1717,64437588-f2b6-4f9e-a971-5658ea26e215,2010-08-15 19:14:50.0,339.0,"I also think that it's not clear what you want. But if you want a set of *deterministically* chosen points, so that they preserve the moments of the initial distribution, you can use the *sigma point* selection method that applies to the [unscented Kalman filter][1].  \\n\\nSay that you want to select $2L+1$ points that fulfill those requirements. Then proceed in the following way:\\n\\n$\\mathcal{X}_0=\\overline{x} \\qquad w_0=\\frac{\\kappa}{L+\\kappa} \\qquad i=0$ \\n\\n$\\mathcal{X}_i=\\overline{x}+\\left(\\sqrt{(\\:L+\\kappa\\:)\\:\\mathbf{P}_x}\\right)_i \\qquad w_i=\\frac{1}{2(L+\\kappa)} \\qquad i=1, \\dots,L$ \\n\\n$\\mathcal{X}_i=\\overline{x}-\\left(\\sqrt{(\\:L+\\kappa\\:)\\:\\mathbf{P}_x}\\right)_i \\qquad w_i=\\frac{1}{2(L+\\kappa)} \\qquad i=L+1, \\dots,2L$ \\n\\nwhere $w_i$ the weight of the i-th point,\\n\\n$\\kappa=3-L$ (in case of Normally distributed data),\\n\\nand $\\left(\\sqrt{(\\:L+\\kappa\\:)\\mathbf{P}_x}\\right)_i$ is the i-th row (or column)* of the *matrix square root* of the weighted covariance $(\\:L+\\kappa\\:)\\:\\mathbf{P}_x$ matrix (usually given by the [Cholesky decomposition][2])\\n\\n\\* If the *matrix square root* $\\mathbf{A}$ gives the original by giving $\\mathbf{A}^T\\mathbf{A}$, then use the *rows* of $\\mathbf{A}$. If it gives the original by giving $\\mathbf{A}\\mathbf{A}^T$, then use the *columns* of $\\mathbf{A}$. The result of the matlab function [chol()][3] falls into the first category.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/UKF#Unscented_Kalman_filter\\n  [2]: http://en.wikipedia.org/wiki/Cholesky_decomposition\\n  [3]: http://www.mathworks.com/access/helpdesk/help/techdoc/ref/chol.html",,
3955,5,1709,a8c71b3f-68aa-4136-afad-72e6b4276bf4,2010-08-15 20:04:03.0,198.0,"I have collected positional data. To visualize the data, I'd like to draw a 'typical' outcome of an experiment. \\n\\nThe data comes from a few hundred experiments, where I identify a variable number of objects at different positions relative to the origin in 2D. Thus, I can calculate the average number of objects, as well as estimate the empirical distribution of the objects. A plot of the 'typical' outcome would then have the average (or possibly mode) number of objects, say, 5. What I'm not sure about is where to position these 5 objects.\\n\\nTo simplify the problem, assume that the data follows a 2D normal distribution. If I were just to randomly draw 5 points from the distribution, I might get one point at [3,3], which would be a very rare outcome, and would thus not reflect the 'typical', or 'average' outcome. However, just drawing 5 points at [0,0] would also not make sense - even though [0,0] is the average position of the objects, 5 overlapping points are not an 'average' outcome of the process, either.\\n\\nIn other words, how can I get a 'likely' draw from a distribution?\\n\\n--------\\n\\n**EDIT** \\n\\nIt looks like I should mention *why* I don't want to use the usual methods (like a 2D smoothed histogram, or plotting all the many points) to look at the 2D distribution. \\n\\n1. The objects (which are vesicles (i.e. little spheres) inside cells) vary in number, size and position (distribution of the distance from the cell center, amount of clustering). I would like to display all these features in one graph. Since there are several hundred cells containing many vesicles each, it is not very useful to combine them all in a single plot. I am well aware that I could use a multipanel graph showing the distributions of all parameters, but this would be a lot less intuitive. \\n2. I would like to show a 'typical' cell that shows all the salient features that characterize a specific phenotype. This way, if I want to image a particular phenotype in a mixed population, I know what kind of cell I'm looking for.\\n3. I think such a plot would be a cool way to display a lot of information at once, and I just want to try.\\n\\nMaybe it would be clearer If I said that I want to simulate a likely experimental result based on my measurements?",clarification; added 118 characters in body,
3956,2,1718,4a48707b-2771-477d-8093-60d4ad7d2872,2010-08-15 22:03:07.0,919.0,"This is a counting problem: there are *b^n* possible birthday assignments for *n* people.  Of those, let *q(k; n, b)* be the number of assignments for which no birthday is shared by more than *k* people but at least one birthday actually is shared by *k* people.  The probability we seek can be found by summing the *q(k;n,b)* for appropriate values of *k* and multiplying the result by *b^(-n)*.\\n\\nThese counts can be found exactly for values of *n* less than several hundred.  However, they will not follow any straightforward formula:  **we have to consider the patterns of ways in which birthdays can be assigned**.  I will illustrate this in lieu of providing a general demonstration.  Let *n* = 4 (this is the smallest interesting situation).  The possibilities are:\\n\\n - Each person has a unique birthday; the code is {4}.\\n - Exactly two people share a birthday; the code is {2,1}.\\n - Two people have one birthday and the other two have another; the code is {0,2}.\\n - Three people share a birthday; the code is {1,0,1}.\\n - Four people share a birthday; the code is {0,0,0,1}.\\n\\nGenerally, the code {*a*[1], *a*[2], ...} is a tuple of counts whose *k* th element stipulates how many distinct birthdates are shared by exactly *k* people.  Thus, in particular,\\n\\n> *1 a[1] + 2a[2] + ... + k a[k] + ... = n*.\\n\\nNote, even in this simple case, that there are two ways in which the maximum of two people per birthday is attained: one with the code {0,2} and another with the code {2,1}.\\n\\nWe can directly count the number of possible birthday assignments corresponding to any given code.  This number is the product of three terms.  One is a multinomial coefficient; it counts the number of ways of partitioning *n* people into *a[1]* groups of 1, *a[2]* groups of 2, and so on.  Because the sequence of groups does not matter, we have to divide this multinomial coefficient by *a*[1]!*a*[2]!... ; its reciprocal is the second term.  Finally, line up the groups and assign them each a birthday: there are *b* candidates for the first group, *b*-1 for the second, and so on.  These values have to be multiplied together, forming the third term.  It is equal to the ""factorial product"" *b^((a[1]+a[2]+...))* where *b^((m))* means *b(b-1)...(b-m+1)*.\\n\\nThere is an obvious and fairly simple recursion relating the count for a pattern *{a[1], ..., a[k]}* to the count for the pattern *{a[1], ..., a[k-1]}*.  This enables rapid calculation of the counts for modest values of *n*.\\n\\nI doubt there is a closed form formula for  *q(k; n, b)*, which is obtained by summing the counts for all partitions of *n* whose maximum term equals *k*.  Let me offer some examples:\\n\\nWith ***b* = 5** (five possible birthdays) and ***n* = 4** (four people), we obtain\\n\\n> *q*(1) = *q*(1;4,5) = 120\\n> \\n> *q*(2) = 360 + 60 = 420\\n> \\n> *q*(3) = 80\\n> \\n> *q*(4) = 5.\\n\\nWhence, for example, the chance that three or more people out of four share the same ""birthday"" (out of 5 possible dates) equals (80 + 5)/625 = 0.136.\\n\\nAs another example, take ***b* = 365** and ***n* = 23**.  Here are the values of *q*( *k*;23,365) for the smallest *k* (to six sig figs only):\\n\\n> *k*=1: 0.49270\\n> \\n> *k*=2: 0.494592\\n> \\n> *k*=3: 0.0125308\\n> \\n> *k*=4: 0.000172844\\n> \\n> *k*=5: 1.80449E-6\\n> \\n> *k*=6: 1.48722E-8\\n> \\n> *k*=7: 9.92255E-11\\n> \\n> *k*=8: 5.45195E-13\\n\\nUsing this technique, we can readily compute that there is about a 50% chance of (at least) a three-way birthday collision among 87 people, a 50% chance of a four-way collision among 187, and a 50% chance of a five-way collision among 310 people.  That last calculation starts taking a few seconds (in Mathematica, anyway) because the number of partitions to consider starts getting large.  For substantially larger *n* we need an approximation.\\n\\nOne approximation is obtained by means of the Poisson distribution with expectation *n/b*, because we can view a birthday assignment as arising from *b* almost (but not quite) independent Poisson variables each with expectation *n/b*: the variable for any given possible birthday describes how many of the *n* people have that birthday.  The distribution of the maximum is therefore approximately *F(k)^b* where *F* is the Poisson CDF.  This is not a rigorous argument, so let's do a little testing.  The approximation for ***n* = 23**, ***b* = 365** gives\\n\\n> *k*=1: 0.498783\\n> \\n> *k*=2: 0.496803\\n> \\n> *k*=3: 0.014187\\n> \\n> *k*=4: 0.000225115\\n\\nBy comparing with the preceding you can see that the relative probabilities can be poor when they are small, but the absolute probabilities are reasonably well approximated to about 0.5%.  Testing with a wide range of *n* and *b* suggests the approximation is usually about this good.\\n\\nTo wrap up, let's consider the original question: take ***n* = 10,000** (the number of observations) and ***b* = 1,000,000** (the number of possible ""structures,"" approximately).  The approximate distribution for the maximum number of ""shared birthdays"" is\\n\\n> *k*=1: 0\\n> \\n> *k*=2: 0.8475+\\n> \\n> *k*=3: 0.1520+\\n> \\n> *k*=4: .0004+\\n> \\n> *k*>4: < 1E-6\\n\\n(This is a fast calculation.)  Clearly, observing one structure 10 times out of 10,000 would be highly significant.  Because *n* and *b* are both large, I expect the approximation to work quite well here.\\n\\nIncidentally, as Shane intimated, simulations can provide useful checks.  A Mathematica simulation is created with a function like\\n\\n`simulate[n_, b_] :=  Max[Last[Transpose[Tally[RandomInteger[{0, b - 1}, n]]]]];`\\n\\nwhich is then iterated and summarized, as in this example which runs 10,000 iterations of the ***n* = 10,000**, ***b* = 1,000,000** case:\\n\\n`Tally[Table[simulate[10000, 1000000], {n, 1, 10000}]] // TableForm`\\n\\nIts output is\\n\\n> 2 8503\\n> \\n> 3 1493\\n> \\n> 4 4\\n\\nThese frequencies closely agree with those predicted by the Poisson approximation.",,
3957,2,1719,273b93b9-3c46-4b3d-a426-33724b68a027,2010-08-15 22:51:09.0,59.0,"Well, I'm an engineer by day. Although most of my work revolves around modeling, we generally do pretty basic stuff. An ""Advanced"" model would be a monte carlo simulation validated using R2 tests. \\n\\nCurrently, in my field, there is a lot of research using Logistic and bayesian analysis. \\n\\nMy question is, which courses would you recommend someone to take from [MIT's open course site][1] or any other sites, for someone who learns best by video/audio first, and reading second?\\n\\nWhat i'd like to learn are the following:\\n\\n - Be able to understand the models and when to employ them\\n - able to take in field data (which is generated once and cannot be regenerated) and design and perform experiments\\n - Able to understand the results, look at them, and figure out if something is off, ""show stopper"" or ""outliers"", or if everything is fine and dandy\\n - Be able to validate and calibrate the model, to actual ""As-built"" results\\n - Be able to forecast the results using appropriate sensitivity analysis\\n - be able to forecast / ""plug"" missing data\\n - be able to write journal papers related to my field\\n\\n\\nmy field in a nutshell is: transportation demand modeling for passenger vehicles, using either the generic four step model, or socio economic activity/tour based models such as PECAS or urbansim\\n\\n\\n\\n  [1]: http://ocw.mit.edu/courses/mathematics/",,
3958,1,1719,273b93b9-3c46-4b3d-a426-33724b68a027,2010-08-15 22:51:09.0,59.0,Which courses to take from the openMIT website,,
3959,3,1719,273b93b9-3c46-4b3d-a426-33724b68a027,2010-08-15 22:51:09.0,59.0,<bayesian><logistic><beginner>,,
3960,4,1719,4309ae14-0115-4c20-a984-958aea6e6232,2010-08-15 22:59:29.0,59.0,Video/Audio online material for getting into Bayesian analysis and logistic-regressions ,edited title,
3961,6,1719,9cbf77af-85db-4006-bc31-7719db5e2495,2010-08-15 23:22:39.0,159.0,<bayesian><logistic>,edited tags,
3962,2,1720,5b6eacb5-831a-4e14-a490-d1f9c04a8404,2010-08-15 23:35:52.0,159.0,"If the Box-Cox transformation yields a symmetric distribution, then the mean of the transformed data is back-transformed to the median on the original scale. This is true for any monotonic transformation, including the Box-Cox transformations, the IHS transformations, etc. So inferences about the means on the transformed data correspond to inferences about the median on the original scale.\\n\\nAs the original data were skewed (or you wouldn't have used a Box-Cox transformation in the first place), why do you want inferences about the means? I would have thought working with medians would make more sense in this situation. I don't understand why this is seen as a ""problem with interpretation on the original scale"".\\n",,
3963,2,1721,e84f5350-fba9-4230-a0b9-8dbe01566107,2010-08-15 23:39:20.0,282.0,"So it turns out the first assumption was actually correct:  U is indeed the first k eigenvectors of C, that we calculate from G by means of the eigendecompposition $(X_tVD^{-\\frac12})$.  \\n\\n",,
3964,2,1722,cec3a284-1585-44c7-9118-c26ad422b82a,2010-08-15 23:43:45.0,959.0,"[This paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.3681&rep=rep1&type=pdf) is very interesting, has good results, and is easy to apply to discriminative models. \\n\\nAnd the term is semi-supervised learning, not unsupervised learning.",,
3965,2,1723,c735c981-a419-4482-9090-ed2449100f18,2010-08-15 23:59:19.0,805.0,"For normality, actual Shapiro-Wilk has good power in fairly small samples. \\n\\nThe main competitor in studies that I have seen is the more general Anderson-Darling, which does fairly well, but I wouldn't say it was better. If you can clarify what alternatives interest you, possibly a better statistic would be more obvious.\\n\\n[I strongly recommend against considering Jarque-Bera in small samples (which probably better known as Bowman-Shenton in statistical circles - they studied the small sample distribution). The asymptotic joint distribution of skewness and kurtosis is *nothing like* the small-sample distribution. It also has very low power against some interesting alternatives - for example it is powerless to pick up a symmetric *bimodal* distribution that has kurtosis close to that of a normal distribution.]\\n\\nFrequently people test goodness of fit for what turn out to be not-particularly-good reasons, or they're answering a question other than the one that they actually want to answer. \\n\\nFor example, you almost certainly already know your data aren't really normal (not exactly), so there's no point in trying to answer a question you know the answer to - and the hypothesis test *doesn't actually answer it anyway*. \\n\\nYour hypothesis test is giving you an answer to a question closer to ""is my sample size large enough to pick up the amount of non-normality that I have"", while the real question is usually closer to ""what is the *impact* of this non-normality on these other things I'm interested in?"". \\n\\nThere are times when testing of normality makes some sense, but those situations almost never occur with small samples.\\n\\nWhy are you testing normality?\\n\\n",,
3966,5,1723,bed27aa7-ad05-41a9-b252-030fe1f71c58,2010-08-16 00:05:54.0,805.0,"For normality, actual Shapiro-Wilk has good power in fairly small samples. \\n\\nThe main competitor in studies that I have seen is the more general Anderson-Darling, which does fairly well, but I wouldn't say it was better. If you can clarify what alternatives interest you, possibly a better statistic would be more obvious.\\n\\n[I strongly recommend against considering Jarque-Bera in small samples (which probably better known as Bowman-Shenton in statistical circles - they studied the small sample distribution). The asymptotic joint distribution of skewness and kurtosis is *nothing like* the small-sample distribution - in the same way a banana doesn't look much like an orange. It also has very low power against some interesting alternatives - for example it is powerless to pick up a symmetric *bimodal* distribution that has kurtosis close to that of a normal distribution.]\\n\\nFrequently people test goodness of fit for what turn out to be not-particularly-good reasons, or they're answering a question other than the one that they actually want to answer. \\n\\nFor example, you almost certainly already know your data aren't really normal (not exactly), so there's no point in trying to answer a question you know the answer to - and the hypothesis test *doesn't actually answer it anyway*. \\n\\nGiven you know don't have exact normality already, your hypothesis test is giving you an answer to a question closer to ""is my sample size large enough to pick up the amount of non-normality that I have"", while the real question is usually closer to ""what is the *impact* of this non-normality on these other things I'm interested in?"". \\n\\nThere are times when testing of normality makes some sense, but those situations almost never occur with small samples.\\n\\nWhy are you testing normality?\\n\\n",added 60 characters in body; added 51 characters in body,
3967,2,1724,dd8e1ceb-d0f8-4eac-8c94-136d15dd651c,2010-08-16 00:30:59.0,805.0,"If you want to do inference about means on the original scale, you could consider using inference that doesn't use a normality assumption. Beware, however, simply plugging through a straight comparison of means via say resampling (either permutation tests or bootstrapping) when the two samples have different variances if your analysis assumes something different (and equal variances on the transformed scale will be difference variances on the original scale if the means differ).\\n\\nAnother approach to consider if you're more interested in estimation or prediction than testing is to use a Taylor expansion of the transformed variables to compute the approximate mean and variance after transforming back - where in the usual Taylor expansion you'd write f(x+h), you now write t[mu + (Y-mu)] where Y is a random variable with mean mu and variance sigma-squared, which you're about to transform back using t(). If you take expectations, the second term drops out, and people usually take the first and third terms (where the third represents an approximation to the bias in just transforming the mean), and if you take the variance of the expansion to the second term, the first term and the first covariance terms drop out - because t(mu) is a constant - leaving you with a single-term approximation for the variance.\\n\\nThe easiest case is when you have normality on the log-scale, and hence a lognormal on the original scale. If your variance is known (which happens very rarely at best), you can cosntruct lognormal CIs and PIs on the original scale, and you can give a predicted mean from the mean of the distribution of the relevant quantity.\\n\\nIf you're estimating both mean and variance on the log-scale, you can construct log-t intervals (prediction intervals for an observation, say), but your original-scale *log-t doesn't have any moments*. So the mean of a prediction just doesn't exist. \\n\\nYou need to think very carefully about precisely what question you're trying to answer.\\n",,
3968,5,1724,b1da68da-b6f2-4019-9e78-a0566c31abed,2010-08-16 00:36:15.0,805.0,"If you want to do inference about means on the original scale, you could consider using inference that doesn't use a normality assumption. Take care, however. Simply plugging through a straight comparison of means via say resampling (either permutation tests or bootstrapping) when the two samples have different variances may be a problem if your analysis assumes the variances to be equal (and equal variances on the transformed scale will be difference variances on the original scale if the means differ). Such techniques don't avoid the necessity to think about what you're doing.\\n\\nAnother approach to consider if you're more interested in estimation or prediction than testing is to use a Taylor expansion of the transformed variables to compute the approximate mean and variance after transforming back - where in the usual Taylor expansion you'd write f(x+h), you now write t[mu + (Y-mu)] where Y is a random variable with mean mu and variance sigma-squared, which you're about to transform back using t(). If you take expectations, the second term drops out, and people usually take the first and third terms (where the third represents an approximation to the bias in just transforming the mean), and if you take the variance of the expansion to the second term, the first term and the first covariance terms drop out - because t(mu) is a constant - leaving you with a single-term approximation for the variance.\\n\\nThe easiest case is when you have normality on the log-scale, and hence a lognormal on the original scale. If your variance is known (which happens very rarely at best), you can cosntruct lognormal CIs and PIs on the original scale, and you can give a predicted mean from the mean of the distribution of the relevant quantity.\\n\\nIf you're estimating both mean and variance on the log-scale, you can construct log-t intervals (prediction intervals for an observation, say), but your original-scale *log-t doesn't have any moments*. So the mean of a prediction just doesn't exist. \\n\\nYou need to think very carefully about precisely what question you're trying to answer.\\n",added 102 characters in body,
3969,5,1621,812b67e2-c2cb-4c14-9e3f-7588d2a0d22a,2010-08-16 00:49:12.0,511.0,"I came across a <a href=""http://www.citeulike.org/user/yaroslavvb/tag/information-geometry"">large</a> body of literature which advocates using Fisher's Information metric as a natural local metric in the space of probability distributions and then integrating over it to define distances and volumes.\\n\\nBut are these ""integrated"" quantities actually useful for anything? I found no theoretical justifications and very few practical applications. One is Guy Lebanon's <a href=""http://www.cs.cmu.edu/~lafferty/pub/hyperplane.pdf""> work</a> where he uses ""Fisher's distance"" to classify documents and another one is Rodriguez' <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.9259&rep=rep1&type=pdf"">ABC of Model Selection…</a> where ""Fisher's volume"" is used for model selection. Apparently, using ""information volume"" gives ""orders of magnitude"" improvement over AIC and BIC for model selection, but I haven't seen any follow up on that work.\\n\\nA theoretical justification might be to have a generalization bound which uses this measure of distance or volume and is better than bounds derived from MDL or asymptotic arguments, or a method relying on one of these quantities that's provably better in some reasonably practical situation, are there any results of this kind?",added 163 characters in body; edited tags,
3970,6,1621,812b67e2-c2cb-4c14-9e3f-7588d2a0d22a,2010-08-16 00:49:12.0,511.0,<model-selection>,added 163 characters in body; edited tags,
3971,2,1725,7ebc3403-8133-4cfb-b940-ec971c9d5952,2010-08-16 02:38:20.0,183.0,"I've only had a little look at this lecture series on Machine Learning, but it looks good.\\n\\n - http://academicearth.org/courses/machine-learning\\n\\n[Lecture 11][1] covers Bayesian Statistics and Regularization.\\n\\n\\n  [1]: http://freevideolectures.com/Course/2257/Machine-Learning/11",,
3972,2,1726,69aab656-e413-4b86-a284-8f211753aad7,2010-08-16 05:45:29.0,128.0,"""Do not trust any statistics you did not fake yourself"" -- Winston Churchill",,
3973,16,1726,69aab656-e413-4b86-a284-8f211753aad7,2010-08-16 05:45:29.0,-1.0,,,
3974,2,1727,4e7ff24a-8fdf-4ff8-aeb4-e22b999407d8,2010-08-16 06:19:27.0,961.0,Python's [matplotlib][1]\\n\\n\\n  [1]: http://matplotlib.sourceforge.net/,,
3975,16,1727,4e7ff24a-8fdf-4ff8-aeb4-e22b999407d8,2010-08-16 06:19:27.0,-1.0,,,
3976,16,188,00000000-0000-0000-0000-000000000000,2010-08-16 06:37:30.0,88.0,,,
3977,16,207,00000000-0000-0000-0000-000000000000,2010-08-16 06:37:30.0,88.0,,,
3978,16,438,00000000-0000-0000-0000-000000000000,2010-08-16 06:37:30.0,88.0,,,
3980,16,1288,00000000-0000-0000-0000-000000000000,2010-08-16 06:37:30.0,88.0,,,
3981,16,165,00000000-0000-0000-0000-000000000000,2010-08-16 06:37:30.0,88.0,,,
3982,2,1728,31e3dc74-2cb1-4aa8-9775-40271da9192d,2010-08-16 07:09:25.0,438.0,"I would go straight to [VideoLectures.net][1]. This is by far the best source--whether free or paid--i have found for very-high quality (both w/r/t the video quality and w/r/t the presentation content) video lectures and tutorials on statistics, forecasting, and machine learning. The target audience for these video lectures ranges from beginner (some lectures are specifically tagged as ""tutorials"") to expert; most of them seem to be somewhere in the middle.\\n\\nAll of the lectures and tutorials are taught to highly experienced professionals and academics, and in many instances, the lecturer is the leading authority on the topic he/she is lecturing on. The site is also 100% free.\\n\\nThe one disadvantage is that you cannot download the lectures and store them in e.g., itunes; however, nearly every lectures has a set of slides which you can download (or, conveniently, you can view them online as you watch the presentation).\\n\\nA few that i've watched and that i can recommend highly:\\n\\n - *Basics of Probability and Statistics*\\n\\n - *Introduction to Machine Learning*\\n\\n - *Gaussian Process Basics*\\n\\n - *Graphical Models*\\n\\n - *k-Nearest Neighbor Models*\\n\\n  [1]: http://videolectures.net/Top/#o=top&t=vl",,
3983,5,1728,765918bf-045f-4f1e-88ae-e1d56e4fb55a,2010-08-16 07:14:37.0,438.0,"I would go straight to [VideoLectures.net][1]. This is by far the best source--whether free or paid--i have found for very-high quality (both w/r/t the video quality and w/r/t the presentation content) video lectures and tutorials on statistics, forecasting, and machine learning. The target audience for these video lectures ranges from beginner (some lectures are specifically tagged as ""tutorials"") to expert; most of them seem to be somewhere in the middle. \\n\\nAll of the lectures and tutorials are taught to highly experienced professionals and academics, and in many instances, the lecturer is the leading authority on the topic he/she is lecturing on. The site is also 100% free.\\n\\nThe one disadvantage is that you cannot download the lectures and store them in e.g., itunes; however, nearly every lectures has a set of slides which you can download (or, conveniently, you can view them online as you watch the presentation).\\n\\nYouTube might have more, but even if you search Y/T through a specific channel, i am sure the signal-to-noise ratio is far higher--on VideoLectures.net, every lecture i've viewed has been outstanding and if you scan the viewer reviews, you'll find that's the consensus opinion towards the entire collection.\\n\\nA few that i've watched and that i can recommend highly:\\n\\n - *Basics of Probability and Statistics*\\n\\n - *Introduction to Machine Learning*\\n\\n - *Gaussian Process Basics*\\n\\n - *Graphical Models*\\n\\n - *k-Nearest Neighbor Models*\\n\\n  [1]: http://videolectures.net/Top/#o=top&t=vl",added two sentences for comparison with another Site.,
3984,2,1729,0bd2a13d-6eea-4879-b776-e4b1b6329f59,2010-08-16 08:11:26.0,913.0,"Dar list, \\n\\nI am trying to format the output from pairwise.t.test into latex, but have not found a way of doing this. Has anyone got any suggestions? \\n\\n**EDIT:** As this is a one-time only report where I do need to customize the variable names, and row-/column headings, I was hoping to avoid using Sweave.",,
3985,1,1729,0bd2a13d-6eea-4879-b776-e4b1b6329f59,2010-08-16 08:11:26.0,913.0,Export/format output from pairwise.t.test to Latex,,
3986,3,1729,0bd2a13d-6eea-4879-b776-e4b1b6329f59,2010-08-16 08:11:26.0,913.0,<t-test><post-hoc>,,
3987,6,1729,3a360208-653b-4dcb-b31f-71aa3044e153,2010-08-16 08:16:09.0,159.0,<r><t-test><post-hoc><latex>,edited tags,
3988,2,1730,5f2f9dac-edf9-4d6d-9ec2-aabc42939858,2010-08-16 08:27:13.0,251.0,"Does this help?\\n\\n    > library(xtable)\\n    > attach(airquality)\\n    > res <- pairwise.t.test(Ozone, Month)\\n    > xtable(res$p.value, caption=res$method)\\n    % latex table generated in R 2.9.2 by xtable 1.5-6 package\\n    % Mon Aug 16 04:24:21 2010\\n    \\begin{table}[ht]\\n    \\begin{center}\\n    \\begin{tabular}{rrrrr}\\n      \\hline\\n     & 5 & 6 & 7 & 8 \\\\ \\n      \\hline\\n    6 & 1.00 &  &  &  \\\\ \\n      7 & 0.00 & 0.05 &  &  \\\\ \\n      8 & 0.00 & 0.05 & 1.00 &  \\\\ \\n      9 & 1.00 & 1.00 & 0.00 & 0.00 \\\\ \\n       \\hline\\n    \\end{tabular}\\n    \\caption{t tests with pooled SD}\\n    \\end{center}\\n    \\end{table}\\n",,
3989,2,1731,75c11447-cebe-4829-9d05-91e025f8bdc1,2010-08-16 08:28:16.0,183.0,"One way that you can do the above task without using Sweave is as follows: \\n\\n1. Write a **custom export function** from R that takes the pairwise.t.test object and outputs latex code.\\n2. Use `\\input{...}` in your LaTeX document to **input** this file into your LaTeX document.\\n\\nWith regards to step 1, there are many functions that are useful. Here are a few:\\n\\n- `paste()` is useful for concatenating elements including latex elements with R objects\\n- `formatC()` is useful for formatting numbers\\n- `write()` is useful for exporting the combined LaTeX/R character vector from R to a text file\\n\\nThere are also a variety of LaTeX table generating functions (e.g., see the `xtable` package)\\n\\n",,
3990,5,1687,9a5fbd67-e353-4034-bb46-66956d0b7956,2010-08-16 09:02:11.0,937.0,"The problem I’m trying to solve is “How do I figure out how much gunpowder should I put into a cartridge so that I can give myself a good probability of making the minimum power factor?”\\n\\nI compete in USPSA/IPSC which requires that a competitors rounds make a minimum power factor. Power Factor is computed to be the FLOOR(average bullet velocity * bullet weight) / 1000) where velocity is in feet per second, and bullet weight is in grains. Note the use of FLOOR. No rounding is done. Only the integral part of the computation is used. The higher the power factor, the higher the felt recoil and harder it is to quickly do follow up shots. Since the sport is about firing shots as quickly and as accurately as possible, the lower the recoil the better.\\n\\nDifferent divisions within the sport have different power factor floors, but the particular division I compete in has a minimum of 165 Power Factor. The bullets I use are 180 gn bullets, and vary by about +/- 0.2 gn and is normally distributed. \\n\\nWhat makes this an interesting problem (and move it out of my meager stats and probability skills) is the testing procedure during a major match. Random sample of 8 rounds are collected. Of the 8 rounds, one is taken apart and the bullet is weighed for use in the formula above. Next, 3 rounds are fired and the average velocity is used. If the resulting power factor is below the minimum, then another 3 rounds are fired. The average of the 3 fastest velocities from the 6 rounds fired is now used to compute the power factor. If the resulting power factor is still below the floor, then the shooter has the option of having the last round taken apart and weighed or the last round fired. If the bullet is taken apart and it is heavier than the first bullet, the heavier weight is used to compute the power factor. If the last round is fired, the average of the 3 fastest velocities from the 7 rounds fired is used to compute the power factor.\\n\\nTo add spice to this problem, not all chronographs used to measure bullet velocities are created equal. The chronograph industry acknowledges that there can be as much as +/- 4% variance between chronographs of different brands. Even more interesting is that the rules allow for the same chronograph used for a particular match to have +/- 4% variance over the duration of a match. I don’t know if either of these 4% variances are normally distributed or not.\\n\\nWith my own chronograph, I test batches of a particular gunpowder load to get the average velocity and standard deviation. After statistical analyses of many different batches, I’ve confirmed that this data is normally distributed.\\n\\nThe way I’m currently determining my minimum load is by finding the load the gives me 165 < FLOOR( target * 179.9 / 1000) where target =  (average velocity - standard deviation) * fudge factor. For fudge factor, I’ve unscientifically chosen 1.04. The 0.04 is the 4% variance between chronographs, but ignores the day-to-day allowable variance. I chose to just subtract just 1 standard deviation because it’ll only be 16% of the time that one bullet will be below the floor. In my mind, the probability of all the first 3 bullets all going below the floor is 0.16^3 which less than half a percent.\\n\\nMy specific questions are: Am I going about computing the target the right way? Should my fudge factor include another 4% for the day-to-day variance allowed? Is the 1 standard deviation too much or too little? How should I write the formula for my target? \\n\\nEdit:\\nAfter Srikant's initial response below let me add a couple of focusing questions and notes.\\n\\nI understand figuring out the error due to my measurements. Not much problem there unless I get really sloppy with quality control or maintenance. \\n\\nMy grasp of probabilities is weak so please bear with me as I ask about computing probabilities:\\n\\n1) One of the key issues I need to deal with is figuring out how to correctly compute the probability for the testing process around the 7 rounds. It's pretty straight forward to me for the case of the first 3 rounds: (probability that the round is below the power flaw)^3. How do I account for the next 3 rounds and the last round?\\n\\nI can see computing the probabilities using combinations of bullets above or below the floor, but it's not quite a binary above or below. Let's assume that 6 rounds have been fired, and the average of the highest 3 rounds is 164.9. If the last round has at least 165.2, then the average of the highest 3 rounds out of 7 will be 165.\\n\\n2) The other issue I need to deal with is figuring out how to account for the 4% variance between my chrono and the match chrono, and how the match chrono is allowed to drift by 4% from day to day. Do I just assume the worst case and make sure that I'm at least 8% above 165 -- that is my rounds are shooting at least 179 power factor? Or do I try to assume some kind of normal distribution over the two 4% variances?\\n",Added some focusing questions so that this problem can be taken a section at a time.,
3991,2,1732,08c4c2d5-4f51-458b-a627-6a568939054f,2010-08-16 09:07:50.0,887.0,"For (1), as ebony1 suggests, there are several incremental or on-line SVM algorithms you could try, the only thing I would mention is that the hyper-parameters (regularisation and kernel parameters) may also need tuning as you go along as well, and there are fewer algorithmic tricks to help with that.  The regularisation parameter will almost certainly benefit from tuning, because as the amount of training data increases, the less regularisation that is normally required.\\n\\nFor (2) you could try fitting a one-class SVM to the training data, which would at least tell you if the data were consistent with the classes you do know about, and then classify as ""unknown"" if the output of the one-class SVM was sufficiently low.  IIRC libsvm has an implementation of one-class SVM.\\n\\nFor (3) if you just use 1-v-1 class component SVMs, then you just need to make three new SVMs, one for each of the unknown-v-known class combinations, and there is no need to retrain the others.\\n\\nHTH",,
3992,2,1733,cd0fc753-9161-4be4-b915-b39a6bb92db9,2010-08-16 09:13:42.0,887.0,"For optimisation, you don't need to perform a grid search; a Nelder-Mead simplex(fminsearch in MATLAB) approach is just as effective and generally much faster, especially if you have a lot of hyper-parameters to tune.  Alternatively you can use gradient descent optimisation - if your implementation doesn't provide gradient information, you can always estimate it by finite differences (as fminunc in MATLAB does).\\n\\nThe Span bound is a good criterion to optimise, as it is fast, but good old cross-validation is hard to beat (but use a continuous statistic such as the squared hinge loss).\\n\\nHTH",,
3993,5,1733,c65a88eb-33b8-435f-9154-2492713d276d,2010-08-16 09:23:45.0,887.0,"For optimisation, you don't need to perform a grid search; a Nelder-Mead simplex(fminsearch in MATLAB) approach is just as effective and generally much faster, especially if you have a lot of hyper-parameters to tune.  Alternatively you can use gradient descent optimisation - if your implementation doesn't provide gradient information, you can always estimate it by finite differences (as fminunc in MATLAB does).\\n\\nThe Span bound is a good criterion to optimise, as it is fast, but good old cross-validation is hard to beat (but use a continuous statistic such as the squared hinge loss).\\n\\nHTH\\n\\nn.b. nu needs to lie in [0,1], however this is not a problem, just re-parameterise as theta = logit(nu), and then optimise theta instead of nu.  You can then use more or less any numerical optimisation technique you like, e.g. Nelder-Mead simplex, gradient descent, local search, genetic algorithms...",added 305 characters in body,
3994,5,1716,f05f7ce0-1615-46bc-a41f-7ec969728150,2010-08-16 09:31:44.0,8.0,"To summarise (please correct me if I'm wrong):\\n\\n * You have a set of points for a number of parameters/states.\\n * The points provide a joint distribution of the parameters states\\n * You want to simulate from a model using some typical states.\\n\\nThe problem you have is that you can't write down a nice closed form density.\\n\\nTo tackle this problem you should use a particle filter. Suppose your model of a cell was this simple ODE:\\n\\n\\begin{equation}\\n\\frac{dX(t)}{dt} = \\lambda X(t)\\n\\end{equation}\\n\\nand your data consists of values of $\\lambda$ and $X(0)$. Put this data in a matrix with two columns and $n$ rows, where $n$ is the number of points. Then\\n\\n1. Choose a **row** at random, to get a particular values of $\\lambda$ and $X(0)$\\n1. Optional step: perturb your parameters with noise.\\n1. Simulate from your model, in this case the ODE.\\n1. Repeat as necessary.\\n\\nThe key point is that step 1 is draw from the joint density of the $\\lambda$ and $X(0)$. \\n\\n____\\n\\nThis answer could be way off if I've misinterpreted what you mean about simulating from the model. Please correct me if I'm wrong.",added 198 characters in body,
3995,5,1717,a6941f18-6981-4b7e-9caa-7f3284c04984,2010-08-16 10:40:58.0,339.0,"I also think that it's not clear what you want. But if you want a set of *deterministically* chosen points, so that they preserve the moments of the initial distribution, you can use the *sigma point* selection method that applies to the [unscented Kalman filter][1].  \\n\\nSay that you want to select $2L+1$ points that fulfill those requirements. Then proceed in the following way:\\n\\n$\\mathcal{X}_0=\\overline{x} \\qquad w_0=\\frac{\\kappa}{L+\\kappa} \\qquad i=0$ \\n\\n$\\mathcal{X}_i=\\overline{x}+\\left(\\sqrt{(\\:L+\\kappa\\:)\\:\\mathbf{P}_x}\\right)_i \\qquad w_i=\\frac{1}{2(L+\\kappa)} \\qquad i=1, \\dots,L$ \\n\\n$\\mathcal{X}_i=\\overline{x}-\\left(\\sqrt{(\\:L+\\kappa\\:)\\:\\mathbf{P}_x}\\right)_i \\qquad w_i=\\frac{1}{2(L+\\kappa)} \\qquad i=L+1, \\dots,2L$ \\n\\nwhere $w_i$ the weight of the i-th point,\\n\\n$\\kappa=3-L$ (in case of Normally distributed data),\\n\\nand $\\left(\\sqrt{(\\:L+\\kappa\\:)\\mathbf{P}_x}\\right)_i$ is the i-th row (or column)* of the *matrix square root* of the weighted covariance $(\\:L+\\kappa\\:)\\:\\mathbf{P}_x$ matrix (usually given by the [Cholesky decomposition][2])\\n\\n\\* If the *matrix square root* $\\mathbf{A}$ gives the original by giving $\\mathbf{A}^T\\mathbf{A}$, then use the *rows* of $\\mathbf{A}$. If it gives the original by giving $\\mathbf{A}\\mathbf{A}^T$, then use the *columns* of $\\mathbf{A}$. The result of the matlab function [chol()][3] falls into the first category.\\n\\nHere is a simple example using R\\n\\n    x <- rnorm(1000,5,2.5)\\n    y <- rnorm(1000,2,1)\\n    \\n    P <- cov(cbind(x,y))\\n    V0 <- c(mean(x),mean(y))\\n    n <- 2;k <- 1\\n    A <- chol((n+k)*P) # matrix square root\\n    \\n    points <- as.data.frame(sapply(1:(2*n),function(i) if (i<=n) A[i,] + V0 else -A[i-n,] + V0))\\n    attach(points)\\n    \\n    #mean (equals V0)\\n    1/(2*(n+k))*(V1+V2+V3+V4) + k/(n+k)*V0\\n    #covariance (equals P)\\n    1/(2*(n+k)) * ((V1-V0) %*% t(V1-V0) + (V2-V0) %*% t(V2-V0) + (V3-V0) %*% t(V3-V0) + (V4-V0) %*% t(V4-V0))\\n\\n \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/UKF#Unscented_Kalman_filter\\n  [2]: http://en.wikipedia.org/wiki/Cholesky_decomposition\\n  [3]: http://www.mathworks.com/access/helpdesk/help/techdoc/ref/chol.html",added 559 characters in body,
3996,2,1734,09eaf597-b51e-477b-81a8-67dc76e27a15,2010-08-16 11:00:11.0,961.0,As a generalization of 'pirates cause global warming': Pick any two quantities which are (monotonically) increasing or decreasing with time and you should see some correlation.,,
3997,2,1735,d708f71c-80ea-4419-a9f3-72b398753aa7,2010-08-16 11:36:42.0,962.0,"I am regressing two butterfly richness variables (summer and winter) \\nagainst a set of environmental variables separately. \\n(variables with continuous numbers)\\nEnvironmental variables are identitcal in each model.\\n\\nIn the summer model,\\nthe weight rank of coefficients is\\ntemp > prec > ndvi.\\n\\nThe weight rank in winter is \\ntemp > ndvi > prec.\\n\\nAs it is almost implausible to compare the coefficients directly, \\npls advise any advanced method other than regression \\nto discriminate such coefficient rank between seasons, \\nsuch as canonical correlation analysis (unsure if it is suitable here)\\n\\nThe spatial info here\\nrichness and environmental variables comprising 2000 grid (continuous distribution) spanning from 100 E to 130 E longitude, 18 to 25 N latitude.\\n",,
3998,1,1735,d708f71c-80ea-4419-a9f3-72b398753aa7,2010-08-16 11:36:42.0,962.0,method to compare variable coefficient in wo regression model,,
3999,3,1735,d708f71c-80ea-4419-a9f3-72b398753aa7,2010-08-16 11:36:42.0,962.0,<regression>,,
4000,16,1719,e13e0355-5aa6-4658-83c6-96b7a535ae82,2010-08-16 12:03:55.0,59.0,,,
4001,2,1736,5cf5e20d-df5f-4c9e-9def-778e81398fbd,2010-08-16 12:10:29.0,172.0,I am wondering if there are any packages for python that is capable of performing survival analysis. I have been using the survival package in R but would like to port my work to python. ,,
4002,1,1736,5cf5e20d-df5f-4c9e-9def-778e81398fbd,2010-08-16 12:10:29.0,172.0,Survival Analysis tools in Python,,
4003,3,1736,5cf5e20d-df5f-4c9e-9def-778e81398fbd,2010-08-16 12:10:29.0,172.0,<survival-analysis><python>,,
4004,4,1485,7d7a674a-79ac-421f-863f-ba761719542c,2010-08-16 12:54:51.0,88.0,Few machine learning problems,Better title.,
4005,5,1729,6b6b74f1-e0c2-40da-b294-52d0e5047371,2010-08-16 12:58:55.0,88.0,"I am trying to format the output from pairwise.t.test into LaTeX, but have not found a way of doing this. Has anyone got any suggestions? \\n\\n**EDIT:** As this is a one-time only report where I do need to customize the variable names, and row-/column headings, I was hoping to avoid using Sweave.",latex becomes LaTeX,
4006,4,1729,6b6b74f1-e0c2-40da-b294-52d0e5047371,2010-08-16 12:58:55.0,88.0,Export/format output from pairwise.t.test to LaTeX,latex becomes LaTeX,
4007,16,48,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4008,16,63,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4009,16,77,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4010,16,443,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4011,16,519,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4012,16,528,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4013,16,618,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4014,16,676,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4015,16,1689,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4016,16,1690,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4017,16,1692,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4018,16,1710,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4019,16,1734,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4020,16,36,00000000-0000-0000-0000-000000000000,2010-08-16 13:01:42.0,88.0,,,
4021,2,1737,3995268e-f5e5-4f7c-ae59-77640ac5fb41,2010-08-16 13:23:52.0,,I want to calculate a summary of a variable in a data.frame for each unique combination of factors in the data.frame. Should I use plyr to do this?,,humble Student
4022,1,1737,3995268e-f5e5-4f7c-ae59-77640ac5fb41,2010-08-16 13:23:52.0,,How to find all unique combinations of factors in a data.frame in R?,,humble Student
4023,3,1737,3995268e-f5e5-4f7c-ae59-77640ac5fb41,2010-08-16 13:23:52.0,,<r>,,humble Student
4024,2,1738,ad2cdac6-5dc6-4510-990b-04bea9a222b1,2010-08-16 13:42:04.0,279.0,"If you want inferences specifically about the mean of the original variable, then don't use Box-Cox transformation. IMO Box-Cox transformations are most useful when the transformed variable has its own interpretation, and the Box-Cox transformation only helps you to find the right scale for analysis - this turns out to be the case surprisingly often. Two unexpected exponents that I found this way were 1/3 (when the response variable was bladder volume) and -1 (when the response variable was breaths per minute).  \\n\\nThe log-transformation is probably the only exception to this. The mean on the log-scale corresponds to the geometric mean in the original scale, which is at least a well-defined quantity.",,
4025,16,197,c8217ecc-2ac3-46bc-bb98-236ca907528b,2010-08-16 13:45:34.0,5.0,,,
4026,2,1739,12018eb9-eaab-40c8-bb53-5f69eea408a0,2010-08-16 13:46:53.0,279.0,"See `aggregate` and `by`. For example, from the help file for `aggregate`:\\n\\n    ## Compute the averages according to region and the occurrence of more\\n    ## than 130 days of frost.\\n    aggregate(state.x77,\\n          list(Region = state.region,\\n               Cold = state.x77[,""Frost""] > 130),\\n          mean)\\n",,
4027,5,1737,ecd347e4-e3ff-4f7e-9ea7-d029298d96f9,2010-08-16 13:57:57.0,,I want to calculate a summary of a variable in a data.frame for each unique combination of factors in the data.frame. Should I use plyr to do this? I am ok with using loops as opposed to apply() ; so just finding out each unique combination would be enough.,added 110 characters in body,humble Student
4028,2,1740,f38e7eee-1663-429b-9cf7-be2a73e5f023,2010-08-16 14:01:14.0,364.0,"Here's the plyr solution, which has the advantage of returning multiple summary stats and producing a progress bar for long computes:\\n\\n	library(ez) #for a data set\\n	data(ANT)\\n	cell_stats = ddply(\\n		.data = ANT #use the ANT data\\n		, .variables = .(cue,flanker) #uses each combination of cue and flanker\\n		, .fun = function(x){ #apply this function to each combin. of cue & flanker\\n			to_return = data.frame(\\n				, acc = mean(x$acc)\\n				, mrt = mean(x$rt[x$acc==1])\\n			)\\n			return(to_return)\\n		}\\n		, .progress = 'text'\\n	)\\n\\n",,
4029,2,1741,ae9d1034-9763-47bc-af59-04aba129d9f7,2010-08-16 14:05:44.0,8.0,"AFAIK, there aren't any survival analysis packages in python. As mbq comments above, the only route available would be to [Rpy][1]. \\n\\nEven if there were a pure python package available, I would be very careful in using it, in particular I would look at:\\n\\n  * How often does it get updated.\\n  * Does it have a large user base?\\n  * Does it have advanced techniques?\\n\\nOne of the benefits of R, is that these standard packages get a massive amount of testing and user feed back. When dealing with real data, unexpected edge cases can creep in.\\n\\n\\n  [1]: http://rpy.sourceforge.net/",,
4030,2,1742,0df14667-b576-4e48-ac98-9f5dc8ae4692,2010-08-16 14:38:47.0,247.0,"I'd suggest Christopher Bishop's ""Pattern Recognition and Machine Learning"". You can see some of it, including a sample chapter, at http://research.microsoft.com/en-us/um/people/cmbishop/PRML/index.htm\\n\\n",,
4031,16,1742,0df14667-b576-4e48-ac98-9f5dc8ae4692,2010-08-16 14:38:47.0,-1.0,,,
4032,2,1743,ec71e676-a202-4069-9c41-39d3eabc795b,2010-08-16 15:29:24.0,887.0,"A correlation on its own can <b>never</b> establish a causal link.  <a href=""http://en.wikipedia.org/wiki/David_Hume"">David Hume</a> (1771-1776) argued quite effectively that we can not obtain <b>certain</b> knowlege of cauasality by purely empirical means.  Kant attempted to address this, the Wikipedia page for <a href=""http://en.wikipedia.org/wiki/Kant"">Kant</a> seems to sum it up quite nicely:\\n\\n<blockquote>Kant believed himself to be creating a compromise between the empiricists and the rationalists. The empiricists believed that knowledge is acquired through experience alone, but the rationalists maintained that such knowledge is open to Cartesian doubt and that reason alone provides us with knowledge. Kant argues, however, that using reason without applying it to experience will only lead to illusions, while experience will be purely subjective without first being subsumed under pure reason.</blockquote>\\n\\n\\nIn otherwords, Hume tells us that we can never know a causal relationship exists just by observing a correlation, but Kant suggests that we may be able to use our reason to distinguish between correlations that do imply a causal link from those who don't.  I don't think Hume would have disagreed, as long as Kant were writing in terms of plausibility rather than certain knowledge.\\n\\nIn short, a correlation provides circumstantial evidence implying a causal link, but the weight of the evidence depends greatly on the particular circumstances involved, and we can never be absolutely sure.  The ability to predict the effects of interventions is one way to gain confidence (we can't prove anything, but we can disprove by observational evidence, so we have then at least attempted to falsify the theory of a causal link).  Having a simple model that explains why we should observed a correlation that also explains other forms of evidence is another way we can apply our reasoning as Kant suggests.\\n\\n\\nCaveat emptor: It is entirely possible I have misunderstood the  philosophy, however it remains the case that a correlation can never provide proof of a causal link.",,
4033,16,1743,ec71e676-a202-4069-9c41-39d3eabc795b,2010-08-16 15:29:24.0,-1.0,,,
4034,6,1737,ec4b1ad2-5ad4-4cfd-9f35-8fca56f9b3cc,2010-08-16 15:29:53.0,8.0,<r><plyr><aggregation>,edited tags,
4035,2,1744,c34f3c58-b47c-47d8-aec0-83c0c84fa2cb,2010-08-16 15:32:04.0,183.0,In addition to other suggestions you may find the `describe.by()` function in the `psych` package useful.\\nIt can be used to show summary statistics on numeric variables across levels of a factor variable.,,
4036,2,1745,6a129fbc-bfa8-4fae-aa3c-0200b6b59d31,2010-08-16 15:46:32.0,729.0,"While I think `aggregate` is probably the solution you are seeking, if you are want to create an explicit list of all possible factor combinations, `expand.grid` will do that for you. e.g.\\n\\n    > expand.grid(height = seq(60, 80, 5), weight = seq(100, 300, 50),\\n                 sex = c(""Male"",""Female""))\\n           height weight    sex\\n    1      60    100   Male\\n    2      65    100   Male\\n    ... \\n    30     80    100 Female\\n    31     60    150 Female\\n\\n\\nYou could then loop over each row in the resulting data frame to pull out records from your original data.",,
4037,2,1746,016c5498-0f99-42a6-855f-55cbb40bdd8e,2010-08-16 16:30:54.0,251.0,"[python-asurv][1] is an effort to port the [asurv][2] software for survival methods in astronomy.   Might be worth keeping an eye on, but cgillespie is right about the things to watch out for: it has a *long* way to go and development doesn't seem active.  (AFAICT only one method exists and even completed, the package may be lacking for, say, biostatisticians.)\\n\\nYou're probably better off using [survival][3] package in R from Python through something like [RPy][4] or [PypeR][5].  I haven't had any problems doing this myself.\\n\\n\\n  [1]: http://sourceforge.net/projects/python-asurv/\\n  [2]: http://www.astrostatistics.psu.edu/statcodes/sc_censor.html\\n  [3]: http://cran.r-project.org/web/packages/survival/index.html\\n  [4]: http://rpy.sourceforge.net/rpy2.html\\n  [5]: http://www.jstatsoft.org/v35/c02\\n\\n",,
4038,2,1747,72ccce3e-769d-4d23-aa6c-074f4f194430,2010-08-16 17:13:58.0,857.0,"2 great answers so far and I promise I'll accept one of them.  But I wanted to post this link as an answer too:\\n[summary of how to compare two numbers][1]\\n\\nYes, for me merely comparing two numbers had some questions, so **that** shows you the level I'm at.  While it won't be relevant to most, here is the VBA function I made based on that info ... well, not based that info exactly, I needed [something simpler][2]:\\n\\n    Private Function fxDifference(sng_A As Single, sng_B As Single) As Variant\\n        ''When calling this function, sng_B must not be zero (Error div/zero)\\n        fxDifference = (sng_A - sng_B)\\n        fxDifference = fxDifference / sng_B\\n        fxDifference = Round(fxDifference * 100, 1)\\n    End Function\\n\\nI suppose when I said 'generic' tools, I meant 'really, really basic' ...\\n\\n  [1]: http://www.statsoft.com/textbook/elementary-concepts-in-statistics/#Two%20basic%20features%20of%20every%20relation%20between%20variables\\n  [2]: http://wiki.answers.com/Q/How_do_you_determine_Percent_of_change_between_two_numbers",,
4039,16,1747,72ccce3e-769d-4d23-aa6c-074f4f194430,2010-08-16 17:13:58.0,-1.0,,,
4040,2,1748,53bf4afb-0610-4470-99fa-44e1069bc495,2010-08-16 17:20:25.0,196.0,"If BPM is staying the same over many samples (or changing infinitesimally in a way you aren't concerned about) you can truncate your data to a significant digit that you actually care about and then do Run Length Encoding.  \\n\\nFor example, in R this data:\\n\\n    0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\\n\\nhas this output\\n\\n    rle(data)\\n    Run Length Encoding\\n      lengths: int [1:3] 10 15 15\\n      values : num [1:3] 0 1 2",,
4041,2,1749,f947b402-e819-412f-b973-4efecc177f9a,2010-08-16 17:27:28.0,196.0,"I'm uncertain whether I should be able to intuit the answer to my question from a question that has [already been asked][1] but I can't, so I am asking the question anyway.  Thus, I am looking for a clear easy to understand answer.  A [recent newspaper article][2] reported that on average couples were able to conceive a child after 104 reproductive acts.  Assuming indpendant binomial trials, that means for each act there was a 1/104 probability of success.  I can do a quick simulation to show myself what the quantiles for this distribution look like, e.g. in R:\\n\\n    NSIM <- 10000\\n    trialsuntilsuccess <- function(N=10000,Pr=1/104)\\n    {\\n    	return(min(seq(1:N)[rbinom(N,1,Pr)==1]))\\n    }\\n    res <- rep(NA,NSIM)\\n    for (i in 1:NSIM)\\n    {\\n    	res[i] <- trialsuntilsuccess()\\n    	if (i %% 10  == 0) {cat(i,""\\r""); flush.console()}\\n    }\\n    quantile(res,c(.025,.975))\\n\\nBut it seems like there should be some simple equation or approximation that could be applied, perhaps a probit or poisson?  Any advice on how to get the quantiles without running a simulation?  Bonus points for providing a way to do the relevant calculations in R.\\n\\n  [1]: http://stats.stackexchange.com/questions/1123/modeling-success-rate-with-gaussian-distribution\\n  [2]: http://www.dailymail.co.uk/news/article-1303442/Pregancy-The-average-couple-sex-104-times-conceive.html",,
4042,1,1749,f947b402-e819-412f-b973-4efecc177f9a,2010-08-16 17:27:28.0,196.0,How can you approximate the number of trials to success given a particular Pr(Success)?,,
4043,3,1749,f947b402-e819-412f-b973-4efecc177f9a,2010-08-16 17:27:28.0,196.0,<probability><poisson><binomial><quantile><probit>,,
4044,2,1750,f111db4e-587e-4f57-ab0e-921c3739a700,2010-08-16 17:31:42.0,196.0,"In library(doBy) there is also the summaryBy() function, e.g.\\nsummaryBy(DV1 + DV2 ~ Height+Weight+Sex,data=my.data)",,
4045,2,1751,c29f9db5-113a-4084-a837-1b7de899f2a2,2010-08-16 17:43:49.0,,"If I understand your question correctly you want to compute the quantiles for the ""No of failures before the first success"" given that $p=\\frac{1}{104}$.\\n\\nThe distribution you should be looking at is the [negative binomial distribution][1]. The wiki discusses the negative binomial as:\\n\\n> In probability theory and statistics, the negative binomial distribution is a discrete probability distribution of the number of successes in a sequence of Bernoulli trials before a specified (non-random) number r of failures occurs.\\n\\nJust invert the interpretation of success and failures with a setting of r=1 would accomplish what you want. The distribution with r=1 is also called the [geometric distribution][2].\\n\\nYou could then use the discrete distribution to compute the quantiles.\\n\\nPS: I do not know R.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Negative_binomial_distribution\\n  [2]: http://en.wikipedia.org/wiki/Geometric_distribution\\n  [3]: http://en.wikipedia.org/wiki/Negative_binomial_distribution#Poisson_distribution",,user28
4047,2,1753,57444f4c-9fec-4783-aac7-e021b7dd09ae,2010-08-16 18:01:43.0,968.0,"The visualization of Standard Deviation is usually associated with frequency distribution graphs (related to normal curve etc.) but I came across an example where it was being plotted on a Cartesian plot (standard 2D with X and Y axes.)\\n\\nThis seems like a valid thing to do but in this case the example only had a single line running across the graph to ""indicate"" standard deviation. This to me seems not very useful and downright dangerous. Don't you need three lines plotted to properly visualize standard deviation on a graph? Thusly:\\n\\n 1. The mean\\n 1. The mean plus one standard deviation value\\n 1. The mean minus one standard deviation value\\n\\nP.S. I am a software guy working on a data visualization package so please take my use of stats terminology with a grain of salt. Any corrections and feedback will be sincerely appreciated.",,
4048,1,1753,57444f4c-9fec-4783-aac7-e021b7dd09ae,2010-08-16 18:01:43.0,968.0,Visualizing standard deviation on a Cartesian plot,,
4049,3,1753,57444f4c-9fec-4783-aac7-e021b7dd09ae,2010-08-16 18:01:43.0,968.0,<data-visualization><standard-deviation>,,
4050,5,1753,7f6e884f-3c55-4e15-8ca4-402dcfe75194,2010-08-16 18:30:26.0,968.0,"I came across an example where standard deviation was being plotted on a Cartesian plot (standard 2D with X and Y axes.)\\n\\nThis seems like a valid thing to do but in this case the example only had a single line running across the graph to ""indicate"" standard deviation. This to me seems not very useful, possibly dangerous and misleading. Don't you need three lines plotted to properly visualize standard deviation on a graph? Thusly:\\n\\n 1. The mean\\n 1. The mean plus one standard deviation value\\n 1. The mean minus one standard deviation value\\n\\nP.S. I am a software developer working on a data visualization package so please take my use of stats terminology with a grain of salt. Any corrections and feedback will be sincerely appreciated.",wording change,
4051,5,1525,e91633ab-512f-474f-8110-2f7ced0ebca8,2010-08-16 19:03:53.0,74.0,"Say I have eaten hamburgers every Tuesday for years. You could say that I eat hamburgers 14% of the time, or that the probability of me eating a hamburger in a given week is 14%.\\n\\nWhat are the main differences between probabilities and proportions?\\n\\nIs a probability an expected proportion?\\n\\nAre probabilities uncertain and proportions are guaranteed?",added 124 characters in body,
4052,6,1749,6cc8f77b-9a36-42e9-8d9d-b67a0a942637,2010-08-16 19:05:00.0,8.0,<probability><binomial><negative-binomial>,edited tags,
4053,5,1251,85d24929-e4b0-4132-b7ef-a5161b2b922a,2010-08-16 19:06:29.0,8.0,"Thanks for all you answers. For completeness I thought I should include what I usually do. I tend to do a combination of the suggestions given: dots, boxplots (when n is large), and se (or sd) ranges.\\n\\n![Dot plot][1]\\n\\nFrom the dot plot, it is clear that data is far more spread out the ""handle bar"" plots suggest. In fact, there is a negative value in A3!\\n\\n\\n\\n\\n\\n  [1]: http://img594.imageshack.us/img594/6883/tmpsc.jpg",deleted 6 characters in body,
4054,5,1753,e07af659-7a37-461b-ba06-21ef34aa078d,2010-08-16 19:36:49.0,968.0,"I came across an example where standard deviation was being plotted on a Cartesian plot (standard 2D with X and Y axes.)\\n\\nThis seems like a valid thing to do but in this case the example only had a single line running across the graph to ""indicate"" standard deviation. This to me seems not very useful, possibly dangerous and misleading. Don't you need three lines plotted to properly visualize standard deviation on a graph? Thusly:\\n\\n 1. The mean\\n 1. The mean plus one standard deviation value\\n 1. The mean minus one standard deviation value\\n\\nP.S. I am a software developer working on a data visualization package so please take my use of stats terminology with a grain of salt. Any corrections and feedback will be sincerely appreciated.\\n\\n**Rephrasing the question:**\\n\\nIf I had a set of five data points to plot on a cartesian plane:\\n\\n    X:    10    20    30    35    50\\n    Y:    20    40    5     55    10\\n\\nFor this sample data set (the Y values) the mean is 20 and the stdev is ~21.036 (x values plotted along the X axis and y values plotted along the Y axis.)\\n\\nWhat would a proper plotting of the mean and the stdev on top of the X/Y data set look like?\\n\\n\\n",rephrased the question; added 75 characters in body,
4055,2,1754,631220cc-b36d-40b9-aaf5-25d8dd21d86f,2010-08-16 19:59:01.0,71.0,"Probably a line for the mean and a line for +/- twice the standard deviation.  That would be the ""default"" plot for that.\\n\\nThat said, I think you may be missing the point of the plot with the single line for the standard deviation.  If what you're trying to represent is change in the variability of Y over X (i.e., heteroscedasticity), then a line plotting SD over X might work.  It really does depend on the data and the questions that you're trying to ask.  There just isn't a set of rules that you can follow to produce good plots every time, and in general the more automated the plotting system gets, the more useless I find it.",,
4056,2,1755,7978bf03-adfd-450c-a7dd-0ca5a882a382,2010-08-16 20:03:34.0,8.0,"What about plotting the point with error bars, say mean +/- sd. Here's what your example data would look like:\\n\\n![dot-plot with error bars][1]\\n\\n\\n______\\n\\nHere's the R code I used to generate the plot:\\n\\n    library(ggplot2)\\n    df = data.frame(values=c(10, 20, 30, 35, 50, 20, 40, 5, 55, 10),\\n      type=rep(c(""X"", ""Y""),  each=5))\\n    \\n    means = tapply(df$values, df$type, mean)\\n    sds = tapply(df$values, df$type, sd)\\n    df_summary = data.frame(means, sds, type=c(""X"", ""Y""))\\n    \\n    g = ggplot(data=df_summary, aes(y=means, x=type)) +\\n      geom_point(data=df,aes(y=values, x=type), col=2) +\\n      geom_errorbar(aes(ymax = means + sd, ymin=means - sd)) +\\n      ylab(""Values"")\\n    \\n    g\\n\\n\\n  [1]: http://i.imgur.com/Zqgbc.png\\n",,
4057,5,1693,cf2b0668-ffe4-481d-8898-86ffc0ef0fde,2010-08-16 20:05:49.0,251.0,"UPDATE\\n\\nTough crowd.  :)  For a concise account of connecting the trace of the Fisher matrix to surface area, please see section 4 (""Isoperimetric Inequalities"") in the paper below.  The crucial part is establishing the relation between differential entropy and the trace of the Fisher matrix, which the authors prove in the appendix.\\n\\n- [On the similarity of the entropy power inequality and the Brunn-Minkowski inequality][1]\\n\\n----\\n\\nThe basic intuition is through the isoperimetric inequality for the surface area of a sphere maximizing the volume.  We can arrive at a similar relationship concerning the trace of the Fisher information matrix and the entropy w.r.t the Gaussian.  The following may be helpful.\\n\\n- [Information Theoretic Inequalities for Contoured Probability Distributions][2]\\n- [On Isoperimetric Inequalities in Minkowski Spaces][3]\\n\\n\\n  [1]: http://statistics.stanford.edu/~ckirby/techreports/NSF/COV%20NSF%2048.pdf\\n  [2]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7743\\n  [3]: http://www.hindawi.com/journals/jia/2010/697954.html",added 523 characters in body,
4058,2,1756,25f6eeec-e35d-4613-9cac-fd9b7aa73d7d,2010-08-16 20:12:03.0,795.0,"The most naive approach I can think of is to regress $Y_i$ vs $X_i$ as $Y_i \\sim \\hat{m}X_i + \\hat{b}$, then perform a $t$-test on the hypothesis $m = 1$. See [t-test for regression slope][1].\\n\\nA less naive approach is the Morgan-Pitman test. Let $U_i = X_i - Y_i, V_i = X_i + Y_i,$ then perform a test of the Pearson Correlation coefficient of $U_i$ vs $V_i$. (One can do this simply using the [Fisher R-Z transform][2], which gives the confidence intervals around the sample Pearson coefficient, or via a bootstrap.) \\n\\nIf you are using R, and don't want to have to code everything yourself, I would use `bootdpci` from Wilcox' Robust Stats package, WRS. (see [Wilcox' page][3].)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/T-test#Slope_of_a_regression_line\\n  [2]: http://en.wikipedia.org/wiki/Fisher_transformation\\n  [3]: http://www-rcf.usc.edu/~rwilcox/",,
4059,5,1721,d0826776-94a6-4b37-881e-9401942e662a,2010-08-16 21:27:59.0,282.0,"So it turns out the first assumption was actually correct:  U is indeed the first k eigenvectors of C, that we calculate from G by means of the eigendecompposition $(X_tVD^{-\\frac12})D^{1/2} = X_tV$.  \\n\\n",added 14 characters in body,
4060,5,1681,65b54aec-cba4-4352-83f3-29e86a80f446,2010-08-16 21:57:35.0,511.0,"Chernoff bound (for <a href=""http://en.wikipedia.org/wiki/Chernoff_bound#Theorem_for_additive_form_.28absolute_error.29"">absolute error</a>) gives a bound on probability of large deviation in terms of sample size and amount of deviation, but it doesn't seem possible to rewrite it to give an explicit bound on the amount of deviation. So, what is good way to bound the largest deviation from the mean in terms of sample size and probability of that deviation?",added 105 characters in body,
4061,2,1757,32d9eacd-26cc-4936-aaee-5ce3c69b6711,2010-08-16 23:36:20.0,569.0,"I am taking the time to learn how to analyze networks and want to test if there are differences between two networks over time.  Since I am new to R and networks in general, I am hoping to get some help how to compare and analyze network graphs.\\n\\nSimply, my dataset will contain information on flows between two geographic units (think Census migration data).  I want to test if the flows, generally, are different with time. Since I am just starting out, I am not even sure if I am phrasing my question correctly.\\n\\nI have created a few basic graphs and generated some very basic ""summary statistics"" on a graph in isolation in R before, so I understand how to get up and running, but I am not really sure where to go from here.\\n\\nAny help you can provide will be very much appreciated!\\n\\nBrock\\n",,
4062,1,1757,32d9eacd-26cc-4936-aaee-5ce3c69b6711,2010-08-16 23:36:20.0,569.0,Significant Difference between two network graphs,,
4063,3,1757,32d9eacd-26cc-4936-aaee-5ce3c69b6711,2010-08-16 23:36:20.0,569.0,<r>,,
4064,2,1758,24c68210-f9fd-4c09-a369-6aab7fc3f176,2010-08-17 00:50:11.0,,"I will try and answer your question for only one of the bounds. Chernoff bounds are given by:\\n\\n$Pr[\\frac{1}{m} \\sum_{i=1}^{i=m} X_i \\ge p+\\epsilon] \\le e^{-D(p+\\epsilon||p)m}$\\n\\nwhere\\n\\n$D(.)$ is the [Kullback-Leibler divergence][1]\\n\\nFor the sake of convenience I will denote the rhs of the above inequality by $r(p,\\epsilon,m)$. Thus, we have:\\n\\n$Pr[\\frac{1}{m} \\sum_{i=1}^{i=m} X_i \\ge p+\\epsilon] \\le r(p,\\epsilon,m)$\\n\\nThe above can be re-written as:\\n\\n$Pr[ \\sum_{i=1}^{i=m} X_i \\ge m\\ (p+\\epsilon)] \\le r(p,\\epsilon,m)$\\n\\nIf we let: $Y = \\sum_{i=1}^{i=m} X_i$ then we know that:\\n\\n$Y \\sim Binomial(m,p)$\\n\\nwhere\\n\\n$p = Prob(X_i=1)$\\n\\nGiven the above it follows that,\\n\\n$Pr[ \\sum_{i=1}^{i=m} X_i \\ge m\\ (p+\\epsilon)] = Pr[ Y_i \\ge m\\ (p+\\epsilon)]$ \\n\\nBut,\\n\\n$Pr[ Y_i \\ge m\\ (p+\\epsilon)] = \\sum_{k=\\lceil m\\ (p+\\epsilon) \\rceil}^{k=m} {m \\choose k} p^k (1-p)^{m-k}$ \\n\\nThus, it follows that:\\n\\n$\\sum_{k=\\lceil p+\\epsilon \\rceil}^{k=m} {m \\choose k} p^k (1-p)^{m-k} \\le r(p,\\epsilon,m)$\\n\\nI am not sure if we can simplify the above to express $\\epsilon$ as a function of $m$ but it may be of some help. Alternatively, you may want to explore the use of the normal approximation to the binomial.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kullback-Leibler_divergence",,user28
4065,2,1759,36b97f3d-f352-4132-ad29-4f4efb437be9,2010-08-17 01:11:36.0,,"I am not sure I will be able to provide a complete answer but here is how I would start. \\n\\nStep 1: Model the data generating process for the flows through the network\\n\\nFor example, you may want to model the flows from one point to another point in the network as a [poisson distribution][1]. The poission distribution is used to model arrivals in a system over time and thus may work well for network flows. Depending on network complexity and your needs you can model each path such that the arrival rate for each path is either different or the same (See the $\\lambda$ parameter for the poisson distribution.)\\n\\nStep 2: Identify a testing strategy which would let you ascertain the strength of evidence for your null model.\\n\\nThe challenge in this step is two-fold. First, you have to define what you mean when you say that the network flows in a network at different times is the same. Are you talking about throughput? or Are you talking about the flows across each one of the paths?\\n\\nThe second challenge is that once you have solved the above issue, you need to find out a way to test your null hypothesis.\\n\\nAs an example: Suppose you want to check that the flows across each path are the same and that your are modeling the path flows for the network by a single parameter (i.e., $\\lambda$ is identical for all paths). Thus, your null hypothesis would assume that $\\lambda$ does not change with time. This how you would go about testing your null hypothesis:\\n\\n**Null Hypothesis is True**\\n\\nPool all network flows across time and estimate a common $\\lambda$ using [maximum likelihood estimation][3] for the poisson distribution.\\n \\n**Null Hypothesis is not true**\\n\\nEstimate $\\lambda$ for each time period separately so that you get two different values  (one for each one of the time periods).\\n\\nYou can then select the model (pooled or separate) that fits the data better on the basis of a criteria such as the [likelihood ratio][4].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Poisson_distribution\\n  [2]: http://en.wikipedia.org/wiki/Test_statistic\\n  [3]: http://en.wikipedia.org/wiki/Poisson_distribution#Maximum_likelihood\\n  [4]: http://en.wikipedia.org/wiki/Likelihood-ratio_test",,user28
4066,6,1757,de390047-675a-4122-aea4-cc80ba680352,2010-08-17 01:29:01.0,159.0,<r><networks>,edited tags,
4067,2,1760,7e30ec15-2010-4be8-af60-cbbf6ada5735,2010-08-17 02:01:44.0,74.0,"""He uses statistics like a drunken man uses a lamp post, more for support than illumination.""\\n\\nAndrew Lang",,
4068,16,1760,7e30ec15-2010-4be8-af60-cbbf6ada5735,2010-08-17 02:01:44.0,-1.0,,,
4069,2,1761,e516ba0e-e59c-42fe-8765-3129dca9db67,2010-08-17 02:52:02.0,253.0,"There was already [a request for Mathematical Statistics Videos][1],  but it explicitly asked from people for\\n\\n>  videos that provide a rigorous\\n> mathematical presentation of\\n> statistics. i.e., videos that might\\n> accompany a course that use a textbook\\n> mentioned in this discussion on...\\n\\nSo at the same time I am wondering, **what recommendation do you have for stat/prob - 101  - video courses?**\\n\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/485/mathematical-statistics-videos",,
4070,1,1761,e516ba0e-e59c-42fe-8765-3129dca9db67,2010-08-17 02:52:02.0,253.0,Statistics/Probability Videos for Beginners ,,
4071,3,1761,e516ba0e-e59c-42fe-8765-3129dca9db67,2010-08-17 02:52:02.0,253.0,<statistics><video>,,
4072,16,1761,e516ba0e-e59c-42fe-8765-3129dca9db67,2010-08-17 02:52:02.0,253.0,,,
4073,2,1762,6a6f3e0b-8b34-48ee-9249-855ac2e1bd10,2010-08-17 03:09:29.0,279.0,"I doubt that you can get an analytical solution to this problem. About the only step that is doable is the probability that the average of the first three velocities is under some cutoff and it is _not_ (probability the one velocity is under the cutoff)^3 as you stated. If $V_i \\sim N (m, s^2)$, then $\\bar{V}_3 \\sim N (m, s^2/3)$, so the probability that $\\bar{V}_3$ is less than $k$ standard deviations above the mean is $P(\\bar{V}_3 < m + k s ) = \\Phi(k/\\sqrt{3})$ where $\\Phi$ is the normal distribution function.  For example, for $k=0$: $P(\\bar{V}_3 < m) = 0.5$, for $k=1$: $P(\\bar{V}_3 < m + s) = 0.718$, etc.  \\n\\nThe calculation involving the next three rounds is complicated by two things: it should somehow be conditional on the fact that the average was too low already, and then should only use the three highest values. While it might work out to be some triple integral, I don't think you want to use such a formula. After that the fact that you have a choice whether the bullet should be weighed or fired also probably depends on the previous values, and things get even more complicated depending on your strategy.\\n\\nIn summary, I think you should write a little simulation program - you will get better answers faster. Note that for the chronometer you will have to define more exactly what does +/-4% mean.",,
4074,5,1762,c31bc6c3-f837-459c-a6d0-e2fa798ab2a1,2010-08-17 03:36:03.0,279.0,"I doubt that you can get an analytical solution to this problem. About the only step that is doable is the probability that the average of the first three velocities is under some cutoff and it is _not_ (probability the one velocity is under the cutoff)^3 as you stated. If $V_i \\sim N (m, s^2)$, then $\\bar{V}_3 \\sim N (m, s^2/3)$, so the probability that $\\bar{V}_3$ is less than $k$ standard deviations above the mean is $P(\\bar{V}_3 < m + k s ) = \\Phi(k/\\sqrt{3})$ where $\\Phi$ is the normal distribution function.  For example, for $k=0$: $P(\\bar{V}_3 < m) = 0.5$, for $k=1$: $P(\\bar{V}_3 < m + s) = 0.718$, etc.  \\n\\nThe calculation involving the next three rounds is complicated by two things: it should somehow be conditional on the fact that the average was too low already, and then should only use the three highest values. While it might work out to be some triple integral, I don't think you want to use such a formula. After that the fact that you have a choice whether the bullet should be weighed or fired also probably depends on the previous values, and things get even more complicated depending on your strategy.\\n\\nEven the simple multiplication of a random bullet weight and one random velocity is not as innocuous as it seems: the product will not be normal (or any other ""regular"" distribution). Its standard deviation can be approximated via standard [error propagation](http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Example_formulas), but calculating probabilities of falling below/above some cutoff is not straightforward.\\n\\nIn summary, I think you should write a little simulation program - you will get better answers faster. Note that for the chronometer you will have to define more exactly what does +/-4% mean.",Added difficulties with multiplication,
4075,2,1763,c6b2c730-10db-4cb1-828f-520956f75f3e,2010-08-17 03:47:24.0,183.0,I think a number of the suggestions put forward on the [mathematical statistics video question][1] probably fall in the stats 101 category:\\n\\n- http://www.khanacademy.org/#Statistics: series of short videos on introductory statistics\\n- http://www.khanacademy.org/#Probability: series of short videos on introductory probability\\n- [Math and probability for life sciences][2]: A whole university course introducing statistics and probability.\\n- [stat579][3]: A complete introductory course emphasising R and statistical computing\\n\\nI also have a [list of maths and statistics videos][4] with a few others.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/485/mathematical-statistics-videos\\n  [2]: http://www.academicearth.org/courses/math-and-proability-for-life-sciences\\n  [3]: http://www.public.iastate.edu/~hofmann/stat579/\\n  [4]: http://jeromyanglim.blogspot.com/2009/05/online-mathematics-video-courses-for.html,,
4076,16,1763,c6b2c730-10db-4cb1-828f-520956f75f3e,2010-08-17 03:47:24.0,-1.0,,,
4077,5,61,33d80175-7255-40b2-ac96-92d114debfd0,2010-08-17 04:11:06.0,74.0,"The standard deviation is a metric, meaning it is a number that represents a concept. The concept in this case is the ""spread"" or ""dispersion"" of the data.\\n\\nThere are other metrics for spread, including range and variance. \\n\\nA distribution with a mean of 1 and a standard deviation of 2 has more ""spread"" than a distribution with a mean of 1 and a standard deviation of 1. \\n\\nIf there is no spread, then the standard deviation is zero. \\n\\nStandard deviation is often used as it is in the same units as the mean, unlike variance. \\n\\nIt is convenient to know that at least 75% of the data points lie within 2 standard deviations of the mean (or around 95% if the distribution is Normal).\\n\\nFor example, if the mean is 100, and the standard deviation is 15, then at least 75% of the values are between 70 and 130. ",added 284 characters in body,
4078,5,1693,b1faa847-961a-4e06-8ea4-529a0a1ba0c0,2010-08-17 04:25:48.0,251.0,"UPDATE\\n\\nTough crowd.  :)  For a concise account of connecting the trace of the Fisher matrix to surface area, please see section 4 (""Isoperimetric Inequalities"") in the paper below.  The crucial part is establishing the relation between differential entropy and the trace of the Fisher matrix, which the authors prove in the appendix.\\n\\n- [On the similarity of the entropy power inequality and the Brunn-Minkowski inequality][1]\\n\\n----\\n\\nThe basic intuition is through the isoperimetric inequality for the surface area of a sphere maximizing the volume.  We can arrive at a similar relationship concerning the trace of the Fisher information matrix and the entropy w.r.t the Gaussian.  The following may be helpful.\\n\\n- [Information Theoretic Inequalities for Contoured Probability Distributions][2]\\n\\n\\n  [1]: http://statistics.stanford.edu/~ckirby/techreports/NSF/COV%20NSF%2048.pdf\\n  [2]: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7743\\n",remvoe link to second paper -- not really relevant here,
4079,2,1764,8058a327-58d0-4c19-a0e8-780edb7af2cd,2010-08-17 05:22:31.0,144.0,"Users are often tempted to break axis values to present data of different orders of magnitude on the same graph (see [here][1]). While this may be convenient it's not always the preferred way of displaying the data (can be misleading at best). What are alternative ways of displaying the data that are different in several orders of magnitude?\\n\\nI can think of two ways, to log-transform the data or use lattice plots. What are other options?\\n\\n\\n  [1]: http://peltiertech.com/Excel/Charts/BrokenYAxis.html",,
4080,1,1764,8058a327-58d0-4c19-a0e8-780edb7af2cd,2010-08-17 05:22:31.0,144.0,alternative ways to broken axes,,
4081,3,1764,8058a327-58d0-4c19-a0e8-780edb7af2cd,2010-08-17 05:22:31.0,144.0,<plotting><logarithm>,,
4082,4,1764,1cd673d8-74dc-4c5c-8eb7-625c8204e106,2010-08-17 05:25:15.0,159.0,What are alternatives to broken axes?,edited tags; edited title; edited tags,
4083,6,1764,1cd673d8-74dc-4c5c-8eb7-625c8204e106,2010-08-17 05:25:15.0,159.0,<data-visualization><logarithm>,edited tags; edited title; edited tags,
4084,6,396,5571f65b-6ffe-454f-ba5f-ba7978009e82,2010-08-17 05:27:03.0,159.0,<data-visualization><best-practices>,edited tags,
4085,6,257,9e5c4fb8-88e9-4924-bdc9-830d45d1e23f,2010-08-17 05:27:15.0,159.0,<data-visualization><csv-file>,edited tags,
4086,2,1765,4c338875-9e2a-49f6-a36b-fce9ef70c2d4,2010-08-17 06:22:29.0,88.0,"Maybe it can be classified as lattice, but I'll try; plot all the bars scaled to the highest in one panel and put another panel showing zoom on lower ones. I used this technique once in case of a scatterplot, and the result was quite nice.",,
4087,2,1766,348e0ee0-5916-45fc-adda-8a51222eb41b,2010-08-17 06:26:59.0,419.0,You can calculate SD for each person and its standard error (for example using bootstrap). Then you can use **rmeta** package to do analysis. I think you should use some transformation of SD for example log (or maybe better log of variance). ,,
4088,2,1767,6cd5b9ca-c773-4281-a89f-3ea44ba5179d,2010-08-17 06:29:42.0,521.0,"From your description I can't see any reason to distinguish the ""baseline test"" from the immediately drawn ""second sample"". They are simply 2 baseline measurements and the variance (at baseline) can be calculated on that basis. It would be better plotting the mean of the two baseline measurements versus the third ""six month"" sample.\\n\\nThe problem is with the 6 month sample. As only one sample is taken at this point there is no way of estimating the ""variability"" at this point, or rather separating sampling variation from longitudinal (real) change in TB reading.\\n\\nIf we consider this a longitudinal data analysis problem we would probably choose the a random intercept (baseline TB) and a random slope (to fit the 6 month TB). The sampling variability would be estimated from the two baseline measurements and the slope from the third 6 month measure. We can't estimate the variability at 6 months without strong distributional assumptions on the chnage over those six months, such as assuming no change.\\n\\n\\n",,
4096,2,1769,46d825e1-1614-4421-8d82-a85314cff275,2010-08-17 07:28:37.0,339.0,"I would suggest [Statistics 401][1] (from Iowa State University). \\nThis is the link for Lesson 1. There are 64 lectures in total. (Here is the [last one][2]). Unfortunately, I haven't found the links gathered together in one page. I think also that lessons 21,26 and 39 are missing. However, I don't remember having any problem in following the course). You can also find the handouts for this class posted [here][3].  \\n\\n\\n  [1]: http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#\\n  [2]: http://video.google.com/videoplay?docid=3835401745697888723&ei=coO0S9aOOJLF-Qb9kJnnBg&q=Statistics+401%3A+Lesson+64&hl=en&view=3#\\n  [3]: http://www.public.iastate.edu/~pcaragea/S401F07/Handouts07.html",,
4097,16,1769,46d825e1-1614-4421-8d82-a85314cff275,2010-08-17 07:28:37.0,-1.0,,,
4099,5,1769,a21fd856-157b-4a75-9829-b0de08b75f17,2010-08-17 07:36:52.0,339.0,"I would suggest [Statistics 401][1] (from Iowa State University). \\nThis is the link for Lesson 1. There are 64 lectures in total. (Here is the [last one][2]). Unfortunately, I haven't found the links gathered together in one page. I think also that lessons 21,26 and 39 are missing. However, I don't remember having any problem in following the course). You can also find the handouts for this class posted [here][3].  \\n\\nSome other introductory courses in Statistics are:\\n\\n[Introductory Probability and Statistics for Business][4]\\n\\n[Basics of probability and statistics][5]\\n\\n[Lesson Videos for Statistics][6]\\n\\n[2007 SLUO Lectures on Statistics][7]\\n\\n\\n  [1]: http://video.google.com/videoplay?docid=-3474013489970580510&hl=en&emb=1#\\n  [2]: http://video.google.com/videoplay?docid=3835401745697888723&ei=coO0S9aOOJLF-Qb9kJnnBg&q=Statistics+401%3A+Lesson+64&hl=en&view=3#\\n  [3]: http://www.public.iastate.edu/~pcaragea/S401F07/Handouts07.html\\n  [4]: http://webcast.berkeley.edu/course_details_new.php?seriesid=2009-B-87384&semesterid=2009-B\\n  [5]: http://videolectures.net/bootcamp07_keller_bss/\\n  [6]: http://sofia.fhda.edu/gallery/statistics/resources.html\\n  [7]: http://www-group.slac.stanford.edu/sluo/lectures/Stat2007_lectures.htm",added 536 characters in body,
4100,5,1757,6a04e8f7-2b29-4326-ae0a-ffec91bb0e4b,2010-08-17 07:37:41.0,8.0,"I am taking the time to learn how to analyze networks and want to test if there are differences between two networks over time.  Since I am new to R and networks in general, I am hoping to get some help how to compare and analyze network graphs.\\n\\nSimply, my dataset will contain information on flows between two geographic units (think Census migration data).  I want to test if the flows, generally, are different with time. Since I am just starting out, I am not even sure if I am phrasing my question correctly.\\n\\nI have created a few basic graphs and generated some very basic ""summary statistics"" on a graph in isolation in R before, so I understand how to get up and running, but I am not really sure where to go from here.\\n",deleted 79 characters in body,
4101,2,1771,bd6405ff-c21b-4ced-bee5-e0733b5d0a61,2010-08-17 07:51:41.0,8.0,"I agree with Srikant, you need to model your process. You mentioned that you had already created some networks in R, what model did you assume?\\n\\nThe way I would tackle this problem, is to form a mathematical model, say an ODE model. For example, \\n\\n\\begin{equation}\\n\\frac{dX_i(t)}{dt} = \\lambda X_{i-1}(t) -\\mu X_{i+1}(t)\\n\\end{equation}\\n\\nwhere $X_i$ depends on the population at geographic unit $i$. Since you are interested in differences in time, your parameters $\\lambda$ may also depend on $t$.\\n\\nYou can fit both models simultaneously and determine if the rates are different.\\n\\nYou problem **isn't** easy and I don't think there's an simple solution to it.\\n\\n\\n",,
4102,2,1772,69bd3550-e0b1-42b2-beab-1249e3c7a48d,2010-08-17 08:29:54.0,956.0,"It's not really *about* statistics, but I think it applies to statistics:\\n\\n> It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts. \\n\\nArthur Conan Doyle",,
4103,16,1772,69bd3550-e0b1-42b2-beab-1249e3c7a48d,2010-08-17 08:29:54.0,-1.0,,,
4104,2,1773,ef8761e6-4d96-4641-8116-d0f6bd8792fd,2010-08-17 09:11:30.0,977.0,"Precision is defined as:\\n\\n    p = true positives / (true positives + false positives)\\n\\nIs it correct that, as `true positives` and `false positives` approach 0, the precision approaches 1?\\n\\nSame question for recall:\\n\\n    r = true positives / (true positives + false negatives)\\n\\nI am currently implementing a statistical test where I need to calculate these values, and sometimes it happens that the denominator is 0, and I am wondering which value to return for this case.\\n\\nP.S.: Excuse the inappropriate tag, I wanted to use `recall`, `precision` and `limit`, but I cannot create new Tags yet.",,
4105,1,1773,ef8761e6-4d96-4641-8116-d0f6bd8792fd,2010-08-17 09:11:30.0,977.0,What are correct values for precision and recall in edge cases?,,
4106,3,1773,ef8761e6-4d96-4641-8116-d0f6bd8792fd,2010-08-17 09:11:30.0,977.0,<statistical-analysis>,,
4107,2,1774,23680ba0-d1f1-4853-9702-aa80d54977ec,2010-08-17 09:21:16.0,159.0,"That would depend on what you mean by ""approach 0"". If false positives and false negatives both approach zero at a faster rate than true positives, then yes to both questions. But otherwise, not necessarily.\\n",,
4108,2,1775,c046441e-bef2-4e2b-91de-5688d4bdc926,2010-08-17 09:45:46.0,170.0,Given a confusion matrix:\\n\\n     predicted\\n     ---------\\n    | TP | FN |\\n     ---------   actual\\n    | FP | TN |\\n     ---------\\n\\nwe know that:\\n\\n    Precision = TP / (TP + FP)\\n    Recall = TP / (TP + FN)\\n\\nLets consider the cases where the denominator is zero:\\n\\n - TP+FN=0 : means that there were no positive cases in the input data\\n - TP+FP=0 : means that all instances were predicted as negative\\n,,
4109,6,1773,8c514151-3a27-4501-a091-b86d94440132,2010-08-17 10:32:45.0,183.0,<statistical-analysis><recall><precision><limit>,edited tags,
4110,6,1773,12c8149b-0a8e-437b-bb2b-019dda7188af,2010-08-17 11:08:09.0,88.0,<statistical-analysis><recall><precision>,edited tags,
4111,2,1776,2c442234-f148-4729-838e-fb208b622613,2010-08-17 11:23:18.0,962.0,"\\nThe purpose to run regressions for butterfly richness again 5 environmental variables is to show the importance rank of the independent variables mainly by AIC.\\n\\nIn non-full models, they reveal that variable A tends to be more influential than the others by delta AIC.\\n\\nHowever, in the full model, the regression coefficient of variable A is slightly second to that of variable B. (R-square of the full model is 1.43)\\n\\nThe conflicting outcomes (non-full model by AIC and full model by slope) seems to make it difficult to ascertain that variable A is the variable mostly weighted.\\n\\nPlease kindly suggest which criterion should be relied on for the specified purpose or any further test should be carried out.\\nThank you.",,
4112,1,1776,2c442234-f148-4729-838e-fb208b622613,2010-08-17 11:23:18.0,962.0,to trust AIC (non-full model) or slope (full model),,
4113,3,1776,2c442234-f148-4729-838e-fb208b622613,2010-08-17 11:23:18.0,962.0,<regression><aic>,,
4115,5,1776,619516b6-7e4c-42a1-8e89-d03c9f0404fe,2010-08-17 11:36:26.0,88.0,"The purpose to run regressions for butterfly richness again 5 environmental variables is to show the importance rank of the independent variables mainly by AIC.\\n\\nIn non-full models, they reveal that variable A tends to be more influential than the others by delta AIC.\\n\\nHowever, in the full model, the regression coefficient of variable A is slightly second to that of variable B. (R-square of the full model is 1.43)\\n\\nThe conflicting outcomes (non-full model by AIC and full model by slope) seems to make it difficult to ascertain that variable A is the variable mostly weighted.\\n\\nPlease kindly suggest which criterion should be relied on for the specified purpose or any further test should be carried out.\\nThank you.",Title improvement.,
4116,4,1776,619516b6-7e4c-42a1-8e89-d03c9f0404fe,2010-08-17 11:36:26.0,88.0,Shall I trust AIC (non-full model) or slope (full model)?,Title improvement.,
4117,6,1776,619516b6-7e4c-42a1-8e89-d03c9f0404fe,2010-08-17 11:36:26.0,88.0,<regression><aic><feature-selection>,Title improvement.,
4118,2,1778,faf5158e-d55e-4205-a2b4-c8d185821d5b,2010-08-17 11:56:05.0,601.0,"R-Squared can't be 1.43...  and other errors make your question hard to interpret.\\n\\nHere's a sort of generic response that might eventually lead to an answer.  \\n\\nThe AIC score tells you how good the model is similar to R-squared but penalizes it based on how many components are in the model.  You can theoretically always get a better fit with more elements to the model, and R-squared reflects that, but at some point adding more explanatory variables doesn't increase the model accuracy as much as it needlessly increases complexity.  Therefore, if you add a factor and the AIC goes down instead of up, that's because you're not adding enough explanatory power to make up for the increased complexity of the model and you should err on the side of parsimony (i.e. remove that factor).\\n",,
4119,2,1779,757361bd-d09a-465f-bb94-88cdbc5647e6,2010-08-17 12:27:00.0,419.0,You can try gamlss.cens package. ,,
4120,2,1780,70b9c674-4083-4316-a443-0ff63c90ba5e,2010-08-17 13:10:09.0,114.0,"(I'm a bit outside my comfort zone, so apologies if this is badly worded, or off-topic)\\n\\nI have a bibliographic database, containign details of about 1200 different papers, books, web sites etc, all with various details, including keywords and an abstract.  I want to somehow analyse this database and produce some graphics showing the correlations between different keywords. (like ""drug"" is often present with either ""pharmacology"" or ""assay"").\\n\\nIdeally this would be in R, but general advise would also be welcome. (I've seen [this][1] question/answer which piqued my interest, and this [heatmap graphic][2] also seem related)\\n\\nMy database could be in bibtex, or could be converted to plain text.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/124/statistical-classification-of-text\\n  [2]: http://www2.warwick.ac.uk/fac/sci/moac/students/peter_cock/r/heatmap/",,
4121,1,1780,70b9c674-4083-4316-a443-0ff63c90ba5e,2010-08-17 13:10:09.0,114.0,how to start to analyse keywords from a bibliography and show correlations,,
4122,3,1780,70b9c674-4083-4316-a443-0ff63c90ba5e,2010-08-17 13:10:09.0,114.0,<r><text-mining>,,
4123,2,1781,524630e4-5d07-428d-9050-9386e7ae705f,2010-08-17 13:10:29.0,919.0,"Chemical analyses of environmental samples are often censored below at reporting limits or various detection/quantitation limits.  The latter can vary, usually in proportion to the values of other variables.  For example, a sample with a high concentration of one compound might need to be diluted for analysis, resulting in proportional inflation of the censoring limits for all other compounds analyzed at the same time in that sample.  As another example, sometimes the presence of a compound can alter the response of the test to other compounds (a ""matrix interference""); when this is detected by the laboratory, it will inflate its reporting limits accordingly.\\n\\nI am seeking a practical way to estimate the entire variance-covariance matrix for such datasets, especially when many of the compounds experience more than 50% censoring, which is often the case.  A conventional distributional model is that the logarithms of the (true) concentrations are multinormally distributed, and this appears to fit well in practice, so a solution for this situation would be useful.\\n\\n(By ""practical"" I mean a method that can reliably be coded in at least one generally available software environment like R, Python, SAS, etc., in a way that executes quickly enough to support iterative recalculations such as occur in multiple imputation, and which is reasonably stable [which is why I am reluctant to explore a BUGS implementation, although Bayesian solutions in general are welcome].)\\n\\nMany thanks in advance for your thoughts on this matter.",,
4124,1,1781,524630e4-5d07-428d-9050-9386e7ae705f,2010-08-17 13:10:29.0,919.0,Unbiased estimation of covariance matrix for multiply censored data,,
4125,3,1781,524630e4-5d07-428d-9050-9386e7ae705f,2010-08-17 13:10:29.0,919.0,<correlation><estimation><censoring>,,
4126,2,1782,f79fcb81-9650-44d3-916a-ca39882c1545,2010-08-17 13:18:59.0,8.0,"Another R package that seems to do what you want, is [pscal][1]. The associated [vignette][2] has lots of examples.\\n\\n\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/pscl/\\n  [2]: http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf",,
4127,2,1783,5ca57f4c-5748-4a92-8531-46bed34657be,2010-08-17 13:20:07.0,521.0,"I am familiar with different terminology. What you call precision I would positive predictive value (PPV). And what you call recall I would call sensitivity (Sens). :\\n\\nhttp://en.wikipedia.org/wiki/Receiver_operating_characteristic\\n\\nIn the case of sensitivity (recall), if the denominator is zero (as Amro points out), there are NO positive cases, so the classification is meaningless. (That does not stop either TP or FN being zero, which would result in a limiting sensitivity of 1 or 0. These points are respectively at the top right and bottom left hand corners of the ROC curve - TPR = 1 and TPR = 0.)\\n\\nThe limit of PPV is meaningful though. It is possible for the test cut-off to be set so high (or low) so that all cases are predicted as negative. This is at the origin of the ROC curve. The limiting value of the PPV just before the cutoff reaches the origin can be estimated by considering the final segment of the ROC curve just before the origin. (This may be better to model as ROC curves are notoriously noisy.)\\n\\nFor example if there are 100 actual positives and 100 actual negatives and the final segnemt of the ROC curve approaches from TPR = 0.08, FPR = 0.02, then the limiting PPV would be PPR ~ 0.08*100/(0.08*100 + 0.02*100) = 8/10 = 0.8 i.e 80% probability of being a true positive.\\n\\nIn practice each sample is represented by a segment on the ROC curve - horizontal for an actual negative and vertical for an actual positive. One could estimate the limiting PPV by the very last segment before the origin, but that would give an estimated limiting PPV of 1, 0 or 0.5, depending on whether the last sample was a true positive, a false positive (actual negative) or made of an equal TP and FP. A modelling approach would be better, perhaps assuming the data are binormal - a common assumption, eg:\\nhttp://mdm.sagepub.com/content/8/3/197.short\\n\\n\\n ",,
4128,5,1651,f4bdb2ee-632d-4973-bb22-5b702849c062,2010-08-17 13:21:27.0,8.0,"I need to fit $Y_{ij} \\sim NegBin(m_{ij},k)$, i.e. a negative binomial distribution to  count data. However, the data I have observed are censored - I know the value of $y_{ij}$, but it could be more than that value. The log-likelihood is\\n\\n\\begin{equation}\\nll = \\sum_{i=1}^n w_i (c_i \\log(P(Y_{ij}=y_{ij}|X_{ij})) + (1- c_i) \\log(1- \\sum_{k=1}^32 P(Y_{ij} = k|X_{ij})))\\n\\end{equation}\\n\\nwhere $X_{ij}$ represent the design matrix (with the covariates of interest), $w_i$ is the weight for each observation, $y_{ij}$ is the response variable and $P(Y_{ij}=y_{ij}|X_{ij})$ is the negative binomial distribution where the $m_{ij}=exp(X_{ij} \\beta)$ and $\\alpha$ is the over-dispersion parameter.\\n\\nDoes anyone know of an R package to tackle this problem?\\n",Fixed latex and made minor changes; added 8 characters in body,
4129,6,1781,e0bd4b12-a3eb-4716-a412-32f0504d32d6,2010-08-17 13:23:59.0,8.0,<correlation><estimation><censoring><variance-covariance>,edited tags,
4130,2,1784,8779f77c-0f64-4022-ad02-fce538d42086,2010-08-17 13:30:03.0,919.0,"Some additional ideas:\\n\\n(1) You needn't confine yourself to a logarithmic transformation.  Search this site for the ""data-transformation"" tag, for example.  Some data lend themselves well to certain transformations like a root or a logit.  (Such transformations--even logs--are usually to be avoided when publishing graphics for a non-technical audience.  On the other hand, they can be excellent tools for seeing patterns in data.)\\n\\n(2) You can borrow a standard cartographic technique of insetting a detail of a chart within or next to your chart.  Specifically, you would plot the extreme values by themselves on one chart and all (or the) rest of the data on another with a more limited axis range, then graphically arrange the two along with indications (visual and/or written) of the relationship between them. Think of a map of the US in which Alaska and Hawaii are inset at different scales.  (This won't work with all kinds of charts, but could be effective with the bar charts in your illustration.) [I see this is similar to mbq's recent answer.]\\n\\n(3) You can show the broken plot side-by-side with the same plot on unbroken axes.\\n\\n(4) In the case of your bar chart example, choose a suitable (perhaps hugely stretched) vertical axis and provide a panning utility.  [This is more of a trick than a genuinely useful technique, IMHO, but it might be useful in some special cases.]\\n\\n(5) Select a different schema to display the data.  Instead of a bar chart that uses length to represent values, choose a chart in which the areas of symbols represent the values, for example.  [Obviously trade-offs are involved here.]\\n\\nYour choice of technique will likely depend on the purpose of the plot: plots created for data exploration often differ from plots for general audiences, for example.",,
4132,2,1785,21d2a52f-09f4-4a7d-a775-a38d6a54cefe,2010-08-17 13:52:29.0,183.0,"I'm also outside my area of expertise, but assuming that you want to use R, here are a few thoughts.\\n\\n- There is a [bibtex package in R][1] for importing bibtex files.\\n- Various [character functions][2] could be used to extract the key words.\\n- The data sounds a little like a [two-mode network][3], which might mean packages like `sna` and `igraph` are useful.\\n- Plots of 2d multidimensional scaling can also also be useful in visualising similarities  (e.g., based on co-occurrence or some other measure) between words ([here's a tutorial][4]).\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/bibtex/bibtex.pdf\\n  [2]: http://www.statmethods.net/management/functions.html\\n  [3]: http://www.faculty.ucr.edu/~hanneman/nettext/C17_Two_mode.html\\n  [4]: http://www.statmethods.net/advstats/mds.html",,
4133,2,1786,19294458-0a2b-4d79-adc9-dce49a0680f2,2010-08-17 14:01:10.0,,You may want to take a look at the [phi coefficient][1] which is a measure of association for [nominal variables][2]. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Phi_coefficient\\n  [2]: http://en.wikipedia.org/wiki/Level_of_measurement#Nominal_scale,,user28
4134,4,1780,19092e64-95e8-40ee-a5f3-658051a3546a,2010-08-17 15:05:28.0,88.0,How to start an analysis of keywords from a bibliography and detect correlations?,Title fixed.,
4135,2,1787,9363d284-cefc-44e0-b842-6d33e4e87ecb,2010-08-17 15:56:39.0,253.0,"I am fitting a GLM model (in R), and would like to get an estimation of the variability of the coefficients estimated by the model.\\n\\nIf I understand it correctly the method to use in such a case is bootstraping (not, cross validation).\\n\\nAm I correct that an easy way to do this is by using the boot command from the boot package, then output the coefficients at each simulation, and at the end calculate their var?  Or is there something I might be missing? ",,
4136,1,1787,9363d284-cefc-44e0-b842-6d33e4e87ecb,2010-08-17 15:56:39.0,253.0,Using bootstrap for glm coefficients variance estimation (in R) ,,
4137,3,1787,9363d284-cefc-44e0-b842-6d33e4e87ecb,2010-08-17 15:56:39.0,253.0,<r><confidence-interval><variance><generalized-linear-model><bootstrap>,,
4138,2,1788,ebe988c1-7a76-4d6d-9ba6-4069bd96d95b,2010-08-17 16:17:00.0,561.0,"From a user on MedStats (Google Group):\\n\\n> ... no scientific worker has a fixed level of significance at which \\nfrom year to year, and in all circumstances, he rejects hypotheses; he \\nrather gives his mind to each particular case in the light of his \\nevidence and his ideas."" - Sir Ronald A. Fisher (1956)",,
4139,16,1788,ebe988c1-7a76-4d6d-9ba6-4069bd96d95b,2010-08-17 16:17:00.0,-1.0,,,
4140,2,1789,e56f490a-72fd-4a1a-86be-a75159806871,2010-08-17 16:27:30.0,979.0,"so you have a document x keyword matrix which basically represents a bipartite graph (or two-mode network depending on your cultural background) with edges between documents and tags. If you're not interested in individual documents - as I understand you -, you can create a network of keywords by counting the number of cooccurrences between each keyword. Simply plotting this graph might already give you a neat idea of what this data looks like. You can further tweak the visualization if you, e.g., scale the size of the keywords by the number of total occurrences, or (in case you have a lot of keywords) introduce a minimum number of total occurrences for a keyword to appear in the first place. \\n\\nAs a tool, I can only recommend [GraphViz][1] which allows you to specify graphs like\\n\\n    keyword1 -- keyword2\\n    keyword1 -- keyword3\\n    keyword1[label=""statistics"", fontsize=...]\\n\\nand ""compile"" them into pngs, pdfs, whatever, yielding very nice results (particularly if you play a bit with the font settings).\\n\\n\\n  [1]: http://www.graphviz.org/\\n",,
4141,2,1790,a3802282-f4f7-46f1-b8e8-f78b49124116,2010-08-17 16:39:24.0,979.0,"I'm trying to compare several methods by their performance on a set of synthetic data samples. For each method, I obtain a performance value between 0 and 1 for each of those samples. Then I plot a graph with the average performance per method\\n\\nThe problem now is that the achievable quality per sample varies strongly between different samples (if you wonder why, I generate random graphs and evaluate community detection methods on them, and sometimes strange things happen, like elements from the same community get disconnected due to sparseness etc). So showing error bars based on standard deviation or standard error tend to get really large. \\n\\nImagine one method yields [1, 0.5, 1], and the other one (one the same three samples) [0.5, 0.25, 0.5]. Which measure can I apply to *de*emphasize the between-sample variance in the series and emphasize the fact that method 1 always outperforms method 2? Or, to put it differently, how can I test whether method 1 is significantly better than method 2 without being mislead by the different range of the indiviual datapoints? (Also note that I typically have more than two methods to compare, this is just for the example)\\n\\nThanks,\\nNic",,
4142,1,1790,a3802282-f4f7-46f1-b8e8-f78b49124116,2010-08-17 16:39:24.0,979.0,Meaningful deviation measure with strongly varying datapoints,,
4143,3,1790,a3802282-f4f7-46f1-b8e8-f78b49124116,2010-08-17 16:39:24.0,979.0,<standard-deviation><statistical-significance><standard-error>,,
4144,2,1791,6df2128f-018e-4028-b830-707f9bee3c4a,2010-08-17 16:49:24.0,88.0,"You need to use some paired test, maybe paired t-test or a sign test is the distribution is really weired.",,
4145,5,1064,80b0d4b9-57f9-4266-92d4-f0be9de5db45,2010-08-17 17:33:06.0,74.0,A nice one I came about:\\n\\n> I think it is much more interesting to\\n> live with uncertainty than to live with\\n> answers that might be wrong.\\n\\nBy Richard Feynman ([link][1])\\n\\n\\n  [1]: http://www.youtube.com/watch?v=zeCHiUe1et0&feature=player_embedded#!,edited body,
4146,2,1792,119515ed-d0f6-4302-a8c3-1368eabb1ffc,2010-08-17 17:35:21.0,25.0,"I am very [wary of using logarithmic axes on bar graphs][1]. The problem is that you have to choose a starting point of the axis, and this is almost always arbitrary. You can choose to make two bars have very different heights, or almost the same height, merely by changing the minimum value on the axis. These three graphs all plot the same data:\\n![alt text][2]\\n\\nAn alternative to discontinuous axes, that no one has mentioned yet,is to simply show a table of values. In many cases, tables are easier to understand than graphs.\\n\\n\\n  [1]: http://www.graphpad.com/faq/viewfaq.cfm?faq=1477\\n  [2]: http://i.stack.imgur.com/RzVdD.png",,
4150,2,1794,48933adb-f504-4dfa-8ec9-637e2088a4fe,2010-08-17 18:31:59.0,339.0,"From my point of view, when there are two explanatory variables and both have just two levels, we have the famous two-by-two contingency table. Fisher’s exact test can take such a matrix as its sole argument. Alternatively you can use Pearson’s chi-squared test. \\n\\nIf your null hypothesis is not the 25:25:25:25 distribution across the four categories (i.e. say it's 9:3:3:1), you'll have to calculate the expected frequencies explicitly.\\n\\nThen perform the chi-squared test (in R) like that:\\n\\n    chisq.test(observed,p=c(9,3,3,1),rescale.p=TRUE)\\n    # rescale.p is needed because the probabilities do not sum to 1.0",,
4151,2,1795,d46071a1-2ee4-4b67-901f-61798ac33341,2010-08-17 18:48:40.0,919.0,"A useful approach when the variable is used as an independent factor in regression is to replace it by two variables: one is a binary indicator of whether it is zero and the other is the value of the original variable or a re-expression of it, such as its logarithm.  This technique is discussed in Hosmer & Lemeshow's book on logistic regression (and in other places, I'm sure).  Truncated probability plots of the positive part of the original variable are useful for identifying an appropriate re-expression.\\n\\nWhen the variable is the dependent one in a linear model, censored regression (like Tobit) can be useful, again obviating the need to produce a started logarithm.  This technique is common among econometricians.",,
4152,5,1483,fb4be71f-ebbb-4268-b351-b19cbc617a9b,2010-08-17 18:56:31.0,251.0,"Assuming the odds ratios are independent, you can proceed as you would in general with any estimate, only you have to look at the log odds.\\n\\nTake the difference of the log odds, $\\delta$.  The standard error of $\\delta$ is $\\sqrt{SE_{1}^2 + SE_{2}^2}$.  Then you can obtain a p-value for the ratio $z = \\delta/SE(\\delta)$ from the standard normal.\\n\\nUPDATE\\n\\nThe standard error of $\\log OR$ is the square root of the sum of the reciprocals of the frequencies:\\n\\n$SE(\\log OR) = \\sqrt{ {1 \\over n_1} + {1 \\over n_2} + {1 \\over n_3} + {1 \\over n_4} }$\\n\\nIn your case, each $n_i$ correspond to TP, FP, TN, FN.  ",added 260 characters in body,
4153,2,1796,6a35eef8-9360-4aa4-9781-c925172614e8,2010-08-17 19:08:05.0,,"I am not at all sure if ignoring the performance spread is a good idea. Ideally, you would want a method to be both reliable (i.e., have low spread) and be valid (i.e., give a performance measure of close to 1). Consider the following two output measures: \\n\\nMethod 1. [0.80, 0.60] \\n\\nMethod 2. [0.71, 0.69].\\n\\nUnlike your example, there is no method that clearly dominates and in fact both methods perform equally well on average. Thus you may want to choose the one that is more reliable (i.e., has lower spread).\\n\\nIf you accept the above reasoning then your null hypothesis should be:\\n\\n$$\\frac{\\mu_1}{\\sigma_1} = \\frac{\\mu_2}{\\sigma_2}$$\\n\\nThe above is analagous to the [Sharpe ratio][1] from finance and I am sure there is an extensive financial literature which discusses how to test hypothesis like the above and its extensions to more than 2 groups. Unfortunately, I am not well read up on that literature to help you.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Sharpe_ratio",,user28
4154,2,1797,5bcd0f65-5d8a-42ad-b590-1fafb4241c84,2010-08-17 19:15:32.0,601.0,"I was looking at this [page][1] and noticed the methods for confidence intervals for lme and lmer in R.  For those who don't know R, those are functions for generating mixed effects or multi-level models.  If I have fixed effects in something like a repeated measures design what would a confidence interval around the predicted value (similar to mean) mean?  I can understand that for an effect you can have a reasonable confidence interval but it seems to me a confidence interval around a predicted mean in such designs seems to be impossible.  It could either be very large to acknowledge the fact that the random variable contributes to uncertainty in the estimate, but in that case it wouldn't be useful at all in an inferential sense comparing across values.  Or, it would have to be small enough to use inferentially but useless as an estimate of the quality of the mean (predicted) value that you could find in the population.\\n\\nAm I missing something here or is my analysis of the situation correct?... [and probably a justification for why it isn't implemented in lmer (but easy to get in SAS). :)]\\n\\n  [1]: http://glmm.wikidot.com/faq",,
4155,1,1797,5bcd0f65-5d8a-42ad-b590-1fafb4241c84,2010-08-17 19:15:32.0,601.0,What would a confidence interval around a predicted value from a mixed effects model mean?,,
4156,3,1797,5bcd0f65-5d8a-42ad-b590-1fafb4241c84,2010-08-17 19:15:32.0,601.0,<r><confidence-interval><mixed-model><repeated-measures><sas>,,
4157,2,1798,70d02a95-7744-4d8d-98dd-5dd54f0b30bd,2010-08-17 19:30:14.0,88.0,"Yes, you are right. What boot does is that it just generates new training sets by drawing with replacement from the original set. So about 2/3 of the original objects are present in each of the new sets, still the size is the same, so it does not influence model building.",,
4158,2,1799,3bc6990a-b0b2-44e5-ad24-4d7ec11cd414,2010-08-17 20:00:59.0,835.0,"I am seeking recommendations and/or best practices for analyzing non-independent data. In particular, I am curious about non-independent data that does not reflect typical repeated-measures time-based data in which data for the same question(s) or stimuli are collected at different time points. Rather the data collected is elicited from similar (but not identical) questions or stimuli that are known to be related. A specific example follows.\\n\\nI have pain perception data for two groups (group A, group B) that can be further divided by gender (i.e., A-women; A-men; B-women; B-men). The pain perception dependent variables include 3 thermal threshold tests (i.e., detection, pain, tolerance) and pain magnitude estimates for a range of specific temperatures (temp X1, X2, X3, X4, X5). These pain perception variables are collected for both heat and cold stimuli (e.g., heat pain tolerance; cold pain tolerance; heat temp X1; cold temp X1). It should be expected (and indeed it is observed) that the various pain perception variables for a given individual are not independent.\\n\\nThe purpose of the analysis is to look for between group differences based on group membership (group A vs. group B) and sex (women vs men). It is desirable to find interactions between group membership and sex. It is also desirable to find interactions between specific pain perception variables (or types of variables; i.e., ""cold stimuli"") and group membership and/or sex. I have tried running a series of repeated-measures ANOVAs separately for each of the different groupings of pain perception variables (i.e., heat stimuli threshold tests; cold stimuli threshold tests; heat pain magnitude estimates; cold pain magnitude estimates); however this solution does not feel optimal or adequate. My specific questions are:\\n\\n1) is it appropriate to analyze data elicited from related (but not identical) questions/stimuli using repeated measures?\\n\\n2) Is it appropriate to analyze chunks of the data (e.g., cold pain magnitude estimates; cold stimuli threshold tests) separately?\\n\\n2) Would a different strategy (such as multilevel analysis / profile analysis / or some type of multivariate repeated measures ANOVA) be more appropriate?\\n\\n3) General recommendations and/or best practices for analyzing data such as this.\\n\\nThank you for your feedback and input\\n\\nPatrick Welch\\n\\nnote: I already searched the site for related questions (i.e., http://stats.stackexchange.com/questions/859/anova-with-non-independent-observations ; http://stats.stackexchange.com/questions/1324/parametric-techniques-for-n-related-samples) but believe the current question to be different enough to warrant unique consideration.",,
4159,1,1799,3bc6990a-b0b2-44e5-ad24-4d7ec11cd414,2010-08-17 20:00:59.0,835.0,Recommendations - or best practices - for analyzing non-independent data. Specific example relating to pain perception data provided.,,
4160,3,1799,3bc6990a-b0b2-44e5-ad24-4d7ec11cd414,2010-08-17 20:00:59.0,835.0,<statistical-analysis><repeated-measures><best-practices><within-subjects>,,
4161,2,1800,7479a9ae-cb4b-4b8f-989f-72dadc447e73,2010-08-17 20:09:56.0,919.0,"Tests and thousands of sample questions are available on the ARTIST (""Assessment Resource Tools for Improving Statistical Thinking"") site, https://app.gen.umn.edu/artist/tests/index.html .  Most are appropriate for an intro stats course.",,
4162,16,1800,7479a9ae-cb4b-4b8f-989f-72dadc447e73,2010-08-17 20:09:56.0,-1.0,,,
4163,2,1801,9ed99d9f-ce84-4566-a499-ec273849ffe5,2010-08-17 20:28:20.0,919.0,"John Tukey strongly and cogently argued for a *proportion* type of measurement in his book on *EDA*.  One thing that makes proportions special and different from the classical (and long outmoded) ""nominal, ordinal, interval, ratio"" taxonomy is that frequently they enjoy an obvious symmetry: A proportion can be thought of as the average of a binary (0/1) indicator variable.  Because it should not make any meaningful difference to recode the indicator, *the data analysis should remain essentially unchanged when you re-express the proportion as its complement.*  Specifically, recoding 0-->1 and 1-->0 changes the original proportion *p* to 1-*p*.  For example, it should make no difference to talk about 60% of people voting ""yes"" or 40% voting ""no"" in a referendum; the two numbers 0.6 and 0.4 represent exactly the same thing.  Thus, statistics, tests, decisions, summaries, etc., should  give the same results (*mutatis mutandis*) regardless of which form of expression is used.\\n\\nAccordingly, Tukey used re-expressions of proportions, and analyses based on those re-expressions, that are (almost) invariant under the conversion *p* <--> 1-*p*.  They are of the form *f* (*p*) +- *f* (1-*p*) for various functions *f*.  (Taking the minus sign is usually best because it continues to distinguish between *p* and 1-*p*: only their signs differ when re-expressed.)  When scaled so that the differential change near *p* = 0.5 equals 1, he called these the ""folded"" values.  Among them are the folded logarithm (""flog""), proportional to ln(*p*) - ln(1-*p*) = ln(*p*/(1-*p*)) = logit(*p*), and the folded root (""froot""), proportional to Sqrt(*p*) - Sqrt(1-*p*).\\n\\nA mathematical exposition of this topic is less convincing than seeing the statistics in action, so I recommend reading chapter 17 of EDA and studying the examples therein.\\n\\nIn sum, then, I am suggesting that the question itself is too limiting and that one should be open to possibilities that go beyond those suggested by the classical taxonomy of variables.",,
4164,2,1802,b088eaaf-eac0-4f08-8962-c0bf8bb19ed2,2010-08-17 20:58:12.0,919.0,"The **second question** seems to ask for a prediction interval for one future observation.  Such an interval is readily calculated under the assumptions that (a) the future observation is from the same distribution and (b) is independent of the previous sample.  When the underlying distribution is Normal, we just have to erect an interval around the difference of two Gaussian random variables.  Note that the interval will be *wider* than suggested by a naive application of a t-test or z-test, because it has to accommodate the variance of the future value, too.  This rules out all the answers I have seen posted so far, so I guess I had better quote one explicitly.  Hahn & Meeker's formula for the endpoints of this prediction interval is\\n\\n    m +- t * Sqrt(1 + 1/n) * s\\n\\nwhere *m* is the sample mean, *t* is an appropriate two-sided critical value of Student's t (for *n*-1 df), *s* is the sample standard deviation, and *n* is the sample size.  Note in particular the factor of Sqrt(1+1/*n*) instead of Sqrt(1/*n*).  That's a big difference!\\n\\nThis interval is used like any other interval: the requested test simply examines whether the new value lies within the prediction interval.  If so, the new value is consistent with the sample; if not, we reject the hypothesis that it was independently drawn from the same distribution as the sample.  Generalizations from one future value to *k* future values or to the mean (or max or min) of *k* future values, etc., exist.\\n\\nThere is a extensive literature on prediction intervals especially in a regression context.  Any decent regression textbook will have formulas.  You could begin with the Wikipedia entry ;-).  Hahn & Meeker's *Statistical Intervals* is still in print and is an accessible read.\\n\\nThe **first question** has an an answer that is so routine nobody seems yet to have given it here (although some of the links provide details).  For completeness, then, I will close by remarking that when the population has approximately a Normal distribution, the sample standard deviation is distributed as the square root of a scaled chi-square variate of *n*-1 df whose expectation is the population variance.  That means (roughly) we expect the sample sd to be close to the population sd and the difference between the two usually will be on the order of Sqrt(*n*-1) times the population sd.  Unlike parallel statements for the sample mean (which invoke the CLT), this statement relies fairly strongly on the assumption of a Normal population.",,
4165,2,1803,9806fcf8-f2c8-44f9-a911-bbc846b3f98e,2010-08-17 21:25:25.0,573.0,"You could try to employ the [theory][1] and [praxis][2] of association analysis or market basket analysis to your problem (just read ""items"" as ""keywords"" / ""cited reference"" and ""market basket"" as ""journal article"").\\n\\nDisclaimer - this is just an idea, I did not do anything like that myself. Just my 2Cents.\\n\\n\\n  [1]: http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf\\n  [2]: http://cran.r-project.org/web/packages/arules/vignettes/arules.pdf",,
4166,5,1657,a51ce4a4-d0f7-4dca-a9d0-3e4deec48d5f,2010-08-17 22:11:08.0,919.0,"[CauseWeb][1] has a collection of statistics quotations.  Many have already been repeated here, but it has plenty that haven't yet been quoted, such as\\n\\n> ""The only statistics you can trust are those you falsified yourself.""\\n\\n(Falsely attributed to Sir Winston Churchill.)  For the rest, follow the CauseWeb links to Resources->Fun->Quote.\\n\\n  [1]: http://www.causeweb.org/",Improved link formatting,
4167,2,1804,b0ed89ec-b4e3-4ac6-9dcd-1dec4e5aca4d,2010-08-17 22:49:17.0,919.0,"Just a thought: you might not need the full SVD for your problem.  Let **M** = **U S V*** be the SVD of your *d* by *n* matrix (*i.e.*, the time series are the columns).  To achieve the dimension reduction you'll be using the matrices **V** and **S**.  You can find them by diagonalizing **M* M** = **V (S*S) V***.  However, because you are missing some values, you cannot compute **M* M**.  Nevertheless, you can estimate it.  Its entries are sums of products of columns of **M**.  When computing any of the SSPs, ignore pairs involving missing values.  Rescale each product to account for the missing values: that is, whenever a SSP involves *n-k* pairs, rescale it by *n/(n-k).*  This procedure is a ""reasonable"" estimator of **M* M** and you can proceed from there.  If you want to get fancier, maybe multiple imputation techniques or *Matrix Completion* will help.\\n\\n(This can be carried out in many statistical packages by computing a pairwise covariance matrix of the transposed dataset and applying PCA or factor analysis to it.)",,
4168,2,1805,4fef1402-c276-4095-96ac-b2de4e4b7a2f,2010-08-17 23:42:12.0,561.0,I was taught to only apply Fisher's Exact Test in contingency tables that were 2x2.\\n\\nQuestions:\\n\\n1) Did Fisher himself ever envision this test to be used in tables larger than 2x2 (I am aware of the tale of him devising the test while trying to guess [whether an old woman could tell if milk was added to tea or tea was added to milk][1])\\n\\n2) Stata allows me to use Fisher's exact test to any contingency table. Is this valid?\\n\\n3) Is it preferable to use FET when expected cell counts in a contingency table are < 5?\\n\\nMany thanks!\\n\\n  [1]: http://en.wikipedia.org/wiki/Fisher's_exact_test,,
4169,1,1805,4fef1402-c276-4095-96ac-b2de4e4b7a2f,2010-08-17 23:42:12.0,561.0,Fisher's Exact Test in contingency tables larger than 2x2,,
4170,3,1805,4fef1402-c276-4095-96ac-b2de4e4b7a2f,2010-08-17 23:42:12.0,561.0,<contingency-tables>,,
4171,2,1806,754e5625-024c-413e-944f-4fa1dd882052,2010-08-18 00:10:56.0,159.0,"The only problem with applying Fisher's exact test to tables larger than 2x2  is that the calculations become much more difficult to do. The 2x2 version is the only one which is even feasible by hand, and so I doubt that Fisher ever imagined the test in larger tables because the computations would have been beyond anything he would have envisaged.\\n\\nNevertheless, the test can be applied to any mxn table and some software including Stata and SPSS provide the facility. Even so, the calculation is often approximated using a Monte Carlo approach.\\n\\nYes, if the expected cell counts are small, it is better to use an exact test as the chi-squared test is no longer a good approximation in such cases.",,
4172,5,1723,cd0d1444-545b-4273-8c07-9dff4d4e5231,2010-08-18 00:52:39.0,805.0,"For normality, actual Shapiro-Wilk has good power in fairly small samples. \\n\\nThe main competitor in studies that I have seen is the more general Anderson-Darling, which does fairly well, but I wouldn't say it was better. If you can clarify what alternatives interest you, possibly a better statistic would be more obvious. [edit: if you estimate parameters, the A-D test should be adjusted for that.]\\n\\n[I strongly recommend against considering Jarque-Bera in small samples (which probably better known as Bowman-Shenton in statistical circles - they studied the small sample distribution). The asymptotic joint distribution of skewness and kurtosis is *nothing like* the small-sample distribution - in the same way a banana doesn't look much like an orange. It also has very low power against some interesting alternatives - for example it is powerless to pick up a symmetric *bimodal* distribution that has kurtosis close to that of a normal distribution.]\\n\\nFrequently people test goodness of fit for what turn out to be not-particularly-good reasons, or they're answering a question other than the one that they actually want to answer. \\n\\nFor example, you almost certainly already know your data aren't really normal (not exactly), so there's no point in trying to answer a question you know the answer to - and the hypothesis test *doesn't actually answer it anyway*. \\n\\nGiven you know you don't have exact normality already, your hypothesis test of normality is really giving you an answer to a question closer to ""is my sample size large enough to pick up the amount of non-normality that I have"", while the real question you're interested in answering is usually closer to ""what is the *impact* of this non-normality on these other things I'm interested in?"". The hypothesis test is measuring sample size, while the question you're interested in answering is not very dependent on sample size.\\n\\nThere are times when testing of normality makes some sense, but those situations almost never occur with small samples.\\n\\nWhy are you testing normality?\\n",added 78 characters in body; added 31 characters in body; added 155 characters in body,
4173,5,1648,ca2302e7-d7eb-45e7-bd03-fc13cef55dbd,2010-08-18 01:11:18.0,5.0,"The [**fBasics**][1] package in R (part of [Rmetrics][2]) includes [several normality tests][3], covering many of the popular [frequentist tests][4] -- Kolmogorov-Smirnov, Shapiro-Wilk's, Jarque–Bera, and D'Agostino -- along with a wrapper for the normality tests in the [**nortest**][5] package -- Anderson–Darling, Cramer–von Mises, Lilliefors (Kolmogorov-Smirnov), Pearson chi–square, and Shapiro–Francia.  The package documentation also provides all the important references.  Here is a demo that shows how to use the [tests from nortest][6].\\n\\nOne approach, if you have the time, is to use more than one test and check for agreement.  The tests vary in a number of ways, so it isn't entirely straightforward to choose ""the best"".  What do other researchers in your field use?  This can vary and it may be best to stick with the accepted methods so that others will accept your work.  I frequently use the Jarque-Bera test, partly for that reason, and Anderson–Darling for comparison.\\n\\nYou can look at [""Comparison of Tests for Univariate Normality""][7] (Seier 2002) and [""A comparison of various tests of normality""][8] (Yazici; Yolacan 2007) for a comparison and discussion of the issues.\\n\\nIt's also trivial to test these methods for comparison in R, thanks to all the [distribution functions][9].  Here's a simple example with simulated data (I won't print out the results to save space), although a more full exposition would be required:\\n\\n    library(fBasics); library(ggplot2)\\n    set.seed(1)\\n\\n    # normal distribution\\n    x1 <- rnorm(1e+06)   \\n    x1.samp <- sample(x1, 200)\\n    qplot(x1.samp, geom=""histogram"")\\n    jbTest(x1.samp)\\n    adTest(x1.samp)\\n\\n    # cauchy distribution\\n    x2 <- rcauchy(1e+06)\\n    x2.samp <- sample(x2, 200)\\n    qplot(x2.samp, geom=""histogram"")\\n    jbTest(x2.samp)\\n    adTest(x2.samp)\\n\\nOnce you have the results from the various tests over different distributions, you can compare which were the most effective.  For instance, the p-value for the Jarque-Bera test above returned 0.276 for the normal distribution (accepting) and < 2.2e-16 for the cauchy (rejecting the null hypothesis).\\n\\n  [1]: http://cran.r-project.org/web/packages/fBasics/index.html\\n  [2]: https://www.rmetrics.org/\\n  [3]: http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/fBasics/html/NormalityTests.html\\n  [4]: http://en.wikipedia.org/wiki/Normality_test#Frequentist_tests\\n  [5]: http://cran.r-project.org/web/packages/nortest/index.html\\n  [6]: http://duncanjg.files.wordpress.com/2008/12/ksdemo3.pdf\\n  [7]: http://interstat.statjournals.net/YEAR/2002/articles/0201001.pdf\\n  [8]: http://www.informaworld.com/smpp/content~db=all~content=a759350109\\n  [9]: http://cran.r-project.org/doc/manuals/R-intro.html#Probability-distributions",added 41 characters in body,
4174,2,1807,eee3ef79-73e6-40f2-9ee0-f427bac0e50c,2010-08-18 01:39:31.0,850.0,"""Student's"" $t$ test requires the sample standard deviation $s$. However, how do I compute for $s$ when only the sample size and sample average are known?\\n\\nFor example, if sample size is $49$ and sample average is $112$, I will then attempt to create a list of $49$ identical samples with values of $112$ each. Expectedly, the sample standard deviation is $0$. This will create a divide-by-zero problem in the $t$ test.",,
4175,1,1807,eee3ef79-73e6-40f2-9ee0-f427bac0e50c,2010-08-18 01:39:31.0,850.0,"How to perform ""Student's"" t test when only sample size, sample average and population average are known",,
4176,3,1807,eee3ef79-73e6-40f2-9ee0-f427bac0e50c,2010-08-18 01:39:31.0,850.0,<standard-deviation><t-test>,,
4179,2,1810,20106aed-08c5-4ecb-94db-a0aad4ec0acd,2010-08-18 02:11:10.0,25.0,"I presume you are referring to a one sample t test. Its goal is to compare the mean of your sample with a hypothetical mean. It then computes (and your population is Gaussian) a P value that answers this question: If the population mean really was the hypothetical value, how unlikely would it be to draw a sample whose mean is as far from that value (or further) than you observed? Of course, the answer to that question depends on sample size. But it also depends on variability. If your data have a huge amount of scatter, they are consistent with a broad range of population means. If your data are really tight, they are consistent with a smaller range of population means. ",,
4180,5,1807,7b0ddc47-cf6f-4272-86c3-e8b22bd0b399,2010-08-18 02:58:17.0,850.0,"""Student's"" $t$ test requires the sample standard deviation $s$. However, how do I compute for $s$ when only the sample size and sample average are known?\\n\\nFor example, if sample size is $49$ and sample average is $112$, I will then attempt to create a list of $49$ identical samples with values of $112$ each. Expectedly, the sample standard deviation is $0$. This will create a divide-by-zero problem in the $t$ test.\\n\\n__ADDITIONAL DATA:__  \\nThe average income of ACME North Factory workers is $\\$200$. It is reported that a random sample of $49$ workers in ACME South Factory had an annual income of $\\$112$. Is this difference statistically significant?\\n\\nAm I correct in saying that the population mean is $\\$200$?",added 304 characters in body,
4181,2,1811,1db862f1-2e68-469e-8ccf-969ba9678433,2010-08-18 05:40:49.0,521.0,"This does look to be a slightly contrived question. 49 is an exact square of 7. The value of a t-distribution with 48 DoF for a two-sided test of p<0.05 is very nearly 2 (2.01).\\n\\nWe reject the null hypothesis of equality of means if |sample_mean - popn_mean| > 2*StdError, i.e. 200-112 > 2*SE so SE < 44, i.e. SD < 7*44 = 308.\\n\\nIt would be impossible to get a normal distribution with a mean of 112 with a standard deviation of 308 (or more) without negative wages. \\n\\nGiven wages are bounded below, they are likely to be skew, so assuming a log-normal distribution would be more appropriate, but it would still require highly variable wages to avoid a p<0.05 on a t-test.",,
4182,5,1807,2fc890d8-1f4e-43e8-a8eb-b1abfffe416d,2010-08-18 06:22:24.0,88.0,"Student's $t$-test requires the sample standard deviation $s$. However, how do I compute for $s$ when only the sample size and sample average are known?\\n\\nFor example, if sample size is $49$ and sample average is $112$, I will then attempt to create a list of $49$ identical samples with values of $112$ each. Expectedly, the sample standard deviation is $0$. This will create a divide-by-zero problem in the $t$ test.\\n\\n__ADDITIONAL DATA:__  \\nThe average income of ACME North Factory workers is $\\$200$. It is reported that a random sample of $49$ workers in ACME South Factory had an annual income of $\\$112$. Is this difference statistically significant?\\n\\nAm I correct in saying that the population mean is $\\$200$?",Fixed title and body.,
4183,4,1807,2fc890d8-1f4e-43e8-a8eb-b1abfffe416d,2010-08-18 06:22:24.0,88.0,"How to perform Student's t-test having only sample size, sample average and population average are known?",Fixed title and body.,
4184,5,1810,fdae09b5-1c9f-46d4-9a41-4dfecb513543,2010-08-18 06:34:34.0,25.0,"I presume you are referring to a one sample t test. Its goal is to compare the mean of your sample with a hypothetical mean. It then computes (assuming your population is Gaussian) a P value that answers this question: If the population mean really was the hypothetical value, how unlikely would it be to draw a sample whose mean is as far from that value (or further) than you observed? Of course, the answer to that question depends on sample size. But it also depends on variability. If your data have a huge amount of scatter, they are consistent with a broad range of population means. If your data are really tight, they are consistent with a smaller range of population means. ",added 5 characters in body,
4186,2,1812,75fdc529-acb7-4560-bacc-4d2650b33e96,2010-08-18 07:36:59.0,339.0,"One of the most important issues in using factor analysis is its interpretation. Factor analysis often uses factor rotation to enhance its interpretation. After a satisfactory rotation, the rotated factor loading matrix **L'** will have the same ability to represent the correlation matrix and it can be used as the factor loading matrix, instead of the unrotated matrix **L**.\\n\\nThe purpose of rotation is to make the rotated factor loading matrix have some desirable properties. One of the methods used is to rotate the factor loading matrix such that the rotated matrix will have a **simple structure**.\\n\\nL. L. Thurstone introduced the Principle of Simple Structure, as a general guide for factor rotation:\\n\\nSimple Structure Criteria:\\n--------------------------\\n\\n1.      Each row of the factor matrix should contain at least one zero\\n2.      If there are m common factors, each column of the factor matrix should have at least m zeros\\n3.      For every pair of columns in the factor matrix, there should be several variables for which entries approach zero in the one column but not in the other\\n4.      For every pair of columns in the factor matrix, a large proportion of the variables should have entries approaching zero in both columns when there are four or more factors\\n5.      For every pair of columns in the factor matrix, there should be only a small number of variables with nonzero entries in both columns\\n\\nThe ideal simple structure is such that:\\n\\n1.     each item has a high, or meaningful, loading on one factor only and\\n2.     each factor have high, or meaningful, loadings for only some of the items.\\n\\nThe problem is that, trying several combinations of rotation methods along with the parameters that each one accepts (especially for oblique ones), the number of candidate matrices increases and it is very difficult to see which one better meets the above criteria.\\n\\nWhen I first faced that problem I realized that I was unable to select the best match by merely 'looking' at them, and that I needed an algorithm to help me decide. Under the stress of project's deadlines, the most I could do was to write the following code in MATLAB, which accepts one rotation matrix at a time and returns (under some assumptions) whether each criterion is met or not.\\nA new version (If I would ever tried to upgrade it) would accept a 3d matrix (a set of 2d matrices) as an argument, and the algorithm should return the one that better fits the above criteria.\\n\\nHow would you extract an algorithm out of those criteria? I am just asking for your opinions (I also think that there's been criticism over the usefulness of the method by itself) and perhaps better approaches to the rotation matrix selection problem. \\n\\nAlso, I would like to know what software do you prefer to perform FA. If it's R, what package do you use? (I must admit that if I had to do FA, I would turn to SPSS again). If someone wants to provide some code, I would prefer R or MATLAB. \\n\\nP.S. The above [Simple Structure Criteria formulation][1] can be found in the book *""Making Sense of Factor Analysis""* by PETT, M., LACKEY, N., SULLIVAN, J.\\n\\nP.S.2 (from the same book): *""A test of successful factor analysis is the extent to which it can reproduce the original corr matrix. If you also used oblique solutions, among all select the one that generated the greatest number of highest and lowest factor loadings.""* \\nThis sounds like another constraint that the algorithm could use.\\n\\nP.S.3 This question has also been asked [here][2]. However, I think it fits better on this site.\\n\\n    function [] = simple_structure_criteria (my_pattern_table)\\n    %Simple Structure Criteria\\n    %Making Sense of Factor Analysis, page 132\\n    \\n    disp(' ');\\n    disp('Simple Structure Criteria (Thurstone):');\\n    disp('1. Each row of the factor matrix should contain at least one zero');\\n    disp( '2. If there are m common factors, each column of the factor matrix should have at least m zeros');\\n    disp( '3. For every pair of columns in the factor matrix, there should be several variables for which entries approach zero in the one column but not in the other');\\n    disp( '4. For every pair of columns in the factor matrix, a large proportion of the variables should have entries approaching zero in both columns when there are four or more factors');\\n    disp( '5. For every pair of columns in the factor matrix, there should be only a small number of variables with nonzero entries in both columns');\\n    disp(' ');\\n    disp( '(additional by Pedhazur and Schmelkin) The ideal simple structure is such that:');\\n    disp( '6. Each item has a high, or meaningful, loading on one factor only and');\\n    disp( '7. Each factor have high, or meaningful, loadings for only some of the items.');\\n    \\n    disp('')\\n    disp('Start checking...')\\n    \\n    %test matrix\\n    %ct=[76,78,16,7;19,29,10,13;2,6,7,8];\\n    %test it by giving: simple_structure_criteria (ct)\\n    \\n    ct=abs(my_pattern_table);\\n    \\n    items=size(ct,1);\\n    factors=size(ct,2);\\n    my_zero = 0.1;\\n    approach_zero = 0.2;\\n    several = floor(items / 3);\\n    small_number = ceil(items / 4);\\n    large_proportion = 0.30;\\n    meaningful = 0.4;\\n    some_bottom = 2;\\n    some_top = floor(items / 2);\\n    \\n    % CRITERION 1\\n    disp(' ');\\n    disp('CRITERION 1');\\n    for i = 1 : 1 : items\\n        count = 0;\\n        for j = 1 : 1 : factors\\n            if (ct(i,j) < my_zero)\\n                count = count + 1;\\n                break\\n            end\\n        end\\n        if (count == 0)\\n            disp(['Criterion 1 is NOT MET for item ' num2str(i)])\\n        end\\n    end\\n    \\n    \\n    % CRITERION 2\\n    disp(' ');\\n    disp('CRITERION 2');\\n    for j = 1 : 1 : factors \\n        m=0;\\n        for i = 1 : 1 : items\\n            if (ct(i,j) < my_zero)\\n                m = m + 1;\\n            end\\n        end\\n        if (m < factors)\\n            disp(['Criterion 2 is NOT MET for factor ' num2str(j) '. m = ' num2str(m)]);\\n        end\\n    end\\n    \\n    % CRITERION 3\\n    disp(' ');\\n    disp('CRITERION 3');\\n    for c1 = 1 : 1 : factors - 1\\n        for c2 = c1 + 1 : 1 : factors\\n            test_several = 0;\\n            for i = 1 : 1 : items\\n                if ( (ct(i,c1)>my_zero && ct(i,c2)<my_zero) || (ct(i,c1)<my_zero && ct(i,c2)>my_zero) ) % approach zero in one but not in the other\\n                    test_several = test_several + 1;\\n                end\\n            end\\n            disp(['several = ' num2str(test_several) ' for factors ' num2str(c1) ' and ' num2str(c2)]);\\n            if (test_several < several)\\n                disp(['Criterion 3 is NOT MET for factors ' num2str(c1) ' and ' num2str(c2)]);\\n            end\\n        end\\n    end\\n    \\n    % CRITERION 4\\n    disp(' ');\\n    disp('CRITERION 4');\\n    if (factors > 3)\\n        for c1 = 1 : 1 : factors - 1\\n            for c2 = c1 + 1 : 1 : factors\\n                test_several = 0;\\n                for i = 1 : 1 : items\\n                    if (ct(i,c1)<approach_zero && ct(i,c2)<approach_zero) % approach zero in both\\n                        test_several = test_several + 1;\\n                    end\\n                end\\n                disp(['large proportion = ' num2str((test_several / items)*100) '% for factors ' num2str(c1) ' and ' num2str(c2)]);\\n                if ((test_several / items) < large_proportion)\\n                    pr = sprintf('%4.2g',  (test_several / items) * 100 );\\n                    disp(['Criterion 4 is NOT MET for factors ' num2str(c1) ' and ' num2str(c2) '. Proportion is ' pr '%']);\\n                end\\n            end\\n        end\\n    end\\n    \\n    % CRITERION 5\\n    disp(' ');\\n    disp('CRITERION 5');\\n    for c1 = 1 : 1 : factors - 1\\n        for c2 = c1 + 1 : 1 : factors\\n            test_number = 0;\\n            for i = 1 : 1 : items\\n                if (ct(i,c1)>approach_zero && ct(i,c2)>approach_zero) % approach zero in both\\n                    test_number = test_number + 1;\\n                end\\n            end\\n            disp(['small number = ' num2str(test_number) ' for factors ' num2str(c1) ' and ' num2str(c2)]);\\n            if (test_number > small_number)\\n                disp(['Criterion 5 is NOT MET for factors ' num2str(c1) ' and ' num2str(c2)]);\\n            end\\n        end\\n    end\\n    \\n    % CRITERION 6\\n    disp(' ');\\n    disp('CRITERION 6');\\n    for i = 1 : 1 : items\\n        count = 0;\\n        for j = 1 : 1 : factors\\n            if (ct(i,j) > meaningful)\\n                count = count + 1;\\n            end\\n        end\\n        if (count == 0 || count > 1)\\n            disp(['Criterion 6 is NOT MET for item ' num2str(i)])\\n        end\\n    end\\n    \\n    % CRITERION 7\\n    disp(' ');\\n    disp('CRITERION 7');\\n    for j = 1 : 1 : factors \\n        m=0;\\n        for i = 1 : 1 : items\\n            if (ct(i,j) > meaningful)\\n                m = m + 1;\\n            end\\n        end\\n        disp(['some items = ' num2str(m) ' for factor ' num2str(j)]);\\n        if (m < some_bottom || m > some_top)\\n            disp(['Criterion 7 is NOT MET for factor ' num2str(j)]);\\n        end\\n    end\\n    disp('')\\n    disp('Checking completed.')\\n    return\\n\\n\\n  [1]: http://books.google.gr/books?id=5Jyaa2LQWbQC&pg=PA132&lpg=PA132&dq=%22The+criteria+of+simple+structure+was+originally+proposed%22&source=bl&ots=OXsCeU0Nzi&sig=qfiLMUyXsgwPunWBgNdSEeBNm34&hl=el&ei=huZeS5nkApTE4gaC67HsCw&sa=X&oi=book_result&ct=result&resnum=1&ved=0CAcQ6AEwAA#v=onepage&q=%22The%20criteria%20of%20simple%20structure%20was%20originally%20proposed%22&f=false\\n  [2]: http://stackoverflow.com/questions/2139703/fa-choosing-rotation-matrix-based-on-simple-structure-criteria",,
4187,1,1812,75fdc529-acb7-4560-bacc-4d2650b33e96,2010-08-18 07:36:59.0,339.0,"FA: Choosing Rotation matrix, based on ""Simple Structure Criteria""",,
4188,3,1812,75fdc529-acb7-4560-bacc-4d2650b33e96,2010-08-18 07:36:59.0,339.0,<r><algorithms><factor-analysis><matlab>,,
4189,2,1813,f29ac1d9-2595-428f-aff0-c9b4ef278927,2010-08-18 08:29:52.0,986.0,"long time since I've had to deal with hypothesis tests, hence I guess it's better to ask here first.\\n\\nI have two implementations of a genetic algorithm which are supposed to behave equivalently. However due to technical restrictions which cannot be resolved their output is not exactly the same, given the same input.\\n\\nStill I'd like to show that there is no significant performance difference.\\n\\nI have 20 runs for each of the two algorithms, using different initial random number seeds. For each run the minimum error in the population was recorded. A run consists of 1000 generations, so I have 1000 values per run. I cannot get more data, as the calculations are very expensive.\\n\\nWhich test should I employ? An easy way would probably be to only compare the error in the final generations (again, which test would I use here)? But one might also think about comparing the convergence behaviour in general.",,
4190,1,1813,f29ac1d9-2595-428f-aff0-c9b4ef278927,2010-08-18 08:29:52.0,986.0,Comparing two genetic algorithms,,
4191,3,1813,f29ac1d9-2595-428f-aff0-c9b4ef278927,2010-08-18 08:29:52.0,986.0,<hypothesis-testing>,,
4192,5,1813,ef492cb8-7cbc-477b-8ef9-bd63de751371,2010-08-18 08:35:41.0,986.0,"long time since I've had to deal with hypothesis tests, hence I guess it's better to ask here first.\\n\\nI have two implementations of a genetic algorithm which are supposed to behave equivalently. However due to technical restrictions which cannot be resolved their output is not exactly the same, given the same input.\\n\\nStill I'd like to show that there is no significant performance difference.\\n\\nI have 20 runs with the same configuration for each of the two algorithms, using different initial random number seeds. For each run the minimum error in the population was recorded. A run consists of 1000 generations, so I have 1000 values per run. I cannot get more data, as the calculations are very expensive.\\n\\nWhich test should I employ? An easy way would probably be to only compare the error in the final generations (again, which test would I use here)? But one might also think about comparing the convergence behaviour in general.",added 28 characters in body,
4193,2,1814,d1203e31-20f4-48ae-8b4b-670b850f0167,2010-08-18 08:37:21.0,183.0,"Great Question. This is not really an answer, but just a few thoughts.\\n\\nIn most of the applications where I have used factor analysis, permitting correlated factors makes more theoretical sense. I tend to rely on the proxmax rotation method. I used to do this in SPSS and now I use the `factanal` function in R. \\n\\n",,
4194,2,1815,a6d4e705-4336-4021-9548-5ec4877a753a,2010-08-18 08:54:23.0,174.0,"What are the panel's recommendations for books on design of experiments?\\n\\nIdeally, books should be still in print or available electronically, although that may not always be feasible. If you feel moved to add a few words on what's so good about the book that would be great too.\\n\\nAlso, aim for one book per answer so that voting can help sort the suggestions.\\n\\n(Community Wiki, please edit the question if you can make it better!)",,
4195,1,1815,a6d4e705-4336-4021-9548-5ec4877a753a,2010-08-18 08:54:23.0,174.0,Recommended books on experiment design?,,
4196,3,1815,a6d4e705-4336-4021-9548-5ec4877a753a,2010-08-18 08:54:23.0,174.0,<books><experiment-design>,,
4197,16,1815,a6d4e705-4336-4021-9548-5ec4877a753a,2010-08-18 08:54:23.0,174.0,,,
4198,2,1816,47b63383-832f-4a6f-9ec7-352f0fd22d96,2010-08-18 08:58:46.0,8.0,"Testing stochastic algorithms can be rather tricky!\\n\\nI work in systems biology and there are many stochastic simulators available to use to simulate a model. Testing these simulators is tricky since any two realizations from a single model will be typically different.\\n\\nIn the [dsmts][1] we have calculate (analytically) the expected value and variance of a particular model. We then perform a hypothesis test to determine if a simulator differs from the truth. Section 3 of the [userguide][2] gives the details. Essentially we do a t-test for the mean values and a chi-squared test for variances.\\n\\nIn your case, you are comparing two simulators so you should just use a two-sampled t-test instead.\\n\\n\\n  [1]: http://code.google.com/p/dsmts/\\n  [2]: http://code.google.com/p/dsmts/downloads/detail?name=dsmts-userguide31.pdf",,
4199,5,1813,1cf72287-5496-4116-adf0-6fe8dcca077d,2010-08-18 08:59:04.0,8.0,"I have two implementations of a genetic algorithm which are supposed to behave equivalently. However due to technical restrictions which cannot be resolved their output is not exactly the same, given the same input.\\n\\nStill I'd like to show that there is no significant performance difference.\\n\\nI have 20 runs with the same configuration for each of the two algorithms, using different initial random number seeds. For each run the minimum error in the population was recorded. A run consists of 1000 generations, so I have 1000 values per run. I cannot get more data, as the calculations are very expensive.\\n\\nWhich test should I employ? An easy way would probably be to only compare the error in the final generations (again, which test would I use here)? But one might also think about comparing the convergence behaviour in general.",deleted 108 characters in body,
4200,6,1813,70d0a3a7-f6ec-4311-be20-c04fc590e3c4,2010-08-18 08:59:40.0,8.0,<hypothesis-testing><genetic-algorithms>,edited tags,
4201,5,1790,fc2e2940-24a5-440e-9d89-a157228e8615,2010-08-18 10:17:34.0,979.0,"I'm trying to compare several methods by their performance on a set of synthetic data samples. For each method, I obtain a performance value between 0 and 1 for each of those samples. Then I plot a graph with the average performance per method\\n\\nThe problem now is that the achievable quality per sample varies strongly between different samples (if you wonder why, I generate random graphs and evaluate community detection methods on them, and sometimes strange things happen, like elements from the same community get disconnected due to sparseness etc). So showing error bars based on standard deviation or standard error tend to get really large. \\n\\nImagine one method yields [1, 0.5, 1], and the other one (one the same three samples) [0.5, 0.25, 0.5]. Which measure can I apply to *de*emphasize the between-sample variance in the series and emphasize the fact that method 1 always outperforms method 2? Or, to put it differently, how can I test whether method 1 is significantly better than method 2 without being mislead by the different range of the indiviual datapoints? (Also note that I typically have more than two methods to compare, this is just for the example)\\n\\nThanks,\\nNic\\n\\n**Update**\\nOne thing I've done is to count, for each method, how many times its performance is within 95% of the top performance. The picture pretty much speaks in favor of sample-based variance, not robust vs less robust methods. However, I'm still uncertain about how to generate a statistically valid statement from that..?",added 331 characters in body,
4202,2,1817,58f9cf51-ec40-4504-b9f0-d7fb65daebee,2010-08-18 10:40:25.0,521.0,Montgomery's Design and Analysis of Experiments is a classic and highly regarded ttext:\\nhttp://www.amazon.com/Design-Analysis-Experiments-Douglas-Montgomery/dp/0470128666/ref=pd_cp_b_0\\n\\nIf you are interested in experimental designb in a particular field (eg. clinical trials) other more specialised texts may be appropriate.\\n,,
4203,16,1817,58f9cf51-ec40-4504-b9f0-d7fb65daebee,2010-08-18 10:40:25.0,-1.0,,,
4204,2,1818,53f5f02b-d986-4de9-8f36-2e7d0174604f,2010-08-18 10:56:21.0,588.0,"I need some help about repeated measurement ANOVA.\\n\\nWe are investigating the effect of some intervention on reducing blood stream infection (BSI) rate in some wards. We plan to capture the BSI rate information at a monthly basis, 12 months without intervention first, then 12 months with intervention.\\n\\nWe are thinking of doing either time-series or repeated measurement ANOVA, I prefer the later one before I don't have much idea to do on the first one (extra question: too little time points, right?), but then here come another problem, how many wards do we need to show that there is really a statistically significant effect of intervention on BSI rate?\\n\\nI think I'll do two ANOVA, one for ""before intervention"", one for ""during intervention"", and I suppose that the ANOVA ""before intervention"" should not have a significant F-ratio test.\\n\\nI consider the term ""sample size"" two-dimensionally, either the number of wards, or the number of repeated measurements.\\n\\nI'm a newbie on this, so I don't know if I have asked anything stupid or missed something obvious, sorry if I have committed these mistake.\\n\\nThanks!",,
4205,1,1818,53f5f02b-d986-4de9-8f36-2e7d0174604f,2010-08-18 10:56:21.0,588.0,how to determine the sample size needed for repeated measurement ANOVA?,,
4206,3,1818,53f5f02b-d986-4de9-8f36-2e7d0174604f,2010-08-18 10:56:21.0,588.0,<anova><repeated-measures><sample-size>,,
4207,2,1819,3c27415e-731d-48b4-b3cb-795949f3da74,2010-08-18 10:56:44.0,561.0,"If you're interested in pharmaceutical trials, two books I recommend:\\n\\n 1. Statistical Issues in Drug Development by Stephen Senn ([Amazon link][1])\\n 2. Cross-over Trials in Clinical Research by Stephen Senn ([Amazon link][2])\\n\\n\\n  [1]: http://www.amazon.com/Statistical-Issues-Drug-Development-Stephen/dp/0471974889\\n  [2]: http://www.amazon.com/Cross-over-Trials-Clinical-Research-Stephen/dp/0471934933",,
4208,16,1819,3c27415e-731d-48b4-b3cb-795949f3da74,2010-08-18 10:56:44.0,-1.0,,,
4209,2,1820,b97a9734-72d7-4376-bda6-86b7fe0e0594,2010-08-18 11:03:02.0,8.0,"You need to look at [power calculations][1]. \\n\\nSearching for ""power calculations for repeated measurements"" in Google provides a good starting point. The [first hit][2] seems to give some good pointers.\\n \\n\\n  [1]: http://en.wikipedia.org/wiki/Statistical_power\\n  [2]: http://statpages.org/",,
4210,6,1818,7fcff005-9e00-4f25-a4cd-a2e15ffedadc,2010-08-18 11:03:13.0,8.0,<anova><repeated-measures><sample-size><power>,edited tags; deleted 154 characters in body; edited title,
4211,5,1818,7fcff005-9e00-4f25-a4cd-a2e15ffedadc,2010-08-18 11:03:13.0,8.0,"I need some help about repeated measurement ANOVA.\\n\\nWe are investigating the effect of some intervention on reducing blood stream infection (BSI) rate in some wards. We plan to capture the BSI rate information at a monthly basis, 12 months without intervention first, then 12 months with intervention.\\n\\nWe are thinking of doing either time-series or repeated measurement ANOVA, I prefer the later one before I don't have much idea to do on the first one (extra question: too little time points, right?), but then here come another problem, how many wards do we need to show that there is really a statistically significant effect of intervention on BSI rate?\\n\\nI think I'll do two ANOVA, one for ""before intervention"", one for ""during intervention"", and I suppose that the ANOVA ""before intervention"" should not have a significant F-ratio test.\\n\\nI consider the term ""sample size"" two-dimensionally, either the number of wards, or the number of repeated measurements.",edited tags; deleted 154 characters in body; edited title,
4212,4,1818,7fcff005-9e00-4f25-a4cd-a2e15ffedadc,2010-08-18 11:03:13.0,8.0,How to determine the sample size needed for repeated measurement ANOVA?,edited tags; deleted 154 characters in body; edited title,
4213,2,1821,63a19983-a44d-4ad8-b809-19c601a9ef5a,2010-08-18 11:09:28.0,521.0,"1) The standard deviation of the sample (stdev(S)) is an unbiased estimate of the standard deviation of the population.\\n\\n2) Given we have estimated both the population mean and variance we need to take this into account when we evaluate whether a new observation x is a member of this population.\\nWe don't use Z = (x - mean(S))/stdev(S), but rather:\\n t = (x - mean(S))/(stdev(S)*sqrt(1 + 1/n)), where n is the sample size of the first sample. We the compare t with a t-distribution with n-1 degrees of freedom to give a p-value. See here:\\n\\nhttp://en.wikipedia.org/wiki/Prediction_interval#Unknown_mean.2C_unknown_variance \\n\\nThis accounts for the sample size both in the divisor (sqrt(1 + 1/n)) and in the degrees of freedom of the t-distribution.",,
4214,2,1822,7cad1df3-51e9-4ba9-909e-113deaafe15d,2010-08-18 11:37:10.0,283.0,How do i test/verify if i can analyze my panel like dataset by simply pooling the individual series?\\n\\nI have a dataset structured as a panel. Now I am wondering if i can simply\\npool the individual series and estimate it via OLS or if I have to use another estimation technique.\\n\\n(Any R hints and references are highly welcomed.)\\n\\nThanks!\\n\\n,,
4215,1,1822,7cad1df3-51e9-4ba9-909e-113deaafe15d,2010-08-18 11:37:10.0,283.0,Test for Poolability of Individual Data Series,,
4216,3,1822,7cad1df3-51e9-4ba9-909e-113deaafe15d,2010-08-18 11:37:10.0,283.0,<test><data>,,
4217,2,1823,7841e614-9a86-4514-a7e2-e876f0b01247,2010-08-18 11:40:44.0,183.0,"**How to perform power analysis on repeated measures ANOVA?**\\n\\n[G*Power 3][1] is free software that provides a user-friendly GUI interface for performing power calculations.\\nIt supports power calculations for repeated measures ANOVA.\\n\\n**What is the appropriate analysis for your design?**\\n\\nHere are a range of points related to what you have mentioned:\\n\\n- More time points will give a clearer indication\\nof how the effect, if any, of your intervention operates over time. Thus, if the improvements decay over time or get grater, more time points will give a clearer sense of these patterns, both on average, and at an individual-level.\\n- If you have 12 time points or more, I'd look at multilevel modelling, particularly if you are expecting any missing observations. You are unlikely to be interested in  whether there is an effect of time. Rather you are likely to be interested in various specific effects (e.g., changes pre and post intervention; perhaps a linear or quadratic improvement effect post-intervention). You could also look at using planned contrasts on top of repeated measures ANOVA. [Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence][2] is a good starting point to learn about multilevel modelling of repeated measures data.\\n- The number of time points over and above pre- and post- wont do much to increase your   power to detect the effect of your intervention. More time points will increase your reliability of measurement, and it might ensure that you capture the time period where the effect applies, but probably the bigger issue will be the sample size in the two conditions.\\n- Assuming you are truly randomly allocating cases to conditions, the populations are by definition equal on the dependent variable, and one could argue that a significance test of baseline differences is meaningless. That said, researchers often still do it, and I suppose it does provide some evidence that random allocation has actually occurred.\\n- There is a fair amount of debate about the best way to test the effect of an intervention in a pre-post-interention-control design. A few options include: (a) the condition * time interaction; (b) the effect of condition but just at post intervention; (c) an ANCOVA looking at the effect of condition, controlling for pre, with post as the DV.\\n\\n\\n\\n\\n  [1]: http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/\\n  [2]: http://www.ats.ucla.edu/stat/examples/alda.htm",,
4218,5,1823,e384949c-ed79-4d79-928f-3f28e38c18bc,2010-08-18 11:52:36.0,183.0,"**How to perform power analysis on repeated measures ANOVA?**\\n\\n[G*Power 3][1] is free software that provides a user-friendly GUI interface for performing power calculations.\\nIt supports power calculations for repeated measures ANOVA.\\n\\n**What is the appropriate analysis for your design?**\\n\\nHere are a range of points related to what you have mentioned:\\n\\n- More time points will give a clearer indication\\nof how the effect, if any, of your intervention operates over time. Thus, if the improvements decay over time or get greater, more time points will give a clearer sense of these patterns, both on average, and at an individual-level.\\n- If you have 12 time points or more, I'd look at multilevel modelling, particularly if you are expecting any missing observations. You are unlikely to be interested in  whether there is an effect of time. Rather you are likely to be interested in various specific effects (e.g., changes pre and post intervention; perhaps a linear or quadratic improvement effect post-intervention). You could also look at using planned contrasts on top of repeated measures ANOVA. [Applied Longitudinal Data Analysis: Modeling Change and Event Occurrence][2] is a good starting point to learn about multilevel modelling of repeated measures data.\\n- The number of time points over and above pre- and post- wont do much to increase your   power to detect the effect of your intervention. More time points will increase your reliability of measurement, and it might ensure that you capture the time period where the effect applies, but probably the bigger issue will be the sample size in the two conditions.\\n- Assuming you are truly randomly allocating cases to conditions, the populations are by definition equal on the dependent variable, and one could argue that a significance test of baseline differences is meaningless. That said, researchers often still do it, and I suppose it does provide some evidence that random allocation has actually occurred.\\n- There is a fair amount of debate about the best way to test the effect of an intervention in a pre-post-interention-control design. A few options include: (a) the condition * time interaction; (b) the effect of condition but just at post intervention; (c) an ANCOVA looking at the effect of condition, controlling for pre, with post as the DV.\\n\\n\\n\\n\\n  [1]: http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/\\n  [2]: http://www.ats.ucla.edu/stat/examples/alda.htm",added 1 characters in body,
4219,2,1824,d0492a26-c9c4-4b31-816a-e5b7435067f2,2010-08-18 12:00:36.0,979.0,"Maybe you could measure the average difference between two runs of the same algorithm to the average difference between two runs from different algorithms. Doesn't solve the problem of how to measure that difference, but might be a more tractable problem. And the individual values of the time series would feed into the difference calculation instead of having to be treated as individual datapoints to be evaluated against each other (I also don't think that the particular difference at the nth step is what you really want to make statements about).",,
4220,5,1816,cf643a1f-417c-4fc9-8b1a-92d2875747b3,2010-08-18 12:23:43.0,8.0,"Testing stochastic algorithms can be rather tricky!\\n\\nI work in systems biology and there are many stochastic simulators available to use to simulate a model. Testing these simulators is tricky since any two realizations from a single model will be typically different.\\n\\nIn the [dsmts][1] we have calculated (analytically) the expected value and variance of a particular model. We then perform a hypothesis test to determine if a simulator differs from the truth. Section 3 of the [userguide][2] gives the details. Essentially we do a t-test for the mean values and a chi-squared test for variances.\\n\\nIn your case, you are comparing two simulators so you should just use a two-sampled t-test instead.\\n\\n\\n  [1]: http://code.google.com/p/dsmts/\\n  [2]: http://code.google.com/p/dsmts/downloads/detail?name=dsmts-userguide31.pdf",added 1 characters in body,
4221,5,1817,ebf46606-5c51-4207-beca-2bb2f468a1bd,2010-08-18 12:50:26.0,174.0,Montgomery's Design and Analysis of Experiments is a classic and highly regarded text:\\nhttp://www.amazon.com/Design-Analysis-Experiments-Douglas-Montgomery/dp/0470128666/ref=pd_cp_b_0\\n\\nIf you are interested in experimental design in a particular field (eg. clinical trials) other more specialised texts may be appropriate.\\n,typos,
4222,2,1825,6786a5d3-cd0f-40ba-8977-ae460cc61a8c,2010-08-18 12:51:41.0,364.0,"If temperature levels X1...X5 are specific degree values, I'm not sure how temperature and stimulus (hot/cold) can be completely crossed. I presume then that ""temperature"" consists of 5 ordinal categories, ranging from ""likely to cause minimal discomfort"" to ""likely to cause maximum discomfort permissible by my research ethics board"", which then permits crossing with hot/cold direction. \\n\\nIf this is the case, you are correct that the 3 response variables might not be expected to be independent, so you might increase your power (relative to 3 seperate analyses) if you do a multivariate test. Frankly, I must admit that my first inclination would be to simply do three separate analyses, one for each response variable, but that's because I'm not very knowledgeable with regards to the mechanics of multivariate tests. That said, given the likely dependence between the 3 response variables, significant results across 3 separate anovas might be most reasonably considered as manifestations of this dependence rather than 3 truly independent sets of phenomena.\\n\\nI am pretty sure that you *shouldn't* lump all 3 response variables into a single univariate analysis by adding ""response variable type"" as a predictor variable; this approach could run into trouble if the scales and variances of your response variables are very different. Presumably, a multivariate analysis has features that take such differences into account. (However, I wonder if a mixed effects analysis might also be able to handle such differences? I'm new to mixed effects, but suspect I it might...)\\n",,
4223,5,1817,c4ef292f-b060-4261-9f8b-318e02a29240,2010-08-18 13:07:11.0,8.0,[Montgomery's Design and Analysis of Experiments][1] is a classic and highly regarded text:\\n\\nIf you are interested in experimental design in a particular field (eg. clinical trials) other more specialised texts may be appropriate.\\n\\n\\n  [1]: http://www.amazon.com/o/ASIN/0470128666,Formating hyperlink,
4224,2,1826,e5288ed4-93d7-4886-9e0b-1d07028cb59a,2010-08-18 13:11:19.0,5.0,How would you describe [**cross-validation**][1] to someone without a data analysis background?\\n\\n  [1]: http://en.wikipedia.org/wiki/Cross-validation_(statistics),,
4225,1,1826,e5288ed4-93d7-4886-9e0b-1d07028cb59a,2010-08-18 13:11:19.0,5.0,Cross-Validation in plain english?,,
4226,3,1826,e5288ed4-93d7-4886-9e0b-1d07028cb59a,2010-08-18 13:11:19.0,5.0,<cross-validation>,,
4227,2,1827,0a58c9bd-7020-4c58-a7a0-50472aa93fed,2010-08-18 13:15:34.0,,"You can fit a hierarchical bayesian (HB) model without pooling and do an ordinary OLS by pooling the data and compare the models in terms of model fit, hold-out predictions etc to evaluate whether pooling outperforms the HB model. The model very briefly will look like so:\\n\\nIf you want I can provide the model set-up and while I do not use R, I do know that there are packages that will do the above for you. Someone more knowledgeable about R can perhaps help you out. ",,user28
4228,2,1828,b6e8d6a9-50c1-4853-a990-9395a125d93d,2010-08-18 13:20:28.0,5.0,"I think that this is best described with [the following picture][1] (in this case showing k-fold cross-validation):\\n\\n![alt text][2]\\n\\nCross-validation is a technique used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited.  In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.\\n\\n  [1]: http://genome.tugraz.at/proclassify/help/pages/XV.html\\n  [2]: http://i.stack.imgur.com/YWgro.gif",,
4229,5,1827,4a66998d-8286-4d38-b6b1-0a42d3d69f30,2010-08-18 13:25:06.0,,"You can fit a hierarchical bayesian (HB) model without pooling and do an ordinary OLS by pooling the data and compare the models in terms of model fit, hold-out predictions etc to evaluate whether pooling outperforms the HB model. The model very briefly will look like so:\\n\\n**Model**\\n\\n$y_i \\sim N(X\\ \\beta_i,\\sigma^2\\ I)$\\n\\n$\\beta_i \\sim N(\\bar{\\beta},\\Sigma)$\\n\\n**Priors**\\n\\n$\\bar{\\beta} \\sim N(\\bar{\\bar{\\beta}},\\Sigma_0)$\\n\\n$\\Sigma \\sim IW(R,d)$\\n\\n$\\sigma^2 \\sim IG(sp,sc)$\\n\\nWhile I do not use R, I do know that there are packages that will do the above for you. Someone more knowledgeable about R can perhaps help you out. ",added 166 characters in body,user28
4230,2,1829,f63e4c20-0785-4a9f-bfa9-a708bead2fed,2010-08-18 13:30:31.0,988.0,"I usually hear about ""ordinary least squares"".  Is that the most widely used algorithm used for linear regression?  Are there reasons to use a different one?",,
4231,1,1829,f63e4c20-0785-4a9f-bfa9-a708bead2fed,2010-08-18 13:30:31.0,988.0,What algorithm is used in linear regression?,,
4232,3,1829,f63e4c20-0785-4a9f-bfa9-a708bead2fed,2010-08-18 13:30:31.0,988.0,<regression><algorithms>,,
4233,2,1830,f07315da-b2b3-44a1-a225-34762e749c21,2010-08-18 13:38:29.0,,"Consider the following situation: \\n\\n> I want to catch the subway to go to my office. My plan is to take my car, park at the subway and then take the train to go to my office. My goal is to catch the train at 8.15 am every day so that I can reach my office on time. I need to decide the following: (a) the time at which I need to leave from my home and (b) the route I will take to drive to the station.\\n\\nIn the above example, I have two parameters (i.e., time of departure from home and route to take to the station) and I need to choose these parameters such that I reach the station by 8.15 am. \\n\\nIn order to solve the above problem I may try out different sets of 'parameters' (i.e., different combination of times of departure and route) on MWF to see which combination is the 'best' one. The idea is that once I have identified the best combination I can use it every day so that I achieve my objective.\\n\\n**Problem of Overfitting**\\n\\nThe problem with the above approach is that I may overfit which essentially means that the best combination I identify may in some sense may be unique to Mon, Wed and Fridays and that combination may not work for Tue and Thu. Overfitting may happen if in my search for the best combination of times and routes I exploit some aspect of the traffic situation on MWF which does not occur on Tue and Thu.\\n\\n**One Solution to Overfitting: Cross-Validation**\\n\\nCross-validation is one solution to overfitting. The idea is that once we have identified our best combination of parameters (in our case time and route) we test the performance of that set of parameters in a different context. Therefore, we may want to test on Tue and Thu as well to ensure that our choices work for those days as well.\\n\\n**Extending the analogy to statistics**\\n\\nIn statistics, we have a similar issue. We often use a limited set of data to estimate the unknown parameters we do not know. If we overfit then our parameter estimates will work very well for the existing data but not as well for when we use them in another context. Thus, cross-validation helps in avoiding the above issue of overfitting by proving us some reassurance that the parameter estimates are not unique to the data we used to estimate them.\\n\\nOf course, cross validation is not perfect. Going back to our example of the subway, it can happen that even after cross-validation, our best choice of parameters may not work one month down the line because of various issues (e.g., construction, traffic volume changes over time etc).\\n",,user28
4234,2,1831,65dc8589-356c-4fef-84fe-245d143bdcef,2010-08-18 13:42:13.0,5.0,"Srikant is right.  The book you want is [""Data Analysis Using Regression and Multilevel/Hierarchical Models""][1] by Gelman and Hill, all [the R code from the book][2], and the associated [arm package in R][3].\\n\\n  [1]: http://www.stat.columbia.edu/~gelman/arm/\\n  [2]: http://www.stat.columbia.edu/~gelman/arm/software/\\n  [3]: http://cran.r-project.org/web/packages/arm/",,
4235,2,1832,29aa2f2c-d451-47bb-a5cd-699cf2950dc9,2010-08-18 13:50:28.0,447.0,"for me, the best book around is by George Box:\\n\\n[Statistics for Experimenters: Design, Innovation, and Discovery][1] \\n\\nof course the book by Maxwell and Delaney is also pretty good:\\n[Designing Experiments and Analyzing Data: A Model Comparison Perspective, Second Edition][2]\\n\\nI personally prefer the first, but they are both top quality. They are a little bit expensive, but you can definitely find a cheap earlier edition for sale.\\n\\n  [1]: http://www.amazon.com/Statistics-Experimenters-Design-Innovation-Discovery/dp/0471718130\\n  [2]: http://www.amazon.com/Designing-Experiments-Analyzing-Data-Perspective/dp/0805837183/ref=sr_1_1?s=books&ie=UTF8&qid=1282139250&sr=1-1",,
4236,16,1832,29aa2f2c-d451-47bb-a5cd-699cf2950dc9,2010-08-18 13:50:28.0,-1.0,,,
4237,2,1833,0361c10c-c2fc-4536-adb3-5239f4964910,2010-08-18 14:01:06.0,,The wiki link: [Estimation Methods for Linear Regression][1] gives a fairly comprehensive list of estimation methods including OLS and the contexts in which alternative estimation methods are used.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Linear_regression#Estimation_methods,,user28
4238,2,1834,ce7e0b2f-3264-42c8-ac39-1d6151e24a4c,2010-08-18 14:01:19.0,334.0,"It is easy to get confused between definitions and terminology.  Both terms are used, sometimes interchangeably.  A quick lookup on Wikipedia should help:\\n\\n - [ordinary least squares](http://en.wikipedia.org/wiki/Ordinary_least_squares)\\n - [lnear regression](http://en.wikipedia.org/wiki/Linear_regression_model)\\n\\nOrdinary Least Squares (OLS) is a method used to fit linear regression models. Because of the demonstrable consistency and efficiency (under supplementary assumptions) of the OLS method, it is the dominant approach.  See the articles for further leads.\\n\\n",,
4239,6,1829,8bb5e2d5-af65-4766-80be-bb7509bf2807,2010-08-18 14:01:28.0,,<regression><ols>,changed tags to appropriate one,user28
4240,5,1824,a95e3213-a094-41bc-83c0-b14cf5d516d1,2010-08-18 14:09:05.0,979.0,"Maybe you could measure the average difference between two runs of the same algorithm to the average difference between two runs from different algorithms. Doesn't solve the problem of how to measure that difference, but might be a more tractable problem. And the individual values of the time series would feed into the difference calculation instead of having to be treated as individual datapoints to be evaluated against each other (I also don't think that the particular difference at the nth step is what you really want to make statements about).\\n\\n**Update**\\nConcerning details - well which features of the time series are you interested in, beyond the final error? I guess you actually got three different questions to solve: \\n\\n 1. What constitues similarity for you, ie what do you mean when you say you don't believe the two methods are different?\\n 2. How do you quantify it - can be answered after 1, and \\n 3. How can you test for significant differences between your two methods?\\n\\nAll I was saying in the first post was that the answer to (1) probably doesn't consider the individual differences at each of the 1000 generations. And that I'd advise coming up with a scalar value for either each time series or at least similarity between time series. Only then you get to the actual statistics question (which I know least about of all three points, but I was advised to use a paired t-test in a similar question I just asked, when having a scalar value per element).",added 893 characters in body; added 42 characters in body,
4241,2,1835,2d03d3fc-9eed-4ec9-ba97-6dc13673fcdf,2010-08-18 14:13:25.0,521.0,"The only further comment I would make is that the approach need not be Bayesian and the model need not be a mixed or random effects model. \\n\\nIn the simplest case if you had two series in x the mean model may be:\\n\\n y = b01 + I*b02 + b11*x + I.b12*x\\n\\nWhere I indicates a sample from the 2nd series. An omnibus F-test can be used to determine whether the additional parameters are required to maintain distinct series (Ho: b02 = b12 = 0). \\nhttp://en.wikipedia.org/wiki/F_test\\nThis can be extended to more series, but it soon becomes more efficient to use a mixed or random effects model. ",,
4242,2,1836,fab54a88-9821-4f15-b5aa-136864f97b58,2010-08-18 14:33:06.0,919.0,"This may surprise many, but to solve this problem you don't necessarily need to estimate *s*.  In fact, you don't need to know *anything* about the spread of the data (although that would be helpful, of course).  For instance, Wall, Boen, and Tweedie in a 2001 article describe how to find a finite confidence interval for the mean of any unimodal distribution based on a *single* draw.\\n\\nIn the present case, we have some basis to view the sample mean of 112 as a draw from an approximately normal distribution (namely, the sampling distribution of the average of a simple random sample of 49 salaries).  We are implicitly assuming there are a fairly large number of factory workers and that their salary distribution is not so skewed or multimodal as to render the central limit theorem inoperable.  Then a conservative 90% CI for the mean extends upwards to\\n\\n112 + 5.84|112|,\\n\\nclearly covering the true mean of 200.  (See Wall *et al* formula 3.)  Given the limited information available and the assumptions made here, we therefore cannot conclude that 112 differs ""significantly"" from 200.\\n\\nReference: ""An Effective Confidence Interval for the Mean With Samples of Size One and Two.""  The American Statistician, May 2001, Vol. 55, No. 2: pp. 102-105.",,
4243,2,1837,5cd47ded-ddbd-4a33-8a3c-3fa52ff77ba2,2010-08-18 14:47:31.0,919.0,"A method that can be very effective--one I have found extremely useful--is to sort the data by time and draw a connected X,Y scatterplot.  (That is, successive points are connected by line segments or a spline.)  This much is straightforward in almost any statistical plotting package.  If the result is too confusing, add graphical indications of directionality.  Depending on the density of the points and their pattern of temporal evolution, options include using arrows on the line segments or otherwise applying a graduated color or thickness to the segments to indicate their times.  You can even dispense with the connecting lines and just color or size the points to indicate time: that works better when there are many points on the plot.  In addition to displaying the bivariate relationship among the data in a conventional form, this method supplies a clear visual indication of temporally local correlations, changes that run counter to the prevailing correlation, etc.",,
4244,2,1838,480dbb72-8b1d-4713-a517-0e70948338f9,2010-08-18 14:55:55.0,990.0,"Greetings,\\n\\nCurrently I'm doing the following in R:\\n\\n    require(zoo)\\n    data <- read.csv(file=""summary.csv"",sep="","",head=TRUE)\\n    cum  = zoo(data$dcomp, as.Date(data$date))\\n    data = zoo(data$compressed, as.Date(data$date))\\n    data <- aggregate(data, identity, tail, 1)\\n    cum  <- aggregate(cum, identity, sum, 1)\\n    days = seq(start(data), end(data), ""day"")\\n    data2 = na.locf(merge(data, zoo(,days)))\\n    \\n    plot(data2,xlab='',ylab='compressed bytes',col=rgb(0.18,0.34,0.55))\\n    lines(cum,type=""h"",col=rgb(0,0.5,0))\\n\\nThe last two lines plot the information I need, and the result resembles the following:\\n![alt text][1]\\nBlue line is the entropy in bytes of the artifact I'm interested. Green lines represent the entropy of the changes.\\n\\nNow, in this graph, it works well because there isn't a huge difference in scales. But I have other graphs where the green lines become so small one cannot see.\\n\\nThe solution I was looking for, involved two things:\\n\\n 1. To move the green vertical lines to a second graph, just below the first one, with its own y axis, but shared x axis.\\n 2. To provide it a logarithmic scale, since I'm more interested in the ""magnitude"", than in the specific values.\\n\\nThanks in advance!\\n\\nP.S. If someone can also tell me how could I put ""minor ticks"" in the x scale referring to the months, I appreciate :-) If these are too much questions for a single post, I can divide them further.\\n\\n  [1]: http://i.stack.imgur.com/ech9l.png",,
4245,1,1838,480dbb72-8b1d-4713-a517-0e70948338f9,2010-08-18 14:55:55.0,990.0,"How do I vertically stack two graphs with the same x scale, but a different y scale in R?",,
4246,3,1838,480dbb72-8b1d-4713-a517-0e70948338f9,2010-08-18 14:55:55.0,990.0,<r><time-series><data-visualization><entropy>,,
4247,2,1839,fd796a1b-cd0c-4db0-8c6e-cfbbab4d7dbb,2010-08-18 14:57:00.0,183.0,"I tend to think of 'least squares' as a criterion for defining the best fitting regression line (i.e., that which makes the sum of 'squared' residuals 'least') and the 'algorithm' in this context as the set of steps used to determine the regression coefficients that satisfy that criterion. This distinction suggests that it is possible to have  different algorithms that would satisfy the same criterion. \\n\\nI'd be curious to know whether others make this distinction and what terminology they use.",,
4248,2,1840,f7f0f730-6a5d-4371-9e35-b834e6c1136c,2010-08-18 15:14:02.0,88.0,"Let's say you investigate some process; you've gathered some data describing it and you have build a model (either statistical or ML, doesn't matter). But now, how to judge if it is ok? Probably it fits suspiciously good to the data it was build on, so no-one will believe that your model is so splendid that you think.  \\nFirst idea is to separate a subset of your data and use it to test the model build by your method on the rest of data. Now the result is definitely overfitting-free, nevertheless (especially for small sets) you could have been (un)lucky and draw (less)more simpler cases to test, making it (harder)easier to predict... Also your accuracy/error/goodness estimate is useless for model comparison/optimization, since you probably know nothing about its distribution.  \\nWhen in doubt, use brute force, so just replicate the above process, gather few estimates of accuracy/error/goodness and average them -- and so you obtain cross validation. Among better estimate you will also get a histogram, so you will be able to approximate distribution or perform some non-parametric tests.  \\nAnd this is it; the details of test-train splitting are the reason for different CV types, still except of rare cases and small strength differences they are rather equivalent. Indeed it is a huge advantage, because it makes it a bulletproof-fair method; it is very hard to cheat it.",,
4249,5,1813,3b37e0fe-1081-4c14-9c2d-f75995314dee,2010-08-18 15:17:21.0,986.0,"I have two implementations of a genetic algorithm which are supposed to behave equivalently. However due to technical restrictions which cannot be resolved their output is not exactly the same, given the same input.\\n\\nStill I'd like to show that there is no significant performance difference.\\n\\nI have 20 runs with the same configuration for each of the two algorithms, using different initial random number seeds. For each run **and generation** the <strike>minimum error</strike> **fitness of the best individual in the population** was recorded. A run consists of 1000 generations, so I have 1000 values per run. I cannot get more data, as the calculations are very expensive.\\n\\nWhich test should I employ? An easy way would probably be to only compare the error in the final generations (again, which test would I use here)? But one might also think about comparing the convergence behaviour in general.",added 48 characters in body; added 4 characters in body; added 19 characters in body,
4250,2,1841,27d2d514-11c8-4cc9-9dc1-cc12027b8fcc,2010-08-18 15:19:44.0,990.0,"Disclaimer: I'm a software engineer, not a statistician, so please forgive any blunt error :-)\\n\\nI have a set of time-series ""curves"", each measuring the entropy of a given artifact. Now, I'm standing over the following premises (please criticize them as you see fit):\\n\\n 1. In order to approximate the upper bounds of the Kolmogorov complexity $K(s)$, of a string $s$, one can simply compress the string $s$ with some method, implement the corresponding decompressor in the chosen language, concatenate the decompressor to the compressed string, and measure the resulting string's length.\\n 2. For this purpose, I've used the *bzip2* application, setting its compression level to the supported maximum (-9).\\n 3. If one is only interested in a time-series analysis of a set of evolving strings, calculating the compressed *deltas* is enough to present a relative measure of entropy between any two strings (at least that's my interpretation after reading Cilibrasi05).\\n 4. For that, I used the *diff* unix tool, with the (--minimal) parameter, again followed by a *bzip2* compression, with the aforementioned settings.\\n\\nI'm doing this to analyze the evolution of the entropy in a software artifact (code, model, whatever). I'm now worried with the absolute values, but with the relative increase (or decrease) in entropy. Now here comes the problem:\\n\\n 1. I've done this for a set of 6 artifacts, which ought to belong to the same population, but I don't know how to provide statistical evidence of that (the corresponding of doing a two-tailed t-test of two samples).\\n 2. One of the artifacts evolution *should* be different from all the others. We're talking something like an exponential v.s. sub-linear growth. How do I provide statistical evidence of that?\\n\\nAgain, the disclaimer of being a software engineer. Although I would appreciate every academic reference (papers, books, etc.) you could handle, I'm looking for something pragmatic that I can use in the next few days, like a script in R, or something in SPSS.\\n\\nP.S. I'm sorry for asking for a *recipe*, instead of a theoretical explanation.\\n",,
4251,1,1841,27d2d514-11c8-4cc9-9dc1-cc12027b8fcc,2010-08-18 15:19:44.0,990.0,Hypothesis testing that one time-series of a measure of entropy doesn't belong to a population,,
4252,3,1841,27d2d514-11c8-4cc9-9dc1-cc12027b8fcc,2010-08-18 15:19:44.0,990.0,<r><time-series><hypothesis-testing><spss><entropy>,,
4253,6,1829,f765d376-f8e1-4e18-b953-29cf460e8bf7,2010-08-18 15:25:08.0,88.0,<regression><least-squares>,edited tags,
4254,5,1813,0dcde429-b713-419a-b671-0cfd4fef6cd9,2010-08-18 15:25:32.0,986.0,"I have two implementations of a genetic algorithm which are supposed to behave equivalently. However due to technical restrictions which cannot be resolved their output is not exactly the same, given the same input.\\n\\nStill I'd like to show that there is no significant performance difference.\\n\\nI have 20 runs with the same configuration for each of the two algorithms, using different initial random number seeds. For each run **and generation** the <strike>minimum error</strike> **fitness of the best individual in the population** was recorded. _The algorithm employs an elite-preserving mechanism, so the fitness of the best individual is monotonically decreasing._ A run consists of 1000 generations, so I have 1000 values per run. I cannot get more data, as the calculations are very expensive.\\n\\nWhich test should I employ? An easy way would probably be to only compare the error in the final generations (again, which test would I use here)? But one might also think about comparing the convergence behaviour in general.",added 122 characters in body,
4255,2,1842,d1621af1-d9b1-46ee-b8ad-657d930d3c2c,2010-08-18 15:25:59.0,8.0,"I think you can get what you want using `ggplot2`. Using the code below, I can produce:\\n\\n![alt text][1]\\n\\nObviously things like line colours can be changed to what ever you want. On the x-axis I specified major lines on years and minor lines on months.\\n\\n    require(ggplot2)\\n    t = as.Date(0:1000, origin=""2008-01-01"")  \\n    y1 = rexp(1001)\\n    y2 = cumsum(y1)\\n    df = data.frame(t=t, values=c(y2,y1), type=rep(c(""Bytes"", ""Changes""), each=1001))\\n    \\n    g = ggplot(data=df, aes(x=t, y=values)) +\\n      geom_line() +\\n      facet_grid(type ~ ., scales=""free"") +\\n      scale_y_continuous(trans=""log10"") +\\n      scale_x_date(major=""years"", minor=""months"") +\\n      ylab(""Log values"")\\n    g\\n\\n\\n  [1]: http://i.stack.imgur.com/qNcAk.png\\n",,
4256,5,1838,a4e83456-d3d5-4746-ba39-5e91846410c7,2010-08-18 15:39:27.0,990.0,"Greetings,\\n\\nCurrently I'm doing the following in R:\\n\\n    require(zoo)\\n    data <- read.csv(file=""summary.csv"",sep="","",head=TRUE)\\n    cum  = zoo(data$dcomp, as.Date(data$date))\\n    data = zoo(data$compressed, as.Date(data$date))\\n    data <- aggregate(data, identity, tail, 1)\\n    cum  <- aggregate(cum, identity, sum, 1)\\n    days = seq(start(data), end(data), ""day"")\\n    data2 = na.locf(merge(data, zoo(,days)))\\n    \\n    plot(data2,xlab='',ylab='compressed bytes',col=rgb(0.18,0.34,0.55))\\n    lines(cum,type=""h"",col=rgb(0,0.5,0))\\n\\nSnip of summary.csv:\\n\\n    date,revision,file,lines,nclass,nattr,nrel,bytes,compressed,diff,dcomp\\n    2007-07-25,16,model.xml,96,11,22,5,4035,991,0,0\\n    2007-07-27,17,model.xml,115,16,26,6,4740,1056,53,777\\n    2007-08-09,18,model.xml,106,16,26,7,4966,1136,47,761\\n    2007-08-10,19,model.xml,106,16,26,7,4968,1150,4,202\\n    2007-09-06,81,model.xml,111,16,26,7,5110,1167,13,258\\n    ...\\n\\nThe last two lines plot the information I need, and the result resembles the following:\\n![alt text][1]\\nBlue line is the entropy in bytes of the artifact I'm interested. Green lines represent the entropy of the changes.\\n\\nNow, in this graph, it works well because there isn't a huge difference in scales. But I have other graphs where the green lines become so small one cannot see.\\n\\nThe solution I was looking for, involved two things:\\n\\n 1. To move the green vertical lines to a second graph, just below the first one, with its own y axis, but shared x axis.\\n 2. To provide it a logarithmic scale, since I'm more interested in the ""magnitude"", than in the specific values.\\n\\nThanks in advance!\\n\\nP.S. If someone can also tell me how could I put ""minor ticks"" in the x scale referring to the months, I appreciate :-) If these are too much questions for a single post, I can divide them further.\\n\\n  [1]: http://i.stack.imgur.com/ech9l.png",added 395 characters in body,
4257,2,1843,fce2ce4e-a7c1-43bc-adf9-f5341b4780d3,2010-08-18 15:43:03.0,442.0,"You can use `par(new=T)` to plot into the same graph using two different y-axes! Thsi should also solve your problem.\\n\\nNext you will find a simple example that plots two random normal variables, one on mean 0 the other one on mean 100 (both *sd* s = 1) in the same plot. The first one in red on the left y-axis, the second one in blue on the right y-axis. Then, axis labels are added.\\n\\nHere you go:\\n\\n    x <- 1:10\\n    y1 <- rnorm(10)\\n    y2 <- rnorm(10)+100\\n\\n    plot(x,y1,pch=0,type=""b"",col=""red"",yaxt=""n"",ylim=c(-8,2))\\n    par(new=T)\\n    plot(x,y2,pch=1,type=""b"",col=""blue"",yaxt=""n"",ylim=c(98,105))\\n\\n    axis(side=2)\\n    axis(side=4)\\n\\nWill put a pic of the example here, when I have time.",,
4258,2,1844,fa9596b0-f76e-49ff-9acf-1b956a09c0be,2010-08-18 15:47:15.0,25.0,"One way to summarize the comparison of two survival curves is to compute the hazard ratio (HR). There are (at least) two methods to compute this value. \\n\\n - Logrank method. As part of the Kaplan-Meier calculations, compute the number of observed events (deaths, usually) in each group (Oa, and Ob), and the number of expected events assuming a null hypothesis of no difference in survival (Ea and Eb). The hazard ratio then is:\\n\\n    HR= (Oa/Ea)/(Ob/Eb)\\n\\n - Mantel-Haenszel method. First compute V, which is the sum of the hypergeometric variances at each time point. Then compute the hazard ratio as:\\n\\n HR= exp[(Oa-Ea)/V]\\n\\nI got both these equations from chapter 3 of  Machin, Cheung and Parmar, [Survival Analysis][1]. That book states that the two methods usually give very similar methods, and indeed that is the case with the example in the book. \\n\\nSomeone sent me an example where the two methods differ by a factor of three. In this particular example, it is obvious that the logrank estimate is sensible, and the Mantel-Haenszel estimate is far off. My question is if anyone has any general advice for when it is best to choose the logrank estimate of the hazard ratio, and when it is best to choose the Mantel-Haenszel estimate? Does it have to do with sample size? Number of ties? Ratio of sample sizes?\\n\\n\\n  [1]: http://www.amazon.com/Survival-Analysis-Practical-David-Machin/dp/0470870400%3FSubscriptionId%3D1EJNRTWJHMX1HXN1FX02%26tag%3Dgraphpadsoftware%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0470870400",,
4259,1,1844,fa9596b0-f76e-49ff-9acf-1b956a09c0be,2010-08-18 15:47:15.0,25.0,What are the pros and cons of using the logrank vs. the Mantel-Haenszel method for computing the Hazard Ratio in survival analysis?,,
4260,3,1844,fa9596b0-f76e-49ff-9acf-1b956a09c0be,2010-08-18 15:47:15.0,25.0,<statistical-analysis><hazard-function><survival><hazard><logrank>,,
4261,5,1843,b8ae7b11-941f-46f3-b7a9-98299f69813c,2010-08-18 15:48:08.0,442.0,"You can use `par(new=T)` to plot into the same graph using two different y-axes! Thsi should also solve your problem.\\n\\nNext you will find a simple example that plots two random normal variables, one on mean 0 the other one on mean 100 (both *sd* s = 1) in the same plot. The first one in red on the left y-axis, the second one in blue on the right y-axis. Then, axis labels are added.\\n\\nHere you go:\\n\\n    x <- 1:10\\n    y1 <- rnorm(10)\\n    y2 <- rnorm(10)+100\\n\\n    plot(x,y1,pch=0,type=""b"",col=""red"",yaxt=""n"",ylim=c(-8,2))\\n    par(new=T)\\n    plot(x,y2,pch=1,type=""b"",col=""blue"",yaxt=""n"",ylim=c(98,105))\\n\\n    axis(side=2)\\n    axis(side=4)\\n\\nlooks like this then (remember red, left, blue right axis): ![alt text][1]\\n\\n\\n  [1]: http://i.stack.imgur.com/311Vo.png",added grap,
4262,5,1841,24983ec8-b3b1-49d3-b170-7b1f0d3bc701,2010-08-18 16:00:37.0,990.0,"Disclaimer: I'm a software engineer, not a statistician, so please forgive any blunt error :-)\\n\\nI have a set of time-series ""curves"", each measuring the entropy of a given artifact. Now, I'm standing over the following premises (please criticize them as you see fit):\\n\\n 1. In order to approximate the upper bounds of the Kolmogorov complexity $K(s)$, of a string $s$, one can simply compress the string $s$ with some method, implement the corresponding decompressor in the chosen language, concatenate the decompressor to the compressed string, and measure the resulting string's length.\\n 2. For this purpose, I've used the *bzip2* application, setting its compression level to the supported maximum (-9).\\n 3. If one is only interested in a time-series analysis of a set of evolving strings, calculating the compressed *deltas* is enough to present a relative measure of entropy between any two strings (at least that's my interpretation after reading Cilibrasi05).\\n 4. For that, I used the *diff* unix tool, with the (--minimal) parameter, again followed by a *bzip2* compression, with the aforementioned settings.\\n\\nI'm doing this to analyze the evolution of the entropy in a software artifact (code, model, whatever). I'm *not* worried with the absolute values, but with the *relative* increase (or decrease) in entropy. Now here comes the problem:\\n\\n 1. I've done this for a set of 6 artifacts, which ought to belong to the same population, but I don't know how to provide statistical evidence of that (the corresponding of doing a two-tailed t-test of two samples).\\n 2. One of the artifacts evolution *should* be different from all the others. We're talking something like an exponential v.s. sub-linear growth. How do I provide statistical evidence of that?\\n\\nAgain, the disclaimer of being a software engineer. Although I would appreciate every academic reference (papers, books, etc.) you could handle, I'm looking for something pragmatic that I can use in the next few days, like a script in R, or something in SPSS.\\n\\nP.S. I'm sorry for asking for a *recipe*, instead of a theoretical explanation.\\n",added 4 characters in body,
4263,2,1845,fdb08037-8824-48af-9488-4f4dff641a5e,2010-08-18 16:19:28.0,339.0,"Regarding the question in the title, about what is the algorithm that is used:\\n\\nIn a linear algebra perspective, the linear regression algorithm is the way to solve a linear system $\\mathbf{A}x=b$ with more equations than unknowns. In most of the cases there is no solution to this problem. And this is because the vector $b$ doesn't belong to the column space of $\\mathbf{A}$, $C(\\mathbf{A})$. \\n\\nThe `best straight line` is the one that makes the overall error $e=\\mathbf{A}x-b$ as small as it takes. And is convenient to think as small to be the squared length, $\\lVert e \\rVert^2$, because it's non negative, and it equals 0 only when $b\\in C(\\mathbf{A})$. \\n\\nProjecting (orthogonally) the vector $b$ to the nearest point in the column space of $\\mathbf{A}$ gives the vector $b^*$ that solves the system (it's components lie on the best straight line) with the minimum error. \\n\\n$\\mathbf{A}^T\\mathbf{A}\\hat{x}=\\mathbf{A}^Tb \\Rightarrow \\hat{x}=(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^Tb$\\n\\nand the projected vector $b^*$ is given by:\\n\\n$b^*=\\mathbf{A}\\hat{x}=\\mathbf{A}(\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^Tb$\\n\\nPerhaps the least squares method is not *exclusively* used because that `squaring`  overcompensates for outliers. \\n\\nLet me give a simple example in R, that solves the regression problem using this algorithm:\\n\\n    reg.data <- read.table(textConnection(""\\n       b      x\\n      12      0\\n      10      1\\n       8      2\\n      11      3\\n       6      4\\n       7      5\\n       2      6\\n       3      7\\n       3      8 ""), header = T)\\n    \\n    attach(reg.data)\\n    \\n    A <- model.matrix(b~x)\\n    \\n    # intercept and slope\\n    inv(t(A) %*% A) %*% t(A) %*% b\\n    \\n    # fitted values - the projected vector b in the C(A)\\n    A %*% inv(t(A) %*%A ) %*% t(A) %*% b\\n    \\n    # The projection is easier if the orthogonal matrix Q is used, \\n    # because t(Q)%*%Q = I\\n    Q <- qr.Q(qr(A))\\n    R <- qr.R(qr(A))\\n    \\n    # intercept and slope \\n    best.line <- inv(R) %*% t(Q) %*% b\\n    \\n    # fitted values \\n    Q %*% t(Q) %*% b\\n    \\n    plot(x,b,pch=16)\\n    abline(best.line[1],best.line[2])\\n      ",,
4264,2,1846,aae094b4-c276-4308-b311-152f7bc942f0,2010-08-18 16:28:57.0,795.0,"Some robust alernatives are discussed in [A class of robust stepwise alternativese to Hotelling's T 2 tests][1], which deals with trimmed means of the marginals of residuals produced by stepwise regression, and in [A comparison of robust alternatives to Hoteslling's T^2 control chart][2], which outlines some robust alternatives based on MVE, MCD, RMCD and trimmed means.\\n\\n\\n  [1]: http://dx.doi.org/10.1080/02664760050076434\\n  [2]: http://dx.doi.org/10.1080/02664760902810813",,
4265,2,1847,dd6fa3c7-92d9-4bd0-9402-d8f9dffb4a73,2010-08-18 16:38:48.0,101.0,Have you thought about using simulate in the arm package?  Gelman & Hill have some nice chapters on this in their book.,,
4266,2,1848,665051bd-7dcd-4314-967d-ba799938b261,2010-08-18 16:43:11.0,991.0,"I'm implementing a rating system to be used on my website, and I think the Bayesian average is the best way to go about it. Every item will be rated in six different categories by the users. I don't want items with only one high rating to shoot to the top though, which is why I want to implement a Bayesian system.\\n\\nHere is the formula:\\n\\n    Bayesian Rating = ( (avg_num_votes * avg_rating) + (this_num_votes * this_rating) ) / (avg_num_votes + this_num_votes)\\n\\nBecause the items will be rated in 6 different categories, should I use the average of the sums of those categories as ""this_rating"" for the Bayesian system? For instance, take one item with two ratings (scale of 0-5):\\n\\n    Rating 1:\\n      Category A: 3\\n      Category B: 1\\n      Category C: 2\\n      Category D: 4\\n      Category E: 5\\n      Category F: 3\\n      Sum: 18\\n\\n    Rating 2:\\n      Category A: 2\\n      Category B: 3\\n      Category C: 3\\n      Category D: 5\\n      Category E: 0\\n      Category F: 1\\n      Sum: 14\\n\\nShould ""this_rating"" be simply the average of the sums listed above? Is my thinking correct, or should a Bayesian system be implemented for each category as well (or is that overthinking it)?",,
4267,1,1848,665051bd-7dcd-4314-967d-ba799938b261,2010-08-18 16:43:11.0,991.0,Bayesian rating system with multiple categories for each rating,,
4268,3,1848,665051bd-7dcd-4314-967d-ba799938b261,2010-08-18 16:43:11.0,991.0,<bayesian>,,
4269,2,1849,9ddf9170-f586-41e3-9ca9-f5e7959c2fa1,2010-08-18 16:58:10.0,279.0,"It depends on whether you want to wind up only with a cumulative rating of each object, or category-specific rating. Having a separate system in each category sounds more realistic, but your particular context might suggest otherwise. You could even do both a category-specific and overall rating!",,
4270,4,1571,33307d68-0341-4f89-9c78-0576b0700890,2010-08-18 17:19:16.0,88.0,Recreating traditional null hypothesis testing with Bayesian methods,edited title,
4271,4,1561,bf2eb395-b20f-4ff9-b686-bc5a655b9f79,2010-08-18 17:21:57.0,88.0,Normalizing data,edited title,
4272,2,1850,e6c3752f-6637-42d9-a14d-3366fa2d502b,2010-08-18 17:35:17.0,559.0,"For an effect size analysis, I am noticing that the differences between Cohen's d, Hedges's g and Hedges' g*. \\n\\n - Are these three metrics normally very similar?  \\n - What would be a case where they would produce results? \\n - Also is it a matter of preference in which when I use or report with?\\n\\n",,
4273,1,1850,e6c3752f-6637-42d9-a14d-3366fa2d502b,2010-08-18 17:35:17.0,559.0,Difference between Cohen's and Hedges' g for effect size metrics,,
4274,3,1850,e6c3752f-6637-42d9-a14d-3366fa2d502b,2010-08-18 17:35:17.0,559.0,<effect-size>,,
4275,5,1843,51c579a9-cd4c-4dfb-9357-cb72e6f7aaba,2010-08-18 17:55:55.0,442.0,"You can use `par(new=T)` to plot into the same graph using two different y-axes! Thsi should also solve your problem.\\n\\nNext you will find a simple example that plots two random normal variables, one on mean 0 the other one on mean 100 (both *sd* s = 1) in the same plot. The first one in red on the left y-axis, the second one in blue on the right y-axis. Then, axis labels are added.\\n\\nHere you go:\\n\\n    x <- 1:10\\n    y1 <- rnorm(10)\\n    y2 <- rnorm(10)+100\\n\\n    plot(x,y1,pch=0,type=""b"",col=""red"",yaxt=""n"",ylim=c(-8,2))\\n    par(new=T)\\n    plot(x,y2,pch=1,type=""b"",col=""blue"",yaxt=""n"",ylim=c(98,105))\\n\\n    axis(side=2)\\n    axis(side=4)\\n\\nlooks like this then (remember red on left axis, blue on right axis): ![alt text][1]\\n\\n\\n  [1]: http://i.stack.imgur.com/311Vo.png",added 10 characters in body,
4279,2,1852,3d6aa912-e083-4636-b5f7-da908a192a4a,2010-08-18 18:21:12.0,795.0,"Suppose there are 999 workers at ACME north factory each making a wage of 112, and 1 CEO making 88112. The population mean salary is $\\mu = 0.999 * 112 + 0.001 * 88112 = 200.$ The probability of drawing the CEO from a sample of 49 people at the factory is $49 / 1000 < 0.05$ (this is from the hypergeometric distribution), thus with 95% confidence, your population mean will be 112. In fact, by adjusting the ratio of workers/CEOs, and the salary of the CEO, we can make it arbitrarily unlikely that a sample of 49 employees will draw a CEO, while fixing the population mean at 200, and the sample mean at 112. Thus, without making _some_ assumptions about the underlying distribution, you cannot draw any inference about the population mean.",,
4280,4,1850,1e4fdb62-5d9d-4ca4-8591-bcd9f5ef3adf,2010-08-18 18:34:52.0,559.0,Difference between Cohen's d and Hedges' g for effect size metrics,add d in title,
4281,2,1853,91dd3e16-d904-4d6b-8762-2d1f6f8ce207,2010-08-18 18:49:48.0,795.0,What tests are available for testing two independent samples for the null hypothesis that they come from populations with the same skew? There is a classical 1-sample test for whether the skew equals a fixed number (the test involves the 6th sample moment!); is there a straightforward translation to a 2-sample test? \\n\\nAre there techniques which don't involve very high moments of the data? (I am anticipating an answer of the form 'bootstrap it': are bootstrap techniques known to be appropriate for this problem?)\\n,,
4282,1,1853,91dd3e16-d904-4d6b-8762-2d1f6f8ce207,2010-08-18 18:49:48.0,795.0,testing 2 independent samples for null of same skew?,,
4283,3,1853,91dd3e16-d904-4d6b-8762-2d1f6f8ce207,2010-08-18 18:49:48.0,795.0,<hypothesis-testing><bootstrap><moments>,,
4284,2,1854,85d3119d-60d9-4d66-8b91-db0b2d0964fe,2010-08-18 19:09:37.0,961.0,"""Avoid learning your training data by heart by making sure the trained model performs well on independent data.""\\n",,
4285,2,1855,c6af1667-4484-403b-984d-bfb5f31010d6,2010-08-18 19:16:05.0,495.0,"It has the same meaning as any other confidence interval: under the assumption that the model is correct, if the experiment and procedure is repeated over and over, 95% of the time the true value of the quantity of interest will lie within the interval. In this case, the quantity of interest is the expected value of the response variable.\\n\\nIt is probably easiest to explain this in the context of a linear model (mixed models are just an extension of this, so the same ideas apply):\\n\\nThe usual assumption is that:\\n\\n$y_i = X_{i1} \\beta_1 + X_{i2} \\beta_2 + \\ldots X_{ip} \\beta_p + \\epsilon $\\n\\nwhere $y_i$ is the response, $X_{ij}$'s are the covariates, $\\beta_j$'s are the parameters, and $\\epsilon$ is the error term which has mean zero. The quantity of interest is then:\\n\\n$E[y_i] = X_{i1} \\beta_1 + X_{i2} \\beta_2 + \\ldots X_{ip} \\beta_p $\\n\\nwhich is a linear function of the (unknown) parameters, since the covariates are known (and fixed). Since we know the sampling distribution of the parameter vector, we can easily calculate the sampling distribution (and hence the confidence interval) of this quantity.\\n\\nSo why would you want to know it? I guess if you're doing out-of-sample prediction, it could tell you how good your forecast is expected to be (though you'd need to take into account model uncertainty).",,
4286,2,1856,ba0c60e6-7f1c-42ca-9ec3-04085146214c,2010-08-18 20:36:59.0,930.0,"What do you think about applying machine learning techniques, like Random Forests or penalized regression (with L1 or L2 penalty, or a combination thereof) in small sample clinical studies when the objective is to isolate interesting predictors in a classification context? It is not a question about model selection, nor am I asking about how to find optimal estimates of variable effect/importance. I don't plan to do strong inference but just to use multivariate modeling, hence avoiding testing each predictor against the outcome of interest one at a time, and taking their interrelationships into account.\\n\\nI was just wondering if such an approach was already applied in this particular extreme case, say 20-30 subjects with data on 10-15 categorical or continuous variables. It is not exactly the n << p case and I think the problem here is related to the number of classes we try to explain (which are often not well balanced), and the (very) small n. I am aware of the huge literature on this topic in the context of bioinformatics, but I didn't find any reference related to biomedical studies with psychometrically measured phenotypes (e.g. throughout neuropsychological questionnaires).\\n\\nAny hint or pointers to relevant papers?",,
4287,1,1856,ba0c60e6-7f1c-42ca-9ec3-04085146214c,2010-08-18 20:36:59.0,930.0,Application of machine learning techniques in small sample clinical studies,,
4288,3,1856,ba0c60e6-7f1c-42ca-9ec3-04085146214c,2010-08-18 20:36:59.0,930.0,<machine-learning><feature-selection>,,
4289,2,1857,65cdb8c0-ffdc-4090-9222-5a8ccd557956,2010-08-18 20:51:32.0,5.0,"One common rule of thumb is to have at least 10 times the number of training data instances (not to speak of any test/validation data, etc.) as there are adjustable parameters in the classifier.  Keep in mind that you have a problem wherein you need to not only have *adequate* data but also *representative* data.  ",,
4290,5,1857,c961603a-3d05-4951-8111-36ad85f1a7d1,2010-08-18 21:00:45.0,5.0,"One common rule of thumb is to have at least 10 times the number of training data instances (not to speak of any test/validation data, etc.) as there are adjustable parameters in the classifier.  Keep in mind that you have a problem wherein you need to not only have *adequate* data but also *representative* data.  \\n\\nIf you are new to this field, I recommend reading this short [""Pattern Recognition""][1] paper from the Encyclopedia of Biomedical Engineering which gives a brief summary of some of the data issues.\\n\\n\\n  [1]: http://users.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/wiley06.pdf",added 279 characters in body,
4291,5,1857,5638adc3-b6fd-432f-b0b5-4181e042f85c,2010-08-18 21:23:54.0,5.0,"One common rule of thumb is to have at least 10 times the number of training data instances (not to speak of any test/validation data, etc.) as there are adjustable parameters in the classifier.  Keep in mind that you have a problem wherein you need to not only have *adequate* data but also *representative* data.  In the end, there is no systematic rule because there are so many variables when making this decision.  As Hastie, Tibshirani, \\nand Friedman say in [The Elements of Statistical Learning][1] (see Chapter 7):\\n\\n> it is too difficult to give a general\\n> rule on how much training data is\\n> enough; among other things, this\\n> depends on the signal-to-noise ratio\\n> of the underlying function, and the\\n> complexity of the models being fit to\\n> the data.\\n\\nIf you are new to this field, I recommend reading this short [""Pattern Recognition""][2] paper from the Encyclopedia of Biomedical Engineering which gives a brief summary of some of the data issues.\\n\\n\\n  [1]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/\\n  [2]: http://users.rowan.edu/~polikar/RESEARCH/PUBLICATIONS/wiley06.pdf",added 514 characters in body,
4292,2,1858,75ac5e88-ccef-498d-abd8-3f2b0c633297,2010-08-18 21:28:17.0,88.0,"I can assure you that RF would work in that case and its importance measure would be pretty insightful (because there will be no large tail of misleading unimportant attributes like in standard (n << p)s). I can't recall now any paper dealing with similar problem, but I'll look for it.\\n",,
4296,2,1860,7bd69975-5a67-41b9-bd50-0f9d957c1be7,2010-08-18 22:58:05.0,78.0,"I am studying a population of individuals who all begin with a measureable score of interest (ranging from -2 to 2) [call it ""old""], then they all undergo a change to a new score (also ranging from -2 to 2) [""new""]. Thus all the variation is in the change (which can be positive or negative), and there are also a variety of predictors that help to explain variation in the amount of change.\\n\\nMy initial model is simply:\\n\\n    change = a + bx + e\\n\\nwhere x is my vector of predictors.\\n\\nBut now I'm concerned that some of these predictors could be correlated with the baseline (old) score. Is this, then, a better specification?\\n\\n    change = a + bx + old +  e\\n\\nOr perhaps\\n\\n    new = a + bx + old + e\\n\\nThanks!",,
4297,1,1860,7bd69975-5a67-41b9-bd50-0f9d957c1be7,2010-08-18 22:58:05.0,78.0,Regression specification choices,,
4298,3,1860,7bd69975-5a67-41b9-bd50-0f9d957c1be7,2010-08-18 22:58:05.0,78.0,<regression>,,
4299,2,1861,497e0213-a2ed-4a1f-8248-12fdaddab092,2010-08-18 23:17:46.0,279.0,"You are right, version 1 is not acceptable. The second or third options (as long as `old` has a coefficient that will be estimated) are both OK, and in fact equivalent with respect to estimates for `a` and `b`. This can be seen if you replace `change` with `new-old` in the second equation, and solve it for `old`. All that happens is that the coefficient of `old` is increased by 1 as compared to the third equation. Other statistics such as R^2 will change, of course, as they are decomposing a different variability.\\n\\nNote, however, that you have a different problem as well. If your scores are restricted to a -2 to 2 range, somebody with `old=-2` cannot possibly get worse, and similarly for `old=2` you can't get any better. Such a range restriction is usually not modeled well by a linear regression.",,
4300,5,1836,5722686c-362a-49e9-bad9-39ed164ced23,2010-08-19 00:30:34.0,159.0,"This may surprise many, but to solve this problem you don't necessarily need to estimate *s*.  In fact, you don't need to know *anything* about the spread of the data (although that would be helpful, of course).  For instance, Wall, Boen, and Tweedie in a 2001 article describe how to find a finite confidence interval for the mean of any unimodal distribution based on a *single* draw.\\n\\nIn the present case, we have some basis to view the sample mean of 112 as a draw from an approximately normal distribution (namely, the sampling distribution of the average of a simple random sample of 49 salaries).  We are implicitly assuming there are a fairly large number of factory workers and that their salary distribution is not so skewed or multimodal as to render the central limit theorem inoperable.  Then a conservative 90% CI for the mean extends upwards to\\n\\n112 + 5.84|112|,\\n\\nclearly covering the true mean of 200.  (See Wall *et al* formula 3.)  Given the limited information available and the assumptions made here, we therefore cannot conclude that 112 differs ""significantly"" from 200.\\n\\nReference: [""An Effective Confidence Interval for the Mean With Samples of Size One and Two.""  The American Statistician, May 2001, Vol. 55, No. 2: pp. 102-105.][1]\\n\\n\\n  [1]: http://www.jstor.org/stable/2685995",Add jstor link for reference.,
4301,2,1862,1d2eb28c-bb2f-4e3e-836b-b4a82fc5b002,2010-08-19 00:31:44.0,840.0,"I've been struggling with the following problem with hopefully is an easy one for statisticians (I'm a programmer with some exposure to statistics).\\n\\nI need to summarize the responses to a survey (for management). The survey has 100+ questions, grouped in different areas (with about 5 to 10 questions per area). All answers are categorical (on an ordinal scale, they are like ""not at all"", ""rarely"" ... ""daily or more frequently"").\\n\\nManagement would like to get a summary for each area and this is my problem: **how to aggregate categorical answers within the related question?**. The questions are too many to make a graph or even a lattice plot for each area. I favor a visual approach if possible, compared to, say, tables with numbers (alas, they won't read them).\\n\\nThe only thing I can come up with is to count the number of answers in each area, then plot the histogram.\\n\\nIs there any thing else available for categorical data?\\n\\nI use R, but not sure if it's relevant, I feel this is more of a general statistics question.\\n\\nThanks for any suggestions.\\n\\n",,
4302,1,1862,1d2eb28c-bb2f-4e3e-836b-b4a82fc5b002,2010-08-19 00:31:44.0,840.0,How to summarize categorical data?,,
4303,3,1862,1d2eb28c-bb2f-4e3e-836b-b4a82fc5b002,2010-08-19 00:31:44.0,840.0,<data-transformation><categorical-data>,,
4305,2,1863,976042c7-ccd8-45f2-a209-87df438c6f14,2010-08-19 00:50:20.0,993.0,"I ran a within subjects repeated measures experiment, where the independent variable had 3 levels. The dependent variable is a measure of correctness and is recorded as either correct / incorrect. Time taken to provide an answer was also recorded.\\n\\nA within subjects repeated measures ANOVA is used to establish whether there is significant differences in correctness (DV) between the 3 levels of the IV, there is significant. Now, I'd like to analyze whether there is significant differences in the time taken to provide the answers when the answers are 1) correct, and 2) incorrect.\\n\\nMy problem is: Across the levels there are different numbers of correct / incorrect answers, e.g. level 1 has 67 correct answers, level 2 has 30, level 3 has 25. \\n\\nHow can I compare the time take taken for all correct answers across the 3 levels? I think this means its unbalanced? Can I do 3 one way ANOVAS to do a pairwise comparison, while adjusting p downwards to account for each comparison?\\n\\nThanks\\n",,
4306,1,1863,976042c7-ccd8-45f2-a209-87df438c6f14,2010-08-19 00:50:20.0,993.0,Might be an unbalanced within subjects repeated measures?,,
4307,3,1863,976042c7-ccd8-45f2-a209-87df438c6f14,2010-08-19 00:50:20.0,993.0,<variance><repeated-measures><unbalanced-classes><within-subjects>,,
4308,2,1864,2cde4bb9-90d3-441e-913e-af514166fc2c,2010-08-19 00:59:56.0,183.0,"I would have very little confidence in the generalisability of results of an exploratory analysis with 15 predictors and a sample size of 20.\\n\\n- The confidence intervals of parameter estimates would be large. E.g., the 95% confidence interval on r = .30 with n = 20 is -0.17 to 0.66 .\\n- Issues tend to be compounded when you have multiple predictors used in an exploratory and data driven way.",,
4309,2,1865,e52b543d-7781-4e7a-8d61-9de736641799,2010-08-19 01:15:27.0,994.0,"Standard deck has 52 cards, 26 Red and 26 Black. A run is a maximum contiguous block of cards, which has the same color.\\n\\nEg.\\n\\n* (R,B,R,B,...,R,B) has 52 runs.\\n* (R,R,R,...,R,B,B,B,...,B) has 2 runs.\\n\\nWhat is the expected number of runs in a shuffled deck of cards?",,
4310,1,1865,e52b543d-7781-4e7a-8d61-9de736641799,2010-08-19 01:15:27.0,994.0,What is the expected number of runs of same color in a standard deck of cards?,,
4311,3,1865,e52b543d-7781-4e7a-8d61-9de736641799,2010-08-19 01:15:27.0,994.0,<probability>,,
4312,2,1866,140ea2c7-57b6-400a-96b4-d7371935b99c,2010-08-19 02:10:15.0,253.0,"Following to the recent questions we had [here][1].\\n\\nI was hopping to know if anyone had come across or can share **R code for performing a custom power analysis based on simulation for a linear model?**\\n\\nLater I would obviously like to extend it to more complex models, but lm seems to right place to start. Thanks.\\n\\n  [1]: http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova/1823#1823",,
4313,1,1866,140ea2c7-57b6-400a-96b4-d7371935b99c,2010-08-19 02:10:15.0,253.0,How to simulate a custom power analysis of an lm model (using R),,
4314,3,1866,140ea2c7-57b6-400a-96b4-d7371935b99c,2010-08-19 02:10:15.0,253.0,<r><power><power-analysis>,,
4315,2,1867,8031eb5b-c684-47f3-b7e8-f592fe980f18,2010-08-19 02:35:58.0,253.0,"So this is a one way repeated measures Anova - with the ""Y"" being time till answer was given, and the first factor having 3 levels (each subject having three of them).\\n\\nI think the easiest way for doing this would be to take the mean response time for each subject for each of the three levels (which will results in 3 numbers per subject).\\n\\nAnd then run a friedman test on that (there is also a [post hoc friedman test in R][1], in case you would want that - I assume you would)\\n\\nThe downside of this is that this assumes, in a sense, that your estimation of the three means (a mean for each of the three levels, per subject), are the same where in fact they are not.  You have more variability in your estimation of level 3 then of level 1. \\n\\nRealistically, I would ignore that.  Theoretically, I hope someone here can offer a better solution so both of us would be able to learn :)\\n\\n\\n  [1]: http://www.r-statistics.com/2010/02/post-hoc-analysis-for-friedmans-test-r-code/",,
4316,2,1868,cb2c9646-2a22-4358-aea9-f53b4e7637fe,2010-08-19 03:03:39.0,183.0,A few references that you might find useful:\\n\\n- Edwards (2001) has a nice article called [Ten Difference Score Myths][1].\\n- I have a [post with some general points on change scores][2].\\n\\n\\n  [1]: http://public.kenan-flagler.unc.edu/faculty/edwardsj/Edwards01b.pdf\\n  [2]: http://jeromyanglim.blogspot.com/2009/09/difference-scores-are-they-okay-to-use.html,,
4317,2,1869,1c1e0285-b9ff-42a5-b936-1239eb1da8dd,2010-08-19 03:15:34.0,995.0,"You really need to figure out what is the question that you are trying to answer- or what question is management most interested in. Then you can select the survey questions that are most relevant to your problem. \\n\\nWithout knowing anything about your problem or dataset, here are some generic solutions:\\n\\n - Visually represent the answers as clusters. My favorite is by either using dendrograms or just plotting on an xy axis (Google ""cluster analysis r"" and go to the first result by statmethods.net)\\n - Rank the questions from greatest to least ""daily or more frequently"" responses. This is an example that may not exactly work for you but perhaps it will inspire you http://www.programmingr.com/content/building-scoring-and-ranking-systems-r\\n - Crosstabs: if for example, you have a question ""How often do you come in late for work?"" and ""How often do you use Facebook?,"" by crosstabbing the two questions you can find out the percentage of people who rarely do both, or who do both everyday. You can also calculate the correlation between two questions and create a correlation matrix. (Google ""r frequency crosstabs"")",,
4318,5,1869,94365ad4-18e8-4569-b52f-4e0fd50a6cc6,2010-08-19 03:21:26.0,995.0,"You really need to figure out what is the question that you are trying to answer- or what question is management most interested in. Then you can select the survey questions that are most relevant to your problem. \\n\\nWithout knowing anything about your problem or dataset, here are some generic solutions:\\n\\n - Visually represent the answers as clusters. My favorite is by either using dendrograms or just plotting on an xy axis (Google ""cluster analysis r"" and go to the first result by statmethods.net)\\n - Rank the questions from greatest to least ""daily or more frequently"" responses. This is an example that may not exactly work for you but perhaps it will inspire you http://www.programmingr.com/content/building-scoring-and-ranking-systems-r\\n - Crosstabs: if for example, you have a question ""How often do you come in late for work?"" and ""How often do you use Facebook?,"" by crosstabbing the two questions you can find out the percentage of people who rarely do both, or who do both everyday.(Google ""r frequency crosstabs"" or go to the aforementioned statmethods.net)\\n - Correlograms. I don't have any experience with these but I saw it also on the statmethods.net website. Basically you find which questions have the highest correlation and then create a table. You may find this useful although it looks kind of ""busy.""\\n",added 206 characters in body,
4319,2,1870,93e45b5d-84be-4b0e-94bb-e4371c9cd20b,2010-08-19 04:35:15.0,253.0,"The question is in the header, but I would extend the context a bit.\\n\\nNext semester I am due to be a TA in a course in statistics, where I would need to help sociology students learn to use SPSS.  I don't know SPSS, yet, and would like to learn how to use it.\\n\\nI was thinking of taking a simple dataset, and start reviewing it with methods I know, thus starting to map out where are methods I know of.  And once finished, to try and explore more options.\\n\\nCan someone propose other/better strategies to master a new statistical GUI? (in my case SPSS, but it could apply to many other GUI's)",,
4320,1,1870,93e45b5d-84be-4b0e-94bb-e4371c9cd20b,2010-08-19 04:35:15.0,253.0,How to learn how to use a new statistical GUI?,,
4321,3,1870,93e45b5d-84be-4b0e-94bb-e4371c9cd20b,2010-08-19 04:35:15.0,253.0,<spss><gui><learning>,,
4322,16,1870,93e45b5d-84be-4b0e-94bb-e4371c9cd20b,2010-08-19 04:35:15.0,253.0,,,
4323,2,1871,77acc753-5d97-44ce-b0e2-bebbb9b56ec3,2010-08-19 04:35:45.0,601.0,"It's not imbalanced because your repeated measures should be averaged across such subgroups within subject beforehand.  The only thing imbalances is the quality of the estimates of your means.\\n\\nJust as you aggregated your accuracies to get a percentage correct and do your ANOVA in the first place you average your latencies as well.  Each participant provides 6 values, therefore it is not imbalanced.\\n\\nMost likely though... the ANOVA was not the best analysis in the first place.  You should probably be using mixed-effect modelling.  For the initial test of the accuracies you'd use mixed effects logistic regression.  For the second one you propose it would be a 3-levels x 2-correctnesses analysis of the latencies.  Both would have subjects as a random effect.\\n\\nIn addition it's often best to do some sort of normality correction on the times like a log or -1/T correction.  This is less of a concern in ANOVA because you aggregate across a number of means first and that often ameliorates the skew of latencies through the central limit theorem.  You could check with a boxcox analysis to see what fits best.\\n\\nOn a more important note though... what are you expecting to find?  Is this just exploratory?  What would it mean to have different latencies in the correct and incorrect groups and what would it mean for them to interact?  Unless you are fully modelling the relationship between accuracy and speed in your experiment, or you have a fully model of such that you are testing, you are probably wasting your time.  A latency with an incorrect response means that someone did something other than what you wanted them to... and it could be anything.  That's why people almost always only work with the latencies to the correct responses. \\n\\n(these two types of responses also often have very different distributions with incorrect much flatter because they disproportionately make up both the short and long latencies)\\n",,
4324,2,1872,89e2e096-dfd8-4007-a8c1-c2e5e2bcf17c,2010-08-19 04:47:46.0,597.0,"Since you are pretty well versed in R, get a copy of Muenchen's ""[R for SAS and SPSS Users][1]"" (Springer, 2009) and work backwards.\\n\\n\\n  [1]: http://www.springer.com/statistics/computanional+statistics/book/978-0-387-09417-5",,
4325,16,1872,89e2e096-dfd8-4007-a8c1-c2e5e2bcf17c,2010-08-19 04:47:46.0,-1.0,,,
4326,2,1873,bd23891b-e3d8-4c41-a2d4-8bc32d3d14fc,2010-08-19 04:50:25.0,997.0,"I posted this on mathoverflow, but they sent me here. This question relates to a problem I had at work a while ago, doing a little data mining at a car rental company. Names changed, of course. I'm using Oracle DBMS if it matters.\\n\\nThere was a flight of steps out the front of our building. It had a dodgy step on it, on which people often stub their toes.\\n\\nI had records for everyone who works in the building, detailing how many times they climbed these steps and how many of these times they stubbed their toes on the dodgy step. There's a total of 3000 stair-climbing incidents and 1000 toe-stubbing incidents.\\n\\nJack climbed the steps 15 times and stubbed his toes 7 times, which is 2 more than you'd expect. What's the probability that this is just random, vs the probability that Joe is actually clumsy?\\n\\nI'm pretty sure from half-remembered statistics 1 that its something to do with chi-squared, but beats me where to go from there.\\n\\n...\\n\\nOf course, we actually had several flights of steps, each with different rates of toe stubbing and instep bashing. How would I combine the stats from those to get a more accurate better likelihood of Joe being clumsy? We can assume that there's no systematic bias in respect of more clumsy people being inclined to use certain flights of steps.\\n",,
4327,1,1873,bd23891b-e3d8-4c41-a2d4-8bc32d3d14fc,2010-08-19 04:50:25.0,997.0,"Based on my data, is Jack likely to be clumsy?",,
4328,3,1873,bd23891b-e3d8-4c41-a2d4-8bc32d3d14fc,2010-08-19 04:50:25.0,997.0,<statistical-analysis>,,
4329,2,1874,20c29658-1a04-4b16-af05-5c1311da9517,2010-08-19 04:53:32.0,996.0,"I'm looking to construct a 3-D surface of a part of the brain based on 2-D contours from cross-sectional slices from multiple angles. Once I get this shape, I want to ""fit"" it to another set of contours via rescaling.\\n\\nI'm aspiring to do this in the context of an MCMC analysis (So as to be able to make inferences, so it would be very nice if I could easily compute the volume of the rescaled surface, and the minimum distance between a given point of the distance. (Accurate approximations are fine). \\n\\nWhat would be a good image reconstruction algorithm that allows for volume and distance to be quickly calculated?",,
4330,1,1874,20c29658-1a04-4b16-af05-5c1311da9517,2010-08-19 04:53:32.0,996.0,Parametric Surface Reconstruction from Contours with Quick Rescaling,,
4331,3,1874,20c29658-1a04-4b16-af05-5c1311da9517,2010-08-19 04:53:32.0,996.0,<bayesian><mcmc><fitting><optimal-scaling><interpolation>,,
4332,5,1864,148dd5b8-194a-4523-a88f-3a49d1f25e2a,2010-08-19 05:53:41.0,183.0,"I would have very little confidence in the generalisability of results of an exploratory analysis with 15 predictors and a sample size of 20.\\n\\n- The confidence intervals of parameter estimates would be large. E.g., the 95% confidence interval on r = .30 with n = 20 is -0.17 to 0.66 .\\n- Issues tend to be compounded when you have multiple predictors used in an exploratory and data driven way.\\n\\nIn such circumstances, my advice would generally be to limit analyses to bivariate relationships.\\nIf you take a bayesian perspective, then I'd say that your prior expectations are equally if not more important than the data.",added 229 characters in body,
4333,2,1875,2f78dcca-ecb1-4452-ba76-9a3f08a4d4a8,2010-08-19 05:56:06.0,997.0,"A question which bothered me for some time, which I don't know how to address:\\n\\nEvery day, my weatherman gives a percentage chance of rain (let's assume its calculated to 9000 digits and he has never repeated a number). Every subsequent day, it either rains or does not rain.\\n\\nI have years of data - pct chance vs rain or not. *Given this weatherman's history*, if he says tonight that tomorrow's chance of rain is X, then what's my best guess as to what the chance of rain really is?",,
4334,1,1875,2f78dcca-ecb1-4452-ba76-9a3f08a4d4a8,2010-08-19 05:56:06.0,997.0,Is my weatherman accurate?,,
4335,3,1875,2f78dcca-ecb1-4452-ba76-9a3f08a4d4a8,2010-08-19 05:56:06.0,997.0,<statistical-analysis>,,
4336,2,1876,d26d6d43-af23-4d4e-b7f1-b1022868261c,2010-08-19 06:13:02.0,183.0,"Standard options include:\\n\\n- getting the mean for items within a scale (e.g., if the scale is 1 to 5, the mean will be 1 to 5)\\n- converting each item to a binary measure (e.g., if item >= 3, then 1, else 0) and then taking the mean of this binary response\\n\\nGiven that you are aggregating over items and over large samples of people in the organisation, both options above (i.e., the mean of 1 to 5 or the mean of percentage above a point) will be reliable at the organisational-level ([see here for further discussion][1]). Thus, either of the above options are basically communicating the same information. \\n\\nIn general I wouldn't be worried about the fact that items are categorical. By the time you create scales by aggregating over items and then aggregate over your sample of respondents, the scale will be a close approximation to a continuous scale.\\n\\nManagement may find one metric easier to interpret. When  I get Quality of Teaching scores (i.e., the average student satisfaction score of say 100 students) , it is the average on a 1 to 5 scale and that's fine. Over the years after seeing my own scores from year to year and also seeing some norms for the university I've developed a frame of reference of what different values mean.\\n However, management sometimes prefers to think about the percentage endorsing a statement, or the percentage of positive responses even when it is in a sense the mean percentage.\\n\\nThe main challenge is to give some tangible frame of reference for the scores. Management will want to know **what the numbers actually mean**. For example, if the mean response for a scale is 4.2, What does that mean? Is it good? Is it bad? Is it just okay?\\n\\n If you are using the survey over multiple years or in different organisations, then you can start to develop some norms. Access to norms is one reason organisations often get an external survey provider or use a standard survey.\\n\\nYou may also wish to run a factor analysis to validate that the assignment of items to scales is empirically justifiable.\\n\\nIn terms of a visual approach, you can have a simple line or bar graph with the scale type on the x-axis and the score on the y-axis. If you have normative data, you could add that also. \\n\\n  [1]: http://jeromyanglim.blogspot.com/2009/10/job-satisfaction-measurement-scales.html",,
4337,2,1877,98c82909-0121-40b0-a6d0-03a4a1727767,2010-08-19 06:16:25.0,339.0,"    chisq.test(c(15,7),p=c(2000,1000),rescale.p=TRUE)\\n    \\n            Chi-squared test for given probabilities\\n    \\n    data:  c(15, 7) \\n    X-squared = 0.0227, df = 1, p-value = 0.8802\\n\\nThere is not enough evidence against the Null hypothesis (that is just a random incident). \\nA difference from the expected value as big as or bigger than the one observed will arise by chance alone in more than 88% of cases and is clearly not statistically significant.\\n\\nThe chi-squared value is\\n\\n    sum((c(15,7) - 22*c(2000,1000)/3000)^2 / (22*c(2000,1000)/3000))\\n    [1] 0.02272727\\n\\nand the p-value comes from the right-hand tail of the cumulative probability function of\\nthe chi-squared distribution 1-pchisq with 1 degrees of freedom (2 comparisons −1 for\\ncontingency; the total count must be 22)\\n\\n    1-pchisq(0.02272727,1)\\n    [1] 0.8801685\\n\\nexactly as we obtained using the built-in chisq.test function, above.",,
4338,2,1878,4d1df636-0dac-4eb5-baa9-e8d7a702b45d,2010-08-19 06:22:09.0,223.0,"Comparison of probability forecast for binary event (or discrete RV) can be done upon the **Brier score** http://en.wikipedia.org/wiki/Brier_score\\n\\nbut you can also use **ROC curve** http://en.wikipedia.org/wiki/Receiver_operating_characteristic since any probability forecast of this type can be transformed into a dicrimination procedure with a varying threshold \\nIndeed you can say ""it will rain"" if your probability is greater than $\\tau$ and evaluate the missed, false discovery,true discovery and  true negatives for different values of $\\tau$.\\n\\nYou should take a look at how the European center fot medium range weather forecast (**ECMWF**) does: http://www.ecmwf.int/products/forecasts/guide/The_verification_of_ECMWF_forecasts.html",,
4339,2,1879,58b06a5c-ff6c-4d14-82e5-4945fb8e69fd,2010-08-19 06:28:05.0,183.0,"As someone who made the shift the other way from SPSS to R, I'd say that SPSS is relatively simple and intuitive relative to R. The menus and dialog boxes guide you through the process. Of course this means that it is also fairly easy to run analyses that don't make sense. And the GUI  leads to less flexible analyses and tedious button pressing especially for repetitive analyses.\\n\\nThus, your approach of taking a dataset and just playing around might be sufficient.\\n\\nThere's plenty of how-to books out there, such as:\\n\\n- Discovering Statistics Using SPSS\\n- SPSS Survival Manual\\n\\nThere's also plenty of websites offering tutorials: \\n\\n- [Andy Field][1]\\n- [My old notes][2]\\n- [UCLA on SPSS][3]\\n\\nI'd also recommend that if you are teaching students about SPSS, you encourage them to use syntax. Using SPSS syntax is not as good as using technologies like [R and Sweave][4].\\nHowever, using syntax is much better than just pressing menus and buttons in an ad hoc way and then wondering later what you've actually done.\\nI wrote a post listing tips for using [SPSS syntax in order to approximate reproducible research with SPSS][5].\\n\\n\\n  [1]: http://www.statisticshell.com/woodofsuicides.html\\n  [2]: http://jeromyanglim.blogspot.com/2009/09/teaching-resources.html\\n  [3]: http://www.ats.ucla.edu/stat/spss/\\n  [4]: http://www.r-bloggers.com/getting-started-with-sweave-r-latex-eclipse-statet-texlipse/\\n  [5]: http://jeromyanglim.blogspot.com/2009/10/introduction-to-spss-syntax-advice-for.html",,
4340,16,1879,58b06a5c-ff6c-4d14-82e5-4945fb8e69fd,2010-08-19 06:28:05.0,-1.0,,,
4341,5,1877,89191546-ead0-49e6-8e2d-d4fe5da7f910,2010-08-19 06:29:38.0,339.0,"    chisq.test(c(15,7),p=c(3000,1000),rescale.p=TRUE)\\n\\n        Chi-squared test for given probabilities\\n\\n    data:  c(15, 7)\\n    X-squared = 0.5455, df = 1, p-value = 0.4602\\n    \\nThere is not enough evidence against the Null hypothesis (that is just a random incident).\\nA difference from the expected value as big as or bigger than the one observed will arise by chance alone in more than 46% of cases and is clearly not statistically significant.\\n\\nThe chi-squared value is\\n\\n    sum((c(15,7) - 22*c(3000,1000)/4000)^2 / (22*c(3000,1000)/4000))\\n    [1] 0.5454545\\n\\nand the p-value comes from the right-hand tail of the cumulative probability function of\\nthe chi-squared distribution 1-pchisq with 1 degrees of freedom (2 comparisons -1 for\\ncontingency; the total count must be 22)\\n\\n    1-pchisq(0.5454545,1)\\n    [1] 0.460181\\n\\nexactly as we obtained using the built-in chisq.test function, above.\\n\\n**EDIT**\\n\\nAlternatively, you could carry out a binomial test:\\n\\n    binom.test(c(15,7),p=3/4)\\n    \\n            Exact binomial test\\n    \\n    data:  c(15, 7)\\n    number of successes = 15, number of trials = 22, p-value = 0.463\\n    alternative hypothesis: true probability of success is not equal to 0.75\\n    95 percent confidence interval:\\n     0.4512756 0.8613535\\n    sample estimates:\\n    probability of success\\n                 0.6818182\\n\\nYou can see that the 95% confidence interval for the proportion of successes (0.45, 0.86)\\ncontains 0.75, so there is no evidence against a 3:1 success ratio in these data. The p value is slightly different than it was in the chi-squared test, but the interpretation is exactly the same.",deleted 16 characters in body; added 747 characters in body; added 3 characters in body,
4342,5,1415,7d85a2b4-c27b-4b71-a9f7-6340100d21ef,2010-08-19 06:34:08.0,223.0,Prostate (gene expression array)\\n\\nk=2\\nn=48+52 \\np=6033\\n\\nAvailable via (among other) R package spls http://cran.r-project.org/web/packages/spls/\\nname of the dataset: prostate\\n\\n\\nerror rate =  3/102 (from http://www.stat.wisc.edu/%7Ekeles/Papers/C_SPLS.pdf) also I think there are paper which show 1/102 error rate. I would say this is an easy test case. \\n,added 98 characters in body,
4343,2,1880,232ca679-82fb-473e-b2ec-45ca4e457a5d,2010-08-19 06:35:02.0,183.0,"Here are a few sources of simulation code in R. I'm not sure if any specifically address linear models, but perhaps they provide enough of an example to get the gist:\\n\\n- Benjamin Bolker has written a great book [Ecological Data and Models with R][1]. An early draft of the whole book along with Sweave code is available online. [Chapter 5][2] addresses power analysis and simulation.\\n\\nThere's another couple of examples of simulation at the following sites:\\n\\n- http://www.personality-project.org/R/r.datageneration.html\\n- http://psy-ed.wikidot.com/simulation\\n\\n\\n  [1]: http://www.math.mcmaster.ca/~bolker/emdbook/\\n  [2]: http://www.math.mcmaster.ca/~bolker/emdbook/chap5A.pdf",,
4344,6,1853,2bb85fa5-b443-4961-814d-8d3e427e091b,2010-08-19 06:39:13.0,183.0,<distributions><hypothesis-testing><bootstrap><moments>,edited tags,
4345,2,1881,86f1685e-b151-4667-8fff-48dc883d2a17,2010-08-19 06:42:15.0,582.0,"I would like an advice on a analysis method I am using, to know if it it statistically sound.\\n\\nI have measured two point processes $T^1 = t^1_1, t^1_2, ..., t^1_n$ and $T^2 = t^2_1, t^2_2, ..., t^2_m$ and I want to determine if the events in $T^1$ are somehow correlated to the events in $T^2$.\\n\\nOne of the methods that I have found in the literature is that of constructing a cross-correlation histogram: for each $t^1_n$ we find the delay to all the events of $T^2$ that fall in a given window of time (before and after $t^1_n$), and then we construct an histogram of all these delays.\\n\\nIf the two processes are not correlated I would expect a flat histogram, as the probability of having an event in $T^2$ after (or before) an event in $T^1$ is equal at all delays. On the other hand if there is a peak in the histogram, this suggests that the two point process are somehow influencing each other (or, at least, have some common input).\\n\\nNow, this is nice and good, but how do I determine whether the histograms do have a peak (I have to say that for my particular set of data they're clearly flat, but still it would be nice to have a statistical way of confirming that)?\\n\\nSo, here what I've done: I've repeated the process of generating the histogram for several (1000) times keeping $T^1$ as it is and using a ""shuffled"" version of $T^2$.\\nTo shuffle $T^2$ I calculate the intervals between all the events, shuffle them and sum them to reconstitute a new point process. In R I simply do this with:\\n\\n    times2.swp <- cumsum(sample(diff(times2)))\\n\\nSo, I end up with 1000 new histogram, that show me the density of events in $T^{2*}$ compared to $T^1$.\\n\\nFor each bin of these histogram (they're all binned in the same way) I calculate the density of 95% of the histogram. In other words I'm saying, for instance: at time delay 5 ms, in 95% of the shuffled point processes there is a probability x of finding an event in $T^{2*}$ after an event in $T^1$.\\n\\nI would then take this 95% value for all of the time delays and use it as some ""confidence limit"" (probably this is not the correct term) so that anything that goes over this limit in the original histogram can be considered a ""true peak"".\\n\\n**Question 1**: is this method statistically correct? If not how would you tackle this problem?\\n\\n**Question 2**: another thing that I want to see is whether there is a ""longer"" type of correlation of my data. For instance there may be similar changes in the rate of events in the two point processes (note that they may have quite different rates), but I'm not sure how to do that. I thought of creating an ""envelope"" of each point process using some sort of smoothing kernel and then performing a cross-correlation analysis of the two envelopes. Could you suggest any other possible type of analysis?\\n\\n\\nThank you and sorry for this very long question.",,
4346,1,1881,86f1685e-b151-4667-8fff-48dc883d2a17,2010-08-19 06:42:15.0,582.0,Analysis of cross correlation between point-processes,,
4347,3,1881,86f1685e-b151-4667-8fff-48dc883d2a17,2010-08-19 06:42:15.0,582.0,<point-process><cross-correlation>,,
4348,2,1882,b0f03b92-8bd2-47a9-b424-c8c6730b90e0,2010-08-19 06:42:28.0,830.0,"To answer the letter of the question, ""ordinary least squares"" is not an algorithm; rather it is a type of problem in computational linear algebra, of which linear regression is one example. Usually one has data $(x_1,y_1)\\dots(x_m,y_m)$ and a tentative function to fit the data against, of the form $f(x)=c_1 f_1(x)+\\dots+c_n f_n(x)$. The $f_j(x)$ are called basis functions and can be anything from monomials $x^j$ to trigonometric functions (e.g. $\\sin(jx)$) and exponential functions ($\\exp(-jx)$). The term ""linear"" in ""linear regression"" here does not refer to the basis functions but to the coefficients $c_j$, in that taking the partial derivative with respect to any of the $c_j$ gives you the factor multiplying $c_j$; that is, $f_j(x)$.\\n\\nOne now has an m-by-n rectangular matrix $\\textbf{A}$ (""design matrix"") that (usually) has more rows than columns, and each entry is of the form $f_j(x_i)$, i being the row index and j being the column index. OLS is now the task of finding the vector $\\textbf{c}=(c_1\\dots c_n)$ that minimizes the quantity $\\|\\textbf{A}\\textbf{c}-\\textbf{y}\\|_2=\\sqrt{\\sum_{j=1}^{m}\\left(y_j-f(x_j)\\right)^2}$ ($\\textbf{y}$ is usually called the ""response vector"").\\n\\nThere are at least three methods used in practice for computing least-squares solutions: the normal equations, QR decomposition, and singular value decomposition. In brief, they are ways to transform the matrix $\\textbf{A}$ into a product of matrices that are easily manipulated to solve for the vector $\\textbf{c}$.\\n\\ngd047 already showed the method of normal equations in his answer; one just solves the n-by-n set of linear equations\\n\\n$\\textbf{A}^T\\textbf{A}\\textbf{c}=\\textbf{A}^T\\textbf{y}$\\n\\nfor $\\textbf{c}$. Due to the fact that the matrix $\\textbf{A}^T\\textbf{A}$ is symmetric positive (semi)definite, the usual method used for this is Cholesky decomposition, which factors $\\textbf{A}^T\\textbf{A}$ into the form $\\textbf{G}\\textbf{G}^T$, $\\textbf{G}$ a lower triangular matrix. The problem with this approach, despite the advantage of being able to compress the m-by-n design matrix into a (usually) much smaller n-by-n matrix, is that this operation is prone to loss of significant figures (this has something to do with the ""condition number"" of the design matrix).\\n\\nA slightly better way is QR decomposition, which directly works with the design matrix. It factors $\\textbf{A}$ as $\\textbf{A}=\\textbf{Q}\\textbf{R}$, where $\\textbf{Q}$ is an orthogonal matrix (the transpose of the matrix is the inverse) and $\\textbf{R}$ is upper triangular. $\\textbf{c}$ is subsequently computed as $\\textbf{R}^{-1}\\textbf{Q}^T\\textbf{y}$. For reasons I won't get into (just see any numerical linear algebra text), this has better numerical properties than the method of normal equations.\\n\\nFinally, the most expensive, yet safest, way of solving OLS is the singular value decomposition (SVD). This time, $\\textbf{A}$ is factored as $\\textbf{A}=\\textbf{U}\\textbf{\\Sigma}\\textbf{V}^T$, where $\\textbf{U}$ and $\\textbf{V}$ are both orthogonal, and $\\textbf{\\Sigma}$ is a diagonal matrix, whose diagonal entries are termed ""singular values"". The power of this decomposition lies in the diagnostic ability granted to you by the singular values, in that if one sees one or more tiny singular values, then it is likely that you have chosen a not entirely independent basis set, thus necessitating a reformulation of your functional form.\\n\\nThis is merely a sketch of these three algorithms; any good book on computational statistics and numerical linear algebra should be able to give you more relevant details.",,
4349,5,1882,ceddc799-6aca-4408-bbc7-d80a4ffffd19,2010-08-19 06:51:15.0,830.0,"To answer the letter of the question, ""ordinary least squares"" is not an algorithm; rather it is a type of problem in computational linear algebra, of which linear regression is one example. Usually one has data $(x_1,y_1)\\dots(x_m,y_m)$ and a tentative function to fit the data against, of the form $f(x)=c_1 f_1(x)+\\dots+c_n f_n(x)$. The $f_j(x)$ are called basis functions and can be anything from monomials $x^j$ to trigonometric functions (e.g. $\\sin(jx)$) and exponential functions ($\\exp(-jx)$). The term ""linear"" in ""linear regression"" here does not refer to the basis functions but to the coefficients $c_j$, in that taking the partial derivative with respect to any of the $c_j$ gives you the factor multiplying $c_j$; that is, $f_j(x)$.\\n\\nOne now has an m-by-n rectangular matrix $\\textbf{A}$ (""design matrix"") that (usually) has more rows than columns, and each entry is of the form $f_j(x_i)$, i being the row index and j being the column index. OLS is now the task of finding the vector $\\textbf{c}=c_1\\dots c_n)$ that minimizes the quantity $\\sqrt{\\sum_{j=1}^{m}\\left(y_j-f(x_j)\\right)^2}$ (in matrix notation, $\\|\\textbf{A}\\textbf{c}-\\textbf{y}\\|_2$ ; here, $\\textbf{y}$ is usually called the ""response vector"").\\n\\nThere are at least three methods used in practice for computing least-squares solutions: the normal equations, QR decomposition, and singular value decomposition. In brief, they are ways to transform the matrix $\\textbf{A}$ into a product of matrices that are easily manipulated to solve for the vector $\\textbf{c}$.\\n\\ngd047 already showed the method of normal equations in his answer; one just solves the n-by-n set of linear equations\\n\\n$\\textbf{A}^T\\textbf{A}\\textbf{c}=\\textbf{A}^T\\textbf{y}$\\n\\nfor $\\textbf{c}$. Due to the fact that the matrix $\\textbf{A}^T\\textbf{A}$ is symmetric positive (semi)definite, the usual method used for this is Cholesky decomposition, which factors $\\textbf{A}^T\\textbf{A}$ into the form $\\textbf{G}\\textbf{G}^T$, $\\textbf{G}$ a lower triangular matrix. The problem with this approach, despite the advantage of being able to compress the m-by-n design matrix into a (usually) much smaller n-by-n matrix, is that this operation is prone to loss of significant figures (this has something to do with the ""condition number"" of the design matrix).\\n\\nA slightly better way is QR decomposition, which directly works with the design matrix. It factors $\\textbf{A}$ as $\\textbf{A}=\\textbf{Q}\\textbf{R}$, where $\\textbf{Q}$ is an orthogonal matrix (the transpose of the matrix is the inverse) and $\\textbf{R}$ is upper triangular. $\\textbf{c}$ is subsequently computed as $\\textbf{R}^{-1}\\textbf{Q}^T\\textbf{y}$. For reasons I won't get into (just see any numerical linear algebra text), this has better numerical properties than the method of normal equations.\\n\\nFinally, the most expensive, yet safest, way of solving OLS is the singular value decomposition (SVD). This time, $\\textbf{A}$ is factored as $\\textbf{A}=\\mathbf{U}\\Sigma\\textbf{V}^T$, where $\\textbf{U}$ and $\\textbf{V}$ are both orthogonal, and $\\mathbf{\\Sigma}$ is a diagonal matrix, whose diagonal entries are termed ""singular values"". The power of this decomposition lies in the diagnostic ability granted to you by the singular values, in that if one sees one or more tiny singular values, then it is likely that you have chosen a not entirely independent basis set, thus necessitating a reformulation of your functional form.\\n\\nThis is merely a sketch of these three algorithms; any good book on computational statistics and numerical linear algebra should be able to give you more relevant details.",added 20 characters in body,
4350,2,1883,3272d8b2-a6d0-4c0b-80ea-b0f5c92d2cae,2010-08-19 07:00:53.0,273.0,"What areas of statistics have been substantially revolutionised in the last 50 years? For example, about 40 years ago, Akaike with colleagues revolutionised the area of statistical model discrimination. About 10 years ago, Hyndman with colleagues revolutionised the area of exponential smoothing. About XX years ago, ... How do I possibly continue the list, with years and names please? By statistics I mean its all four types from Bartholomew's 1995 presidential address, Chambers's greater and lesser statistics together, as featuring in Hand's recent presidential address on 'Modern statistics' and so on - anything professionally relevant.",,
4351,1,1883,3272d8b2-a6d0-4c0b-80ea-b0f5c92d2cae,2010-08-19 07:00:53.0,273.0,Revolutions in statistics for the last 50 years?,,
4352,3,1883,3272d8b2-a6d0-4c0b-80ea-b0f5c92d2cae,2010-08-19 07:00:53.0,273.0,<statistics>,,
4353,5,1850,6964e3a8-31df-409f-a328-7aed9e41828f,2010-08-19 07:03:46.0,159.0,"For an effect size analysis, I am noticing that there are differences between Cohen's d, Hedges's g and Hedges' g*. \\n\\n - Are these three metrics normally very similar?  \\n - What would be a case where they would produce different results? \\n - Also is it a matter of preference which I use or report with?\\n\\n",Fixed typos,
4354,16,1883,00000000-0000-0000-0000-000000000000,2010-08-19 07:05:16.0,159.0,,,
4355,6,1850,53604313-9063-45b5-8385-c55eeaec7c28,2010-08-19 08:02:01.0,183.0,<effect-size><cohens-d>,edited tags,
4356,2,1884,0740e298-3eca-4662-8abf-2bcd7fdf086b,2010-08-19 08:34:51.0,994.0,"Suppose $X_n$ denotes the color of the $n$th card in the shuffled deck.\\n\\nThen note that the last card always denotes the end of a run. Other ends of runs are characterized by $X_n\\ne X_{n+1}$ which indicates a run ending at $n$.\\n\\nNote that $P(X_n\\ne X_{n+1})=26/51$ (since once you fix a card, you can choose another card from remaining 51 out of which 26 will have a different color).\\n\\nSo summing up the indicators $X_n\\ne X_{n+1}$ we get the number of runs - \\n\\n$\\\\#runs=1+\\sum_{n=1}^{51}\\mathbb{I}_{X_n\\ne X_{n+1}}$.\\n\\nSo $E(\\\\#runs)=1+\\sum_{n=1}^{51}P(X_n\\ne X_{n+1})=1+\\sum_{n=1}^{51}26/51=27$.",,
4357,2,1885,6368d0cd-fbb7-4bf1-98ba-90a88706057c,2010-08-19 08:50:44.0,930.0,"To my understanding, Hedges's g is a somewhat more accurate version of Cohen's d (with pooled SD) in that we add a correction factor for small sample. Both measures generally agree when the homoscedasticity assumption is not violated, but we may found situations where this is not the case, see e.g. McGrath & Meyer, *Psychological Methods* 2006, **11(4)**: 386-401 ([pdf][1]). Other papers are listed at the end of my reply.\\n\\nI generally found that in almost every psychological or biomedical studies, this is the Cohen's d that is reported; this probably stands from the well-known rule of thumb for interpreting its magnitude (Cohen, 1988). I don't know about any recent paper considering Hedges's g (or Cliff delta as a non-parametric alternative). Bruce Thompson has a [revised version][2] of the APA section on effect size.\\n\\nGoogling about Monte Carlo studies around effect size measures, I found this paper which might be interesting (I only read the abstract and the simulation setup): [Robust Confidence Intervals for Effect Sizes: A Comparative Study of Cohen’s d and Cliff’s Delta Under Non-normality and Heterogeneous Variances][3] (pdf).\\n\\nAbout your 2nd comment, the `MBESS` R package includes various utilities for ES calculation (e.g., `smd` and related functions).\\n\\n**Other references**\\n\\n1. Zakzanis, K.K. (2001). Statistics to tell the truth, the whole truth, and nothing but the truth: Formulae, illustrative numerical examples, and heuristic interpretation of effect size analyses for neuropsychological researchers. *Archives of Clinical Neuropsychology*, 16(7), 653-667. ([pdf][4])\\n2. Durlak, J.A. (2009). How to Select, Calculate, and Interpret Effect Sizes. *Journal of Pediatric Psychology* ([pdf][5])\\n\\n\\n  [1]: http://www.bobmcgrath.org/Pubs/When_effect_sizes_disagree.pdf\\n  [2]: http://people.cehd.tamu.edu/~bthompson/apaeffec.htm\\n  [3]: http://www.coedu.usf.edu/main/departments/me/documents/cohen.pdf\\n  [4]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VDJ-43TFJ8B-4&_user=10&_coverDate=10%2F31%2F2001&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1435354249&_rerunOrigin=google&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=12f831e4b738bda3a217d40d6cfd329f\\n  [5]: http://jpepsy.oxfordjournals.org/cgi/content/full/jsp004v1",,
4361,6,1865,cc014f2b-2007-4c6d-93bd-b64cf1666da5,2010-08-19 09:48:17.0,8.0,<probability><games><cards>,edited tags,
4362,5,1843,9e485cb7-89fd-42de-a0a5-a44ee5ffc48f,2010-08-19 09:59:28.0,442.0,"You can use `par(new=T)` to plot into the same graph using two different y-axes! Thsi should also solve your problem.\\n\\nNext you will find a simple example that plots two random normal variables, one on mean 0 the other one on mean 100 (both *sd* s = 1) in the same plot. The first one in red on the left y-axis, the second one in blue on the right y-axis. Then, axis labels are added.\\n\\nHere you go:\\n\\n    x <- 1:10\\n    y1 <- rnorm(10)\\n    y2 <- rnorm(10)+100\\n\\n    plot(x,y1,pch=0,type=""b"",col=""red"",yaxt=""n"",ylim=c(-8,2))\\n    par(new=T)\\n    plot(x,y2,pch=1,type=""b"",col=""blue"",yaxt=""n"",ylim=c(98,105))\\n\\n    axis(side=2)\\n    axis(side=4)\\n\\nlooks like this then (remember red on left axis, blue on right axis): ![alt text][1]\\n\\n**UPDATE:**  \\nBased on comments I produced an updated version of my graph. Now I dig a little deeper into base graph functionality using `par(mar=c(a,b,c,d))` to create a bigger margin around the graph (needed for right axis label), `mtext` to show the axis labels and and advanced use of the `axis` function:\\n\\n\\n    x <- 1:100\\n    y1 <- rnorm(100)\\n    y2 <- rnorm(100)+100\\n    \\n    par(mar=c(5,5,5,5))\\n\\n    plot(x,y1,pch=0,type=""b"",col=""red"",yaxt=""n"",ylim=c(-8,2),ylab="""")\\n    axis(side=2, at=c(-2,0,2))\\n    mtext(""red line"", side = 2, line=2.5, at=0)\\n    \\n    par(new=T)\\n    plot(x,y2,pch=1,type=""b"",col=""blue"",yaxt=""n"",ylim=c(98,108), ylab="""")\\n    axis(side=4, at=c(98,100,102), labels=c(""98%"",""100%"",""102%""))\\n    mtext(""blue line"", side=4, line=2.5, at=100)\\n\\n![alt text][2]\\n\\nAs you see it is pretty straight forward. You can define the position of your data with `ylim` in the `plot` function, then use `at` in the `axis` function to select which axis ticks you wanna see. Furthermore, you can even provide the labels for the axis ticks (pretty useful for nominal x-axis) via `labels` in the `axis` function (done here on the right axis). To add axis labels, use `mtext` with `at` for vertical positioning (`line` for horizontal positioning).  \\n\\nMake sure to check `?plot`, `?par`, `?axis`, and `?mtext` for further info.  \\nGreat web resources are: [Quick-R][3] for Graphs: [1][4], [2][5], and [3][6].\\n\\n\\n  [1]: http://i.stack.imgur.com/311Vo.png\\n  [2]: http://i.stack.imgur.com/14zqJ.png\\n  [3]: http://www.statmethods.net/index.html\\n  [4]: http://www.statmethods.net/advgraphs/parameters.html\\n  [5]: http://www.statmethods.net/graphs/index.html\\n  [6]: http://www.statmethods.net/advgraphs/index.html",added updated grapoh based on comments; added 11 characters in body,
4363,2,1887,76948551-0878-44ce-9954-e4134fd69dd8,2010-08-19 10:00:00.0,930.0,"Maybe too late but I add my answer anyway...\\n\\nIt depends on what you intend to do with your data: If you are interested in showing that scores differ when considering different group of participants (gender, country, etc.), you may treat your scores as numeric values, provided they fulfill usual assumptions about variance (or shape) and sample size. If you are rather interested in highlighting how response patterns vary across subgroups, then you should consider item scores as discrete choice among a set of answer options and look for log-linear modeling, ordinal logistic regression, item-response models or any other statistical model that allows to cope with polytomous items.\\n\\nAs a rule of thumb, one generally considers that having 12 distinct points on a scale is sufficient to approximate an interval scale (for interpretation purpose). Lickert items may be regarded as true ordinal scale, but they are often used as numeric and we can compute their mean or SD. This is often done in attitude surveys, although it is wise to report both mean/SD and % of response in, e.g. the two highest categories.\\n\\nWhen using summated scale scores (i.e., we add up score on each item to compute a ""total score""), usual statistics may be applied, but you have to keep in mind that you are now working with a latent variable so the underlying construct should make sense! In psychometrics, we generally check that (1) unidimensionnality of the scale holds, (2) scale reliability is sufficient. When comparing two such scale scores (for two different instruments), we might even consider using attenuated correlation measures instead of classical Pearson correlation coefficient. \\n\\nClassical textbooks include:  \\n1. Nunnally, J.C. and Bernstein, I.H. (1994). *Psychometric Theory* (3rd ed.). McGraw-Hill Series in Psychology.  \\n2. Streiner, D.L. and Norman, G.R. (2008). *Health Measurement Scales. A practical guide to their development and use* (4th ed.). Oxford.  \\n3. Rao, C.R. and Sinharay, S., Eds. (2007). *Handbook of Statistics, Vol. 26: Psychometrics*. Elsevier Science B.V.  \\n4. Dunn, G. (2000). *Statistics in Psychiatry*. Hodder Arnold.\\n\\nYou may also have a look at [Applications of latent trait and latent class models in the social sciences][1], from Rost & Langeheine, and W. Revelle's website on [personality research][2].\\n\\nWhen validating a psychometric scale, it is important to look at so-called ceiling/floor effects (large asymmetry resulting from participants scoring at the lowest/highest response category), which may seriously impact on any statistics computed when treating them as numeric variable (e.g., country aggregation, t-test). This raises specific issues in cross-cultural studies since it is known that overall response distribution in attitude or health surveys differ from one country to the other (e.g. chinese people vs. those coming from western countries tend to highlight specific response pattern, the former having generally more extreme scores at the item level, see e.g. Song, X.-Y. (2007) Analysis of multisample structural equation models with applications to Quality of Life data, in *Handbook of Latent Variable and Related Models*, Lee, S.-Y. (Ed.), pp 279-302, North-Holland).\\n\\nMore generally, you should look at the psychometric-related literature which makes extensive use of Lickert items if you are interested with measurement issue. Various statistical models have been developed and are currently headed under the Item Response Theory framework.\\n\\n\\n  [1]: http://www.ipn.uni-kiel.de/aktuell/buecher/rostbuch/inhalt.htm\\n  [2]: http://www.personality-project.org/",,
4365,2,1888,ba585d0a-2f96-4b84-beb8-de4a619e5b78,2010-08-19 10:19:35.0,930.0,"Clason & Dormody discussed the issue of statistical testing for Lickert items ([Analyzing data measured by individual Likert-type items][1]). I think that a bootstraped test is ok when the two distributions look similar (bell shaped and equal variance). However, a test for categorical data (e.g. trend or Fisher test, or ordinal logistic regression) would be interesting too since it allows to check for response distribution across the item categories, see Agresti's book on *Categorical Data Analysis* (Chapter 7 on *Logit models for multinomial responses*). \\n\\nAside from this, you can imagine situations where the t-test or any other non-parametric tests would fail if the response distribution is strongly imbalanced between the two groups. For example, if all people from group A answer 1 or 5 (in equally proportion) whereas all people in group B answer 3, then you end up with identical within-group mean and the test is not meaningful at all, though in this case the homoscedasticity assumption is largely violated.\\n\\n\\n  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.113.2197&rep=rep1&type=pdf",,
4366,2,1889,8bf187cd-2b89-4cab-903c-62075ff3227c,2010-08-19 10:29:27.0,334.0,"Efron's work on the [Bootstrap](http://en.wikipedia.org/wiki/Bootstrapping_(statistics) comes to mind.\\n\\n*Grr, anybody know how to fix the URL ending in a ) ?*",,
4367,16,1889,8bf187cd-2b89-4cab-903c-62075ff3227c,2010-08-19 10:29:27.0,-1.0,,,
4368,5,1889,26797ab7-6185-4b33-b1e3-bf4fbb5188a5,2010-08-19 10:44:00.0,88.0,Efron's work on the [Bootstrap][1] comes to mind.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bootstrapping_(statistics),deleted 43 characters in body,
4369,2,1890,179101fe-a8bd-4d52-8a4f-9d4dcf0350a1,2010-08-19 10:45:32.0,88.0,The creation of this site ;-),,
4370,16,1890,179101fe-a8bd-4d52-8a4f-9d4dcf0350a1,2010-08-19 10:45:32.0,-1.0,,,
4371,2,1891,0e46b780-9595-4ab9-bf8c-8addcd6a0eb0,2010-08-19 11:03:39.0,521.0,"<p><a href=""http://en.wikipedia.org/wiki/Generalized_linear_model"" rel=""nofollow"">Generalized linear models</a> due to the recently deceased <a href=""http://en.wikipedia.org/wiki/John_Nelder"" rel=""nofollow"">John Nelder</a> and <a href=""http://en.wikipedia.org/wiki/Robert_Wedderburn_%28statistician%29"" rel=""nofollow"" >Robert Wedderburn</a>.</p>\\n",,
4372,16,1891,0e46b780-9595-4ab9-bf8c-8addcd6a0eb0,2010-08-19 11:03:39.0,-1.0,,,
4373,2,1892,c9c80c6c-067e-4e66-8c3d-ee6ec510fefd,2010-08-19 11:06:22.0,183.0,- Revolution 1: **S** ([ACM Software Systems Award][1])\\n- Revolution 2: **R** ([Ross Ihaka (1998)][2] on the history of R to that point)\\n\\n\\n  [1]: http://www.acm.org/announcements/ss99.html\\n  [2]: http://cran.r-project.org/doc/html/interface98-paper/paper.html,,
4374,16,1892,c9c80c6c-067e-4e66-8c3d-ee6ec510fefd,2010-08-19 11:06:22.0,-1.0,,,
4375,2,1893,f79fb3c5-68e1-4e86-a7bb-c5726f5786ed,2010-08-19 11:21:55.0,5.0,The application of Bayesian statistics with Monte Carlo methods. ,,
4376,16,1893,f79fb3c5-68e1-4e86-a7bb-c5726f5786ed,2010-08-19 11:21:55.0,-1.0,,,
4377,2,1894,b025204b-db4f-4c61-b647-80def0864bfa,2010-08-19 11:22:51.0,334.0,"[Ensemble methods](http://en.wikipedia.org/wiki/Ensemble_learning) like boosting, bagging, ... etc are another potential candidate.",,
4378,16,1894,b025204b-db4f-4c61-b647-80def0864bfa,2010-08-19 11:22:51.0,-1.0,,,
4379,2,1895,3682b2a5-e911-42ae-9b03-85d76d2fc0ee,2010-08-19 11:33:01.0,1001.0,"I have a software benchmark which is quite noisy. I am trying to for the bugs which are causing the noise, and I need to be able to measure it somehow.\\n\\nThe benchmark is comprised of a number of subbenchmarks, for example:\\n\\n    ""3d-cube"": 31.56884765625,\\n    ""3d-morph"": 21.89599609375,\\n    ""3d-raytrace"": 51.802978515625,\\n    ""access-binary-trees"": 15.09521484375,\\n    ""access-fannkuch"": 45.578857421875,\\n    ""access-nbody"": 8.651123046875,\\n\\nThe times are in milliseconds. The times typically vary between runs. For example, on my machine, the ""3d-cube"" benchmark tends to take around 35ms, but I've seen it go as high as 44ms, and 31ms (above) is uncharacteristically low.\\n\\nMy aim is to change the benchmark so that minor improvements to the run-time can be visible in a benchmark result. What I need is a number that tells me whether I have reduced the ""variability"" of the benchmark.\\n\\n\\n### My own solution\\n\\nI run it the benchmark 1000 times, the took the sum of the differences between each subbenchmark's mean and its actual run-times. In pseudo-code:\\n\\n    v = 0\\n    for s in subbenchmarks:\\n      x = mean of all iterations of s\\n      for i in iteration\\n        v += absolute_value(results[s][i] - x)\\n\\nI'm sure this isn't statistically valid (having asked someone), but what is a ""correct"" way of measuring this ""variability"" so that I can reduce it.",,
4380,1,1895,3682b2a5-e911-42ae-9b03-85d76d2fc0ee,2010-08-19 11:33:01.0,1001.0,"Determining the ""variability"" of a benchmark",,
4381,3,1895,3682b2a5-e911-42ae-9b03-85d76d2fc0ee,2010-08-19 11:33:01.0,1001.0,<variance>,,
4382,2,1896,ad527e50-e301-4737-96cd-5cd8e2ad2647,2010-08-19 11:58:09.0,339.0,"I guess that your method is the one described [here][1], and it's apparently valid. You could also have used the [standard deviation][2] as a measure of variability (which according to the article, it's not as [robust][3] as your absolute deviation)\\n\\nCheck out [this][4], for other measures of statistical dispersion.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Absolute_deviation\\n  [2]: http://en.wikipedia.org/wiki/Standard_deviation\\n  [3]: http://en.wikipedia.org/wiki/Robust_statistics\\n  [4]: http://en.wikipedia.org/wiki/Statistical_dispersion",,
4383,6,1895,9f7eed73-097a-4802-a2de-7e98a76cf1b2,2010-08-19 11:59:53.0,8.0,<variance><software>,edited tags,
4384,2,1897,c4ea9880-0be7-4d4c-b461-00134b2e6810,2010-08-19 12:29:30.0,8.0,"As gd047 mentioned, the standard way of measuring variability is to use the [variance][1]. So your pseudo-code will be:\\n\\n    v_new = vector of length subbenchmarks\\n    for s in subbenchmarks:\\n      v_new[i] = variance(s)\\n\\nNow the problem is, even if  you don't change your code, `v_new` will be different for each run - there is noise. To determine if a change is significant, we need to perform a [hypothesis test][2], i.e. can the change be explained as random variation or is likely that something has changed. A quick and dirty rule would be:\\n\\n\\begin{equation}\\nY_i = sqrt{n/2} ({v_new}_i/{v_old}_i -1) \\sim N(0,1)\\n\\end{equation}\\n\\nThis means any values of $Y_i < -1.96$ (at a 5% significance level) can be considered significant, i.e. an improvement. However, I would probably increase this to -3 or -4. This would test for improvement in **individual** benchmarks. \\n\\nIf you want to combine all your benchmarks into a single test, then let\\n\\n\\begin{equation}\\n\\bar Y = \\frac{1}{n} \\sum Y_i\\n\\end{equation}\\n\\nSo \\n\\n\\begin{equation}\\n\\sqrt \\bar Y \\sim N(0, 1)\\n\\end{equation}\\n\\nHence, an appropriate test would be to consider values of $\\bar Y < 1.96$ to indicate an improvement. \\n\\n____\\n\\nFurther details of the mathematical reasoning are found at Section 3.2 of this [document][3].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance\\n  [2]: http://en.wikipedia.org/wiki/Statistical_hypothesis_testing\\n  [3]: http://dsmts.googlecode.com/files/dsmts-userguide31.pdf",,
4385,5,1897,44cf6b7f-8375-4d22-8b60-c76e83db660c,2010-08-19 12:36:24.0,8.0,"As gd047 mentioned, the standard way of measuring variability is to use the [variance][1]. So your pseudo-code will be:\\n\\n    vnew = vector of length subbenchmarks\\n    for s in subbenchmarks:\\n      vnew[i] = variance(s)\\n\\nNow the problem is, even if  you don't change your code, `vnew` will be different for each run - there is noise. To determine if a change is significant, we need to perform a [hypothesis test][2], i.e. can the change be explained as random variation or is likely that something has changed. A quick and dirty rule would be:\\n\\n\\begin{equation}\\nY_i = \\sqrt{n/2} (\\frac{vnew_i}{vold_i} -1) \\sim N(0,1)\\n\\end{equation}\\n\\nThis means any values of $Y_i < -1.96$ (at a 5% significance level) can be considered significant, i.e. an improvement. However, I would probably increase this to -3 or -4. This would test for improvement in **individual** benchmarks. \\n\\nIf you want to combine all your benchmarks into a single test, then let\\n\\n\\begin{equation}\\n\\bar Y = \\frac{1}{n} \\sum Y_i\\n\\end{equation}\\n\\nSo \\n\\n\\begin{equation}\\n\\sqrt{n} \\bar Y \\sim N(0, 1)\\n\\end{equation}\\n\\nHence, an appropriate test would be to consider values of $\\bar Y < 1.96$ to indicate an improvement. \\n\\n\\n____\\n\\n1. Further details of the mathematical reasoning are found at Section 3.2 of this [document][3].\\n1. I've made a approximation by assuming that v_old represents the true underlying variance.\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance\\n  [2]: http://en.wikipedia.org/wiki/Statistical_hypothesis_testing\\n  [3]: http://dsmts.googlecode.com/files/dsmts-userguide31.pdf",Fixed latex,
4386,2,1898,da8a7f67-0bdf-4738-bbeb-89a17739e68d,2010-08-19 12:45:55.0,253.0,"There was a great discussion on metaoptimize called ""[Most Influential Ideas 1995 - 2005][1]"" \\nWhich holds a great collection of ideas.\\n\\nThe one I mentioned there, and will repeat here, is the ""revolution"" in the concept of multiple comparisons, specifically the shift from using FWE to FDR methods, for testing very many hypothesis (like in micro array or fMRI and so on)\\n\\nHere is one of the first articles that introduced this notion to the scientific community: [Benjamini, Yoav; Hochberg, Yosef (1995). ""Controlling the false discovery rate: a practical and powerful approach to multiple testing"". Journal of the Royal Statistical Society][2]\\n \\n\\n\\n  [1]: http://metaoptimize.com/qa/questions/867/most-influential-ideas-1995-2005\\n  [2]: http://www.math.tau.ac.il/~ybenja/MyPapers/benjamini_hochberg1995.pdf",,
4387,16,1898,da8a7f67-0bdf-4738-bbeb-89a17739e68d,2010-08-19 12:45:55.0,-1.0,,,
4388,2,1899,69465e9c-1115-456a-8c78-8839cad3cf9d,2010-08-19 12:50:42.0,521.0,Cox proportional hazards survival analysis:\\nhttp://en.wikipedia.org/wiki/Cox_proportional_hazards_model,,
4389,16,1899,69465e9c-1115-456a-8c78-8839cad3cf9d,2010-08-19 12:50:42.0,-1.0,,,
4390,5,1897,042fd6b3-c300-40ae-b774-c5ff885a54ba,2010-08-19 12:53:59.0,8.0,"As gd047 mentioned, the standard way of measuring variability is to use the [variance][1]. So your pseudo-code will be:\\n\\n    vnew = vector of length subbenchmarks\\n    for s in subbenchmarks:\\n      vnew[i] = variance(s)\\n\\nNow the problem is, even if  you don't change your code, `vnew` will be different for each run - there is noise. To determine if a change is significant, we need to perform a [hypothesis test][2], i.e. can the change be explained as random variation or is likely that something has changed. A quick and dirty rule would be:\\n\\n\\begin{equation}\\nY_i = \\sqrt{n/2} (\\frac{vnew_i}{vold_i} -1) \\sim N(0,1)\\n\\end{equation}\\n\\nThis means any values of $Y_i < -1.96$ (at a 5% significance level) can be considered significant, i.e. an improvement. However, I would probably increase this to -3 or -4. This would test for improvement in **individual** benchmarks. \\n\\nIf you want to combine all your benchmarks into a single test, then let\\n\\n\\begin{equation}\\n\\bar Y = \\frac{1}{n} \\sum Y_i\\n\\end{equation}\\n\\nSo \\n\\n\\begin{equation}\\n\\sqrt{n} \\bar Y \\sim N(0, 1)\\n\\end{equation}\\n\\nHence, an appropriate test would be to consider values of $\\bar Y < 1.96$ to indicate an improvement. \\n\\n_____\\n\\n**Edit**\\n\\nIf the benchmarks aren't Normal, then I would try working with log(benchmarks). It also depends on what you want to do. I read your question as ""You would like a good rule of thumb"". In this case, taking logs is probably OK.\\n\\n\\n\\n\\n\\n____\\n\\n1. Further details of the mathematical reasoning are found at Section 3.2 of this [document][3].\\n1. I've made a approximation by assuming that v_old represents the true underlying variance.\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Variance\\n  [2]: http://en.wikipedia.org/wiki/Statistical_hypothesis_testing\\n  [3]: http://dsmts.googlecode.com/files/dsmts-userguide31.pdf",added 255 characters in body,
4391,5,1861,afae5f95-49ae-47d6-8836-42559aa863d9,2010-08-19 12:59:30.0,279.0,"You are right, version 1 is not acceptable. The second or third options (as long as `old` has a coefficient that will be estimated) are both OK, and in fact equivalent with respect to estimates for `a` and `b`. This can be seen if you replace `change` with `new-old` in the second equation, and solve it for `new`. All that happens is that the coefficient of `old` is increased by 1 as compared to the third equation. Other statistics such as R^2 will change, of course, as they are decomposing a different variability.\\n\\nNote, however, that you have a different problem as well. If your scores are restricted to a -2 to 2 range, somebody with `old=-2` cannot possibly get worse, and similarly for `old=2` you can't get any better. Such a range restriction is usually not modeled well by a linear regression.",fixed typo,
4392,2,1900,ddf43eef-b76d-486a-bd8e-1725fc03ee5c,2010-08-19 13:01:46.0,521.0,John Tukey's truly strange idea: exploratory data analysis.\\nhttp://en.wikipedia.org/wiki/Exploratory_data_analysis,,
4393,16,1900,ddf43eef-b76d-486a-bd8e-1725fc03ee5c,2010-08-19 13:01:46.0,-1.0,,,
4394,2,1901,a6fa104f-d853-4c71-9bae-3fd6c4ba8195,2010-08-19 13:07:22.0,919.0,"A standard method to analyze this problem in two or more dimensions is **Ripley's (cross) K function**, but there's no reason not to use it in one dimension, too.  (A Google search does a good job of digging up references.)  Essentially, it plots the CDF of all distances between points in the two realizations rather than a histogram approximation to the PDF of those distances.  (A variant, the L function, plots the difference between K and the null distribution for two uniform uncorrelated processes.)  This neatly sidesteps most of the issues you are confronting with the need to choose bins, to smooth, etc.  Confidence bands for K are typically created through simulation.  This is easy to do in R.  Many spatial stats packages for R can be used directly or readily adapted to this 1D case.  Roger Bivand's [overview page][1] on CRAN lists these packages: refer to the section on ""Point Pattern Analysis.""\\n\\n\\n  [1]: http://cran.r-project.org/web/views/Spatial.html",,
4395,2,1902,ddc1d16e-88e8-4be2-8d22-5d1fc108a38c,2010-08-19 13:09:50.0,521.0,The Box-Jenkins approach to time-series modelling: ARIMA models etc.\\n\\nhttp://en.wikipedia.org/wiki/Box-Jenkins\\n,,
4396,16,1902,ddc1d16e-88e8-4be2-8d22-5d1fc108a38c,2010-08-19 13:09:50.0,-1.0,,,
4397,2,1903,77e02a1c-de83-4092-9220-e10415c1128e,2010-08-19 13:21:56.0,919.0,"In effect you are thinking of a model in which the *true* chance of rain, *p*, is a function of the *predicted* chance *q*: *p* = *p(q*).  Each time a prediction is made, you observe one realization of a Bernoulli variate having probability *p(q)* of success.  This is a classic logistic regression setup if you are willing to model the true chance as a linear combination of basis functions *f1*, *f2*, ..., *fk*; that is, the model says\\n\\n>Logit(*p*) = *b0* + *b1 f1(q)* + *b2 f2(q)* + ... + *bk fk(q)* + *e*\\n\\nwith iid errors *e*.  If you're agnostic about the form of the relationship (although if the weatherman is any good *p(q) - q* should be reasonably small), consider using a set of splines for the basis.  The output, as usual, consists of estimates of the coefficients and an estimate of the variance of *e*.  Given any future prediction *q*, just plug the value into the model with the estimated coefficients to obtain an answer to your question (and use the variance of *e* to construct a prediction interval around that answer if you like).\\n\\nThis framework is flexible enough to include other factors, such as the possibility of changes in the quality of predictions over time.  It also lets you test hypotheses, such as whether *p* = *q* (which is what the weatherman implicitly claims).",,
4398,2,1904,67f1d723-4f5d-42d7-8b6a-1b3a213febe6,2010-08-19 13:25:53.0,5.0,What are the most significant annual Statistics conferences?\\n\\nRules:\\n\\n1. One conference per answer\\n2. Include a link to the conference ,,
4399,1,1904,67f1d723-4f5d-42d7-8b6a-1b3a213febe6,2010-08-19 13:25:53.0,5.0,Statistics conferences?,,
4400,3,1904,67f1d723-4f5d-42d7-8b6a-1b3a213febe6,2010-08-19 13:25:53.0,5.0,<conferences>,,
4401,16,1904,67f1d723-4f5d-42d7-8b6a-1b3a213febe6,2010-08-19 13:25:53.0,5.0,,,
4402,2,1905,a3e98825-8305-4847-8e0a-302693db679f,2010-08-19 13:30:01.0,334.0,"Shameless plug: [R/Finance](http://www.RinFinance.com) which relevant for its intersection of domain-specifics as well as tools, and so far well received by participants of the 2009 and 2010 conference. .\\n\\nDisclaimer: I am one of the organizers.",,
4403,16,1905,a3e98825-8305-4847-8e0a-302693db679f,2010-08-19 13:30:01.0,-1.0,,,
4404,2,1906,8cfff56c-ccdc-40d3-aede-45902dba59da,2010-08-19 13:37:35.0,5.0,What are the most significant annual Data Mining conferences?\\n\\nRules:\\n\\n1. One conference per answer\\n2. Include a link to the conference ,,
4405,1,1906,8cfff56c-ccdc-40d3-aede-45902dba59da,2010-08-19 13:37:35.0,5.0,Data mining conferences?,,
4406,3,1906,8cfff56c-ccdc-40d3-aede-45902dba59da,2010-08-19 13:37:35.0,5.0,<conferences>,,
4407,2,1907,aa54cef2-8af1-4a76-9a6f-b2ec13263945,2010-08-19 13:40:35.0,5.0,[KDD][1] (ACM Special Interest Group on Knowledge Discovery and Data Mining) \\n\\n- [KDD 2010][2]\\n\\n\\n  [1]: http://www.sigkdd.org/conferences.php\\n  [2]: http://www.sigkdd.org/kdd2010/,,
4408,16,1906,da7684f4-0dc4-4b77-96d0-b1ce5a619306,2010-08-19 13:44:40.0,5.0,,,
4409,16,1907,aa54cef2-8af1-4a76-9a6f-b2ec13263945,2010-08-19 13:40:35.0,5.0,,,
4410,2,1908,b25db9a3-3b18-461e-9aa9-7b7dc6958d8d,2010-08-19 13:45:36.0,5.0,What are the most significant annual Machine Learning conferences?\\n\\nRules:\\n\\n1. One conference per answer\\n2. Include a link to the conference ,,
4411,1,1908,b25db9a3-3b18-461e-9aa9-7b7dc6958d8d,2010-08-19 13:45:36.0,5.0,Machine Learning conferences?,,
4412,3,1908,b25db9a3-3b18-461e-9aa9-7b7dc6958d8d,2010-08-19 13:45:36.0,5.0,<conferences>,,
4413,16,1908,b25db9a3-3b18-461e-9aa9-7b7dc6958d8d,2010-08-19 13:45:36.0,5.0,,,
4414,2,1909,0c243ab7-43d1-4a56-af2a-5471a2d3d046,2010-08-19 13:46:41.0,183.0,"UseR!\\n\\n- [2011: University of Warwick, Coventry, UK][1]\\n- [Videos of some keynote speakers from 2010][2]\\n\\n\\n  [1]: http://www.warwick.ac.uk/statsdept/useR-2011/\\n  [2]: http://www.r-bloggers.com/RUG/?p=180",,
4415,16,1909,0c243ab7-43d1-4a56-af2a-5471a2d3d046,2010-08-19 13:46:41.0,-1.0,,,
4416,2,1910,7798305b-d43d-41b8-8f97-b8be772d8edd,2010-08-19 13:47:24.0,5.0,[**ICML**][1] (International Conference on Machine Learning)\\n\\n - [ICML 2010][2]\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/ICML\\n  [2]: http://icml2010.haifa.il.ibm.com/,,
4417,16,1910,7798305b-d43d-41b8-8f97-b8be772d8edd,2010-08-19 13:47:24.0,-1.0,,,
4418,2,1911,cf24d6ee-6747-4f72-b6ac-ec8b32e3604a,2010-08-19 14:13:27.0,919.0,"In 1960 most people doing statistics were calculating with a four-function manual calculator or a slide rule or by hand; mainframe computers were just beginning to run some programs in Algol and Fortran; graphical output devices were rare and crude.  Because of these limitations, Bayesian analysis was considered formidably difficult due to the calculations required.  Databases were managed on punch cards and computer tape drives limited to a few megabytes.  Statistical education focused initially on learning formulas for t-testing and ANOVA.  Statistical practice usually did not go beyond such routine hypothesis testing (although some brilliant minds had just begun to exploit computers for deeper analysis, as exemplified by Mosteller & Wallace's book on the Federalist papers, for instance).\\n\\nI recounted this well-known history as a reminder that *all* of statistics has undergone a revolution due to the rise and spread of computing power during this last half century, a revolution that has made possible almost every other innovation in statistics during that time (with the notable exception of Tukey's pencil-and-paper EDA methods, as Thylacoleo has already observed).",,
4419,16,1911,cf24d6ee-6747-4f72-b6ac-ec8b32e3604a,2010-08-19 14:13:27.0,-1.0,,,
4420,2,1912,87db068c-e7e2-406d-9fc6-da7f893494e6,2010-08-19 14:15:28.0,5.0,"[Gary King][1] made the [following statement on Twitter][2]:\\n\\n> scale invariance sounds cool but is\\n> usually statisticians shirking\\n> responsibility & losing power by\\n> neglecting subject matter info\\n\\nWhat is an example of this phenomena, where [scale invariance][3] causes a loss of power?\\n\\n  [1]: http://gking.harvard.edu/\\n  [2]: http://twitter.com/kinggary/status/21513150698\\n  [3]: http://www.statistics.com/resources/glossary/s/scaleinv.php",,
4421,1,1912,87db068c-e7e2-406d-9fc6-da7f893494e6,2010-08-19 14:15:28.0,5.0,Why would scale invariance cause a loss of power?,,
4422,3,1912,87db068c-e7e2-406d-9fc6-da7f893494e6,2010-08-19 14:15:28.0,5.0,<scale-invariance>,,
4423,2,1913,fbe173e9-ac27-43d3-81f2-0cf4e456b25e,2010-08-19 14:38:51.0,,"In terms of overall breadth, I would say that the ASA/IMS Joint Statistical Meetings are the most significant.  Next year, the statisticians are taking their talents to South Beach...or [Miami Beach][1] is more correct.  I just couldn't help to use that line from Lebron James' infamous press conference.  Having said that, I prefer smaller conferences like the UseR! conferences, ICORS (robust statistics), etc.\\n\\n\\n  [1]: http://www.amstat.org/meetings/jsm/2011/index.cfm",,rtelmore
4424,16,1913,fbe173e9-ac27-43d3-81f2-0cf4e456b25e,2010-08-19 14:38:51.0,-1.0,,,
4425,10,1883,a6aeca13-3ab5-4a00-90c8-4913cf5027ac,2010-08-19 15:10:48.0,-1.0,"{""Voters"":[{""Id"":223,""DisplayName"":""robin girard""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":88,""DisplayName"":""mbq""}]}",3,
4426,5,1883,2dbd189e-e990-42ec-801f-b9c6103a6427,2010-08-19 15:12:06.0,88.0,"Let's say [chat][1] is a better place for such discussions. --mbq\\n\\nWhat areas of statistics have been substantially revolutionised in the last 50 years? For example, about 40 years ago, Akaike with colleagues revolutionised the area of statistical model discrimination. About 10 years ago, Hyndman with colleagues revolutionised the area of exponential smoothing. About XX years ago, ... How do I possibly continue the list, with years and names please? By statistics I mean its all four types from Bartholomew's 1995 presidential address, Chambers's greater and lesser statistics together, as featuring in Hand's recent presidential address on 'Modern statistics' and so on - anything professionally relevant.\\n\\n\\n  [1]: http://chat.meta.stackoverflow.com/rooms/170/stats",added 126 characters in body; added 6 characters in body,
4427,2,1914,829d219b-46bc-4a38-b21c-279526a612c3,2010-08-19 15:13:49.0,1004.0,"I doing spatial bayesian data analysis, I am assuming a no-nugget exponential covariance. I have tried a variety of priors for the sills and range parameters (gamma, inverse gamma etc.) , unfortunately the convergence diagonstics are typically horrible. \\n\\nI am wondering how to figure out the poor mixing I observe, is there something I can do to make the MCMC chain behave better?",,
4428,1,1914,829d219b-46bc-4a38-b21c-279526a612c3,2010-08-19 15:13:49.0,1004.0,choice for priors for exponential spatial covariance,,
4429,3,1914,829d219b-46bc-4a38-b21c-279526a612c3,2010-08-19 15:13:49.0,1004.0,<bayesian><spatial>,,
4430,6,1914,54064445-8145-4645-a8cd-a12f29c479ef,2010-08-19 15:29:24.0,8.0,<bayesian><mcmc><spatial>,edited tags,
4431,2,1915,70e5f36b-5331-4d7d-8162-581873209ec1,2010-08-19 16:09:23.0,1007.0,"I am interested in running Newman's [modularity clustering][1] algorithm on a large graph. If you can point me to a library (or R package, etc) that implements it I would be most grateful.\\n\\nbest ~lara\\n\\n\\n  [1]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1482622/",,
4432,1,1915,70e5f36b-5331-4d7d-8162-581873209ec1,2010-08-19 16:09:23.0,1007.0,Newman's modularity clustering for graphs,,
4433,3,1915,70e5f36b-5331-4d7d-8162-581873209ec1,2010-08-19 16:09:23.0,1007.0,<clustering><graphical-techniques><networks>,,
4434,6,1915,f8d73090-911e-4dc0-b74e-a346c934944a,2010-08-19 16:17:44.0,8.0,<clustering><networks><data-visualization><partitioning>,edited tags,
4435,2,1916,a6981d53-ffbc-422e-9fcd-7dfe473cedf9,2010-08-19 16:31:49.0,919.0,"Diggle and Ribeiro discuss this in their book (""Model-based Geostatistics""): see section 5.4.2.  They quote some research suggesting that re-parameterization might help a little.  For an exponential model (a Matern model with kappa = 1/2) this research suggests using the equivalent of log(sill/range) and log(range).  Diggle and Ribeiro themselves recommend a profile likelihood method to investigate the log-likelihood surface.  Their software is implemented in the R package geoRglm.\\n\\nHave you looked at an experimental variogram to check that a zero nugget and an exponential shape are appropriate?",,
4436,2,1917,d47f7c3a-970f-4e47-bc0a-3b38e3df5f93,2010-08-19 16:59:14.0,251.0,The [igraph][1] library implements some algorithms for community structure based on Newman's optimization of modularity.  You can consult the [reference manual][2] for details and citations.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/igraph/index.html\\n  [2]: http://cran.r-project.org/web/packages/igraph/igraph.pdf\\n\\n,,
4437,5,1852,0d36a6ed-c125-4666-979f-442cf4f2f7c7,2010-08-19 17:09:15.0,795.0,"Suppose there are 999 workers at ACME north factory each making a wage of 112, and 1 CEO making 88112. The population mean salary is $\\mu = 0.999 * 112 + 0.001 * 88112 = 200.$ The probability of drawing the CEO from a sample of 49 people at the factory is $49 / 1000 < 0.05$ (this is from the hypergeometric distribution), thus with 95% confidence, your <strike>population</strike> sample mean will be 112. In fact, by adjusting the ratio of workers/CEOs, and the salary of the CEO, we can make it arbitrarily unlikely that a sample of 49 employees will draw a CEO, while fixing the population mean at 200, and the sample mean at 112. Thus, without making _some_ assumptions about the underlying distribution, you cannot draw any inference about the population mean.",correcting brain-o sample vs. population.,
4438,2,1918,7374bf7d-5fb0-48b3-8089-59ca0d71cfe4,2010-08-19 17:13:01.0,251.0,"I'm not sure you need simulation for a simple regression model.  For example, see the paper [Portable Power][1].  For more complex models, specifically mixed effects, the [pamm][2] package in R performs power analyses through simulations.  Also see Todd Jobe's [post][3] which has R code for simulation.\\n\\n\\n\\n  [1]: http://www.bobwheeler.com/statistics/Papers/PortablePower.PDF\\n  [2]: http://cran.r-project.org/web/packages/pamm/index.html\\n  [3]: http://www.unc.edu/~toddjobe/blog/2009/09/power-analysis-for-mixed-effect-models.html\\n\\n",,
4439,2,1919,9888f33d-3365-4aed-a015-dcfef4bd5826,2010-08-19 17:14:39.0,11.0,"Not a ""statistics"" conference in the technical sense, but Predictive Analytics World is a case study conference on how companies are using predictive and other analytics in theis businesses.\\n\\n[Predictive Analytics World][1]\\n\\n\\n  [1]: http://www.predictiveanalyticsworld.com/",,
4440,16,1919,9888f33d-3365-4aed-a015-dcfef4bd5826,2010-08-19 17:14:39.0,-1.0,,,
4441,2,1920,860af2b3-4763-40d1-af85-ecca9cc4c1c7,2010-08-19 17:19:52.0,11.0,[ACM SIGKDD 2010][1]\\n\\n[KDD 2011 in San Diego][2]\\n\\n\\n  [1]: http://www.kdd.org/kdd2010/index.shtml\\n  [2]: http://kdd.org/kdd/2011/,,
4442,16,1920,860af2b3-4763-40d1-af85-ecca9cc4c1c7,2010-08-19 17:19:52.0,-1.0,,,
4443,2,1921,01ed4762-bc2d-4c1d-a6f9-7de433e963f8,2010-08-19 17:24:19.0,364.0,"I just want to emphasize the importance of *not* analyzing accuracies on the proportion scale. While lamentably pervasive across a number of disciplines, this practice can yield frankly incorrect conclusions. See: http://dx.doi.org/10.1016/j.jml.2007.11.004\\n\\nAs John Christie notes, the best way to approach analysis of accuracy data is a mixed effects model using the binomial link and participants as a random effect, eg:\\n\\n	#R code\\n	library(lme4)\\n	fit = lmer(\\n		formula = acc ~ my_IV + (1|participant)\\n		, family = 'binomial'\\n		, data = my_data\\n	)\\n	print(fit)\\n",,
4444,2,1922,b7b3abf8-0d41-437f-b5bd-75e44ffa9206,2010-08-19 17:28:18.0,251.0,"There's a nice paper on visualization techniques you might use by Michael Friendly:\\n\\n- [Visualizing Categorical Data: Data, Stories, and Pictures][1]\\n\\n(Actually, there's a whole [book][2] devoted to this by the same author.)  The [vcd][3] package in R implements many of these techniques.\\n\\n\\n \\n \\n\\n\\n  [1]: http://www.math.yorku.ca/SCS/vcd/vcdstory.pdf\\n  [2]: http://books.google.com/books?id=eG0phz62f1cC&lpg=PP1&dq=michael%20friendly%20visualizing%20categorical%20data&pg=PP1#v=onepage&q&f=false\\n  [3]: http://cran.r-project.org/web/packages/vcd/index.html",,
4445,5,1921,9d61d63e-39f0-4024-a11b-41288754d98a,2010-08-19 17:43:34.0,364.0,"I just want to emphasize the importance of *not* analyzing accuracies on the proportion scale. While lamentably pervasive across a number of disciplines, this practice can yield frankly incorrect conclusions. See: http://dx.doi.org/10.1016/j.jml.2007.11.004\\n\\nAs John Christie notes, the best way to approach analysis of accuracy data is a mixed effects model using the binomial link and participants as a random effect, eg:\\n\\n	#R code\\n	library(lme4)\\n	fit = lmer(\\n		formula = acc ~ my_IV + (1|participant)\\n		, family = 'binomial'\\n		, data = my_data\\n	)\\n	print(fit)\\n\\nNote that ""my_data"" should be the raw, trial-by-trial data such that ""acc"" is either 1 for accurate trials or 0 for inaccurate trials. That is, data should *not* be aggregated to proportions before analysis.",added last sentence clarifying that data should not be aggregated to proportions before analysis,
4446,2,1923,d2cde4b9-596c-47ed-a681-fb1c6a69e06f,2010-08-19 17:56:58.0,511.0,"Suppose instead of maximizing likelihood I maximize some other function g. Like likelihood, this function decomposes over x's (ie, g({x1,x2})=g({x1})g({x2}), and ""maximum-g"" estimator is consistent. How do I compute asymptotic variance of this estimator?",,
4447,1,1923,d2cde4b9-596c-47ed-a681-fb1c6a69e06f,2010-08-19 17:56:58.0,511.0,How to compute efficiency?,,
4448,3,1923,d2cde4b9-596c-47ed-a681-fb1c6a69e06f,2010-08-19 17:56:58.0,511.0,<estimation><efficiency>,,
4449,2,1924,dd946743-df52-4c82-87fa-d2b4a8eefd1a,2010-08-19 18:29:24.0,,The consistency and asymptotic normality of the maximum likelihood estimator is demonstrated using some regularity conditions on the likelihood function. The wiki link on [consistency][1] and [asymptotic normality][2] has the conditions necessary to prove these properties. The conditions at the wiki may be stronger than what you need as they are used to prove asymptotic normality whereas you simply want to compute the variance of the estimator.\\n\\nI am guessing that if your function satisfies the same conditions then the proof will carry over to your function as well. If not then we need to know one or both of the following: (a) the specific condition that $g(.)$ does not satisfy from the list at the wiki and (b) the specifics of $g(.)$ to give a better answer to your question.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Maximum_likelihood#Consistency\\n  [2]: http://en.wikipedia.org/wiki/Maximum_likelihood#Asymptotic_normality,,user28
4450,2,1925,50e9e75f-1156-4725-b51d-973565129463,2010-08-19 18:30:39.0,419.0,Proportional odds ratio model is better then t-test for Likert item scale.,,
4451,2,1926,8d491d99-e31e-40ad-b023-01bd8f0c2f3b,2010-08-19 18:43:16.0,279.0,For biostatistics the largest US conferences are the meetings of the local sections of the International Biometrics Society (IBS):  \\n - ENAR for the Eastern region: http://www.enar.org/meetings.cfm  \\n - WNAR for the Western region: http://www.wnar.org/ \\n\\nOf these ENAR is by far larger.,,
4452,16,1926,8d491d99-e31e-40ad-b023-01bd8f0c2f3b,2010-08-19 18:43:16.0,-1.0,,,
4453,6,1923,0184a81c-f046-4037-bb3f-3d0f2a7ba027,2010-08-19 18:53:07.0,,<estimation><efficiency><asymptotics>,added tag,user28
4454,2,1927,a74bfbfe-2ecf-4bd2-8c46-d4cbbd936059,2010-08-19 19:36:01.0,1011.0,"If so, what?\\nIf not, why not?\\n\\nFor a sample on the line, the median minimizes the total absolute deviation. It would seem natural to extend the definition to R2, etc., but I've never seen it. But then, I've been out in left field for a long time.",,
4455,1,1927,a74bfbfe-2ecf-4bd2-8c46-d4cbbd936059,2010-08-19 19:36:01.0,1011.0,"Is there an accepted definition for the median of a sample on the plane, or higher ordered spaces?",,
4456,3,1927,a74bfbfe-2ecf-4bd2-8c46-d4cbbd936059,2010-08-19 19:36:01.0,1011.0,<median>,,
4457,2,1928,32a9dd03-9afc-4835-ba08-1f363fca0760,2010-08-19 19:48:16.0,251.0,"I'm not sure there is one accepted definition for a multivariate median.  The one I'm familiar with is [Oja's median point][1], which minimizes the sum of volumes of simplices formed over subsets of points.  (See the link for a technical definition.)\\n\\n\\n  [1]: http://cgm.cs.mcgill.ca/~athens/Geometric-Estimators/oja.html",,
4458,2,1929,1be5052c-82ab-41a3-82fe-827cc07f574c,2010-08-19 19:53:51.0,,"I do not know if any such definition exists but I will try and extend the [standard definition of the median][1] to $R^2$. I will use the following notation:\\n\\n$X$, $Y$: the random variables associated with the two dimensions.\\n\\n$m_x$, $m_y$: the corresponding medians.\\n\\n$f(x,y)$: the joint pdf for our random variables\\n\\nTo extend the definition of the median to $R^2$, we choose $m_x$ and $m_y$ to minimize the following:\\n\\n$E(|(x,y) - (m_x,m_y)|$\\n\\nThe problem now is that we need a definition for what we mean by:\\n\\n$|(x,y) - (m_x,m_y)|$\\n\\nThe above is in a sense a distance metric and several possible candidate definitions are possible. \\n\\n**[Eucliedan Metric][2]**\\n\\n$|(x,y) - (m_x,m_y)| = \\sqrt{(x-m_x)^2 + (y-m_y)^2}$ \\n\\nComputing the median under the euclidean metric will require computing the expectation of the above with respect to the joint density $f(x,y)$.\\n\\n**[Taxicab Metric][3]**\\n\\n$|(x,y) - (m_x,m_y)| = |x-m_x| + |y-m_y|$ \\n\\nComputing the median in the case of the taxicab metric involves computing the median of $X$ and $Y$ separately as the metric is separable in $x$ and $y$. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Median#An_optimality_property\\n  [2]: http://en.wikipedia.org/wiki/Euclidean_metric\\n  [3]: http://en.wikipedia.org/wiki/Taxicab_geometry",,user28
4459,5,1928,91fb70ba-a8e0-4e1b-825c-e4e45c3464f8,2010-08-19 19:58:31.0,251.0,"I'm not sure there is one accepted definition for a multivariate median.  The one I'm familiar with is [Oja's median point][1], which minimizes the sum of volumes of simplices formed over subsets of points.  (See the link for a technical definition.)\\n\\nUpdate: The site referenced for the Oja definition above also has a nice paper covering a number of definitions of a multivariate median:\\n\\n- [Geometric Measures of Data Depth][2]\\n\\n\\n  [1]: http://cgm.cs.mcgill.ca/~athens/Geometric-Estimators/oja.html\\n  [2]: http://cgm.cs.mcgill.ca/~athens/Papers/depth.pdf\\n",added 243 characters in body,
4460,2,1930,2794dd28-7c76-4457-bf04-5667402fab96,2010-08-19 20:52:10.0,1013.0,"Both Cohen's d and Hedges' g pool variances on the assumption of equal population variances, but g pools using n - 1 for each sample instead of n, which provides a better estimate, especially the smaller the sample sizes.  Both d and g are somewhat positively biased, but only negligibly for moderate or larger sample sizes.  The bias is reduced using g*. The d by Glass does not assume equal variances, so it uses the sd of a control group or baseline comparison group as the standardizer for the difference between the two means.\\n\\nThese effect sizes and Cliff's and other nonparametric effect sizes are discussed in detail in my book:\\n\\nGrissom, R. J., & Kim, J, J. (2005). Effect sizes for research: A broad practical approach. Mahwah, NJ: Erlbaum.",,
4461,2,1931,c170c011-f27c-4e35-9101-0b89cb0655d8,2010-08-19 20:58:59.0,919.0,"There are distinct ways to generalize the concept of median to higher dimensions.  One not yet mentioned, but which was proposed long ago, is to construct a convex hull, peel it away, and iterate for as long as you can: what's left in the last hull is a set of points that are all candidates to be ""medians.""\\n\\n[""Head-banging""][1] is another more recent attempt (c. 1980) to construct a robust center to a 2D point cloud.\\n\\nThe principal reason why there are multiple distinct generalizations and no one obvious solution is that R1 can be ordered but R2, R3, ... cannot be.\\n\\n\\n  [1]: http://srab.cancer.gov/headbang/",,
4462,2,1932,6b9460eb-fc6d-4bcd-abef-ebb68387a116,2010-08-19 21:44:04.0,881.0,"[NIPS (Neural Information Processing Systems)][1]. It's actually an intersection of machine learning, and application areas such as speech/language, vision, neuro-science, and other related areas.\\n\\n\\n  [1]: http://nips.cc",,
4463,16,1932,6b9460eb-fc6d-4bcd-abef-ebb68387a116,2010-08-19 21:44:04.0,-1.0,,,
4464,2,1933,573f1ff1-797a-4a07-9469-c51d3216328b,2010-08-19 21:50:48.0,881.0,"Since you don't have access to the test data at the time of training, and you want your model to do well on the future test data, you ""pretend"" that you have access to some test data by repeatedly subsampling a small part of your training data, hiding it while training the model, and then treating it as a proxy to the test data. You hope that by randomly sampling various subsets from the training data, you might make them look like the test data (in the average behavior sense).",,
4465,2,1934,dd7175be-c504-47e1-82d7-d28494c05c67,2010-08-19 22:08:44.0,919.0,"Thomas Ryan (""Statistical Methods for Quality Improvement"", Wiley, 1989) describes several procedures.  He tends to try to reduce all control charting to the Normal case, so his procedures are not as creative as they could be, but he claims they work pretty well.  One is to treat the values as Binomial data and use the ArcSin transformation, then run standard CUSUM charts.  Another is to view the values as Poisson data and use the square root transformation, then again run a CUSUM chart.  For these approaches, which are intended for process quality control, you're supposed to know the number of potentially exposed individuals during each period.  If you don't, you probably have to go with the Poisson model.  Given that the infections are rare, the square root transformation sets your upper control limit a tiny bit above (u/2)^2 where typically u = 3 (corresponding to the usual 3-SD UCL in a Normal chart), whence any count of Ceiling((3/2)^2) = 3 or greater would trigger an out-of-control condition.\\n\\nOne wonders whether control charting is the correct conceptual model for your problem, though.  You're not really running any kind of quality control process here: you probably know, on scientific grounds, when the infection rate is alarming.  You might know, as a hypothetical example, that fewer than ten infections over a week-long period is rarely a harbinger of an outbreak.  Why not set your upper limit on this kind of basis rather than employing an almost useless statistical limit?",,
4466,5,1933,f7e0bf09-d1cd-4018-b3b3-8ae6a05fd63f,2010-08-19 23:49:54.0,881.0,"Since you don't have access to the test data at the time of training, and you want your model to do well on the unseen test data, you ""pretend"" that you have access to some test data by repeatedly subsampling a small part of your training data, hold out this set while training the model, and then treating the held out set as a proxy to the test data (and choose model parameters that give best performance on the held out data). You hope that by randomly sampling various subsets from the training data, you might make them look like the test data (in the average behavior sense), and therefore the learned model parameters will be good for the test data as well (i.e., your model generalizes well for unseen data).",added 135 characters in body; added 100 characters in body,
4467,2,1935,06eb3042-9c5b-47bd-ad27-2b4053ea7dc0,2010-08-20 00:10:33.0,561.0,"Despite several attempts at reading about bootstrapping, I seem to always hit a brick wall. I wonder if anyone can give a reasonably non-technical definition of bootstrapping?\\n\\nI know it is not possible in this forum to provide enough detail to enable me to fully understand it, but a gentle push in the right direction with the main goal and mechanism of bootstrapping would be *much* appreciated! Thanks.",,
4468,1,1935,06eb3042-9c5b-47bd-ad27-2b4053ea7dc0,2010-08-20 00:10:33.0,561.0,Whither bootstrapping - can someone provide a simple explanation to get me started?,,
4469,3,1935,06eb3042-9c5b-47bd-ad27-2b4053ea7dc0,2010-08-20 00:10:33.0,561.0,<statistics><bootstrap>,,
4470,2,1936,572d4f61-fd97-4572-b7eb-bcd2247a2c62,2010-08-20 00:16:09.0,530.0,One of the only machine learning conferences for those in Australia and New Zealand is:\\n\\n - [23rd Australasian Joint Conference on Artificial Intelligence][1]\\n\\nIt's held in Adelaide this year.\\n\\n  [1]: http://ai10.cis.unisa.edu.au/,,
4471,16,1936,572d4f61-fd97-4572-b7eb-bcd2247a2c62,2010-08-20 00:16:09.0,-1.0,,,
4472,2,1937,f515fa70-5450-4277-99b0-b636333d4ddc,2010-08-20 00:20:49.0,,"The wiki on [bootstrapping][1] gives the following description:\\n\\n> Bootstrapping allows one to gather many alternative versions of the single statistic that would ordinarily be calculated from one sample. For example, assume we are interested in the height of people worldwide. As we cannot measure all the population, we sample only a small part of it. From that sample only one value of a statistic can be obtained, i.e one mean, or one standard deviation etc., and hence we don't see how much that statistic varies. When using bootstrapping, we randomly extract a new sample of n heights out of the N sampled data, where each person can be selected at most t times. By doing this several times, we create a large number of datasets that we might have seen and compute the statistic for each of these datasets. Thus we get an estimate of the distribution of the statistic. The key to the strategy is to create alternative versions of data that ""we might have seen"".\\n\\nI will provide more detail if you can clarify what part of the above description you do not understand.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Bootstrapping_(statistics)",,user28
4473,2,1938,5ce633e6-78aa-48ad-8501-81f5cbd79646,2010-08-20 00:23:03.0,159.0,"The main regular conference in Australia is the ""Australian Statistics Conference"", held every second year. The next one is [ASC 2010][1], to be held in Western Australia in December.\\n\\n\\n  [1]: http://www.promaco.com.au/2010/asc/",,
4474,16,1938,5ce633e6-78aa-48ad-8501-81f5cbd79646,2010-08-20 00:23:03.0,-1.0,,,
4475,2,1939,b83c7669-1039-4814-a2ef-931fb82d2fcc,2010-08-20 00:35:28.0,521.0,"The Wikipedia entry on Bootstrapping is actually very good:\\n\\nhttp://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29\\n\\nThe most common reason bootstrapping is applied is when the form of the underlying distribution from which a sample is taken is unknown. Traditionally statisticians assume a normal distribution (for very good reasons related to the central limit theorem), but statistics (such as the standard deviation, confidence intervals, power calculations etc) estimated via normal distribution theory are only strictly valid if the underlying population distribution is normal.\\n\\nBy repeatedly re-sampling the sample itself, bootstrapping enables estimates that are distribution independent. Traditionally each ""resample"" of the original sample randomly selects the same number of observations as in the original sample. However these are selected with replacement. If the sample has N observations, each bootstrap resample will have N observations, with many of the original sample repeated and many excluded. \\n\\nThe parameter of interest (eg. odds ratio etc) can then be estimated from each bootstrapped sample. Repeating the bootstrap say 1000 times allows an estimate of the ""median"" and 95% confidence interval on the statistic (eg odds ratio) by selecting the 2.5th, 50th and 97.5th percentile.\\n\\n\\n\\n",,
4476,2,1940,a08ccc2b-86f0-43f7-97b7-751e96ffdf43,2010-08-20 01:08:12.0,251.0,The American Scientist recently had a nice article by Cosma Shalizi on [the bootstrap][1] which is fairly easy reading and gives you the essentials to grasp the concept.  \\n\\n\\n  [1]: http://www.americanscientist.org/issues/pub/2010/3/the-bootstrap/1\\n\\n,,
4477,5,1909,5b9c810e-8a90-4283-a8c1-c5ee320e62bc,2010-08-20 01:43:17.0,183.0,"UseR!\\n\\n- [List of previous and upcoming R confrences on r-project][1]\\n\\nRelated Links:\\n\\n- [2011: University of Warwick, Coventry, UK][2]\\n- [Videos of some keynote speakers from 2010][3]\\n\\n\\n  [1]: http://www.r-project.org/conferences.html\\n  [2]: http://www.warwick.ac.uk/statsdept/useR-2011/\\n  [3]: http://www.r-bloggers.com/RUG/?p=180",added 90 characters in body; added 2 characters in body; added 42 characters in body,
4478,2,1941,906e8cd1-fa5a-4172-80e0-5c50579e2040,2010-08-20 01:47:20.0,,"I have not full internalized the issue of matrix interference but here is one approach. Let:\\n\\n$Y$ be a vector that represents the concentration of all the target compounds in the undiluted sample.\\n\\n$Z$ be the corresponding vector in the diluted sample.\\n\\n$d$ be the dilution factor i.e., the sample is diluted $d$:1.\\n\\nOur model is:\\n\\n$Y \\sim N(\\mu,\\Sigma)$\\n\\n$Z = \\frac{Y}{d} + \\epsilon$\\n\\nwhere $\\epsilon \\sim N(0,\\sigma^2\\ I)$ represents the error due to dilution errors.\\n\\nTherefore, it follows that:\\n\\n$Z \\sim N(\\frac{\\mu}{d}, \\Sigma + \\sigma^2\\ I)$\\n\\nDenote the above distribution of $Z$ by $f_Z(.)$.\\n\\nLet $O$ be the observed concentrations and $\\tau$ represent the test instrument's threshold below which it cannot detect a compound. Then, for the $i^{th}$ compound we have:\\n\\n$O_i = Z_i I(Z_i > \\tau) + 0 I(Z_i \\le \\tau)$ \\n\\nWithout loss of generality let the first $k$ compounds be such that they are below the threshold. Then the likelihood function can be written as:\\n\\n$L(O_1, ... O_k, O_{k+1},...O_n |- ) = [\\prod_{i=1}^{i=k}{Pr(Z_i \\le \\tau)}]  [\\prod_{i=k+1}^{i=n}{f(O_i |-)}]$ \\n\\nwhere\\n\\n$f(O_i |-) = \\int_{j\\neq i}{f_Z(O_i|-) I(O_i > \\tau)}$\\n\\nEstimation is then a matter of using either maximum likelihood or bayesian ideas. I am not sure how tractable the above is but I hope it gives you some ideas.\\n",,user28
4479,2,1942,01134c04-69eb-4519-b7d7-21f74ebe6c80,2010-08-20 02:25:32.0,183.0,"What are the most significant annual conferences focusing on quantitative methods in psychology?\\n\\nThis could include but is not limited to psychometrics, mathematical psychology, and statistical methods in psychology.\\n\\nRules:\\n\\n- One conference per answer\\n- Include a link to the conference",,
4480,1,1942,01134c04-69eb-4519-b7d7-21f74ebe6c80,2010-08-20 02:25:32.0,183.0,Quantitative methods and statistics conferences in psychology?,,
4481,3,1942,01134c04-69eb-4519-b7d7-21f74ebe6c80,2010-08-20 02:25:32.0,183.0,<conferences><psychometrics><psychology>,,
4482,16,1942,01134c04-69eb-4519-b7d7-21f74ebe6c80,2010-08-20 02:25:32.0,183.0,,,
4483,5,1909,f440a64f-438f-490e-8e59-bc615cedc29b,2010-08-20 03:23:15.0,183.0,"UseR!\\n\\n- [List of previous and upcoming R conferences on r-project][1]\\n\\nRelated Links:\\n\\n- [2011: University of Warwick, Coventry, UK][2]\\n- [Videos of some keynote speakers from 2010][3]\\n\\n\\n  [1]: http://www.r-project.org/conferences.html\\n  [2]: http://www.warwick.ac.uk/statsdept/useR-2011/\\n  [3]: http://www.r-bloggers.com/RUG/?p=180",added 1 characters in body,
4484,6,1561,9f5c7a86-258c-45c9-b7f2-cb6325b54aca,2010-08-20 05:54:14.0,183.0,<proportion>,edited tags,
4485,2,1943,4fc9be20-2373-475d-b387-e4a31635e630,2010-08-20 06:46:21.0,251.0,"If I'm not mistaken, the log-rank estimator you reference is also known as the Pike estimator.  I believe it's generally recommended for HR < 3 because it exhibits less bias in that range.  The following paper may be of interest (note that the paper refers to it as O/E):\\n\\n- [Estimation of the Proportional Hazard in Two-Treatment-Group Clinical Trials][1] (Bernstein, Anderson, Pike)\\n\\n> [...] The O/E method is biased but, within the range of values of the ratio of the hazard rates of interest in clinical trials, it is more efficient in terms of mean square error than either CML or the Mantel-Haenszel method for all but the largest trials. The Mantel-Haenszel method is minimally biased, gives answers very close to those obtained using CML, and may be used to provide satisfactory approximate confidence intervals.\\n\\n\\n  [1]: http://www.jstor.org/pss/2530564\\n\\n",,
4486,6,1927,71bb607b-3e8f-4bd4-98be-3cf50ceda012,2010-08-20 08:25:44.0,8.0,<multivariable><median>,edited tags,
4487,5,1916,634073d1-fd4c-4bc2-a80f-27a82e5cb505,2010-08-20 08:27:30.0,8.0,"Diggle and Ribeiro discuss this in their book (""[Model-based Geostatistics][1]""): see section 5.4.2.  They quote some research suggesting that re-parameterization might help a little.  For an exponential model (a Matern model with kappa = 1/2) this research suggests using the equivalent of log(sill/range) and log(range).  Diggle and Ribeiro themselves recommend a profile likelihood method to investigate the log-likelihood surface.  Their software is implemented in the R package [geoRglm][2].\\n\\nHave you looked at an experimental variogram to check that a zero nugget and an exponential shape are appropriate?\\n\\n\\n  [1]: http://www.amazon.com/Model-based-Geostatistics-Springer-Statistics-Diggle/dp/0387329072\\n  [2]: http://gbi.agrsci.dk/~ofch/geoRglm/",Add in links to book and R package,
4488,6,1915,406ceeb1-5289-47bd-84ef-dca1c0602428,2010-08-20 08:27:54.0,8.0,<clustering><networks><data-visualization><partitioning><igraph>,edited tags,
4489,2,1944,a5e1b4d2-8430-4e22-b978-4e4b5406b5cc,2010-08-20 08:31:16.0,582.0,"This may be a stupid question but... is there a specific name for normalizing some data so that it has mean=0 and sd=1? \\n\\nOr do I just say ""data was normalized to have mean=0 and sd=1""?\\n\\nthanks\\nnico",,
4490,1,1944,a5e1b4d2-8430-4e22-b978-4e4b5406b5cc,2010-08-20 08:31:16.0,582.0,What is the name of this normalization?,,
4491,3,1944,a5e1b4d2-8430-4e22-b978-4e4b5406b5cc,2010-08-20 08:31:16.0,582.0,<normalization><nomenclature>,,
4492,2,1945,5922cfc8-2e92-4a41-9afc-34a0eb8af45e,2010-08-20 08:42:39.0,251.0,"The quantity $z = \\frac{X - \\mu}{\\sigma}$ is a [standard score][1].  So, standardization is a common way to refer to it.  \\n\\n  [1]: http://en.wikipedia.org/wiki/Standard_score\\n\\n",,
4493,2,1946,7cd37d37-40dc-418f-88e6-0aadd88441ef,2010-08-20 08:45:26.0,88.0,I think it is just called Z-score.,,
4494,5,1946,e4afc0a5-9028-4078-9c56-86370d5ff201,2010-08-20 09:24:29.0,88.0,I think it is just called z-score.,edited body,
4495,2,1947,5bc3563f-c089-432b-b7ec-40e737ce9799,2010-08-20 09:52:03.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$.\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)) . Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can define quantile in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",,
4496,2,1948,00c3165d-44f6-46db-b2db-a5e6aa9ae4f3,2010-08-20 10:05:35.0,603.0,"A definition that comes close to it, for unimodal distributions, is the tukey halfspace median \\n\\nhttp://cgm.cs.mcgill.ca/~athens/Geometric-Estimators/halfspace.html\\nhttp://www.isical.ac.in/~statmath/html/publication/Tukey_tech_rep.pdf",,
4497,2,1949,298e5879-d847-4dc0-a545-65fdaff5e602,2010-08-20 10:19:39.0,603.0,"Really, these have been discredited long ago. For univariate Outliers, the optimal (most efficent) filter is median+/-$\\delta \\times$ MAD, or better yet (if you have access to R)\\nmedian+/-$\\delta \\times$ Qn,  \\n\\nThe Qn estimator is implemented in package robustbase.\\n\\nSee:\\n\\nRousseeuw, P.J. and Croux, C. (1993) Alternatives to the Median\\n     Absolute Deviation, _Journal of the American Statistical\\n     Association_ *88*, 1273-1283.",,
4498,5,1949,186dd095-8c74-488c-80f6-f41403d41d4e,2010-08-20 10:34:11.0,603.0,"Really, these have been discredited long ago. For univariate Outliers, the optimal (most efficent) filter is median+/-$\\delta \\times$ MAD, or better yet (if you have access to R)\\nmedian+/-$\\delta \\times$ Qn (so you don't assume the underlying distribution to be symmetric),  \\n\\nThe Qn estimator is implemented in package robustbase.\\n\\nSee:\\n\\nRousseeuw, P.J. and Croux, C. (1993) Alternatives to the Median\\n     Absolute Deviation, _Journal of the American Statistical\\n     Association_ *88*, 1273-1283.",added 66 characters in body,
4499,6,1944,86dda75e-ec8b-4704-806e-378a43a021c8,2010-08-20 11:03:54.0,582.0,<terminology><normalization>,edited tags,
4500,2,1950,22a3dda0-2751-48b2-8637-89f1fc42897f,2010-08-20 11:16:27.0,88.0,"Bootstrap is essentially a simulation of repeating experiment; let's say you have a box with balls an want to obtain an average size of a ball -- so you draw some of them, measure and take a mean. Now you want to repeat it to get the distribution, for instance to get a standard deviation -- but you found out that someone stole the box.  \\nWhat can be done now is to use what you have -- this one series of measurements. The idea is to put the balls to the new box and simulate the original experiment by drawing the same number of balls with replacement -- both to have same sample size and some variability. Now this can be replicated many times to get a series of means which can be finally used to approximate the mean distribution. ",,
4501,5,1947,e36c3cf6-8bd9-4942-94ef-c0b6b17a8255,2010-08-20 11:22:08.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer false into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$ ($x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)) . Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can define quantile in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",added 353 characters in body; deleted 1 characters in body; deleted 9 characters in body,
4502,5,1947,53537ff7-61c8-487c-b662-308e41d5cafc,2010-08-20 11:28:20.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer false into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$  (with $x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)) . Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can define quantile in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",added 6 characters in body,
4503,2,1951,bc363190-b96b-4d1c-b090-cc0fc3ccfbac,2010-08-20 12:28:09.0,364.0,The annual meeting of the Society for Computers in Psychology often features content on quantitative methods:\\n\\nhttp://sites.google.com/site/scipws/,,
4504,16,1951,bc363190-b96b-4d1c-b090-cc0fc3ccfbac,2010-08-20 12:28:09.0,-1.0,,,
4505,2,1952,192eed57-a355-4336-a7ae-2d7f9ae2e01a,2010-08-20 12:52:31.0,364.0,"I like to think of it as follows: If you obtain a random sample data set from a population, then presumably that sample will have characteristics that roughly match that of the source population. So, if you're interested in obtaining confidence intervals on on a particular feature of the distribution, its skewness for example, you can treat the sample as a pseudo-population from which you can obtain many sets of random pseudo-samples, computing the value of the feature of interest in each. The assumption that the original sample roughly matches the population also means that you can obtain the pseudo-samples by sampling from the pseudo-population ""with replacement"" (eg. you sample a value, record it, then put it back; thus each value has a chance of being observed multiple times.). Sampling with replacement means that the computed value of the feature of interest will vary from pseudo-sample to pseudo-sample, yielding a distribution of values from which you can compute, say, the 2.5th and 97.5th percentiles to obtain the 95% confidence interval for the value of the feature of interest.\\n",,
4506,2,1953,c2b08d28-ce42-4976-85a9-0de406753f8e,2010-08-20 13:42:01.0,449.0,"L-moments might be useful here?\\n\\n<a href=""http://en.wikipedia.org/wiki/L-moment"">Wikipedia article<a/>\\n\\n<a href=""http://www.research.ibm.com/people/h/hosking/lmoments.html"">The L-moments page (Jonathan R.M. Hosking, IBM Research)</a>\\n\\nThey provide quantities analogous to conventional moments such as skewness and kurtosis, called the l-skewness and l-kurtosis. These have the advantage that they don't require calculation of high moments as they are computed from linear combinations of the data and defined as linear combinations of expected values of order statistics. This also means they are less sensitive to outliers.\\n\\nI believe you only need second-order moments to calculate their sample variances, which presumably you'd need for your test. Also their asymptotic distribution converges to a normal distribution much faster than conventional moments.\\n\\nIt seems the expressions for their sample variances get quite complicated (<a href=""http://dx.doi.org/10.1016/S0378-3758(03)00213-1"">Elamir and Seheult 2004</a>), but i know they've been programmed in downloadable packages for both R and Stata (available from their standard repositories), and maybe in other packages too for all i know. As your samples are independent once you've got the estimates and standard errors you could just plug them into a two-sample z-test if your sample sizes are ""large enough"" (Elamir and Seheult report some limited simulations that appear to show that 100 isn't large enough, but not what is). Or you could bootstrap the l-skewness divided by its SE. The above properties suggest that should perform considerably better than bootstrapping the conventional skewness divided by its SE.",,
4507,5,1953,303ade36-f634-41aa-85a6-a4dfa7999ccf,2010-08-20 13:53:11.0,449.0,"L-moments might be useful here?\\n\\n<a href=""http://en.wikipedia.org/wiki/L-moment"">Wikipedia article<a/>\\n\\n<a href=""http://www.research.ibm.com/people/h/hosking/lmoments.html"">The L-moments page (Jonathan R.M. Hosking, IBM Research)</a>\\n\\nThey provide quantities analogous to conventional moments such as skewness and kurtosis, called the l-skewness and l-kurtosis. These have the advantage that they don't require calculation of high moments as they are computed from linear combinations of the data and defined as linear combinations of expected values of order statistics. This also means they are less sensitive to outliers.\\n\\nI believe you only need second-order moments to calculate their sample variances, which presumably you'd need for your test. Also their asymptotic distribution converges to a normal distribution much faster than conventional moments.\\n\\nIt seems the expressions for their sample variances get quite complicated (<a href=""http://dx.doi.org/10.1016/S0378-3758(03)00213-1"">Elamir and Seheult 2004</a>), but i know they've been programmed in downloadable packages for both R and Stata (available from their standard repositories), and maybe in other packages too for all i know. As your samples are independent once you've got the estimates and standard errors you could just plug them into a two-sample z-test if your sample sizes are ""large enough"" (Elamir and Seheult report some limited simulations that appear to show that 100 isn't large enough, but not what is). Or you could bootstrap the difference in l-skewness. The above properties suggest that may perform considerably better than bootstrapping based on the conventional skewness.",deleted 16 characters in body,
4508,2,1954,242ecb8c-a8ec-482e-bce5-29a5c2e38570,2010-08-20 13:54:04.0,521.0,"There are actually several more methods and the choice often depends on whether you are most interested in looking for early differences, later differences or - as for the log-rank test & the Mantel-Haenszel test - give equal weight to all time points.\\n\\nTo the question at hand. The log-rank test is in fact a form of the Mantel-Haenszel test applied to survival data. The Mantel-Haenszel test is usually used to test for independence in stratified contingency tables.\\n\\nIf we try to apply the MH test to survival data, we can start by assuming that events at each failure time are independent. We then stratify by failure time. We use the MH methods for by making each failure time a strata. Not surprisingly they often give the same result.\\n\\nThe exception occcurs when more than one event occurs simultaneously - multiple deaths at exactly the same time point. I can't remember how the treatment then differs. I think the log-rank test averages over the possible orderings of the tied failure times. \\n\\nSo the log-rank test is the MH test for survival data and can deal with ties. I've never used the MH test for survival data.",,
4509,2,1955,a3698a48-7f2e-4e53-8fbe-c1b20b6f78a8,2010-08-20 13:59:34.0,174.0,"Suppose we have time series data sampling something we can approximate as a Markov process: that is, there are some number of discrete states and associated probabilities of transitioning from one state to another in a memory-free fashion.\\n\\nThe data are fairly extensive, say millions of samples per series, but noisy. We cannot observe the states directly, and instead sample another, effectively continuous, quantity as a proxy for it. This sampling is subject to measurement error. Also, the precise levels of the proxy value associated with each state vary according to a range of experimental factors, only some of which we are able to control with any accuracy. So there isn't a straightforward ""key"" that unambiguously maps a given value of the proxy to the corresponding state -- we need to infer this from the data itself.\\n\\n(There are other complications that I'll gloss over here but may come back to in future questions. Some -- such as experimental interventions that change the transition probabilities in predictable ways -- may make the problem more tractable.)\\n\\n**Given two such series, how would you go about estimating the likelihood that both are sampling the same underlying process?**\\n\\nWe can assume that the structure of the underlying process is known -- ie, the number of states and the approximate transition probabilities between them -- although if there are approaches that do not depend on this that would be interesting.\\n\\n*Note: this problem arises in a particular field of laboratory experimentation for which a body of specialised literature already exists. But I'm interested in seeing what people make of it outside that context, so I'm trying not to be too specific at this stage.*",,
4510,1,1955,a3698a48-7f2e-4e53-8fbe-c1b20b6f78a8,2010-08-20 13:59:34.0,174.0,Comparing noisy time series to estimate the likelihood of them being produced by the same Markov process,,
4511,3,1955,a3698a48-7f2e-4e53-8fbe-c1b20b6f78a8,2010-08-20 13:59:34.0,174.0,<time-series><markov-process>,,
4512,2,1956,328463c3-1503-4d36-9ff3-d736c109f215,2010-08-20 14:08:11.0,,You can perhaps use a [hidden markov model][1] (HMM). I know that there is a R package that estimates HMMs but cannot recall its name right now.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Hidden_Markov_model,,user28
4513,2,1957,1f5ccadb-001f-435d-99e3-2ab5da007ac4,2010-08-20 14:37:06.0,919.0,"*Very* broadly: the intuition, as well as the origin of the name (""pulling oneself up by the bootstraps""), derive from the observation that in using properties of a sample to draw inferences about a population (the ""inverse"" problem of statistical inference), we expect to err.  To find out the nature of that error, treat the sample itself as a population in its own right and study how your inferential procedure works when you draw samples from *it.*  That's a ""forward"" problem: you know all about your sample-*qua*-population and don't have to guess anything about it.  Your study will suggest (a) the extent to which your inferential procedure may be biased and (b) the size and nature of the statistical error of your procedure.  So, use this information to adjust your original estimates.  In many (but definitely not all) situations, the adjusted bias is asymptotically much lower.\\n\\nOne insight provided by this schematic description is that bootstrapping does not *require* simulation or repeated subsampling: those just happen to be omnibus, computationally tractable ways to study any kind of statistical procedure when the population is known.  There exist plenty of bootstrap estimates that can be computed mathematically.\\n\\nThis answer owes much to Peter Hall's book ""The Bootstrap and Edgeworth Expansion"" (Springer 1992), especially his description of the ""Main Principle"" of bootstrapping.",,
4514,5,1912,103cd38f-7e01-4970-a8fc-c37b1c31ab1d,2010-08-20 15:19:17.0,5.0,"[Gary King][1] made the [following statement on Twitter][2]:\\n\\n> scale invariance sounds cool but is\\n> usually statisticians shirking\\n> responsibility & losing power by\\n> neglecting subject matter info\\n\\nWhat is an example of this phenomena, where [scale invariance][3] causes a loss of power?\\n\\n*Edit:*\\n\\n[Gary responded][4]:\\n\\n> not [statistical] power, but scale invariance\\n> loses the power that can be extracted\\n> from knowledge of the substance\\n\\nNow this makes more sense.  How can scale invariance cause a loss of explanatory power from the resulting analysis?\\n\\n\\n  [1]: http://gking.harvard.edu/\\n  [2]: http://twitter.com/kinggary/status/21513150698\\n  [3]: http://www.statistics.com/resources/glossary/s/scaleinv.php\\n  [4]: http://twitter.com/kinggary/status/21624260888",added 338 characters in body; edited title,
4515,4,1912,103cd38f-7e01-4970-a8fc-c37b1c31ab1d,2010-08-20 15:19:17.0,5.0,Why can scale invariance cause a loss of explanatory power?,added 338 characters in body; edited title,
4516,2,1958,2e073e54-0f34-4293-b2df-96ebd89c1d9a,2010-08-20 15:35:33.0,5.0,A few thoughts:\\n\\n 1. Can you not just use a goodness-of-fit test?  Choose a distribution and compare both samples.  Or use a qqplot.  Possible do this with returns instead of the original series.\\n 2. You could look at whether the two series are cointegrated.\\n,,
4517,5,1958,35859b7f-afec-465d-ae3a-97dcb8b5c822,2010-08-20 15:45:08.0,5.0,"A few thoughts:\\n\\n 1. Can you not just use a goodness-of-fit test?  Choose a distribution and compare both samples.  Or use a qqplot.  You may want to do this with returns (i.e. changes) instead of the original series, since this is often easier to model.  There are also relative distribution functions (see, for instance, the [reldist package][1]).\\n 2. You could look at whether the two series are cointegrated (use [the Johansen test][2]).  This is available in the [urca package][3] (and related book).\\n 3. There many multivariate time series models such as [VAR][4] that could be applied to model the dependencies (see the [vars package][5]).\\n 4. You could trying using a [copula][6], which is used for dependence modeling, and is available in [the copula package][7].\\n \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/reldist/index.html\\n  [2]: http://en.wikipedia.org/wiki/Johansen_test\\n  [3]: http://cran.r-project.org/web/packages/urca/index.html\\n  [4]: http://en.wikipedia.org/wiki/Vector_autoregression\\n  [5]: http://cran.r-project.org/web/packages/vars/index.html\\n  [6]: http://en.wikipedia.org/wiki/Copula_(statistics)\\n  [7]: http://cran.r-project.org/web/packages/copula/index.html",added 945 characters in body,
4518,5,1958,63eec64a-242c-426e-8d49-f794711375ba,2010-08-20 15:50:42.0,5.0,"A few thoughts:\\n\\n 1. Can you not just use a goodness-of-fit test?  Choose a distribution and compare both samples.  Or use a qqplot.  You may want to do this with returns (i.e. changes) instead of the original series, since this is often easier to model.  There are also relative distribution functions (see, for instance, the [reldist package][1]).\\n 2. You could look at whether the two series are cointegrated (use [the Johansen test][2]).  This is available in the [urca package][3] (and related book).\\n 3. There many multivariate time series models such as [VAR][4] that could be applied to model the dependencies (see the [vars package][5]).\\n 4. You could trying using a [copula][6], which is used for dependence modeling, and is available in [the copula package][7].\\n \\nIf the noise is serious concern, then try using a filter on the data before analyzing it.\\n\\n  [1]: http://cran.r-project.org/web/packages/reldist/index.html\\n  [2]: http://en.wikipedia.org/wiki/Johansen_test\\n  [3]: http://cran.r-project.org/web/packages/urca/index.html\\n  [4]: http://en.wikipedia.org/wiki/Vector_autoregression\\n  [5]: http://cran.r-project.org/web/packages/vars/index.html\\n  [6]: http://en.wikipedia.org/wiki/Copula_(statistics)\\n  [7]: http://cran.r-project.org/web/packages/copula/index.html",added 89 characters in body,
4519,2,1959,72cfc5af-9cf4-4259-b5e4-c37a0603a6cf,2010-08-20 16:52:39.0,919.0,"I still feel negatively about what amounts to a gratuitous insult on King's part--it's a churlish and unprofessional way to communicate an idea--but I can see where he might be coming from.  ""Scale-invariance"" is a restriction on a statistical procedure.  Thus, limiting our choice of procedures to scale-invariant ones (or to linear ones or to unbiased ones or minimax ones, etc.) potentially excludes procedures that might perform better.  Whether this is actually the case or not depends.  In many situations, data are reported in units that are essentially independent of what is being studied.  It shouldn't matter whether you measure distances in angstroms or parsecs, for example.  In this context, any procedure that is *not* scale invariant is therefore an *arbitrary* one--and arbitrariness is not a positive attribute in this field.  In other situations, though, there is a natural scale.  The most obvious of these concern counted data.  A procedure that treats counted data as if they were measurements on a continuous scale (e.g., using OLS for a counted response) is potentially inferior to other available procedures and may be (likely is, I suspect) inadmissible in the decision-theoretic sense.  This can be a tricky and subtle point because it's not always obvious when we have counted data.  One example I'm familiar with concerns many chemical or radioactivity measurements, which ultimately originate as counts on some machine.  Said counts get converted by the laboratory into a concentration or activity that forever after is treated as a real number.  (However, attempts to exploit this fact in the chemometrics literature have not yielded superior statistical procedures.)\\n\\nJust to stave off one possible misunderstanding, since King has not been too obliging with his lack of explanation: I wouldn't view a selection of an informative prior for a scale parameter (in a Bayesian analysis) as a scale-dependent procedure.  Such a prior obviously favors some ranges of values over others, but does not affect the scale invariance of the procedure itself.",,
4520,2,1960,0aa39f26-e98b-4949-9777-24b6dd5c1686,2010-08-20 16:54:44.0,,M2010 - 13th Annual Data Mining Conference http://www.sas.com/m2010,,Michele
4521,16,1960,0aa39f26-e98b-4949-9777-24b6dd5c1686,2010-08-20 16:54:44.0,-1.0,,,
4522,2,1961,b9c0260e-f02e-4d20-b7dc-0c08f976c88a,2010-08-20 17:15:43.0,253.0,"When one wants to compute the correlation of two vectors of a continues variables, one uses pearson (or spearman) correlation.\\n\\nBut what should (can) one use for the case of two vectors with 2 (or 3) levels only?  Is spearman enough, or does it require another method?\\n\\nI remember coming across someone who once claimed to me that OR (odds ratio) is more fitting for such situations, is this true?\\n\\nHere is an example R code, for allowing of answers relating to the same example:\\n\\n    seed(10)\\n    \\n    x2 <- sample(c(-1,1),50, T)\\n    x3 <- sample(c(-1:1),50, T)\\n    y3 <- sample(c(-1:1),50, T)\\n    y2 <- sample(c(-1,1),50, T)\\n    \\n    cor(x3,y3, method = c(""spearman""))\\n    cor(x2,y2, method = c(""spearman""))\\n    cor(x3,y2, method = c(""spearman""))\\n\\n",,
4523,1,1961,b9c0260e-f02e-4d20-b7dc-0c08f976c88a,2010-08-20 17:15:43.0,253.0,Correlation measure for vectors with only 2 (or 3) levels (in R?),,
4524,3,1961,b9c0260e-f02e-4d20-b7dc-0c08f976c88a,2010-08-20 17:15:43.0,253.0,<r><correlation><association-measure><ordered-variables>,,
4525,5,1955,825e5686-56b8-4fb7-a064-d746514c4511,2010-08-20 17:34:44.0,174.0,"**Edit:** Looks like I've muddied the water by referring to time series data, since that suggests a common timeframe with shared seasonality and exposure to external factors, which is not the case here. The series share the same time *scale* but there is no common *origin* and the underlying processes can be assumed (at least for the purposes of this question) to be stationary. Proposals for a better term (and tag) welcome!\\n\\n----\\n\\nSuppose we have time series data sampling something we can approximate as a Markov process: that is, there are some number of discrete states and associated probabilities of transitioning from one state to another in a memory-free fashion.\\n\\nThe data are fairly extensive, say millions of samples per series, but noisy. We cannot observe the states directly, and instead sample another, effectively continuous, quantity as a proxy for it. This sampling is subject to measurement error. Also, the precise levels of the proxy value associated with each state vary according to a range of experimental factors, only some of which we are able to control with any accuracy. So there isn't a straightforward ""key"" that unambiguously maps a given value of the proxy to the corresponding state -- we need to infer this from the data itself.\\n\\n(There are other complications that I'll gloss over here but may come back to in future questions. Some -- such as experimental interventions that change the transition probabilities in predictable ways -- may make the problem more tractable.)\\n\\n**Given two such series, how would you go about estimating the likelihood that both are sampling identical underlying processes?**\\n\\nWe can assume that the structure of the expected process -- ie, the number of states and the transition probabilities between them -- is approximately known, although if there are approaches that do not depend on this that would be interesting.\\n\\n*Note: this problem arises in a particular field of laboratory experimentation for which a body of specialised literature already exists. But I'm interested in seeing what people make of it outside that context, so I'm trying not to be too specific at this stage.*","flagged up possible misunderstanding of ""time series""",
4526,2,1962,70b3286e-4834-4a09-8c6d-90975916a4eb,2010-08-20 17:38:51.0,251.0,"The OR is a good measure of association, but sometimes people prefer a correlation coefficient for interpretation because it has a [-1, 1] scale.  \\n\\nFor binary variables, the analog to Pearson is the Phi statistic.  Cramer's V is applicable when you have more than 2x2 cases.  For details, see the following references:\\n\\n- [Effect size][1]\\n- [Correlation][2]\\n- [Cramer's V][3]\\n\\nI've never used any of these, so hopefully someone will jump in and say if there are good reasons for preferring them.\\n\\n  [1]: http://en.wikipedia.org/wiki/Effect_size\\n  [2]: http://faculty.chass.ncsu.edu/garson/PA765/correl.htm\\n  [3]: http://edpsych.ed.sc.edu/seaman/edrm711/questions/categorical.htm\\n\\n",,
4527,4,1961,bb1f6a87-e0e7-4ab5-be7f-6d7dd490dfdc,2010-08-20 18:05:10.0,253.0,Measure of association (correlation) for vectors with only 2 (or 3) levels (in R?),edited title,
4528,2,1963,cb3b45fe-0365-4d43-8286-73ab8c536783,2010-08-20 18:06:04.0,919.0,"A (non-statistician) colleague has been encountering meta-analyses in papers he reviews for medical journals and is looking for a good introductory level treatment so he can educate himself.  Any recommendations?  Favorites?  Books, monographs, nontechnical survey articles would all be fine.\\n\\n(Yes, he's familiar with the Wikipedia entry and other stuff readily accessible by a Google search, such as [Jerry Dallal's nice little article][1].)\\n\\n\\n  [1]: http://www.jerrydallal.com/LHSP/meta.htm ",,
4529,1,1963,cb3b45fe-0365-4d43-8286-73ab8c536783,2010-08-20 18:06:04.0,919.0,Looking for good introductory treatment of meta-analysis,,
4530,3,1963,cb3b45fe-0365-4d43-8286-73ab8c536783,2010-08-20 18:06:04.0,919.0,<modeling><beginner>,,
4531,16,1963,6a83d970-eb2c-40dc-a5cd-b9613a0cfbf4,2010-08-20 18:16:10.0,919.0,,,
4532,6,1963,0131145a-6892-4ec7-8ded-cfb180915c4f,2010-08-20 18:22:35.0,5.0,<modeling><meta-analysis>,edited tags,
4533,2,1964,763ac291-e8b0-4520-9c44-74f09e513acb,2010-08-20 18:34:22.0,1025.0,"I am doing some Kernel density estimation, with a weighted points set (ie., each sample has a weight which is not necessary one), in N dimensions. Also, these samples are just in a metric space (ie., we can define a distance between them) but nothing else. For example, we cannot determine the mean of the sample points, nor the standard deviation, nor scale one variable compared to another. The Kernel is just affected by this distance, and the weight of each sample: f(x) = 1./(sum_weights) * sum(weight_i/h * Kernel(distance(x,x_i)/h))\\n\\nIn this context, I am trying to find a robust estimation for the kernel bandwidth 'h', possibly spatially varying, and preferably which gives an exact reconstruction on the training dataset x_i. If necessary, we could assume that the function is relatively smooth.\\n\\nI tried using the distance to the first or second nearest neighbor but it gives quite bad results. I tried with leave-one-out optimization, but I have difficulties finding a good measure to optimize for in this context in N-d, so it finds very bad estimates, especially for the training samples themselves. I cannot use the greedy estimate based on the normal assumption since I cannot compute the standard deviation. I found references using covariance matrices to get anisotropic kernels, but again, it wouldn't hold in this space...\\n\\nSomeone has an idea or a reference ?\\n\\nThank you very much in advance!",,
4534,1,1964,763ac291-e8b0-4520-9c44-74f09e513acb,2010-08-20 18:34:22.0,1025.0,Kernel bandwidth in Kernel density estimation,,
4535,3,1964,763ac291-e8b0-4520-9c44-74f09e513acb,2010-08-20 18:34:22.0,1025.0,<smoothing><kde><kernel><pdf>,,
4536,2,1965,3e0be4bd-050f-463d-a5dd-b386e777ddd7,2010-08-20 19:39:20.0,795.0,"(I will delete my other non-answer, the edit of which had this nugget in it)\\n\\n**No.** Asymptotically, the 'trivial' upper bound is the least upper bound. To see this, Let $P_n = P(Z = n)$. Trivially, $E[\\exp{(Z^2)}] \\ge P_n \\exp{(n^2)} = L$, where $L$ is the lower bound of interest. Since $Z$ is binomial, we have $P_n = {n\\choose n} (n^{-\\beta})^n (1-n^{-\\beta})^0 = n^{-n\\beta}$. Then $\\log{L} = -\\beta n \\log{n} + n^2$. It is easily shown that this is $\\omicron{(n^2)}$, and thus, $L$ is $\\omicron{(\\exp{(n^2)})}$. Thus the trivial upper bound is, asymptotically, the least upper bound, i.e. $E[\\exp{(Z^2)}] \\in \\Theta{(\\exp{(n^2)})}$.",,
4538,5,1949,4a3d374c-4304-4642-bb35-009f32209467,2010-08-20 19:47:54.0,603.0,"Really, these have been discredited long ago. For univariate Outliers, the optimal (most efficent) filter is median+/-$\\delta \\times$ MAD, or better yet (if you have access to R)\\nmedian+/-$\\delta \\times$ Qn (so you don't assume the underlying distribution to be symmetric),  \\n\\nThe Qn estimator is implemented in package robustbase.\\n\\nSee:\\n\\nRousseeuw, P.J. and Croux, C. (1993) Alternatives to the Median\\n     Absolute Deviation, _Journal of the American Statistical\\n     Association_ *88*, 1273-1283.\\n\\n\\nResponse to comment:\\n\\nTwo levels. \\n\\nA) Philosophical.\\n\\nBoth the Dixon and Grub tests are only able to detect a particular type of (isolated) outliers. For the last 20-30 years the concept of outliers has involved unto ""any observation that departs from the main body of the data"". Without further specification of what the particular departure is. This characterization-free approach renders the idea of building tests to detect outliers void. The emphasize shifted to the concept of estimators (a classical example of which is the median) that retain there values (i.e. are insensitive) even for large rate of contamination by outliers -such estimator is then said to be robust-  and the question of detecting outliers becomes void.\\n\\nB) Weakness,\\n\\nYou can see that the Grub and Dixon tests easily break down: one can easily generated contaminated data that would pass either test like a bliss (i.e. without breaking the null).\\nThis is particularly obvious in the Grubb test, because outliers will break down the mean and s.d. used in the construction of the test stat. It's less obvious in the Dixon, until one learns that order statistics are not robust to outliers either.\\n\\nI think you will find more explanation of these facts in papers oriented towards the general non-statistician audience such as the one cited above (I can also think of the Fast-Mcd paper by Rousseeuw). If you consult any recent book/intro to robust analysis, you will notice that neither Grubb nor Dixon are mentioned. ",added 1515 characters in body,
4539,2,1966,105ee53d-49b5-4f66-a1c5-c0b5bd9f8acb,2010-08-20 20:10:24.0,961.0,What non-/semiparametric methods to estimate a probability density from a data sample are you using ?\\n\\n(Please do not include more than one method per answer) ,,
4540,1,1966,105ee53d-49b5-4f66-a1c5-c0b5bd9f8acb,2010-08-20 20:10:24.0,961.0,Density estimation methods ?,,
4541,3,1966,105ee53d-49b5-4f66-a1c5-c0b5bd9f8acb,2010-08-20 20:10:24.0,961.0,<probability><estimation><nonparametric>,,
4542,2,1967,c7beb388-8fd5-4053-98ea-d501b84fc64e,2010-08-20 20:37:23.0,919.0,"Numerical experiments (for 2 <= n <= 4000 and all values of beta) indicate the estimate n^2 - (n Ln(n))*beta exceeds the *logarithm* of the expectation by an amount on the order of beta*Exp(-n).  The error appears to increase monotonically in beta for each n.  This should provide some useful clues about how to proceed (for those with the time and interest).  In particular, an upper bound for the expectation exists of the form Exp(n^2 - (n Ln(n))*beta + C*Exp(-n)*beta) with C << 1. ",,
4543,2,1968,fd3daa2b-85e6-4efe-98d5-267c2c8a76be,2010-08-20 20:45:24.0,1028.0,I haven't had the problem you're talking about for a while but I used to.  It might be that when you're selecting the output that you want you release your mouse drag somewhere off of the output (ie you're selecting a large region and you start at the bottom right and drag to the upper left and let go of the mouse somewhere in the explorer area).  I know when I did that it wouldn't go through with the copy.  I needed to release the drag inside of the output window to allow a copy to go through.  Like I said the version I'm using now doesn't have this problem so I can't actually test this but I believe that's how I fixed that problem before.,,
4544,2,1969,dfde1762-146b-43e5-a522-3f7308a80e17,2010-08-20 20:51:48.0,795.0,I use Silverman's Adaptive Kernel Density estimator. see e.g [`akj` help page][1]\\n\\n\\n  [1]: http://bm2.genes.nig.ac.jp/RGM2/R_current/library/quantreg/man/akj.html,,
4545,16,1966,9c2f5fae-7b80-46ab-9386-a90b35207e70,2010-08-20 20:55:10.0,961.0,,,
4546,16,1969,07555ddf-b9a3-4a19-a413-603a94bfbfbf,2010-08-20 21:03:32.0,5.0,,,
4547,2,1970,8adb6961-c746-4a72-828a-b1dba473ec0e,2010-08-20 21:30:13.0,168.0,"What are good statistical journals with quick turnaround (fast review cycle), suitable for short notes in mathematical statistics and preferably with open access. An example is Statistics & Probability Letters, however, that journal only has sponsored open access.",,
4548,1,1970,8adb6961-c746-4a72-828a-b1dba473ec0e,2010-08-20 21:30:13.0,168.0,What is a statistical journal with quick turnaround?,,
4549,3,1970,8adb6961-c746-4a72-828a-b1dba473ec0e,2010-08-20 21:30:13.0,168.0,<open-source>,,
4550,16,1970,8adb6961-c746-4a72-828a-b1dba473ec0e,2010-08-20 21:30:13.0,168.0,,,
4551,5,1967,85073e4f-7397-48fb-8de9-88003f3c5f12,2010-08-20 21:38:08.0,919.0,"Numerical experiments (for 2 <= n <= 4000 and all values of beta) indicate the estimate n^2 - (n Ln(n))*beta exceeds the *logarithm* of the expectation by an amount on the order of beta*Exp(-n).  The error appears to increase monotonically in beta for each n.  This should provide some useful clues about how to proceed (for those with the time and interest).  In particular, an upper bound for the expectation exists of the form Exp(n^2 - (n Ln(n))*beta + C*Exp(-n)*beta) with C << 1. \\n\\n**Update**\\n\\nAfter staring at the summation expression for the expectation, it became evident where the nLn(n)*beta term comes from: break each binomial coefficient Comb(n,k) into its sum Comb(n-1,k) + Comb(n-1,k-1) and write Exp(k^2) = Exp((k-1)^2)*Exp(2k-1).  This decomposes the expectation e(n,beta) into the sum of two parts, one of which looks like the expectation e(n-1,beta) and the other of which is messy (because each term is multiplied by Exp(2k-1)) but can be bounded above by replacing all those exponential terms by their obvious upper bound Exp(2n-1).  (This is not too bad, because the last term with the highest exponent strongly dominates the entire sum.)  This gives a recursive inequality for the expectation,\\n\\n>e(n,beta) <= (n^-beta * Exp(2n-1) + 1 - n^-beta) * e(n-1,beta)\\n\\nDoing this n times creates a polynomial whose highest term is in fact Exp(n^2)*n^(-n*beta), with the remaining terms decreasing fairly rapidly.  At this point any reasonable bound on the remainder will produce an improved bound for the expectation essentially of the form suggested by the numerical experiments.  At this point you have to decide how hard you want to work to obtain a tighter upper bound; the numerical experiments suggest this additional work is not going to pay off unless you're interested in the smallest values of n.",Update added.,
4552,2,1971,93ec28f6-9555-4705-aee0-1993305b51b4,2010-08-20 21:52:06.0,168.0,"[AISTATS][1] -- Conference on Artificial Intelligence and Statistics\\n\\nSimilar flavor of papers to NIPS, although papers may be of slightly lower quality. It is much smaller than ICML or NIPS, which allows people to have deeper interactions.\\n\\n\\n  [1]: http://www.aistats.org/",,
4553,16,1971,93ec28f6-9555-4705-aee0-1993305b51b4,2010-08-20 21:52:06.0,-1.0,,,
4554,2,1972,13b86148-8c75-4c9f-adc2-8499540dbdd6,2010-08-20 22:12:36.0,1029.0,"I have a bunch of variables organized into 10 different levels of a grouping factor. I'm doing some ANCOVA on particular variables and also plotting the data using boxplots. I'd like to add 84% confidence intervals to all the groups (since non-overlapping 84% CIs indicate a significant difference at alpha .05 - at least for two groups). I can do all this quite easily in R.\\n\\nMy question is - should I be applying a ""family-wise"" 84% CIs to all the groups? In other words, just as one would devalue an alpha level by the number of groups to obtain a family-wise alpha, should I inflate the CI a reciprocal amount to achieve a family-wise interval? This seems reasonable to me, but I haven't seen this discussed in the literature.\\n\\nIf alpha were CI were interchangeable for two or more groups the the family-wise 84% CI would be 99.5%, but i've read that alpha and CI are only interchangable for 1-sample situations. If this is the case, how would I go about calculating the family-wise confidence intervals for 10 (or any number) groups?\\n\\nAny advice would be welcome.\\n\\nbest,\\n\\nSteve",,
4555,1,1972,13b86148-8c75-4c9f-adc2-8499540dbdd6,2010-08-20 22:12:36.0,1029.0,Family-wise confidence intervals,,
4556,3,1972,13b86148-8c75-4c9f-adc2-8499540dbdd6,2010-08-20 22:12:36.0,1029.0,<confidence-interval>,,
4557,2,1973,5a943b03-c816-45d7-8a5c-f5116afe7409,2010-08-20 22:35:55.0,603.0,Half-space depth a.k.a. bag-plots.\\n\\nhttp://www.r-project.org/user-2006/Slides/Mizera.pdf\\n,,
4558,16,1973,5a943b03-c816-45d7-8a5c-f5116afe7409,2010-08-20 22:35:55.0,-1.0,,,
4559,2,1974,02b8893f-94c4-4ae9-903b-2b3b82d4e583,2010-08-20 22:40:51.0,253.0,"It sounds a reasonable solution **if this is what important for you to present in the plot**.\\n\\nWhat this will give you  (besides many questions, in case you are working with people who like statistics less then you), is a CI that is applicable to your situation which requires correction for multiple hypothesis.\\n\\nWhat this won't give you, is the ability to compare difference between groups based on the CI.\\n\\nRegarding the computation of the CI, you could use the p.adjust with something like simes which will still keep your FWE (family wise error), but will give you a wider window.\\n\\nAs to why you didn't find people writing about this, that is a good question, I don't know.",,
4560,2,1975,58f99fec-ee5d-4885-a5a5-bf5eaa90dee5,2010-08-20 22:45:24.0,603.0,"If you have more than two values, you can use (M)CA:\\nhttp://en.wikipedia.org/wiki/Correspondence_analysis",,
4561,2,1976,fe4c7d4b-7b54-4d24-b6b2-902cd5b6ab71,2010-08-20 23:58:49.0,561.0,"I have two suggestions:\\n\\n 1. Systematic Reviews in Health Care: Meta-Analysis in Context ([Amazon link][1])\\n 2. Introduction to Meta-Analysis (Statistics in Practice) ([Amazon link][2])\\n\\nBoth books are very good, including introductory information as well as detailed information about how to actually perform meta-analyses.\\n\\n  [1]: http://www.amazon.com/Systematic-Reviews-Health-Care-Meta-Analysis/dp/072791488X\\n  [2]: http://www.amazon.com/Introduction-Meta-Analysis-Statistics-Practice-Borenstein/dp/0470057246/ref=sr_1_fkmr1_1?ie=UTF8&qid=1282348649&sr=1-1-fkmr1",,
4562,16,1976,fe4c7d4b-7b54-4d24-b6b2-902cd5b6ab71,2010-08-20 23:58:49.0,-1.0,,,
4563,2,1977,f987b378-8177-47f8-b4fd-8ecfab767f08,2010-08-21 00:00:26.0,881.0,"[Dirichlet Process][1] mixture models can be very flexible nonparametric Bayesian approach for density modeling, and can also be used as building blocks in more complex models. They are essentially an infinite generalization of parametric Gaussian mixture models and don't require specifying in advance the number of components in the mixture.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Dirichlet_process",,
4564,16,1977,f987b378-8177-47f8-b4fd-8ecfab767f08,2010-08-21 00:00:26.0,-1.0,,,
4565,2,1978,7fe86b17-838d-491b-b469-707518fc4188,2010-08-21 00:04:13.0,881.0,Gaussian Processes can also be another nonparametric Bayesian approach for density estimation. See this [Gaussian Process Density Sampler][1] paper.\\n\\n\\n  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.3937&rep=rep1&type=pdf,,
4566,16,1978,7fe86b17-838d-491b-b469-707518fc4188,2010-08-21 00:04:13.0,-1.0,,,
4567,5,1961,b4b90432-537e-4fcd-80d8-dd1bf80e33d6,2010-08-21 03:43:11.0,253.0,"When one wants to compute the correlation of two vectors of a continues variables, one uses pearson (or spearman) correlation.\\n\\n**But what should (can) one use for the case of two vectors with 2 (or 3) ordered levels only?  Is spearman enough, or does it require another method?**\\n\\nI remember coming across someone who once claimed to me that OR (odds ratio) is more fitting for such situations (for 2 by 2 tables, where order has no meaning), is this true?\\n\\nHere is an example R code, for allowing of answers relating to the same example:\\n\\n    seed(10)\\n    \\n    x2 <- sample(c(-1,1),50, T)\\n    x3 <- sample(c(-1:1),50, T)\\n    y3 <- sample(c(-1:1),50, T)\\n    y2 <- sample(c(-1,1),50, T)\\n    \\n    cor(x3,y3, method = c(""spearman""))\\n    cor(x2,y2, method = c(""spearman""))\\n    cor(x3,y2, method = c(""spearman""))\\n\\n\\np.s: for the 2 by 2 case, I followed from the comments that categorical ""measures of association"" is the term to look for.  However, Part of the time I am comparing 2 on 3 tables, where on the factor with 3 levels there is order - so I would like to take use of that information.\\n",clarifying that I am referring to ordered factors.,
4568,4,1961,b4b90432-537e-4fcd-80d8-dd1bf80e33d6,2010-08-21 03:43:11.0,253.0,"Can you use normal correlation for vectors with only 2 (or 3), ordered, levels? ",clarifying that I am referring to ordered factors.,
4569,2,1979,bfb4fa79-c2d9-43fb-8297-814b8d01f26c,2010-08-21 04:42:39.0,183.0,"I wrote a post a while back on [getting started with meta analysis][1] with:\\n(a) tips on getting started,\\n(b) links to online introductory texts, \\nand (c) links to free software for meta analysis.\\n\\nSpecifically, you might want to read [James DeCoster's notes][2].\\n\\n\\n  [1]: http://jeromyanglim.blogspot.com/2009/12/meta-analysis-tips-resources-and.html\\n  [2]:  http://www.stat-help.com/meta.pdf",,
4570,16,1979,bfb4fa79-c2d9-43fb-8297-814b8d01f26c,2010-08-21 04:42:39.0,-1.0,,,
4571,2,1980,e5d208b8-5861-4fb5-b2a5-85677b3b120c,2010-08-21 04:58:11.0,183.0,"Are there any good examples of [reproducible research][1] using R that are freely available online?\\n\\nSpecifically, ideal examples would provide:\\n\\n- The raw data (and ideally meta data explaining the data),\\n- All R code including data import, processing, analyses, and output generation,\\n- Sweave or some other approach for linking the final output to the final document,\\n- All in a format that is easily downloadable and compilable on a reader's computer.\\n\\nIdeally, the example would be a journal article or a thesis where the emphasis is on an actual applied topic as opposed to a statistical teaching example.\\n\\n\\n  [1]: http://reproducibleresearch.net/index.php/Main_Page",,
4572,1,1980,e5d208b8-5861-4fb5-b2a5-85677b3b120c,2010-08-21 04:58:11.0,183.0,Complete substantive examples of reproducible research online using R,,
4573,3,1980,e5d208b8-5861-4fb5-b2a5-85677b3b120c,2010-08-21 04:58:11.0,183.0,<r><reproducible-research><sweave>,,
4574,2,1981,343a5f58-758b-46e5-8b26-7a82da5729c5,2010-08-21 05:30:40.0,183.0,"A few thoughts: \\n\\n- There are many different binary-binary and ordinal-ordinal measures of association.\\nSPSS provides names and algorithms for many of them under [proximities][1]\\nand [crosstabs][2].\\n- I'm also intrigued by  [tetrachoric][3] (binary-binary) and [polychoric][4] (ordinal-ordinal) correlations that\\n aim to estimate the correlation between theorised latent continuous variables.\\n- You can use Pearson's correlation. However, it is not always the most meaningful metric of association. Also, confidence intervals and p-values that assume continuous normal variables wont be perfectly accurate.\\n\\n\\n  [1]: ftp://ftp.spss.com/pub/spss/statistics/spss/algorithms/proximit.pdf\\n  [2]: http://support.spss.com/productsext/spss/documentation/statistics/algorithms/14.0/crosstabs.pdf\\n  [3]: http://jeromyanglim.blogspot.com/2009/09/tetrachoric-correlations-overview-and.html\\n  [4]: http://www.john-uebersax.com/stat/tetra.htm",,
4575,2,1982,c85262e0-d5ce-4df5-9b04-7010356fafed,2010-08-21 05:47:09.0,251.0,"Charles Geyer's [page on Sweave][1] has an example from a thesis, which meets some of your requirements (the raw data is simply from an R package, but the R/sweave code and final PDF are available):\\n\\n> A paper on the theory in Yun Ju Sung's thesis, Monte Carlo Likelihood Inference for Missing Data Models (preprint) contained computing examples. Every number in the paper and every plot was taken (by cut-and-paste, I must admit) from a ""supplementary materials"" document done in Sweave.\\n\\n(The [source file][2] is linked under the ""Supplementary Materials for a Paper"" section.)\\n\\nI know I've come across at least one R example browsing the [ReproducibleResearch.net material][2] page before, but unfortunately didn't bookmark it.  \\n\\n\\n  [1]: http://www.stat.umn.edu/~charlie/Sweave/#exam\\n  [2]: http://www.stat.umn.edu/geyer/bernor/library/bernor/doc/examples.Rnw\\n  [3]: http://reproducibleresearch.net/index.php/RR_material\\n\\n\\n\\n\\n\\n",,
4576,5,1962,67a4966e-836a-40ed-b4f2-96cd503e74f6,2010-08-21 05:49:42.0,251.0,"The OR is a good measure of association, but sometimes people prefer a correlation coefficient for interpretation because it has a [-1, 1] scale.  \\n\\nFor binary variables, the Phi statistic provides Pearson's correlation (see Jeromy's comment).  Cramer's V is applicable when you have more than 2x2 cases.  For details, see the following references:\\n\\n- [Effect size][1]\\n- [Correlation][2]\\n- [Cramer's V][3]\\n\\nI've never used any of these, so hopefully someone will jump in and say if there are good reasons for preferring them.\\n\\n  [1]: http://en.wikipedia.org/wiki/Effect_size\\n  [2]: http://faculty.chass.ncsu.edu/garson/PA765/correl.htm\\n  [3]: http://edpsych.ed.sc.edu/seaman/edrm711/questions/categorical.htm\\n\\n",added 29 characters in body; edited body,
4577,4,1980,21beeead-b3dc-4d9b-9459-3c55f4b17121,2010-08-21 05:58:41.0,183.0,Complete substantive examples of reproducible research  using R,edited title,
4578,6,1980,82bfb792-fa94-4eef-a173-ba507aee2895,2010-08-21 07:01:22.0,183.0,<r><latex><reproducible-research><sweave>,edited tags,
4579,2,1983,25c3edd8-0edc-4281-876d-bb75b7cf6d25,2010-08-21 07:15:43.0,5.0,"I have found good ones in the past and will post once I dig them up, but some quick general suggestions:\\n\\n 1. You may be able to find some interesting examples by searching google with keywords and ext:rnw (which will search for files with the sweave extension). Here's <a HREF=""http://www.google.com/m/search?oe=UTF-8&client=safari&hl=en&aq=f&oq=&aqi=-k0d0t0&fkt=1178&fsdt=6703&q=ext%3Arnw+paper"">an example search</a>.  This is the third result from my search: http://www.ne.su.se/paper/araietal_source.Rnw\\n 2. Many R packages have interesting vignettes which essentially amount to the same thing. An example: https://r-forge.r-project.org/scm/viewvc.php/paper/maxLik.Rnw\\n  ",,
4580,5,1871,73c6e122-3e83-474d-9df8-3a1b4d969f09,2010-08-21 07:21:06.0,601.0,"It's not imbalanced because your repeated measures should be averaged across such subgroups within subject beforehand.  The only thing imbalanced is the quality of the estimates of your means.\\n\\nJust as you aggregated your accuracies to get a percentage correct and do your ANOVA in the first place you average your latencies as well.  Each participant provides 6 values, therefore it is not imbalanced.\\n\\nMost likely though... the ANOVA was not the best analysis in the first place.  You should probably be using mixed-effect modelling.  For the initial test of the accuracies you'd use mixed effects logistic regression.  For the second one you propose it would be a 3-levels x 2-correctnesses analysis of the latencies.  Both would have subjects as a random effect.\\n\\nIn addition it's often best to do some sort of normality correction on the times like a log or -1/T correction.  This is less of a concern in ANOVA because you aggregate across a number of means first and that often ameliorates the skew of latencies through the central limit theorem.  You could check with a boxcox analysis to see what fits best.\\n\\nOn a more important note though... what are you expecting to find?  Is this just exploratory?  What would it mean to have different latencies in the correct and incorrect groups and what would it mean for them to interact?  Unless you are fully modelling the relationship between accuracy and speed in your experiment, or you have a full model that you are testing, then you are probably wasting your time.  A latency with an incorrect response means that someone did something other than what you wanted them to... and it could be anything.  That's why people almost always only work with the latencies to the correct responses. \\n\\n(these two types of responses also often have very different distributions with incorrect much flatter because they disproportionately make up both the short and long latencies)\\n",deleted 4 characters in body,
4581,2,1984,65f811fa-a6b5-4148-8ea9-c3fb693dc351,2010-08-21 07:30:48.0,88.0,Also look at [Journal Of Statistical Software][1]; they encourage making papers in Sweave.\\n\\n\\n  [1]: http://www.jstatsoft.org/,,
4582,5,1983,a5753e47-5f28-41cd-8932-85996a40a14d,2010-08-21 07:36:31.0,5.0,"I have found good ones in the past and will post once I dig them up, but some quick general suggestions:\\n\\n 1. You may be able to find some interesting examples by searching google with keywords and ext:rnw (which will search for files with the sweave extension). Here's <a HREF=""http://www.google.com/m/search?oe=UTF-8&client=safari&hl=en&aq=f&oq=&aqi=-k0d0t0&fkt=1178&fsdt=6703&q=ext%3Arnw+paper"">an example search</a>.  This is the third result from my search: http://www.ne.su.se/paper/araietal_source.Rnw. Here's another example from my search: http://www.stat.umn.edu/geyer/gdor/. \\n 2. Many R packages have interesting vignettes which essentially amount to the same thing. An example: https://r-forge.r-project.org/scm/viewvc.php/paper/maxLik.Rnw\\n  ",added 78 characters in body,
4583,5,1980,384aa6f0-c1aa-4dce-afbc-44e8734103c1,2010-08-21 07:53:32.0,183.0,"**The Question:** Are there any good examples of [reproducible research][1] using R that are freely available online?\\n\\n**Ideal Example:** \\nSpecifically, ideal examples would provide:\\n\\n- The raw data (and ideally meta data explaining the data),\\n- All R code including data import, processing, analyses, and output generation,\\n- Sweave or some other approach for linking the final output to the final document,\\n- All in a format that is easily downloadable and compilable on a reader's computer.\\n\\nIdeally, the example would be a journal article or a thesis where the emphasis is on an actual applied topic as opposed to a statistical teaching example.\\n\\n**Reasons for interest:**\\nI'm particularly interested in applied topics in journal articles and theses, because in these situations, several additional issues arise:\\n\\n- Issues arise related to data cleaning and processing\\n- Issues arise related to managing metadata\\n- Journals and theses often have style guide expectations regarding the appearance and formatting of tables and figures\\n- Many journals and theses often have a wide range of analyses which raise issues regarding workflow (i.e., how to sequence analyses) and processing time (e.g., issues of caching analyses, etc.).\\n\\nSeeing complete working examples could provide good instructional material for researchers starting out with reproducible research.\\n\\n\\n  [1]: http://reproducibleresearch.net/index.php/Main_Page",added 765 characters in body,
4584,5,1975,470dc711-3544-4298-883a-a5eeefe76872,2010-08-21 10:48:49.0,603.0,"If you have more than two levels, you can use (M)CA:\\nhttp://en.wikipedia.org/wiki/Correspondence_analysis",edited body,
4585,2,1985,a02c501c-8557-4a23-b72e-73d629c5526d,2010-08-21 14:03:03.0,334.0,"Frank Harrell has been beating the drum on reproducible research and reports for many, many years.  You could start \\n[at this wiki page](http://biostat.mc.vanderbilt.edu/wiki/Main/StatReport) which lists plenty of other resources, including published research and also covers Charles Geyer's page.",,
4586,5,1947,167f5efd-6e4e-41a0-ae54-dbc33ff1085f,2010-08-21 16:46:55.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer falls into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$  (with $x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)) . Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can define quantile in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",edited body,
4587,2,1986,61533795-b0fd-4960-84c0-3f8039eb28ad,2010-08-21 17:13:07.0,919.0,"For nearly equal sample sizes you can translate *Tukey's HSD* (Google it) into a set of individual CIs.  For unequal sample sizes your approach may be doomed, because all pairwise comparisons cannot be reduced to pairwise comparisons of intervals: check out the literature on the *Tukey-Kramer Method* for details.  (I know Stata and SAS both do these computations; contributed package DTK does it in R.)",,
4588,2,1987,78f5a16c-737a-4d7c-af97-7e00beb4123b,2010-08-21 17:59:43.0,603.0,"You can also use the HST (mentioned here)\\n\\nhttp://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1630#1630\\n\\nIf $x$ is p.c. consumption, create a variable $x'=x-\\bar{x}$ (i.e. de-mean $x$).\\nThen use $f(\\bar{x},theta=1)$ as your explanatory variable (where $f$ is the inverse hyperbolic sin transform).\\n",,
4589,5,1987,e832046c-35b1-4861-806e-f9e68bd0e83a,2010-08-21 20:02:58.0,603.0,"You can also use the HST (mentioned here)\\n\\nhttp://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1630#1630\\n\\nIf $x$ is p.c. consumption, create a variable $x'=x-\\bar{x}$ (i.e. de-mean $x$).\\nThen use $f(x',theta=1)$ as your explanatory variable (where $f$ is the inverse hyperbolic sin transform).\\n",deleted 5 characters in body,
4590,2,1988,fc63e157-d01c-4908-b3f1-f4d9c6efc5e9,2010-08-21 20:33:14.0,666.0,"You can do it all on Excel.\\n\\nPlotting the six time series should give you a hint of the shapes of the curves. Let's say that, as you mentioned, five of the curves look like they're exponential and the sixth looks like it grows sub-linearly.\\n\\nInsert a trendline for each curve. If you are right, five of them will provide the best fit (as measured by r squared) with an exponential trendline, while the sixth will be best fitted to a logarithmic trendline.\\n\\nThis may sound non-deterministic, but if all six values of r squared are close to 1 you can be pretty confident of your result.",,
4591,2,1989,3a99dfe6-6d0c-4ea7-a860-edebabbbbab2,2010-08-21 21:19:39.0,930.0,"Maybe [Statistics Surveys][1] (but I think they are seeking review more than short note), [Statistica Sinica][2], or the [Electronic Journal of Statistics][3]. They are not as quoted as SPL, but I hope this may help.\\n\\n\\n  [1]: http://www.i-journals.org/ss/\\n  [2]: http://www3.stat.sinica.edu.tw/statistica/\\n  [3]: http://www.imstat.org/ejs/",,
4592,16,1989,3a99dfe6-6d0c-4ea7-a860-edebabbbbab2,2010-08-21 21:19:39.0,-1.0,,,
4593,2,1990,4ff37833-d771-46ec-9e2a-9d49dacd0fe6,2010-08-21 21:24:35.0,930.0,"The [European Association of Methodology][1] has a meeting turning around statistics and psychometrics for applied research in social, educational and psychological science every two years. The latest was held in [Postdam][2] two months ago.\\n\\n\\n  [1]: http://www.eam-online.org/\\n  [2]: http://www.iqb.hu-berlin.de/veranst/EAM-SMABS",,
4594,16,1990,4ff37833-d771-46ec-9e2a-9d49dacd0fe6,2010-08-21 21:24:35.0,-1.0,,,
4595,5,1987,a4827b08-e8ee-4983-9d92-c1e55c019a45,2010-08-21 21:26:16.0,603.0,"You can also use the HST (mentioned here)\\n\\nhttp://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1630#1630\\n\\nIf x is p.c. consumption, create a variable x'=x-\\bar{x} (i.e. de-mean x).\\nThen use f(x',theta=1) as your explanatory variable (where f is the inverse hyperbolic sin transform).\\n",deleted 2 characters in body; deleted 8 characters in body,
4596,2,1991,30f698c8-033c-412b-8d95-da18eadbdf33,2010-08-21 21:34:42.0,666.0,"We have a tendency to crunch data according to pre-established algorithms and methods, and forget that ""data"" is actually information about the real world. I recall as a child in school solving a second-degree equation where the teacher had stated that the answer represented the length of a pencil. Some students actually reported that the answer was ""one inch plus or minus two inches"".\\n\\nBefore you plug your data into any software, you should first get to *really* know and understand it, which you can only accomplish if you keep the subject matter in mind. That's the only way you can spot any quirky data points (such as a pencil measuring -1 inch) or determine which scales make sense in the real world.",,
4597,2,1992,621c900b-714a-4c4c-b113-0239d31c8c00,2010-08-21 21:36:07.0,1033.0,[AAAI (in Atlanta this year)][1]\\n\\n\\n  [1]: http://www.aaai.org/Conferences/conferences.php,,
4598,16,1992,621c900b-714a-4c4c-b113-0239d31c8c00,2010-08-21 21:36:07.0,-1.0,,,
4599,2,1993,4a92e2fd-217d-426d-93ce-ff9438863b55,2010-08-21 21:59:56.0,8.0,"We wrote a paper explaining how to use R/Bioconductor when analysing microarray data. The paper was written in Sweave and all the code used to generate the graphs is given included as supplementary material.\\n\\nGillespie, C. S., Lei, G., Boys, R. J., Greenall, A. J., Wilkinson, D. J., 2010. [Analysing yeast time course microarray data using BioConductor: a case study using yeast2 Affymetrix arrays BMC][1] Research Notes,  3:81.\\n\\n\\n  [1]: http://www.mas.ncl.ac.uk/~ncsg3/microarray/",,
4600,5,1993,400450bf-a3fd-4bb5-be6e-3e93df49e02b,2010-08-21 22:41:19.0,8.0,"We wrote a paper explaining how to use R/Bioconductor when analysing microarray data. The paper was written in Sweave and all the code used to generate the graphs is included as supplementary material.\\n\\nGillespie, C. S., Lei, G., Boys, R. J., Greenall, A. J., Wilkinson, D. J., 2010. [Analysing yeast time course microarray data using BioConductor: a case study using yeast2 Affymetrix arrays BMC][1] Research Notes,  3:81.\\n\\n\\n  [1]: http://www.mas.ncl.ac.uk/~ncsg3/microarray/",deleted 6 characters in body,
4601,2,1994,57c87523-3d32-41ec-ba7c-1836908fb2c1,2010-08-21 22:50:51.0,1034.0,"Using variables in logs is actually quite common in economics, since the estimated coefficients can be interpreted as sensitivities to relative changes in RHS variables (or elasticities, if both LHS and RHS variables are in logs). For example, say that you have model *y = b ln(x)*, and *x* changes to *x(1+r)*. Then you can use the approximation $ln(1+t) \\approx t$ to see how *y* changes:\\n$$y = b \\ln(x(1+r)) = b \\ln(x) + b \\ln(1+r) \\approx b \\ln(x) + b r.$$\\nSo if *r* is 0.01 (*x* increases by 1%), *y* increases by *b r = 0.01 b* (of course, this works only for small *r*). In case of your probit model, if coefficient for log-consumption is *b*, it can be interpreted so that increase in consumption by 1% would increase probability of enrollment by *b* %.",,
4602,5,1987,8686ac54-dec4-48ce-9a96-822c6507e385,2010-08-21 23:25:15.0,603.0,"You can also use the HST (mentioned here)\\n\\nhttp://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros/1630#1630\\n\\nIf x is p.c. consumption, create a variable x'=x-\\bar{x} (i.e. de-mean x).\\nThen use f(x',theta=1) as your explanatory variable (where f is the inverse hyperbolic sin transform).\\n\\n\\nfor positive values of x' (i.e. people who consume more than the average) \\nf(x',theta=1) behaves as log(x').\\n\\nfor negative values of x' (i.e. people who consume less than the average) \\nf(x',theta=1) behaves as -log(-x').\\n\\n(f(x',theta=1) looks like a big 'S', passing by the origin).\\n\\n\\n",added 298 characters in body,
4603,2,1995,fa517249-b7e1-41ed-865e-7729db77b8ee,2010-08-22 00:22:33.0,835.0,"Under which conditions should someone consider using multilevel/hierarchical analysis as opposed to more basic/traditional analyses (e.g., ANOVA, OLS regression, etc.)? Are there any situations in which this could be considered mandatory? Are there situations in which using multilevel/hierarchical analysis is inappropriate? Finally, what are some good resources for beginners to learn multilevel/hierarchical analysis?",,
4604,1,1995,fa517249-b7e1-41ed-865e-7729db77b8ee,2010-08-22 00:22:33.0,835.0,Under what conditions should one use multilevel/hierarchical analysis,,
4605,3,1995,fa517249-b7e1-41ed-865e-7729db77b8ee,2010-08-22 00:22:33.0,835.0,<statistical-analysis><statistics>,,
4606,2,1996,4accebfb-42f3-4d12-b09b-e615c7bddc29,2010-08-22 00:34:17.0,74.0,"""It ain’t what you don’t know that gets you into trouble. It’s what you know for sure that just ain’t so.""\\n\\nMark Twain (fine, so he's not a statistician)",,
4607,16,1996,4accebfb-42f3-4d12-b09b-e615c7bddc29,2010-08-22 00:34:17.0,-1.0,,,
4608,2,1997,27a87a51-c184-417c-a620-eeb72ef21ee2,2010-08-22 00:40:31.0,251.0,"When the structure of your data is naturally hierarchical or nested, multilevel modeling is a good candidate.  More generally, it's one method to model interactions.\\n\\nA natural example is when your data is from an organized structure such as country, state, districts, where you want to examine effects at those levels.  Another example where you can fit such a structure is is longitudinal analysis, where you have repeated measurements from many subjects over time (e.g. some biological response to a drug dose).  One level of your model assumes a group mean response for all subjects over time.  Another level of your model then allows for perturbations (random effects) from the group mean, to model individual differences.  \\n\\nA popular and good book to start with is Gelman's [*Data Analysis Using Regression and Multilevel/Hierachical Models*][1].  \\n\\n\\n  [1]: http://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/\\n  \\n",,
4609,2,1998,6444a790-a5ce-4a59-8a49-4aab449b7ba2,2010-08-22 00:53:28.0,835.0,"Miller and Chapman (2001) argue that it is absolutely inappropriate to control for non-independent covariates that are related to both the independent and dependent variables in an observational (non-randomized) study - even though this is routinely done in the social sciences. How problematic is it to do so? How is the best way to deal with this problem? If you routinely control for non-independent covariates in an observational study in your own research, how do you justify it? Finally, is this a fight worth picking when arguing methodology with ones colleagues (i.e., does it really matter)?\\n\\nThanks\\n----------------------\\nMiller, G. A., & Chapman, J. P. (2001). Misunderstanding analysis of covariance. Journal\\nof Abnormal Psychology, 110, 40-48. - http://mres.gmu.edu/pmwiki/uploads/Main/ancova.pdf",,
4610,1,1998,6444a790-a5ce-4a59-8a49-4aab449b7ba2,2010-08-22 00:53:28.0,835.0,"How problematic is it to control for non-independent covariates in an observational (i.e., non-randomized) study?",,
4611,3,1998,6444a790-a5ce-4a59-8a49-4aab449b7ba2,2010-08-22 00:53:28.0,835.0,<statistical-analysis><statistics><analysis>,,
4612,2,1999,33b77ba0-f140-42ef-9843-2d04a2ce5d26,2010-08-22 01:04:54.0,,"Generally, speaking a hierarchical bayesian (HB) analysis will lead to efficient and stable individual level estimates unless your data is such that individual level effects are completely homogeneous (an unrealistic scenario). The efficiency and stable parameter estimates of HB models becomes really important when you have sparse data (e.g., less no of obs than the no of parameters at the individual level) and when you want to estimate individual level estimates.\\n\\nHowever, HB models are not always easy to estimate. Therefore, while HB analysis usually trumps non-HB analysis you have to weigh the relative costs vs benefits based on your past experience and your current priorities in terms of time and cost.\\n\\nHaving said that if you are not interested in individual level estimates then you can simply estimate an aggregate level model but even in these contexts estimating aggregating models via HB using individual level estimates may make a lot of sense. \\n\\nIn summary, fitting HB models is the recommended approach as long as you have the time and the patience to fit them. You can then use aggregate models as a benchmark to assess the performance of your HB model.",,user28
4613,2,2000,48d03a2b-3f3b-457b-aabe-87d7e2bedde5,2010-08-22 01:22:31.0,,"I read the first page of their paper and so I may have misunderstood their point but it seems to me that they are basically discussing the problem of including multi-collinear independent variables in the analysis. The example they take of age and grade illustrates this idea as they state that:\\n\\n> Age is so intimately associated with grade in school that\\nremoval of variance in basketball ability associated with age would\\nremove considerable (perhaps nearly all) variance in basketball\\nability associated with grade\\n\\n\\nANCOVA is linear regression with the levels represented as dummy variables and the covariates also appearing as independent variables in the regression equation. Thus, unless I have misunderstood their point (which is quite possible as I have not read their paper completely) it seems they are saying 'do not include dependent covariates' which is equivalent to stating avoid multi-collinear variables.",,user28
4614,5,997,dcafaf66-489d-4c7b-94b2-73cc9ff7ff31,2010-08-22 01:43:00.0,25.0,"The huge denominators throw off one's intuition. Since the sample sizes are identical, and the proportions low, the problem can be recast: 13 events occurred, and were expected (by null hypothesis) to occur equally in both groups. In fact the split was 3 in one group and 10 in the other. How rare is that? The binomial test answers.\\n\\nEnter this line into R:\\nbinom.test(3,13,0.5,alternative=""two.sided"")\\n\\nThe two-tail P value is 0.09229, identical to four digits to the results of Fisher's test. \\n\\nLooked at that way, the results are not surprising. The problem is equivalent to this one: If you flipped a coin 13 times, how surprising would it be to see three or fewer, or ten or more, heads. One of those outcomes would occur 9.23% of the time. \\n\\n",typo,
4615,2,2001,832d5a66-945e-4d56-88f1-14698a43158a,2010-08-22 02:10:22.0,601.0,"It is as problematic as the degree of correlation.  \\n\\nThe irony is that you wouldn't bother controlling if there weren't some expected correlation with one of the variables.  And, if you expect your independent variable to affect your dependent then it's necessarily somewhat correlated with both.  However, if it's highly correlated them perhaps you shouldn't be controlling for it since it's tantamount to controlling out the actual independent or dependent variable.\\n\\n",,
4616,2,2002,59b2ea1e-1a9b-41a6-9e2c-a3047d2e49f7,2010-08-22 02:24:24.0,,"I am running LOESS regression models in R, and I want to compare the outputs of 12 different models with varying sample sizes. I can describe the actual models in more details if it helps with answering the question. \\n\\nHere are the sample sizes: \\n\\n    Fastballs vs RHH 2008-09: 2002\\n    Fastballs vs LHH 2008-09: 2209\\n    Fastballs vs RHH 2010: 527 \\n    Fastballs vs LHH 2010: 449\\n    \\n    Changeups vs RHH 2008-09: 365\\n    Changeups vs LHH 2008-09: 824\\n    Changeups vs RHH 2010: 201\\n    Changeups vs LHH 2010: 330\\n    \\n    Curveballs vs RHH 2008-09: 488\\n    Curveballs vs LHH 2008-09: 483\\n    Curveballs vs RHH 2010: 213\\n    Curveballs vs LHH 2010: 162\\n\\nThe LOESS regression model is a surface fit, where the X location and the Y location of each baseball pitch is used to predict sw, swinging strike probability. However, I'd like to compare between all 12 of these models, but setting the same span (i.e. span = 0.5) will bear different results since there is such a wide range of sample sizes.\\n\\nMy basic question is how do you determine the span of your model? A higher span smooths out the fit more, while a lower span captures more trends but introduces statistical noise if there is too little data. I use a higher span for smaller sample sizes and a lower span for larger sample sizes. \\n\\nWhat should I do? What's a good rule of thumb when setting span for LOESS regression models in R? Thanks in advance!",,Think Blue Crew
4617,1,2002,59b2ea1e-1a9b-41a6-9e2c-a3047d2e49f7,2010-08-22 02:24:24.0,,How do I decide what span to use in LOESS regression in R?,,Think Blue Crew
4618,3,2002,59b2ea1e-1a9b-41a6-9e2c-a3047d2e49f7,2010-08-22 02:24:24.0,,<r><regression>,,Think Blue Crew
4619,2,2003,72143e51-eab0-4059-a942-47d354b7bd87,2010-08-22 02:46:05.0,364.0,"I suggest checking out generalized additive models (GAM). I'm just learning about them myself, but they seem to automatically figure out how much ""wiggly-ness"" is justified by the data. I also see that you're dealing with binomial data (strike vs not a strike), so be sure to analyze the raw data (i.e. don't aggregate to proportions) and use family='binomial' (assuming that you're going to use R). If you have information about what individual pitchers and hitters are contributing to the data, you can probably increase your power by doing a generalized additive mixed model (GAMM, see the gamm4 package in R) and specifying pitcher and hitter as random effects (and again, setting family='binomial').",,
4620,2,2004,dc953971-55a1-4948-b76a-da74afc56a05,2010-08-22 02:48:20.0,1036.0,"[The Centre for Multilevel Modelling][1] has some good free online tutorials for multi-level modeling, and they have software tutorials for fitting models in both their MLwiN software and STATA.\\n\\n\\nTake this as heresy, because I have not read more than a chapter in the book, but Hierarchical linear models: applications and data analysis methods By Stephen W. Raudenbush, Anthony S. Bryk comes highly recommended. I also swore there was a book on multi level modeling using R software in the Springer Use R! series, but I can't seem to find it at the moment (I thought it was written by the same people who wrote the A Beginner’s Guide to R book). \\n\\ngood luck\\n\\n  [1]: http://www.cmm.bristol.ac.uk/learning-training/index.shtml\\n",,
4621,5,2003,c847568f-f700-486d-88e6-fa0a0782c5ca,2010-08-22 02:59:41.0,364.0,"I suggest checking out generalized additive models (GAM, see the mgcv package in R). I'm just learning about them myself, but they seem to automatically figure out how much ""wiggly-ness"" is justified by the data. I also see that you're dealing with binomial data (strike vs not a strike), so be sure to analyze the raw data (i.e. don't aggregate to proportions) and use family='binomial' (assuming that you're going to use R). If you have information about what individual pitchers and hitters are contributing to the data, you can probably increase your power by doing a generalized additive mixed model (GAMM, see the gamm4 package in R) and specifying pitcher and hitter as random effects (and again, setting family='binomial'). Finally, you probably want to allow for an interaction between the smooths of X & Y, but I've never tried this myself so I don't know how to go about that.","added reference to mgcv package in R, added mention of X&Y interaction",
4622,2,2005,48cae99f-eb44-4ac2-a402-11c9bdcef4d0,2010-08-22 03:05:50.0,5186.0,"You're probably already aware of it, but the Society for Mathematical Psychology has an annual conference, MathPsych, which is attached to CogSci (generally happnens in the same city either before or after) and blends statistical methodology and Psychological modeling.\\n\\nThey do a pretty good job getting big names to come present, it's pretty cutting edge.\\n\\n2010 conference site: http://www.mathpsych.org/conferences/2010/",,
4623,16,2005,48cae99f-eb44-4ac2-a402-11c9bdcef4d0,2010-08-22 03:05:50.0,-1.0,,,
4624,2,2006,0deea70a-8763-4cc5-ba0b-9352975c5898,2010-08-22 03:11:06.0,485.0,"As I see it, there are two basic problems with observational studies that ""control for"" a number of independent variables.  1) You have the problem of missing explanatory variables and thus model misspecification.  2) You have the problem of multiple correlated independent variables--a problem that does not exist in (well) designed experiment and the fact that regression coefficients and ANCOVA tests of covariates are based on partials--the effects of one variable after controlling for the others in the model.  The first is intrinsic to the nature of observational research and is addressed in scientific context and the process of competitive elaboration.  The latter is an issue of interpretation and relies on a clear understanding of regression and ANCOVA models and exactly what those coefficients represent.\\n\\nWith respect to the first issue, it is easy enough to demonstrate that if all of the influences on some dependent variable are known and included in a model, statistical methods of control are effective and produce good estimates of effects.  The problem in the ""soft sciences"" is that all of the relevant influences are rarely included or even known and thus the models are poorly specified and difficult to interpret.  Yet, many worthwhile problems exist in these domains.  They simply lack certainty.  The beauty of the scientific process is that it is self corrective and models are questioned, elaborated, and refined.  The alternative is to suggest that we cannot investigate these issues scientifically.\\n\\nThe second issue is a technical issue in the nature of ANCOVA and regression models.  Analysts need to be clear about what these coefficients and tests represent.  Correlations among the independent variables influence regression coefficients and ANCOVA tests.  They are tests of partials.  These models take out the variance in a given independent variable and the dependent variable that are associated with all of the other variables in the model and then examine the relationship in those residuals.  As a result, the individual coefficients and tests are very difficult to interpret outside of the context of a clear conceptual understanding of the entire set of variables included and their interrelationships.  This, however, produces NO problems for prediction--just be cautious about interpreting specific tests and coefficients.\\n\\n**A side note:**  This issue is related to a problem discussed previously in this forum on the reversing of regression signs--e.g., from negative to positive--when other predictors are intorduced.  In the presence of correlated predictors and without a clear understanding of the multiple and complex relationships among the set of predictors, there is no reason to EXPECT a (by nature partial) regression coefficient to have a particular sign.  When there is strong theory and a clear understanding of those interrelationships, such sign ""reversals"" can be enlightening and theoretically useful.  Though, given the complexity of many social science problems sufficient understanding would not be common, I would expect.\\n\\n**Disclaimer:**  I'm a sociologist and public policy analyst by training.\\n\\n",,
4625,5,2003,9673c392-7efb-4849-a66d-f8c9e232eeb1,2010-08-22 03:14:30.0,364.0,"I suggest checking out generalized additive models (GAM, see the mgcv package in R). I'm just learning about them myself, but they seem to automatically figure out how much ""wiggly-ness"" is justified by the data. I also see that you're dealing with binomial data (strike vs not a strike), so be sure to analyze the raw data (i.e. don't aggregate to proportions, use the raw pitch-by-pitch data) and use family='binomial' (assuming that you're going to use R). If you have information about what individual pitchers and hitters are contributing to the data, you can probably increase your power by doing a generalized additive mixed model (GAMM, see the gamm4 package in R) and specifying pitcher and hitter as random effects (and again, setting family='binomial'). Finally, you probably want to allow for an interaction between the smooths of X & Y, but I've never tried this myself so I don't know how to go about that. A gamm4 model without the X*Y interaction would look like:\\n\\n	fit = gamm4(\\n		formula = strike ~ s(X) + s(Y) + pitch_type*batter_handedness + (1|pitcher) + (1|batter)\\n		, data = my_data\\n		, family = 'binomial'\\n	)\\n	summary(fit$gam)\\n\\nCome to think of it, you probably want to let the smooths vary within each level of pitch type and batter handedness. This makes the problem more difficult as I've not yet found out how to let the smooths vary by multiple variables in a way that subsequently produces meaninful analytic tests ([see my queries to the R-SIG-Mixed-Models list][1]). You could try:\\n\\n	my_data$dummy = factor(paste(my_data$pitch_type,my_data$batter_handedness))\\n	fit = gamm4(\\n		formula = strike ~ s(X,by=dummy) + s(Y,by=dummy) + pitch_type*batter_handedness + (1|pitcher) + (1|batter)\\n		, data = my_data\\n		, family = 'binomial'\\n	)\\n	summary(fit$gam)\\n\\nBut this won't give meaningful tests of the smooths. In attempting to solve this problem myself, I've used bootstrap resampling where on each iteration I obtain the model predictions for the full data space then compute the bootstap 95% CIs for each point in the space and any effects I care to compute.\\n\\n  [1]: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004170.html",added example of gamm4 and discussion of troubles with characterizing a smooth that interacts with multiple predictor variables,
4626,2,2007,2d146ee6-037d-45ea-9346-1d7b9a7e0139,2010-08-22 03:24:28.0,1026.0,"Although I was trained as an engineer, I find that I'm becoming more interested in data mining.  Right now I'm trying to investigate the field further.  In particular, I would like to understand the different categories of software tools that exist and which tools are notable in each category and why. (Note that I didn't say the ""best"" tools, just the notable ones lest we start a flame war.)  Especially make note of the tools that are open-source and freely available - although don't take this to mean that I'm only interested in open-source and free.",,
4627,1,2007,2d146ee6-037d-45ea-9346-1d7b9a7e0139,2010-08-22 03:24:28.0,1026.0,A survey of data-mining software tools.,,
4628,3,2007,2d146ee6-037d-45ea-9346-1d7b9a7e0139,2010-08-22 03:24:28.0,1026.0,<data-mining>,,
4629,2,2008,99ea9781-db34-456d-b1af-f86c4749f230,2010-08-22 04:16:03.0,253.0,"I am looking through articles citing an article I'm reading - and I wish to judge how much that citing article is ""important"" or ""good"" by itself.\\n\\none way of knowing would be if I had known how ""distinguished"" that journal was.  Which leads me to my question:\\n\\n**What measures are there (and where can I find them) for a journal ""importance"" or ""impact""?**\\n\\n(I know of Impact Factor score.  But I wonder if there are other such measures, and which are more applicable to statistical journals)",,
4630,1,2008,99ea9781-db34-456d-b1af-f86c4749f230,2010-08-22 04:16:03.0,253.0,"Measures of publication ""importance"" in statistics ?",,
4631,3,2008,99ea9781-db34-456d-b1af-f86c4749f230,2010-08-22 04:16:03.0,253.0,<journals>,,
4632,16,2008,99ea9781-db34-456d-b1af-f86c4749f230,2010-08-22 04:16:03.0,253.0,,,
4636,2,2010,c1b05ae1-4518-48a9-b2fb-c0ae79ce9715,2010-08-22 04:51:46.0,862.0,"If <i>x1, x2,..</i> are identically distributed random variables with given mean, and N is a random variable >= 0 and N is independent of </i>x1,x2,..</i>. If <code>y=x_1+x_2+...x_n</code>. How do we find <code>E(y|N=n)</code>?\\n\\n<code>\\nE(y|N=n) = sum_Y(y*P(Y=y|N=n)) = sum_Y(y*P(Y=y,N=n)/P(N=n))\\n</code>\\n\\nI am not sure how to count/compute <code>P(Y=y,N=n)</code>. How do compute it?",,
4637,1,2010,c1b05ae1-4518-48a9-b2fb-c0ae79ce9715,2010-08-22 04:51:46.0,862.0,conditional expectation given number of outcomes,,
4638,3,2010,c1b05ae1-4518-48a9-b2fb-c0ae79ce9715,2010-08-22 04:51:46.0,862.0,<probability><conditional-probability>,,
4640,2,2012,91214d05-a3d7-42b7-b317-81444ae35d79,2010-08-22 06:37:15.0,74.0,this might be a good place to start:\\n\\n[http://www.eigenfactor.org/whyeigenfactor.htm][1]\\n\\n\\n  [1]: http://www.eigenfactor.org/whyeigenfactor.htm,,
4641,16,2012,91214d05-a3d7-42b7-b317-81444ae35d79,2010-08-22 06:37:15.0,-1.0,,,
4642,2,2013,36dff0d2-6a46-48a0-82a7-0d77581dd6af,2010-08-22 07:27:32.0,930.0,Have a look at \\n\\n- [Weka][1] (java)\\n- [Orange][2]\\n- the open-source [R][3] statistical software (Check the [Machine Learning][4] taskview)\\n\\nand the [UCI Machine Learning Repository][5].\\n\\n\\n  [1]: http://www.cs.waikato.ac.nz/ml/weka/\\n  [2]: http://www.ailab.si/orange/\\n  [3]: http://www.r-project.org\\n  [4]: http://cran.r-project.org/web/views/MachineLearning.html\\n  [5]: http://archive.ics.uci.edu/ml/,,
4651,16,2013,00000000-0000-0000-0000-000000000000,2010-08-22 09:47:37.0,88.0,,,
4652,16,2007,00000000-0000-0000-0000-000000000000,2010-08-22 09:47:37.0,88.0,,,
4653,2,2016,1c6aad66-aaf6-4d95-9a2d-24121fc79c11,2010-08-22 10:02:58.0,183.0,"Anne-Wil Harzing has a useful site with some free software called [Publish or Perish][1].\\nThe site discusses a number of journal, article, and author impact factor metrics.\\nThe software uses Google Scholar to calculate citation based impact factor metrics.\\n\\n  [1]: http://www.harzing.com/pop.htm",,
4654,16,2016,1c6aad66-aaf6-4d95-9a2d-24121fc79c11,2010-08-22 10:02:58.0,-1.0,,,
4655,2,2017,60b203ad-33a1-492a-a82f-1496f8cdccf9,2010-08-22 10:07:23.0,183.0,[Rattle][1] is a data mining GUI that provides a front end to a wide range of R packages.\\n\\n\\n  [1]: http://rattle.togaware.com/,,
4656,16,2017,60b203ad-33a1-492a-a82f-1496f8cdccf9,2010-08-22 10:07:23.0,-1.0,,,
4657,2,2018,a042bc73-f202-49e8-97db-cc2f05a936fb,2010-08-22 10:19:09.0,183.0,Some of the matching tools developed by Gary King and colleagues look promising:\\n\\n- [Software][1]\\n- [Video providing a tutorial in R][2]\\n\\n\\n  [1]: http://gking.harvard.edu/stats.shtml\\n  [2]: http://www.vcasmo.com/video/drewconway/8105,,
4658,2,2019,96ffc978-0dbe-4e08-9e18-a1ffb8536a3d,2010-08-22 10:29:46.0,,You can use the fact that expectation is [linear][1] and compute $E(Y|N=n)$. The fact that $N$ is a random variable does not matter as you are computing the expected value of $Y$ conditional on a specific value of $N$. \\n\\n  [1]: http://en.wikipedia.org/wiki/Expected_value#Linearity,,user28
4659,2,2020,dc22448a-7393-4f75-9987-99d04d283a3e,2010-08-22 10:30:13.0,183.0,"> This is the essence of bootstrapping:\\n> taking different samples of your data,\\n> getting a statistic for each sample\\n> (e.g., the mean, median, correlation,\\n> regression coefficient, etc.), and\\n> using the variability in the statistic\\n> across samples to indicate something\\n> about the standard error and\\n> confidence intervals for the\\n> statistic.\\n- [Bootstrapping and the boot package in R][1]\\n\\n\\n  [1]: http://jeromyanglim.blogspot.com/2009/05/bootstrapping.html",,
4660,2,2021,5c1c5519-98ea-4420-a59e-947c08305878,2010-08-22 10:47:03.0,979.0,"There's a nice website: http://www.arnetminer.org/page/conference-rank/html/Journal.html - however, it only contains computer science conferences. \\n\\nIt's an interesting question because publication venue is an extremely important factor for the reception of an article - most of these concepts however live inside the heads of the people working in the field; all these metrics imo just try to mimic those concepts. Although having argued in a comment to your question in favor of metrics, actually looking (even superficially) at as many articles as possible from different sources should let you develop an intuition about good and bad publication venues, which is, on the long run, probably the most efficient tool you can get.",,
4661,16,2021,5c1c5519-98ea-4420-a59e-947c08305878,2010-08-22 10:47:03.0,-1.0,,,
4662,2,2022,1f7504ab-27d5-42d8-b7d0-3ad4b5d25823,2010-08-22 11:24:41.0,183.0,"Assume:\\n\\n - A previous study looking at the relationship between $X$ and $Y$ obtained a correlation of $r = 0.50$ using a sample of $n = 100$. The raw data is not available.\\n - The current study also looking at the relationship between $X$ and $Y$ obtained a correlation of $r = 0.45$ with $n = 50$.\\n\\nHow would you do the following tasks:\\n\\n 1. Give your best estimate of the population correlation between $X$ and $Y$ assuming the two studies are estimating the same correlation.\\n 2. Give your best estimate of the population correlation between $X$ and $Y$ in the current study\\n   assuming that the first study is slightly different than the current study (e.g., it used a different measurement procedure, a different type of sample, etc.). Of course the weight given to the previous study would depend on perceived similarity with the second study. Thus, are there standard ways of quantifying similarity between studies in such calculations?",,
4663,1,2022,1f7504ab-27d5-42d8-b7d0-3ad4b5d25823,2010-08-22 11:24:41.0,183.0,Estimating population correlation based on current data and a previous study,,
4664,3,2022,1f7504ab-27d5-42d8-b7d0-3ad4b5d25823,2010-08-22 11:24:41.0,183.0,<bayesian><correlation><meta-analysis>,,
4665,5,1763,3cb5e5cf-91f8-4c29-8540-5f70fb2aa6f6,2010-08-22 12:05:01.0,183.0,I think a number of the suggestions put forward on the [mathematical statistics video question][1] probably fall in the stats 101 category:\\n\\n- http://www.khanacademy.org/#Statistics: series of short videos on introductory statistics\\n- http://www.khanacademy.org/#Probability: series of short videos on introductory probability\\n- [Math and probability for life sciences][2]: A whole university course introducing statistics and probability.\\n\\nI also have a [list of maths and statistics videos][4] with a few others.\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/485/mathematical-statistics-videos\\n  [2]: http://www.academicearth.org/courses/math-and-proability-for-life-sciences\\n  [3]: http://www.public.iastate.edu/~hofmann/stat579/\\n  [4]: http://jeromyanglim.blogspot.com/2009/05/online-mathematics-video-courses-for.html,deleted 88 characters in body,
4666,2,2023,27158ddc-6c63-4b3e-903f-22b65c2a0596,2010-08-22 13:52:09.0,,The article on [Combinative Properties of Correlation Coefficients][1] may have the answer to your question.\\n\\n\\n  [1]: http://www.jstor.org/stable/20150454,,user28
4667,5,1955,d81dd937-749b-415b-b421-c3f5ef163e0c,2010-08-22 14:07:19.0,174.0,"*(Prompted to some extent by the answers already given by Shane and Srikant, I've rewritten this to try to clarify what I'm getting at, if only to myself.)*\\n\\nSuppose we have several similar systems, each with behaviour that approximates a continuous time Markov process. That is, there are some number of discrete states the system can be in and associated probabilities of transitioning from one state to another at any instant, depending solely on the current state. For now, consider the processes to be *stationary*, ie the transition probabilities do not change over time, and unaffected by seasonality or other external considerations.\\n\\nUnfortunately, we cannot measure the state of any system directly, but have instead to measure a proxy quantity, which varies with state but is *not* discrete and is subject to various sources of noise and error.\\n\\nThe principal question is this:\\n\\n**Q1: Given two data sequences produced independently from two such systems, how can we decide whether the underlying Markov processes are the same?**\\n\\nNow, it *may* be that the best way to approach this is as two separate problems:\\n\\n1. Convert the imperfect proxy sequence into an idealised time series of (categorical) states\\n2. Determine whether the state sequences correspond\\n\\nOn the other hand, such a separation *might* involve discarding some information from the data in step 1 (eg, about its variability) that would be useful in step 2. Which leads to:\\n\\n**Q2: Does it make sense to decompose the problem in this way or is it better to compare the proxy data directly?**\\n\\nIf such a decomposition does make sense, that opens up a whole other issue about how to do the idealisation, but that's definitely a question for another day.\\n\\nShane, below, mentions goodness-of-fit and distributional tests such as Anderson-Darling, and that seems like a promising approach. But I'd like to check I'm understanding the idea correctly.\\n\\nGiven sufficient samples in a sequence, we would expect the proportion of time spent in each state to tend to the stationary distribution. So one could test the distributions of occupancies in the two sequences for similarity. (I have the vague sense a two-sample Kolmogorov-Smirnov might suit for this, but please set me right about that.)\\n\\nThe thing is, I'm not sure how good this can be as evidence. If the distributions are very different, that seems like a reasonable strike against the underlying processes being the same, but what if they're very similar? Can we draw conclusions in that direction?\\n\\n**Q3: Does a good fit of occupancy distributions tell us anything useful?**\\n\\nIt seems like there could be an infinite number of processes that will tend to the same stationary distribution. I *think* this is very unlikely in practice, and that different systems will tend to have distinctly different behaviour, but still it's worth considering.\\n\\nFinally, we will often have a model of the underlying process that we are looking for, although it may not be perfect. So we could compare each sequence with the expected behaviour of the model, instead of with each other. We may also have more than two sequences to test.\\n\\n**Q4: Is it better to compare multiple sequences to a single model, even an approximate one, or are we better off comparing data directly?**","substantial rewrite, breaking down into several different questions",
4668,4,1955,d81dd937-749b-415b-b421-c3f5ef163e0c,2010-08-22 14:07:19.0,174.0,Comparing noisy data sequences to estimate the likelihood of them being produced by different instances of an identical Markov process,"substantial rewrite, breaking down into several different questions",
4669,6,1955,d81dd937-749b-415b-b421-c3f5ef163e0c,2010-08-22 14:07:19.0,174.0,<time-series><markov-process><goodness-of-fit>,"substantial rewrite, breaking down into several different questions",
4670,2,2024,f27f7eaa-1af0-4486-8b63-d277fc524ee6,2010-08-22 14:18:56.0,,This is a very basic questions however I have not been able to find an answer.\\nWhen you plot your regular bar plot with your standard errors of the means for comparison do you plot the standard errors and means from the raw data ?  \\nOr do you plot the ones predicted by the model you are fitting. Thank you in advance for your help. I usually use the latter please advice.\\n\\n\\nAlfred,,Alfred
4671,1,2024,f27f7eaa-1af0-4486-8b63-d277fc524ee6,2010-08-22 14:18:56.0,,Plotting standard errors,,Alfred
4672,3,2024,f27f7eaa-1af0-4486-8b63-d277fc524ee6,2010-08-22 14:18:56.0,,<confidence-interval>,,Alfred
4675,2,2026,c015dc56-dbe0-4e4e-b1cf-d885db895830,2010-08-22 15:08:47.0,635.0,This is probably the most comprehensive list you'll find: [mloss.org][1]\\n\\n\\n  [1]: http://mloss.org/software/,,
4676,16,2026,c015dc56-dbe0-4e4e-b1cf-d885db895830,2010-08-22 15:08:47.0,-1.0,,,
4677,2,2027,f4afe05e-51c0-4f64-bb78-65e92f05384a,2010-08-22 15:33:16.0,601.0,"The list of things to say here...\\n\\nAs Tai said, it's hard to directly answer your question without information on the actual model.  Nevertheless, it's usually good to present data reflective of the model.  Typically that is the means with t-tests or ANOVAs.  With something else it's probably close to that.\\n\\nHow does a standard error on a graph work for comparison?  Are you going to be putting the N on the graph and the multiplying factor needed for comparisons?\\n\\nHow many data points do you have?  You could probably just put up the entire data set, with a line indicating your predicted value and some kind of measure of variability around it.  Perhaps even an overlayed boxplot.\\n\\nThe measure of variability should reflect what you want to say about the data.  If you just want people to compare values then std. err. isn't a very good idea because it's dependent upon n and requires some value of multiplication greater than 2 (i.e. bars have to not overlap by some amount for significant effects).  If you want something like that put up 0.5*LSD bars (comparison bars) (about an 84% confidence interval, or a 0.5 * 95%CI * sqrt(2)).  Those bars would show significant differences at the point of bar overlap.  \\n\\nOr, you could be wanting to represent how well you estimated the predicted values.  In that case a more convenient confidence interval (about 95%) or even the std. err would be good.  Or, if you want to reflect the estimated variability of the population you put up the standard deviation.\\n",,
4678,5,1763,258c5256-a799-4ccd-8314-54fb847420a4,2010-08-22 15:34:38.0,183.0,I think a number of the suggestions put forward on the [mathematical statistics video question][1] probably fall in the stats 101 category:\\n\\n- http://www.khanacademy.org/#Statistics: series of short videos on introductory statistics\\n- http://www.khanacademy.org/#Probability: series of short videos on introductory probability\\n- [Math and probability for life sciences][2]: A whole university course introducing statistics and probability.\\n- I also have a [list of maths and statistics videos][4] with a few others.\\n\\n\\nThe Stat 579 Videos are also quite good:\\n\\n- http://connect.extension.iastate.edu/p31588910/\\n- http://connect.extension.iastate.edu/p45341752/\\n- http://connect.extension.iastate.edu/p39131597/\\n- http://connect.extension.iastate.edu/p21949344/\\n- http://connect.extension.iastate.edu/p77697317/\\n- http://connect.extension.iastate.edu/p62985380/\\n- http://connect.extension.iastate.edu/p77697317/\\n- http://connect.extension.iastate.edu/p54242569/\\n- http://connect.extension.iastate.edu/p95967207/\\n- http://connect.extension.iastate.edu/p80966639/\\n- http://connect.extension.iastate.edu/p51894198/\\n- http://connect.extension.iastate.edu/p39993940/\\n\\n\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/485/mathematical-statistics-videos\\n  [2]: http://www.academicearth.org/courses/math-and-proability-for-life-sciences\\n  [3]: http://www.public.iastate.edu/~hofmann/stat579/\\n  [4]: http://jeromyanglim.blogspot.com/2009/05/online-mathematics-video-courses-for.html,added 658 characters in body; added 6 characters in body,
4679,5,2027,f79c1dd6-b4bd-4bdb-b432-706e02ee4cd3,2010-08-22 15:54:54.0,601.0,"The list of things to say here...\\n\\nAs Tai's question suggests, it's hard to directly answer your question without information on the actual model.  Nevertheless, it's usually good to present data reflective of the model.  Typically that is the means with t-tests or ANOVAs.  With something else it's probably close to that.\\n\\nHow does a standard error on a graph work for comparison?  Are you going to be putting the N on the graph and the multiplying factor needed for comparisons?\\n\\nHow many data points do you have?  You could probably just put up the entire data set, with a line indicating your predicted value and some kind of measure of variability around it.  Perhaps even an overlayed boxplot.\\n\\nThe measure of variability should reflect what you want to say about the data.  If you just want people to compare values then std. err. isn't a very good idea because it's dependent upon n and requires some value of multiplication greater than 2 (i.e. bars have to not overlap by some amount for significant effects).  Instead, put up 0.5*LSD bars (comparison bars) (about an 84% confidence interval, or a 0.5 * 95%CI * sqrt(2)).  Those bars would show significant differences at the point of bar overlap.  \\n\\nOr, you could be wanting to represent how well you estimated the predicted values.  In that case a more convenient confidence interval (about 95%) would be best or even the std. err would be ok.  If you want to reflect the estimated variability of the population you put up the standard deviation.\\n",added 15 characters in body; deleted 15 characters in body,
4680,5,2006,a461aa33-0c52-4d25-857c-3af349be0df5,2010-08-22 16:22:23.0,485.0,"As I see it, there are two basic problems with observational studies that ""control for"" a number of independent variables.  1) You have the problem of missing explanatory variables and thus model misspecification.  2) You have the problem of multiple correlated independent variables--a problem that does not exist in (well) designed experiments--and the fact that regression coefficients and ANCOVA tests of covariates are based on partials, making them difficult to interpret.  The first is intrinsic to the nature of observational research and is addressed in scientific context and the process of competitive elaboration.  The latter is an issue of education and relies on a clear understanding of regression and ANCOVA models and exactly what those coefficients represent.\\n\\nWith respect to the first issue, it is easy enough to demonstrate that if all of the influences on some dependent variable are known and included in a model, statistical methods of control are effective and produce good predictions and estimates of effects for individual variables.  The problem in the ""soft sciences"" is that all of the relevant influences are rarely included or even known and thus the models are poorly specified and difficult to interpret.  Yet, many worthwhile problems exist in these domains.  The answeres simply lack certainty.  The beauty of the scientific process is that it is self corrective and models are questioned, elaborated, and refined.  The alternative is to suggest that we cannot investigate these issues scientifically when we can't design experiments.\\n\\nThe second issue is a technical issue in the nature of ANCOVA and regression models.  Analysts need to be clear about what these coefficients and tests represent.  Correlations among the independent variables influence regression coefficients and ANCOVA tests.  They are tests of partials.  These models take out the variance in a given independent variable and the dependent variable that are associated with all of the other variables in the model and then examine the relationship in those residuals.  As a result, the individual coefficients and tests are very difficult to interpret outside of the context of a clear conceptual understanding of the entire set of variables included and their interrelationships.  This, however, produces NO problems for prediction--just be cautious about interpreting specific tests and coefficients.\\n\\n**A side note:**  The latter issue is related to a problem discussed previously in this forum on the reversing of regression signs--e.g., from negative to positive--when other predictors are introduced into a model.  In the presence of correlated predictors and without a clear understanding of the multiple and complex relationships among the entire set of predictors, there is no reason to EXPECT a (by nature partial) regression coefficient to have a particular sign.  When there is strong theory and a clear understanding of those interrelationships, such sign ""reversals"" can be enlightening and theoretically useful.  Though, given the complexity of many social science problems sufficient understanding would not be common, I would expect.\\n\\n**Disclaimer:**  I'm a sociologist and public policy analyst by training.\\n\\n",added 66 characters in body,
4683,2,2029,2ebbf8ad-17d1-40fa-86ca-a432fc06e991,2010-08-22 18:15:09.0,22.0,Have a look at [KNIME][1]. \\n\\nVery easy to learn. With lots of scope for further progress. Integrates nicely with Weka and R.\\n\\n\\n  [1]: http://www.knime.org/,,
4684,16,2029,2ebbf8ad-17d1-40fa-86ca-a432fc06e991,2010-08-22 18:15:09.0,-1.0,,,
4685,2,2030,cad7763f-2580-45d3-b59e-f02d73a91883,2010-08-22 19:24:35.0,881.0,"From the popularity perspective, this paper (2008) surveys [top 10 algorithms in data mining][1].\\n\\n\\n  [1]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.5575&rep=rep1&type=pdf",,
4686,16,2030,cad7763f-2580-45d3-b59e-f02d73a91883,2010-08-22 19:24:35.0,-1.0,,,
4687,2,2031,d6f6f1f6-7625-4f97-b1fb-c5f5dbc9fcde,2010-08-22 19:33:49.0,1048.0,"IMHO you cannot use a t-test for Likert scales. The Likert scale is ordinal and ""knows"" only about relations of values of a variable: e.g. ""totally dissatisfied"" is worse than ""somehow dissatisfied"". A t-test on the other hand needs to calculate means and more and thus needs interval data. You can map Likert scale scores to interval data (""totally dissatisfied"" is 1 and so on) but nobody guarantees that ""totally dissatisfied"" is the same distance to ""somehow dissatisfied"" as ""somehow dissatisfied"" is from ""neither nor"". By the way: what is the difference between ""totally dissatisfied"" and ""somehow dissatisfied""? So in the end, you'd do a t-test on the coded values of your ordinal data but that just doesn't make any sense. \\n\\n",,
4688,2,2032,e11bcc8d-c050-4e1c-8a7a-ebf8ad5d3fae,2010-08-22 19:49:20.0,1049.0,What learning material would you suggest for a CS person / novice statistician / novice mathematician to get into predictive analytics?,,
4689,1,2032,e11bcc8d-c050-4e1c-8a7a-ebf8ad5d3fae,2010-08-22 19:49:20.0,1049.0,Recommend some books/articles/guides to enter predictive analytics?,,
4690,3,2032,e11bcc8d-c050-4e1c-8a7a-ebf8ad5d3fae,2010-08-22 19:49:20.0,1049.0,<books><predictive-models>,,
4693,2,2033,81854399-802f-4a32-9e5d-d996267efd33,2010-08-22 21:41:03.0,1043.0,"<pre><code>\\nset.seed(10); x <- matrix(rnorm(15000), ncol=15)\\nplot(density(rowSums(x^2)), col=2, xlim=c(-100, 100))\\nfor (i in 3:10) {\\n   lines(density(rowSums(x^i)), col=i)\\n}\\n</pre></code>\\n\\nCan give us a plot.\\n\\nI don't have the perfect theoretical results.\\n",,
4694,2,2034,bfe30cee-d2fd-45b5-9893-d5f75becce7b,2010-08-22 22:56:57.0,919.0,"You can't hope to combine correlations with any legitimacy unless you also know the means and variances of the X's and Y's in each case, as well as their counts (*n*) and correlations (*r*).  The article Srikant Vadali refers to (Jack Dunlap, 1937) starts off by making exactly this assumption.  (It's easy to construct examples with the given values of your *n* and *r* statistics where the combined value of *r* is arbitrarily close to +1 or -1 or anything in between.)  Having these full second-moment statistics is crucial in the case of question (2) where one should expect there to be some systematic differences between the two studies.\\n\\n",,
4695,5,2033,4bb5a0d7-1c90-4114-8c1a-d087c219dcf7,2010-08-23 01:06:56.0,1043.0,"<pre><code>\\nset.seed(10); x <- matrix(rnorm(15000), ncol=15)\\nplot(density(rowSums(x^2)), col=2, xlim=c(-100, 100))\\nfor (i in 3:10) {\\n   lines(density(rowSums(x^i)), col=i)\\n}\\n</pre></code>\\n\\nCan give us a plot.\\n\\nI don't have the perfect theoretical results.\\n\\nAnother thing is, what's will it be if $n$ is not an integer?\\n\\nFor example, what is $(-3.5)^{3.4}$?",added 103 characters in body,
4696,5,2033,8a075fdb-976d-46e8-8e6b-ec56bbed2fab,2010-08-23 02:22:47.0,1043.0,"<pre><code>\\nset.seed(10); x <- matrix(rnorm(15000), ncol=15)\\nplot(density(rowSums(x^2)), col=2, xlim=c(-100, 100))\\nfor (i in 3:10) {\\n   lines(density(rowSums(x^i)), col=i)\\n}\\n</pre></code>\\n\\nCan give us a plot.\\n\\nI don't have the perfect theoretical results.\\n\\nAnother thing is, what's will it be if $n$ is not an integer?\\n\\nFor example, what is $(-3.5)^{3.4}$?\\n\\nOh, maybe you're looking for this [article][1]?\\n\\n\\n  [1]: http://biomet.oxfordjournals.org/cgi/pdf_extract/46/3-4/296",added 141 characters in body; deleted 18 characters in body,
4697,2,2035,6594e04d-1b37-4127-b632-f736c8296196,2010-08-23 03:12:13.0,1043.0,"If $X_i\\sim\\Gamma(\\alpha_i,\\beta_i)$ for $1\\leq i\\leq n$, let \\n$Y = \\sum_{i=1}^n c_iX_i$ where $c_i$ are positive real numbers.\\nAssume all the parameters $\\alpha_i$'s and $\\beta_i$'s are all known, what is $Y$'s distribution ?",,
4698,1,2035,6594e04d-1b37-4127-b632-f736c8296196,2010-08-23 03:12:13.0,1043.0,The distribution of the linear combination of  Gamma random variables,,
4699,3,2035,6594e04d-1b37-4127-b632-f736c8296196,2010-08-23 03:12:13.0,1043.0,<distributions>,,
4700,2,2036,59261dac-180d-400c-9105-b0d3c833f61f,2010-08-23 06:07:35.0,251.0,See Theorem 1 given in [Moschopoulos][1] (1985) for the distribution of a sum of independent gamma variables.  You can extend this result using the [scaling property][2] for linear combinations.\\n\\n\\n  [1]: http://www.ism.ac.jp/editsec/aism/pdf/037_3_0541.pdf\\n  [2]: http://en.wikipedia.org/wiki/Gamma_distribution#Scaling\\n  \\n,,
4701,2,2037,872017b3-1613-48d9-b72f-eab724009ec1,2010-08-23 09:34:51.0,,"Suppose I have a black box that generates data following a normal distribution with mean m and standard deviation s. Suppose, however, that whenever it outputs a value < 0 it does not record anything (can't even tell that it's outputted such a value). We have a truncated gaussian distribution without a spike.\\n\\nHow can I estimate these parameters?\\n\\n",,Catrin Campbell-Moore
4702,1,2037,872017b3-1613-48d9-b72f-eab724009ec1,2010-08-23 09:34:51.0,,estimating mean and st dev of a truncated gaussian curve without spike,,Catrin Campbell-Moore
4703,3,2037,872017b3-1613-48d9-b72f-eab724009ec1,2010-08-23 09:34:51.0,,<distributions><standard-deviation><mean><gaussian>,,Catrin Campbell-Moore
4704,2,2038,47cda457-996a-4ce4-8793-951888b7e76e,2010-08-23 09:48:44.0,862.0,I am reviewing PCA concepts. I came across this nice tutorial - http://cran.r-project.org/web/packages/HSAUR/vignettes/Ch_principal_components_analysis.pdf on how to do PCA in R language. I don't understand the interpretation of Figure 13.3. So I am plotting first eigenvector vs the second eigenvector. What does that mean? Suppose eigenvalue corresponding to first eigenvector explains 60% of variation in data set and second eigenvalue-eigenvector explain 20% of variation. What does it mean to plot these against each other?,,
4705,1,2038,47cda457-996a-4ce4-8793-951888b7e76e,2010-08-23 09:48:44.0,862.0,principal component analysis in R ,,
4706,3,2038,47cda457-996a-4ce4-8793-951888b7e76e,2010-08-23 09:48:44.0,862.0,<r><pca>,,
4708,2,2039,382aeb74-e809-4adb-abd9-118b8928bd9a,2010-08-23 10:22:08.0,183.0,"The plot is showing:\\n\\n- the score of each case (i.e., athlete) on the first two principal components\\n- the loading of each variable (i.e., each sporting event) on the first two principal components.\\n\\nThe left and bottom axes are showing the loadings; the top and right axes are showing principal component scores.\\n\\nIn general it assumes that two components explain a sufficient amount of the variance to provide\\n a meaningful visual representation of the structure of cases and variables.\\n\\nYou can look to see which events are close together in the space. Where this applies, this may suggest that athletes who are good at one event are likely also to be good at the other proximal events. Alternatively you can use the plot to see which events are distant. For example, javelin appears to be bit of an outlier and a major event defining the second principal component. Perhaps a different kind of athlete is good at javelin than is good at most of the other events. \\n\\nOf course, more could be said about substantive interpretation.\\n\\n\\n\\n",,
4709,2,2040,0818d653-d28d-4071-a795-c3d49869ec2e,2010-08-23 10:23:50.0,930.0,"PCA is one of the many ways to analyse the structure of a given correlation matrix. By construction, the first principal axis is the one which maximizes the variance (reflected by the eigenvalue) when data are projected onto a line (which stands for a direction in the p-space, assuming you have p variables) and the second one is orthogonal to it, and still maximizes the remaining variance. This is the reason why using the first two axes should yield the better approximation of the original variables space (say, a matrix X of dim n x p) when it is projected onto a plane.\\n\\nPrincipal components are just linear combinations of the original variable. Therefore, plotting individual factor scores (defined as Xu, where u is the vector of loadings of any principal component) may help to highlight groups of homogeneous individuals, for example, or to interpret one's overall scoring when considering all variables at the same time. In other words, this is a way to summarize one's location with respect to his value on the p variables, or a combination thereof. In your case, Fig. 13.3 for HSAUR shows that Joyner-Kersee (Jy-K) has a high (negative) score on the 1st axis, suggesting he performed overall quite good on all events. The same line of reasonning applies for interpreting the second axis. I take a very short look at the figure so I will not go into details and my interpretation is certainly superficial. I assume that you will find further information in the HSAUR textbook. Here it is worth noting that both variables and individuals are shown on the same diagram (this is called a *biplot*), which helps to interpret the factorial axes while looking at individuals location. Usually, we plot the variables into a so-called correlation circle (where angle formed by any two variables, represented here as vectors, reflects their actual pairwise correlation, since r(x1,x2)=cos<sup>2</sup>(x1,x2)).\\n\\nI think, however, you'd better start reading some introductory book on multivariate analysis to get deep insight into PCA-based methods. For example, B.S. Everitt wrote an excellent textbook on this topic, *An R and S-Plus<sup>®</sup> Companion to Multivariate Analysis*, and you can check the [companion website][1] for illustration. There are other great R package for applied multivariate data analysis, like [ade4][2] and [FactoMineR][3].\\n\\n\\n  [1]: http://biostatistics.iop.kcl.ac.uk/publications/everitt/\\n  [2]: http://cran.r-project.org/web/packages/ade4/index.html\\n  [3]: http://cran.r-project.org/web/packages/FactoMineR/index.html",,
4710,5,2040,96e123e2-00fa-40f5-b7fa-b690f59ab44e,2010-08-23 10:29:54.0,930.0,"PCA is one of the many ways to analyse the structure of a given correlation matrix. By construction, the first principal axis is the one which maximizes the variance (reflected by its eigenvalue) when data are projected onto a line (which stands for a direction in the p-space, assuming you have p variables) and the second one is orthogonal to it, and still maximizes the remaining variance. This is the reason why using the first two axes should yield the better approximation of the original variables space (say, a matrix X of dim n x p) when it is projected onto a plane.\\n\\nPrincipal components are just linear combinations of the original variables. Therefore, plotting individual factor scores (defined as Xu, where u is the vector of loadings of any principal component) may help to highlight groups of homogeneous individuals, for example, or to interpret one's overall scoring when considering all variables at the same time. In other words, this is a way to summarize one's location with respect to his value on the p variables, or a combination thereof. In your case, Fig. 13.3 in HSAUR shows that Joyner-Kersee (Jy-K) has a high (negative) score on the 1st axis, suggesting he performed overall quite good on all events. The same line of reasonning applies for interpreting the second axis. I take a very short look at the figure so I will not go into details and my interpretation is certainly superficial. I assume that you will find further information in the HSAUR textbook. Here it is worth noting that both variables and individuals are shown on the same diagram (this is called a *biplot*), which helps to interpret the factorial axes while looking at individuals' location. Usually, we plot the variables into a so-called correlation circle (where the angle formed by any two variables, represented here as vectors, reflects their actual pairwise correlation, since r(x1,x2)=cos<sup>2</sup>(x1,x2)).\\n\\nI think, however, you'd better start reading some introductory book on multivariate analysis to get deep insight into PCA-based methods. For example, B.S. Everitt wrote an excellent textbook on this topic, *An R and S-Plus<sup>®</sup> Companion to Multivariate Analysis*, and you can check the [companion website][1] for illustration. There are other great R packages for applied multivariate data analysis, like [ade4][2] and [FactoMineR][3].\\n\\n\\n  [1]: http://biostatistics.iop.kcl.ac.uk/publications/everitt/\\n  [2]: http://cran.r-project.org/web/packages/ade4/index.html\\n  [3]: http://cran.r-project.org/web/packages/FactoMineR/index.html",fix some typos; added 1 characters in body,
4711,2,2041,a6920d89-9afe-45da-b8ac-3166c1902186,2010-08-23 10:46:36.0,,"The model for your data would be:\\n\\n$y_i \\sim N(\\mu,\\sigma^2) I(y_i > 0)$\\n\\nThus, the density function is:\\n\\n$$f(y_i|-) = \\frac{exp(-\\frac{(y_i-\\mu)^2}{2 \\sigma^2})}{\\sqrt{2 \\pi \\sigma}\\ (1 - \\phi(-\\frac{\\mu}{\\sigma}))}$$\\n\\nwhere,\\n\\n$\\phi(.)$ is the standard normal cdf.\\n\\nYou can then estimate the parameters $\\mu$ and $\\sigma$ using either maximum likelihood or bayesian methods.",,user28
4712,2,2042,806bd826-9c8f-455e-89c4-157fa013aee1,2010-08-23 10:48:27.0,930.0,"Have a look at the [mvoutlier][1] package which relies on ordered robust mahalanobis distances, as suggested by @drknexus.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/mvoutlier/index.html",,
4713,6,2037,17b96bea-d09c-4f5b-ad1d-c46b4b2edf51,2010-08-23 10:52:31.0,,<standard-deviation><estimation><mean><truncated-gaussian>,changed tags to appropriate ones,user28
4714,2,2043,1379668c-8355-49bb-a1da-aefeb58d40a9,2010-08-23 11:18:49.0,930.0,"The R [psych][1] package includes various routines to apply Factor Analysis (whether it be PCA-, ML- or FA-based), but see my short review on [crantastic][2]. Most of the usual rotation techniques are available, as well as algorithm relying on simple structure criteria; you might want to have a look at W. Revelle's paper on this topic, [Very Simple Structure: An Alternative Procedure For Estimating The Optimal Number Of Interpretable Factors][3] (MBR 1979 (14)) and the `VSS()` function. \\n\\nMany authors are using orthogonal rotation (VARIMAX), considering loadings higher than, say 0.3 or 0.4 (which amounts to 9 or 16% of variance explained by the factor), as it provides simpler structures for interpretation and scoring purpose (e.g., in quality of life research); others (e.g. Cattell, 1978; Kline, 1979) would recommend oblique rotations since ""in the real world, it is not unreasonable to think that factors, as important determiners of behavior, would be correlated"" (I'm quoting Kline, *Intelligence. The Psychometric View*, 1991, p. 19). \\n\\nTo my knowledge, researchers generally start with FA (or PCA), using a scree-plot together with simulated data (parallel analysis) to help choosing the right number of factors. I often found that item cluster analysis and VSS nicely complement such an approach. When one is interested in second-order factors, or to carry on with SEM-based methods, then obviously you need to use oblique rotation and factor out the resulting correlation matrix.\\n\\nOther packages/software:\\n\\n - [lavaan][4], for latent variable analysis in R;\\n - [OpenMx][6] based on [Mx][5], a general purpose software including a matrix algebra interpreter and numerical optimizer for structural equation modeling.\\n\\n**References**  \\n 1. Cattell, R.B. (1978). The scientific use of factor analysis in behavioural and life sciences. New York, Plenum.  \\n 2. Kline, P. (1979). Psychometrics and Psychology. London, Academic Press.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/psych/index.html\\n  [2]: http://crantastic.org/packages/psych\\n  [3]: http://personality-project.org/revelle/publications/vss.pdf\\n  [4]: http://lavaan.ugent.be/\\n  [5]: http://www.vcu.edu/mx/\\n  [6]: http://openmx.psyc.virginia.edu/",,
4715,16,2032,075189ec-d318-4b0a-a0b7-9325c3356e88,2010-08-23 13:39:23.0,1049.0,,,
4716,2,2044,d34b1fd2-9889-41be-870b-1c221e2b814e,2010-08-23 13:47:21.0,,"If you torture the data enough, nature will always confess.  Ronald Coase (quoted from Coase, R. H. 1982. How should economists chose? American Enterprise Institute, Washington, D. C.).  I think most who hear this quote misunderstand it's profound message against data dredging.",,Eric Stolen
4717,16,2044,d34b1fd2-9889-41be-870b-1c221e2b814e,2010-08-23 13:47:21.0,-1.0,,,
4718,2,2045,686986e1-b1c0-4f4f-bec2-d0cf48bccc25,2010-08-23 14:14:25.0,919.0,"As Srikant Vadali has suggested, Cohen and Hald solved this problem using ML (with a Newton-Raphson root finder) around 1950.  Another paper is Max Halperin's ""Estimation in the Truncated Normal Distribution"" available on JSTOR [link text][1] (for those with access).  Googling ""truncated gaussian estimation"" produces lots of useful-looking hits.\\n\\n\\n  [1]: http://www.jstor.org/pss/2281315",,
4720,16,2046,f2139ea8-e05d-46f4-a5a4-7b413b53ac79,2010-08-23 14:14:57.0,-1.0,,,
4722,6,2037,a21578e3-3e92-4b82-a04a-541f43890615,2010-08-23 14:19:37.0,919.0,<standard-deviation><estimation><mean><truncation>,edited tags,
4724,2,2048,1b55d1a8-e34a-45ef-97dd-6f95fac3084e,2010-08-23 14:53:02.0,976.0,"[European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning][1]\\n\\n\\n  [1]: http://www.dice.ucl.ac.be/esann/",,
4725,16,2048,1b55d1a8-e34a-45ef-97dd-6f95fac3084e,2010-08-23 14:53:02.0,-1.0,,,
4726,2,2049,5a2d4de2-5c12-4dcd-9c57-2023d8ff015a,2010-08-23 14:55:35.0,976.0,[IEEE World Congress on Computational Intelligence][1]. Note that it is the link for 2010 conference.\\n\\n\\n  [1]: http://www.wcci2010.org/topics/ijcnn-2010,,
4727,16,2049,5a2d4de2-5c12-4dcd-9c57-2023d8ff015a,2010-08-23 14:55:35.0,-1.0,,,
4728,2,2050,8a8375f7-a2bd-4334-bf15-7fef9ae35e46,2010-08-23 14:58:22.0,976.0,[International Conference on Artificial Neural Networks][1]. Note that the link is for the 2010 conference.\\n\\n\\n  [1]: http://delab.csd.auth.gr/icann2010/,,
4729,16,2050,8a8375f7-a2bd-4334-bf15-7fef9ae35e46,2010-08-23 14:58:22.0,-1.0,,,
4730,6,1908,7449bc7f-a335-4933-ae46-1dd406fa813f,2010-08-23 15:27:33.0,877.0,<machine-learning><conferences>,edited tags,
4731,2,2051,8ce90d9f-e36f-4f71-9c62-19bc6799a60a,2010-08-23 15:35:18.0,930.0,"[Artificial Intelligence In Medicine][1] (AIME), odd years starting from 1985.\\n\\n\\n  [1]: http://aimedicine.info/aime/",,
4732,16,2051,8ce90d9f-e36f-4f71-9c62-19bc6799a60a,2010-08-23 15:35:18.0,-1.0,,,
4733,5,2004,366b1a15-9b1e-4e01-973a-722ebb207dd8,2010-08-23 16:26:02.0,1036.0,"[The Centre for Multilevel Modelling][1] has some good free online tutorials for multi-level modeling, and they have software tutorials for fitting models in both their MLwiN software and STATA.\\n\\n\\nTake this as heresy, because I have not read more than a chapter in the book, but Hierarchical linear models: applications and data analysis methods By Stephen W. Raudenbush, Anthony S. Bryk comes highly recommended. I also swore there was a book on multi level modeling using R software in the Springer Use R! series, but I can't seem to find it at the moment (I thought it was written by the same people who wrote the A Beginner’s Guide to R book). \\n\\nedit: The book on using R for multi-level models is [Mixed Effects Models and Extensions in Ecology with R by Zuur, A.F., Ieno, E.N., Walker, N., Saveliev, A.A., Smith, G.M.][2]\\n\\ngood luck\\n\\n\\n  [1]: http://www.cmm.bristol.ac.uk/learning-training/index.shtml\\n  [2]: http://www.springer.com/life+sciences/ecology/book/978-0-387-87457-9",added 258 characters in body,
4734,5,1965,538d9f11-4a36-4ae2-aa5a-4235fb18b3af,2010-08-23 16:37:56.0,795.0,"(I will delete my other non-answer, the edit of which had this nugget in it)\\n\\n**No.** Asymptotically, the 'trivial' upper bound is the least upper bound. To see this, Let $P_n = P(Z = n)$. Trivially, $E[\\exp{(Z^2)}] \\ge P_n \\exp{(n^2)} = L$, where $L$ is the lower bound of interest. Since $Z$ is binomial, we have $P_n = {n\\choose n} (n^{-\\beta})^n (1-n^{-\\beta})^0 = n^{-n\\beta}$. Then $\\log{L} = -\\beta n \\log{n} + n^2$. It is easily shown that this is $\\Omega{(n^2)}$, and thus, $L$ is $\\Omega{(\\exp{(n^2)})}$. Thus the trivial upper bound is, asymptotically, the least upper bound, i.e. $E[\\exp{(Z^2)}] \\in \\Theta{(\\exp{(n^2)})}$.\\n\\n","duh, had a 'little-o' where I should have had an 'Omega' oops.",
4735,5,1947,22e528ed-646c-4309-a85c-1602f6770ff2,2010-08-23 17:00:13.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer falls into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$  (with $x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)) . Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can generalize the definition of quantiles in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",added 23 characters in body,
4739,4,2038,046e9581-75f7-48a0-bd35-5bbbd6e64764,2010-08-23 17:40:17.0,88.0,Interpretation of biplots in principal components analysis in R,edited title,
4740,2,2053,d8346e13-6068-4e47-b1fc-66a073a7c7d8,2010-08-23 18:03:15.0,25.0,"I think I figured out the answer (to my own question). If the assumption of proportional hazards is true, the two methods give similar estimates of the hazard ratio. The discrepancy I found in one particular example, I now think, is due to the fact that that assumption is dubious. \\n\\nIf the assumption of proportional hazards is true, then a graph of log(time) vs. log(-log(St)) (where St is the proportional survival at time t) should be linear, with the two lines having the same slope. Below is the graph created from the problem data set. It seems far from linear. If the assumption of proportional hazards is not valid, then the concept of a hazard ratio is meaningless.\\n\\n![alt text][1]\\n\\n\\n  [1]: http://i.stack.imgur.com/cgsYv.png\\n\\nI wonder if the discrepancy between the logrank and Mantel-Haenszel estimates of the hazard ratio can be used as a method to test the assumption of proportional hazards?",,
4741,2,2054,3bacf793-abce-401e-8c29-12d4d273201f,2010-08-23 18:34:05.0,1068.0,"The Tukey halfspace median can be extended to >2 dimensions using DEEPLOC, an algorithm due to Struyf and Rousseeuw; see <A HREF=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.2052"">here</A> for details.\\n\\nThe algorithm is used to approximate the point of greatest depth efficiently; naive methods which attempt to determine this exactly usually run afoul of (the computational version of) ""the curse of dimensionality"", where the runtime required to calculate a statistic grows exponentially with the number of dimensions of the space.",,
4742,2,2055,84b10f7e-dfaf-428d-b907-6528871e86ba,2010-08-23 19:47:39.0,1036.0,"As to your suggestion of using Multi-level models in this and the other thread, I see no benefit of approaching your analysis in this manner over repeated measures ANOVA. MLM are simply an extension of OLS regression, and offer an explicit framework to model group level (often referred to as ""contextual"") effects on lower level estimates. I would guess from your example you have no specific ""contexts"" besides that measures are repeated within individuals, and this is accounted for within the repeated ANOVA design (and your not interested in measuring effects of specific individuals anyway, only to control for this non-independence). Neither groups nor gender with what your description says are ""contexts"" in this sense, they can only be direct effects (or at least you can only observe if they have direct effects).\\n\\nMLM just complicates things in this example IMO. It doesn't directly solve your problem that several dependent or measures are non-independent, you can't measure any group level characteristic on your outcome because you only have two groups, and your hierarchy is cross classified and has three levels, (an observation is nested within only 1 individual, but the gender and group nestings are not mutually exclusive). All of the things you listed as purposes of the project can be accomplished using repeated ANOVA simply including group, gender, and group gender interaction effects into the models.\\n\\nIt is beyond the scope of your question, but I would never agree that gender is a group observations are nested within. \\n",,
4743,2,2056,0e0368ea-2bdb-451b-9302-8acc0f2d8f1f,2010-08-23 20:39:21.0,187.0,"According to Davison and Hinckley (""Bootstrap methods and their application"", 1997, Section 3.8), the third algorithm is conservative.  They advocate a fourth approach: simply resampling the subjects.",,
4744,5,2053,a70ee09c-495b-4826-8506-382dd4d84e83,2010-08-23 21:10:58.0,25.0,"I think I figured out the answer (to my own question). If the assumption of proportional hazards is true, the two methods give similar estimates of the hazard ratio. The discrepancy I found in one particular example, I now think, is due to the fact that that assumption is dubious. \\n\\nIf the assumption of proportional hazards is true, then a graph of log(time) vs. log(-log(St)) (where St is the proportional survival at time t) should show two parallel lines. Below is the graph created from the problem data set. It seems far from linear. If the assumption of proportional hazards is not valid, then the concept of a hazard ratio is meaningless, and so it doesn't matter which method is used to compute the hazard ratio.\\n\\n![alt text][1]\\n\\n\\n  [1]: http://i.stack.imgur.com/cgsYv.png\\n\\nI wonder if the discrepancy between the logrank and Mantel-Haenszel estimates of the hazard ratio can be used as a method to test the assumption of proportional hazards?",minor edits in wording,
4745,2,2057,e4bc9c53-053f-4eda-931d-5460827408dd,2010-08-23 22:04:33.0,919.0,"I have a relatively simple solution to propose, Hugo.   Because you're forthright about not being a statistician (often a plus ;-) but obviously can handle technical language, I'll take some pains to be technically clear but avoid statistical jargon.\\n\\nLet's start by checking my understanding: you have six series of data (t[j,i], h[j,i]), 1 <= j <= 6, 1 <= i <= n[j], where t[j,i] is the time you measured the entropy h[j,i] for artifact j and n[j] is the number of observations made of artifact j.\\n\\nWe may as well assume t[j,i] <= t[j,i+1] is always the case, but it sounds like you cannot necessarily assume that t[1,i] = ... = t[6,i] for all i (synchronous measurements) or even that t[j,i+1] - t[j,i] is a constant for any given j (equal time increments). We might as well also suppose j=1 designates your special artifact.\\n\\nWe do need a model for the data.  ""Exponential"" versus ""sublinear"" covers a lot of ground, suggesting we should adopt a very broad (non-parametric) model for the behavior of the curves.  One thing that simply distinguishes these two forms of evolution is that the increments h[j,i+1] - h[j,i] in the exponential case will be increasing whereas for concave sublinear growth the increments will decreasing.  Specifically, the *increments of the increments*,\\n\\nd2[j,i] = h[j,i+1] - 2*h[j,i+1] + h[j,i], 1 <= i <= n[j]-2,\\n\\nwill either tend to be positive (for artifact 1) or negative (for the others).\\n\\nA big question concerns the nature of variation: the observed entropies might not exactly fit along any nice curve; they might oscillate, seemingly at random, around some ideal curve.  Because you don't want to do any statistical modeling, we aren't going to learn much about the nature of this variation, but let's hope that the amount of variation for any given artifact j is typically about the same size for all times t[i].  This lets us write each entropy in the form\\n\\nh[j,i] = y[j,i] + e[j,i]\\n\\nwhere y[j,i] is the ""true"" entropy for artifact j at time t[j,i] and e[j,i] is the difference between the observed entropy h[j,i] and the true entropy.  It might be reasonable, as a first cut at this problem, to hope that the e[j,i] act randomly and appear to be statistically independent of each other and of the y[j,i] and t[j,i].\\n\\nThis setup and these assumptions imply that the set of second increments for artifact j, {d2[j,i] | 1 <= i <= n[j]-2}, will not necessarily be entirely positive or entirely negative, but that each such set should look like a bunch of (potentially different) positive or negative numbers plus some fluctuation:\\n\\nd[j,i] = (y[j,i+2] - 2*y[j,i+1] + y[j,i]) + (e[j,i+2] - 2*e[j,i+1] + e[j,i]).\\n\\nWe're still not in a classic probability context, but we're close if we (incorrectly, but perhaps not fatally) treat the correct second increments (y[j,i+2] - 2*y[j,i+1] + y[j,i]) as if they were numbers drawn randomly from some box.  In the case of artifact 1 your hope is that this is a box of all positive numbers; for the other artifacts, your hope is that it is a box of all negative numbers.\\n\\n**At this point we can apply some standard machinery for hypothesis testing**.  The null hypothesis is that the true second increments are all (or most of them) negative; the alternative hypothesis covers all the other 2^6-1 possibilities concerning the signs of the six batches of second increments.  This suggests **running a t-test separately for each collection of actual second increments to compare them against zero**.  (A non-parametric equivalent, such as a sign test, would be fine, too.)  Use a Bonferroni correction with these planned multiple comparisons; that is, if you want to test at a level of *alpha* (e.g., 5%) to attain a desired ""probability value,"" use the *alpha*/6 critical value for the test.  This can readily be done even in a spreadsheet if you like.  It's fast and straightforward.\\n\\nThis approach is not going to be the best one because among all those that could be conceived: it's one of the less powerful and it still makes some assumptions (such as independence of the errors); but if it works--that is, if you find the second increments for j=1 to be significantly above 0 and all the others to be significantly below 0--then it will have done its job.  If this is not the case, your expectations might still be correct, but it would take a greater statistical modeling effort to analyze the data.  (The next phase, if needed, might be to look at the runs of increments for each artifact to see whether there's evidence that *eventually* each curve becomes exponential or sublinear.  It should also involve a deeper analysis of the nature of variation in the data.)",,
4746,2,2058,2250aefb-343e-4328-8234-419d681b4d87,2010-08-23 22:41:05.0,919.0,"You can find candidates for ""outliers"" among the support points of the minimum volume bounding ellipsoid.  (Efficient algorithms to find these points in fairly high dimensions, both exactly and approximately, were invented in a spate of papers in the 1970's because this problem is intimately connected with a question in experimental design.)",,
4748,2,2059,985a2417-8272-4a12-bdd0-5409f3f5f869,2010-08-23 23:21:15.0,1072.0,"It seems that EM is widely used (mostly as a heuristic) in machine learning / statistics to learn the parameters of a mixture of Gaussians. I'm assuming we're given random samples from the mixture. \\n\\nMy question is: Are there any proven bounds on the error in terms of the number of samples? \\n\\nIdeally, these bounds would not assume that we start in a local neighborhood of the optimal solution or any such thing. (If EM is not the method of choice and there is a better way of doing it, please point this out, as well.)",,
4749,1,2059,985a2417-8272-4a12-bdd0-5409f3f5f869,2010-08-23 23:21:15.0,1072.0,Learning parameters of a mixture of Gaussian using EM,,
4750,3,2059,985a2417-8272-4a12-bdd0-5409f3f5f869,2010-08-23 23:21:15.0,1072.0,<estimation><mixed-model><normal-distribution>,,
4751,2,2060,06a171bf-0b42-4e25-997c-b5ed5776bc5b,2010-08-24 00:27:18.0,881.0,"EM essentially solves the maximum likelihood problem and therefore has the same properties w.r.t. sample sizes. EM for Gaussian mixture models is known to converge asymptotically to a local maximum and exhibits first order convergence (see [this paper][1]).\\n\\nIf you don't want to use EM for mixture models, you can take a [fully Bayesian approach][2].\\n\\n\\n  [1]: http://dspace.mit.edu/bitstream/handle/1721.1/7195/AIM-1520.pdf;jsessionid=DEB41C50ECD9DA8161DC80F7057BDCBE?sequence=2\\n  [2]: http://research.microsoft.com/en-us/um/people/cmbishop/downloads/Bishop-robust-mixture-Neurocomputing-04.pdf",,
4752,5,2059,2ef0a28d-0528-4360-b504-f68b62feb772,2010-08-24 01:02:22.0,1072.0,"It seems that MLE (via EM) is widely used in machine learning / statistics to learn the parameters of a mixture of Gaussians. I'm assuming we're given random samples from the mixture. \\n\\nMy question is: Are there any proven _quantitative_ bounds on the error in terms of the number of samples (and perhaps the parameters of the Gaussian)?\\n\\nFor example, what is the runtime required to estimate the parameters up to a certain error?\\n\\nIdeally, these bounds would not assume that we start in a local neighborhood of the optimal solution or any such thing. (If EM is not the method of choice and there is a better way of doing it, please point this out, as well.)",added 140 characters in body; edited title,
4753,4,2059,2ef0a28d-0528-4360-b504-f68b62feb772,2010-08-24 01:02:22.0,1072.0,Learning parameters of a mixture of Gaussian using MLE,added 140 characters in body; edited title,
4754,5,2060,b293de6c-eb48-45f9-8645-625aaf441194,2010-08-24 01:24:08.0,881.0,"EM essentially solves the maximum likelihood problem and therefore has the same properties w.r.t. sample sizes. EM for Gaussian mixture models is known to converge asymptotically to a local maximum and exhibits first order convergence (see [this paper][1]).\\n\\nBTW, there are some results which quantify how good the EM solution is in terms of the parameters of the data distribution. See [this paper][2] which shows that the goodness depends on the separation of mixture components (measured by variances). A lot of papers have analyzed mixture models using this criteria.\\n\\nIf you don't want to use EM for mixture models, you can take a [fully Bayesian approach][3].\\n\\n\\n  [1]: http://dspace.mit.edu/bitstream/handle/1721.1/7195/AIM-1520.pdf;jsessionid=DEB41C50ECD9DA8161DC80F7057BDCBE?sequence=2\\n  [2]: http://www.cs.caltech.edu/~schulman/Papers/em-uaif.pdf\\n  [3]: http://research.microsoft.com/en-us/um/people/cmbishop/downloads/Bishop-robust-mixture-Neurocomputing-04.pdf\\n  ",added 383 characters in body,
4755,5,2055,f971f512-6a9d-44db-ae8b-40eda4e2393c,2010-08-24 01:49:46.0,1036.0,"As to your suggestion of using Multi-level models in this and the other thread, I see no benefit of approaching your analysis in this manner over repeated measures ANOVA. MLM are simply an extension of OLS regression, and offer an explicit framework to model group level (often referred to as ""contextual"") effects on lower level estimates. I would guess from your example you have no specific ""contexts"" besides that measures are repeated within individuals, and this is accounted for within the repeated ANOVA design (and your not interested in measuring effects of specific individuals anyway, only to control for this non-independence). Neither groups nor gender with what your description says are ""contexts"" in this sense, they can only be direct effects (or at least you can only observe if they have direct effects).\\n\\nMLM just complicates things in this example IMO. It doesn't directly solve your problem that several dependent measures are non-independent, you can't measure any group level characteristic on your outcome because you only have two groups, and your hierarchy is cross classified and has three levels, (an observation is nested within only 1 individual, but the gender and group nestings are not mutually exclusive). All of the things you listed as purposes of the project can be accomplished using repeated ANOVA simply including group, gender, and group*gender interaction effects into the models.\\n\\nIt is beyond the scope of your question, but I would never agree that gender is a group observations are nested within. \\n",deleted 3 characters in body,
4756,2,2061,789c194e-7d0d-4df0-9198-67e3d8d75dd4,2010-08-24 02:48:08.0,183.0,"When lecturing statistics, it can be useful to incorporate the occasional short video.\\n\\nMy first thoughts included: \\n\\n 1. Animations and visualisations of statistical concepts\\n 2. Stories regarding the application of a particular technique\\n 3. Humorous videos that have relevance to a statistical idea \\n 4. Interviews with a statistician or a researcher who uses statistics\\n\\nAre there any videos that you use when teaching statistics or that you think would be useful?\\n\\nPlease provide:\\n\\n- Link to the video online\\n- Description of content\\n- To what statistical topic the video might relate\\n\\n",,
4757,1,2061,789c194e-7d0d-4df0-9198-67e3d8d75dd4,2010-08-24 02:48:08.0,183.0,Short online videos that could assist a lecture on statistics,,
4758,3,2061,789c194e-7d0d-4df0-9198-67e3d8d75dd4,2010-08-24 02:48:08.0,183.0,<teaching><books><lecture>,,
4759,2,2062,4a6149ce-7cac-4e1e-9030-ab3249f15022,2010-08-24 02:57:23.0,253.0,"Link:\\n\\nhttp://animation.yihui.name/\\n\\nA big list of animation ""clips"" (gif's or other formats), through the use of the ""[animation][1]"" package (R).  Including the following topics:\\n\\n\\n**Topics**(in each of them there are 1 or more animation, by topic) \\n\\n-  Theory of Probability\\n-  Mathematical Statistics\\n-  Sampling Survey\\n-  Linear Models\\n-  Multivariate Statistics\\n-  Nonparametric Statistics (no videos yet)\\n-  Computational Statistics\\n-  Time Series Analysis\\n-  Data Mining, Machine Learning\\n-  Statistical Data Analysis\\n-  R Graphics\\n-  Package ''animation''\\n-  Dynamic Graphics\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/animation/index.html\\n\\n\\nThrough the use of the package, the animations can be reproduced in various formats (such as gif, aws, and others)",,
4761,2,2063,83990d7e-0d0a-4980-aae2-1ad487c8d0be,2010-08-24 06:06:54.0,401.0,"Perhaps I'm misunderstanding the OP, but shouldn't the test be for 8 successes (of not stubbing the fellow's toe) and 7 failures, for a total of 15 trials? And shouldn't the comparison be to a probability of 2/3 (2000 incidents of not stubbing a toe relative to the 1000 observed ascents)? This, of course, is taking the true population probability of stubbing a toe to be 1/3. \\n\\nOne way to permit both quantities to be random is to bootstrap. If stubbing is 1 and not stubbing is 0, draw (3000-15) observations with replacement from the total pool. Note that we exclude Joe from the total pool in order to compare him to others. Sum the observations (i.e., count the number of toe stubs) and divide by the total number of observations, 3000-15. Draw 15 observations from Joe's sample, sum, and divide by 15. Subtract Joe's proportion of stubs from the population proportion. Simulate many (1000, 10000 maybe) times, look at the 2.5 and 97.5 percentiles (for a 95% test against the null hypothesis that Joe's stub rate is the same as that of the population; a different level or one-sided test could also be used as desired) and see whether 0 is in that interval. If it is, then you cannot reject the null hypothesis that Joe is as clumsy as everyone else.",,
4762,2,2064,d719e80f-00fb-4f18-b99e-ea08888476d8,2010-08-24 06:38:43.0,401.0,"In the social sciences, we often call this issue ""post treatment bias."" If you are considering the effect of some treatment (your independent variable), including variables that arise after treatment (in a causal sense), then your estimate of the treatment effect can be biased. If you include these variables, then you are, in some sense, controlling for the impact of treatment. If treatment T causes outcome Y and other variable A and A causes Y, then controlling for A ignores the impact that T has on Y via A. This bias can be positive or negative.\\n\\nIn the social sciences, this can be especially difficult because A might cause T, which feeds back on A, and A and T both cause Y. For example, high GDP can lead to high levels of democratization (our treatment), which leads to higher GDP, and higher GDP and higher democratization both lead to less government corruption, say. Since GDP causes democratization, if we don't control for it, then we have an endogeneity issue or ""omitted variables bias."" But if we do control for GDP, we have post treatment bias. Other than use randomized trials when we can, there is little else that we can do to steer our ship between Scylla and Charybdis. Gary King talks about these issues as his nomination for Harvard's ""Hardest Unsolved Problems in the Social Sciences"" initiative [here](http://www.slideshare.net/HardProblemsSS/gary-king-hard-unsolved-problem-in-social-sciencepost-treatment-bias).",,
4763,2,2065,027bd14f-4e67-4d53-a8a4-9930e41c44fd,2010-08-24 06:49:15.0,521.0,"I thought I'd stumbled across a web site and reference that deals exactly with this question:\\n\\nhttp://www.graphpad.com/faq/viewfaq.cfm?faq=1226\\nStart from ""The two methods compared"".\\n\\nThe site references the Berstein paper ars linked (above):\\n\\nhttp://www.jstor.org/stable/2530564?seq=1\\n\\nThe site summarises Berstein et al's results nicely, so I'll quote it:\\n\\n> The two usually give identical (or\\n> nearly identical) results. But the\\n> results can differ when several\\n> subjects die at the same time or when\\n> the hazard ratio is far from 1.0.\\n> \\n> Bernsetin and colleagues analyzed\\n> simulated data with both methods (1).\\n> In all their simulations, the\\n> assumption of proportional hazards was\\n> true. The two methods gave very\\n> similar values.  The logrank method\\n> (which they refer to as the O/E\\n> method) reports values that are closer\\n> to 1.0 than the true Hazard Ratio,\\n> especially when the hazard ratio is\\n> large or the sample size is large.\\n> \\n> When there are ties, both methods are\\n> less accurate. The logrank methods\\n> tend to report hazard ratios that are\\n> even closer to 1.0 (so the reported\\n> hazard ratio is too small when the\\n> hazard ratio is greater than 1.0, and\\n> too large when the hazard ratio is\\n> less than 1.0). The Mantel-Haenszel\\n> method, in contrast, reports hazard\\n> ratios that are further from 1.0  (so\\n> the reported hazard ratio is too large\\n> when the hazard ratio is greater than\\n> 1.0, and too small when the hazard ratio is less than 1.0). \\n> \\n> They did not test the two methods with\\n> data simulated where the assumption of\\n> proportional hazards is not true. I\\n> have seen one data set where the two\\n> estimate of HR were very different (by\\n> a factor of three), and the assumption\\n> of proportional hazards was dubious\\n> for those data. It seems that the\\n> Mantel-Haenszel method gives more\\n> weight to differences in the hazard at\\n> late time points, while the logrank\\n> method gives equal weight everywhere\\n> (but I have not explored this in\\n> detail).  If you see very different HR\\n> values with the two methods, think\\n> about whether the assumption of\\n> proportional hazards is reasonable. If\\n> that assumption is not reasonable,\\n> then of course the entire concept of a\\n> single hazard ratio describing the\\n> entire curve is not meaningful\\n\\nThe site also refer to the dataset in which ""the two estimate of HR were very different (by a factor of three)"", and suggest that the PH assumption is a key consideration. \\n\\nThen I thought, ""Who authored the site?"" After a little searching I found it was Harvey Motulsky. So Harvey I've managed to reference you in answering your own question. You've become the authority!\\n\\nIs the ""problem dataset"" a publicly available dataset?\\n\\n\\n\\n\\n",,
4764,2,2066,0a95ac4b-2f4b-4067-98f9-5c6196f7dc82,2010-08-24 09:48:08.0,956.0,"Is there a numerically stable way to calculate values of a [beta distribution][1] for large integer alpha, beta (e.g. alpha,beta > 1000000)?\\n\\nActually, I only need a 99% confidence interval around the mode, if that somehow makes the problem easier.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Beta_distribution",,
4765,1,2066,0a95ac4b-2f4b-4067-98f9-5c6196f7dc82,2010-08-24 09:48:08.0,956.0,How can I (numerically) approximate values for a beta distribution with large alpha & beta,,
4766,3,2066,0a95ac4b-2f4b-4067-98f9-5c6196f7dc82,2010-08-24 09:48:08.0,956.0,<confidence-interval><algorithms>,,
4767,2,2067,8ab3eff0-99ab-423e-837f-f6b9e39e3f2d,2010-08-24 10:05:42.0,442.0,"I am currently finishing a paper and stumbled upon [this question from yesterday][1] which led me to pose the same question to myself. Is it better to provide my graph with the actual standard error from the data or the one estimated from my ANOVA?  \\nAs the question from yesterday was rather unspecific and mine is pretty specific I thought it would be appropriate to pose this follow-up question.\\n\\n**Details:**  \\nI have run an experiment in some cognitive psychology domain (conditional reasoning) comparing two groups (inductive and deductive instructions, i.e., a between-subjects manipulation) with two within-subjects manipulations (problem type and content of the problem, each with two factor levels).  \\n\\nThe results look like this (left panel with SE-estimates from the ANOVA Output, right panel with SEs estimated from the data):\\n![alt text][2]  \\nNote that the different lines represent the two different groups (i.e., the between-subjects manipulation) and the within-subjects manipulations are plotted on the x-axis (i.e., the 2x2 factor levels).\\n\\nIn the text I provide the respective results of the ANOVA and even planned comparisons for the critical cross-over interaction in the middle. **The SEs are there to give the reader some hint about the variability of the data.** I prefer SEs over standard deviations and confidence intervals as it is not common to plot SDs and there are severe problems when comparing within- and between-subjects CIs (as the same surely applys for SEs, it is not so common to falsely infer significant differences from them).\\n\\nTo repeat my question: **Is it better to plot the SEs estimated from the ANOVA** (actually I am not 100% sure what they really are, I think they should be the SDs of the residuals, but I cannot reproduce this with SPSS) **or should I plot the SEs estimated from the raw data?** \\n\\n\\n  [1]: http://stats.stackexchange.com/questions/2024/plotting-standard-errors\\n  [2]: http://i.stack.imgur.com/fHmrK.png",,
4768,1,2067,8ab3eff0-99ab-423e-837f-f6b9e39e3f2d,2010-08-24 10:05:42.0,442.0,Follow up: In a mixed within-between ANOVA plot estimated SEs or actual SEs?,,
4769,3,2067,8ab3eff0-99ab-423e-837f-f6b9e39e3f2d,2010-08-24 10:05:42.0,442.0,<anova><data-visualization><within-subjects><standard-error><between-subjects>,,
4770,2,2068,157fe4d7-b6c7-4a43-bde3-ec5f2188a6a2,2010-08-24 10:35:37.0,601.0,"You will not find a single reasonable error bar for inferential purposes with this type of experimental design.  This is an old problem with no clear solution.\\n\\nIt seems impossible to have the estimate SE's you have here.  There are two main kinds of error in such a design, the between and within S error.  They are usually very different from one another and not comparable.  There just really is no good single error bar to represent your data.\\n\\nOne might argue that the raw SEs or SDs from the data are most important in a descriptive rather than inferential sense.  They either tell about the quality of the central tendency estimate (SE) or the variability of the data (SD). However, even then it's somewhat disingenuous because the thing you're testing and measuring within S is not that raw value but rather the effect of the within S variable.  Therefore, reporting variability of the raw values is either meaningless or misleading with respect to within S effects.\\n\\nI have typically endorsed no error bars on such graphs and adjacent effects graphs indicating the variability of the effects.  One might have CI's on that graph that are perfectly reasonable.  See Masson & Loftus (2003) for examples of the effects graphs.  Simply eliminate their ((pretty much completely useless) error bars around the mean values they show and just use the effect error bars.\\n\\nFor your study I'd first replot the data as the 2 x 2 x 2 design it is (2-panel 2x2) and then plot immediately adjacent a graph with confidence intervals of the validity, plausibility, instruction, and interaction effects.  Put SDs and SEs for the instruction groups in a table or in the text.\\n\\n(waiting for expected mixed effects analysis response ;) )",,
4771,2,2069,239b8f8a-d18d-4ff1-8c1f-8ad9306fd990,2010-08-24 11:21:58.0,521.0,"This looks like a very nice experiment, so congratulations!\\n\\nI agree with John Christie, it is a mixed model, but provided it can be properely specified in an ANOVA design (& is balanced) I don't see why it can't be so formulated. Two factor within and 1-factor between subjects, but the between subjects factor (inductive/deductive) clearly interacts (modifies) the within-subjects effects. I assume the plotted means are from the ANOVA model (LHS) and so the model is correctly specified. Well done -  this is non-trivial! \\n\\nSome points:\\n1) The ""estimated"" vs ""actual"" ""error"" is a false dichotomy. Both assume an underlying model and make estimates on that basis. If the model is reasonable, I would argue it is better to use the model-based estimates (they are based on the pooling of larger samples). But as James mentions, the errors differ depending on the comparison you are making, so no simple representation is possible.\\n\\n2) I would prefer to see box-plots or individual data points plotted (if there are not too many), perhaps with some sideways jitter, so points with the same value can be distinguished.\\n\\nhttp://en.wikipedia.org/wiki/Box_plot\\n\\n3) If you must plot an estimate of the error of the mean, never plot SDs - they are an estimate of the standard deviation of the sample and relate to population variablility, not a statistical comparison of means. It is generally preferable to plot 95% confidence intervals rather than SEs, but not in this case (see 1 and John's point)\\n\\n4) The one issue with this data that concerns me is the assumption of uniform variance is probably violated as the ""MP Valid and Plausible"" data are clearly constrained by the 100% limit, especially for the deductive people. I'm tossing up in my own mind how important this issue is. It might be best to let others answer.\\n\\n",,
4772,5,2067,5c41b4c8-ecca-4081-b84e-0765fd182906,2010-08-24 11:31:24.0,442.0,"I am currently finishing a paper and stumbled upon [this question from yesterday][1] which led me to pose the same question to myself. Is it better to provide my graph with the actual standard error from the data or the one estimated from my ANOVA?  \\nAs the question from yesterday was rather unspecific and mine is pretty specific I thought it would be appropriate to pose this follow-up question.\\n\\n**Details:**  \\nI have run an experiment in some cognitive psychology domain (conditional reasoning) comparing two groups (inductive and deductive instructions, i.e., a between-subjects manipulation) with two within-subjects manipulations (problem type and content of the problem, each with two factor levels).  \\n\\nThe results look like this (left panel with SE-estimates from the ANOVA Output, right panel with SEs estimated from the data):\\n![alt text][2]  \\nNote that the different lines represent the two different groups (i.e., the between-subjects manipulation) and the within-subjects manipulations are plotted on the x-axis (i.e., the 2x2 factor levels).\\n\\nIn the text I provide the respective results of the ANOVA and even planned comparisons for the critical cross-over interaction in the middle. **The SEs are there to give the reader some hint about the variability of the data.** I prefer SEs over standard deviations and confidence intervals as it is not common to plot SDs and there are severe problems when comparing within- and between-subjects CIs (as the same surely applys for SEs, it is not so common to falsely infer significant differences from them).\\n\\nTo repeat my question: **Is it better to plot the SEs estimated from the ANOVA or should I plot the SEs estimated from the raw data?** \\n\\n**Update:**  \\nI think I should be a little bit clearer in what the estimated SEs are. The ANOVA Output in SPSS gives me `estimated marginal means` with corresponding SEs and CIs. This is what is plotted in the left graph. As far as I understand this, they should be the SDs of the residuals. But, when saving the residuals their SDs are not somehow near the estimated SEs. So a secondary (potentially SPSS specific) question would be:   \\n**What are these SEs?**\\n\\n  [1]: http://stats.stackexchange.com/questions/2024/plotting-standard-errors\\n  [2]: http://i.stack.imgur.com/fHmrK.png",added a descricption of what the estimated means are,
4773,2,2070,0fae3b2c-ff37-482d-8f7d-463336c59a4c,2010-08-24 11:32:58.0,364.0,"Lately I've been using mixed effects analysis, and in attempting to develop an accompanying visual data analysis approach I've been using bootstrapping ([see my description here][1]), which yields confidence intervals that are not susceptible to the within-versus-between troubles of conventional CIs.\\n\\nAlso, I would avoid mapping multiple variables to the same visual aesthetic, as you have done in the graph above; you have 3 variables (MP/AC, valid/invalid, plausible/implausible) mapped to the x-axis, which makes it rather difficult to parse the design and patterns. I would suggest instead mapping, say, MP/AC to the x-axis, valid/invalid to facet columns, and plausible/implausible to facet rows. Check out ggplot2 in R to easily achieve this, eg:\\n\\n	library(ggplot2)\\n	ggplot(\\n		data = my_data\\n		, mapping = aes(\\n			y = mean_endorsement\\n			, x = mp_ac\\n			, linetype = deductive_inductive\\n			, shape = deductive_inductive\\n	)+\\n	geom_point()+\\n	geom_line()+\\n	facet_grid(\\n		plausible_implausible ~ valid_invalid\\n	)\\n\\n\\n  [1]: https://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q3/004369.html\\n",,
4774,5,2069,9e3d45be-dddc-4a71-8b19-edae081bff16,2010-08-24 11:39:14.0,521.0,"This looks like a very nice experiment, so congratulations!\\n\\nI agree with John Christie, it is a mixed model, but provided it can be properely specified in an ANOVA design (& is balanced) I don't see why it can't be so formulated. Two factor within and 1-factor between subjects, but the between subjects factor (inductive/deductive) clearly interacts (modifies) the within-subjects effects. I assume the plotted means are from the ANOVA model (LHS) and so the model is correctly specified. Well done -  this is non-trivial! \\n\\nSome points:\\n1) The ""estimated"" vs ""actual"" ""error"" is a false dichotomy. Both assume an underlying model and make estimates on that basis. If the model is reasonable, I would argue it is better to use the model-based estimates (they are based on the pooling of larger samples). But as James mentions, the errors differ depending on the comparison you are making, so no simple representation is possible.\\n\\n2) I would prefer to see box-plots or individual data points plotted (if there are not too many), perhaps with some sideways jitter, so points with the same value can be distinguished.\\n\\nhttp://en.wikipedia.org/wiki/Box_plot\\n\\n3) If you must plot an estimate of the error of the mean, never plot SDs - they are an estimate of the standard deviation of the sample and relate to population variablility, not a statistical comparison of means. It is generally preferable to plot 95% confidence intervals rather than SEs, but not in this case (see 1 and John's point)\\n\\n4) The one issue with this data that concerns me is the assumption of uniform variance is probably violated as the ""MP Valid and Plausible"" data are clearly constrained by the 100% limit, especially for the deductive people. I'm tossing up in my own mind how important this issue is. Moving to a mixed-effects logit (binomial probability) is probably the ideal solution, but it's a hard ask. It might be best to let others answer.\\n\\n",added 108 characters in body,
4775,5,2068,44a1ff76-fb2e-4af4-b15d-3d5661215708,2010-08-24 12:12:36.0,601.0,"You will not find a single reasonable error bar for inferential purposes with this type of experimental design.  This is an old problem with no clear solution.\\n\\nIt seems impossible to have the estimate SE's you have here.  There are two main kinds of error in such a design, the between and within S error.  They are usually very different from one another and not comparable.  There just really is no good single error bar to represent your data.\\n\\nOne might argue that the raw SEs or SDs from the data are most important in a descriptive rather than inferential sense.  They either tell about the quality of the central tendency estimate (SE) or the variability of the data (SD). However, even then it's somewhat disingenuous because the thing you're testing and measuring within S is not that raw value but rather the effect of the within S variable.  Therefore, reporting variability of the raw values is either meaningless or misleading with respect to within S effects.\\n\\nI have typically endorsed no error bars on such graphs and adjacent effects graphs indicating the variability of the effects.  One might have CI's on that graph that are perfectly reasonable.  See Masson & Loftus (2003) for examples of the effects graphs.  Simply eliminate their ((pretty much completely useless) error bars around the mean values they show and just use the effect error bars.\\n\\nFor your study I'd first replot the data as the 2 x 2 x 2 design it is (2-panel 2x2) and then plot immediately adjacent a graph with confidence intervals of the validity, plausibility, instruction, and interaction effects.  Put SDs and SEs for the instruction groups in a table or in the text.\\n\\n(waiting for expected mixed effects analysis response ;) )\\n\\nUPDATE:  OK, after editing it's clear the only thing you want is an SE to be used to show the quality of the estimate of the value.  In that case use your model values.  Both values are based on a model and there is no 'true' value in your sample.  Use the ones from the model you applied to your data.  BUT, make sure you warn readers in the figure caption that these SEs have no inferential value whatsoever for your within S effects or interactions.",added 322 characters in body; added 134 characters in body,
4776,2,2071,757167a6-8fc5-44ec-b501-1f83f469b462,2010-08-24 12:38:01.0,449.0,"A quick graphical experiment suggests that the beta distribution looks very like a normal distribution when alpha and beta are both very large. By googling ""beta distribution limit normal"" i found http://nrich.maths.org/discus/messages/117730/143065.html?1200700623, which gives a handwaving 'proof'.\\n\\nThe wikipedia page for the beta distribution gives its mean, mode (v close to mean for large alpha and beta) and variance, so you could use a normal distribution with the same mean & variance to get an approximation. Whether it's a good enough approximation for your purposes depends on what your purposes are.",,
4777,2,2072,d4245d3c-1191-4461-a322-a708f2f31c92,2010-08-24 13:07:33.0,5.0,"It isn't clear to me how to calculate cointegration with irregular time series (ideally using [the Johansen test][1] with VECM).  My initial thought would be to regularize the series and interpolate missing values, although that may bias the estimation.\\n\\nIs there any literature on this subject?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Johansen_test",,
4778,1,2072,d4245d3c-1191-4461-a322-a708f2f31c92,2010-08-24 13:07:33.0,5.0,Does a cointegration model exist for irregularly spaced time series?,,
4779,3,2072,d4245d3c-1191-4461-a322-a708f2f31c92,2010-08-24 13:07:33.0,5.0,<time-series><cointegration>,,
4780,6,2067,758a49c1-eb7e-4a34-96ce-011e0e3c2ba9,2010-08-24 13:17:24.0,442.0,<data-visualization><anova><mixed-model><standard-error>,changed tags,
4781,2,2073,12e79dea-f53d-4c2f-91c0-2f50e2a2ce02,2010-08-24 15:06:00.0,795.0,"I am going to infer you want an interval $[l,r]$ such that the probability that a random draw from the Beta RV is in the interval with probability 0.99, with bonus points for $l$ and $r$ being symmetric around the mode. By [Gauss' Inequality][1] or the [Vysochanskii-Petunin inequality][2], you can construct intervals that contain the interval $[l,r]$, and would be fairly decent approximations. For sufficiently large $\\alpha, \\beta$, you will have numerical underflow issues in even _representing_ $l$ and $r$ as distinct numbers, so this route may be good enough.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Gauss's_inequality\\n  [2]: http://en.wikipedia.org/wiki/Vysochanskiï–Petunin_inequality",,
4782,2,2074,0cf3d44b-613e-4bce-bd77-768c0226b7c4,2010-08-24 15:43:19.0,919.0,"A Normal approximation works extremely well, *especially* in the tails.  Use a mean of alpha/(alpha+beta) and a variance of alpha*beta/((alpha+beta)^2*(1+alpha+beta)).  For example, the absolute relative error in the tail probability in a tough situation (where skewness might be of concern) such as alpha = 10^6, beta = 10^8 peaks around 0.00026 and is less than 0.00006 when you're more than 1 SD from the mean.  (This is *not* because beta is so large: with alpha = beta = 10^6, the absolute relative errors are bounded by 0.0000001.)  Thus, this approximation is excellent for essentially any purpose involving 99% intervals.",,
4783,5,2066,de44690b-803f-4a54-abcb-11d8bf257417,2010-08-24 15:54:59.0,956.0,"Is there a numerically stable way to calculate values of a [beta distribution][1] for large integer alpha, beta (e.g. alpha,beta > 1000000)?\\n\\nActually, I only need a 99% confidence interval around the mode, if that somehow makes the problem easier.\\n\\n**Add**: I'm sorry, my question wasn't as clearly stated as I thought it was. What I want to do is this: I have a machine that inspects products on a conveyor belt. Some fraction of these products is rejected by the machine. Now if the machine operator changes some inspection setting, I want to show him/her the estimated reject rate and some hint about how reliable the current estimate is.\\n\\nSo I thought I treat the actual reject rate as a random variable X, and calculate the probability distribution for that random variable based on the number of rejected objects N and accepted objects M. If I assume a uniform prior distribution for X, this is a beta distribution depending on N and M. I can either display this distribution to the user directly or find an interval [l,r] so that the actual reject rate is in this interval with p >= 0.99 (using shabbychef's terminology) and display this interval. For small M, N (i.e. immediately after the parameter change), I can calculate the distribution directly and approximate the interval [l,r]. But for large M,N, this naive approach leads to underflow errors, because x^N*(1-x)^M is to small to be represented as a double precision float.\\n\\nI guess my best bet is to use my naive beta-distribution for small M,N and switch to a normal distribution with the same mean and variance as soon as M,N exceed some threshold. Does that make sense?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Beta_distribution",Add,
4784,2,2075,690fa9f5-f1e2-440b-93c1-df5445ea0471,2010-08-24 16:17:53.0,253.0,I also just realized the [freakonomics has a podcast][1]\\n\\n\\n  [1]: http://itunes.apple.com/us/podcast/freakonomics-radio/id354668519,,
4785,16,2075,690fa9f5-f1e2-440b-93c1-df5445ea0471,2010-08-24 16:17:53.0,-1.0,,,
4786,2,2076,ad13077e-e8c7-46f6-88d2-2083655c76d5,2010-08-24 17:08:00.0,1076.0,"If you wanted to check non-parametrically for significance, you could bootstrap the confidence intervals on the ratio, or you could do a permutation test on the two classes. For example, to do the bootstrap, create two arrays: one with 3 ones and 999,997 zeros, and one with 10 ones and 999,990 zeros. Then draw with replacement a sample of 1m items from the first population and a sample of 1m items from the second population. The ratio we're interested in is the ratio of ""hits"" in the first group to the ratio of ""hits"" in the second group, or: (proportion of ones in the first sample) / (proportion of ones in the second sample). We do this 1000 times. I don't have matlab handy but here's the R code to do it:\\n\\n    # generate the test data to sample from\\n    v1 <- c(rep(1,3),rep(0,999997))\\n    v2 <- c(rep(1,10),rep(0,999990))\\n    # set up the vectors that will hold our proportions\\n    t1 <- vector()\\n    t2 <- vector()\\n    # loop 1000 times each time sample with replacement from the test data and\\n    # record the proportion of 1's from each sample\\n    # note: this step takes a few minutes. There are ways to write it such that\\n    # it will go faster in R (applies), but it's more obvious what's going on this way:\\n    for(i in 1:1000) {\\n       t1[i] <- length(which(sample(v1,1000000,replace=TRUE)==1)) / 1000000\\n       t2[i] <- length(which(sample(v2,1000000,replace=TRUE)==1)) / 1000000\\n    }\\n    # what was the ratio of the proportion of 1's between each group for each random draw?\\n    ratios <- t1 / t2\\n    # grab the 95% confidence interval over the bootstrapped samples\\n    quantile(ratios,c(.05,.95))\\n    # and the 99% confidence interval\\n    quantile(ratios,c(.01,.99))\\nThe output is:\\n    5%       95% \\n    0.0000000 0.8333333 \\nand:\\n    1%  99% \\n    0.00 1.25 \\nSince the 95% confidence interval doesn't overlap the null hypothesis (1), but the 99% confidence interval does, I believe that it would be correct to say that this is significant at an alpha of .05 but not at .01. \\n\\nAnother way to look at it is with a permutation test to estimate the distribution of ratios given the null hypothesis. In this case you'd mix the two samples together and randomly divide it into two 1,000,000 item groups. Then you'd see what the distribution of ratios under the null hypothesis looks like, and your empirical p-value is how extreme the true ratio is given this distribution of null ratios. Again, the R code:\\n\\n    # generate the test data to sample from\\n    v1 <- c(rep(1,3),rep(0,999997))\\n    v2 <- c(rep(1,10),rep(0,999990))\\n    v3 <- c(v1,v2)\\n    \\n    # vectors to hold the null hypothesis ratios\\n    t1 <- vector()\\n    t2 <- vector()\\n    \\n    # loop 1000 times; each time randomly divide the samples\\n    # into 2 groups and see what those two random groups' proportions are\\n    for(i in 1:1000) {\\n      idxs <- sample(1:2000000,1000000,replace=FALSE)\\n      s1 <- v3[idxs]\\n      s2 <- v3[-idxs]\\n      t1[i] <- length(which(s1==1)) / 1000000\\n      t2[i] <- length(which(s2==1)) / 1000000\\n    }\\n    \\n    # vector of the ratios\\n    ratios <- t1 / t2\\n    \\n    # take a look at the distribution\\n    plot(density(ratios))\\n    \\n    # calculate the sampled ratio of proportions\\n    sample.ratio <- ((3/1000000)/(10/1000000))\\n    # where does this fall on the distribution of null proportions?\\n    plot(abline(v=sample.ratio))\\n    \\n    # this ratio (r+1)/(n+1) gives the p-value of the true sample\\n    (length(which(ratios <= sample.ratio)) + 1) / (1001)\\n\\nThe output is ~ .0412 (of course this will vary run to run since it's based on random draws). So again, you could potentially call this significant at the .05 value.\\n\\nI should issue the caveats: it depends too on how your data was collected and the type of study, and I'm just a grad student so don't take my word as gold. If anyone has any criticism of my methods I'd love to hear them since I'm doing this stuff for my own work as well and I'd love to find out the methods are flawed here rather than in peer review. For more stuff like this check out Efron & Tibshirani 1993, or chapter 14 of Introduction to the Practice of Statistics by David Moore (a good general textbook for practitioners).\\n",,
4787,5,2074,90d65016-bbbb-45d5-a5a0-25aa30a335e6,2010-08-24 17:16:41.0,919.0,"A Normal approximation works extremely well, *especially* in the tails.  Use a mean of alpha/(alpha+beta) and a variance of alpha*beta/((alpha+beta)^2*(1+alpha+beta)).  For example, the absolute relative error in the tail probability in a tough situation (where skewness might be of concern) such as alpha = 10^6, beta = 10^8 peaks around 0.00026 and is less than 0.00006 when you're more than 1 SD from the mean.  (This is *not* because beta is so large: with alpha = beta = 10^6, the absolute relative errors are bounded by 0.0000001.)  Thus, this approximation is excellent for essentially any purpose involving 99% intervals.\\n\\nIn light of the edits to the question, note that one does not compute beta integrals by actually integrating the integrand: of course you'll get underflows (although they don't really matter, because they don't contribute appreciably to the integral).  There are many, many ways to compute the integral or approximate it, as documented in Johnson & Kotz (Distributions in Statistics).  An online calculator is found at http://www.danielsoper.com/statcalc/calc37.aspx .  You actually need the inverse of this integral.  Some methods to compute the inverse are documented on the Mathematica site at http://functions.wolfram.com/GammaBetaErf/InverseBetaRegularized/ .  Code is provided in Numerical Recipes (www.nr.com).  A really nice online calculator is the Wolfram Alpha site (www.wolframalpha.com ): enter ""inverse beta regularized (.005, 1000000, 1000001)"" for the left endpoint and ""inverse beta regularized (.995, 1000000, 1000001)"" for the right endpoint (alpha=1000000, beta=1000001, 99% interval).",added 1001 characters in body; added 4 characters in body; added 4 characters in body,
4788,2,2077,d0836078-2167-4246-9b9c-0c79e9dad814,2010-08-24 18:40:42.0,5.0,"Besides taking differences, what are other techniques for making a non-stationary time series, stationary?\\n\\nOrdinarily one refers to a series as ""[integrated of order p][1]"" if it can be made stationary through a lag operator $(1-L)^P X_t$.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Order_of_integration",,
4789,1,2077,d0836078-2167-4246-9b9c-0c79e9dad814,2010-08-24 18:40:42.0,5.0,How to make a time series stationary?,,
4790,3,2077,d0836078-2167-4246-9b9c-0c79e9dad814,2010-08-24 18:40:42.0,5.0,<time-series><stationarity>,,
4791,2,2078,1c124592-5391-4322-bc63-e296dd376c82,2010-08-24 19:25:14.0,511.0,"<a href=""http://en.wikipedia.org/wiki/Geometric_median"">Geometric median</a> is the point with the smallest average euclidian distance from the samples",,
4792,5,1947,2807ce3a-df5f-43ba-b7b2-7696527fd6e4,2010-08-24 19:36:16.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer falls into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$  (with $x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{\\alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)) . Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can generalize the definition of quantiles in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",added 1 characters in body,
4793,2,2079,a564325c-3b3e-4b22-a512-239f6459371a,2010-08-24 20:09:45.0,511.0,"There seems to be a large body of applied research where distribution q is picked to minimize <a href=""http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"">KL(q,p)</a> where p is empirical distribution. Are there theoretical reasons to prefer this estimator? For instance, a theoretical reason to prefer MLE for estimating mean of a one-dimensional Gaussian with mean between -1 and 1 because it's admissible, and also as sample size goes to infinity, it minimizes maximum risk and Bayes risk.",,
4794,1,2079,a564325c-3b3e-4b22-a512-239f6459371a,2010-08-24 20:09:45.0,511.0,What's good about I-projections?,,
4795,3,2079,a564325c-3b3e-4b22-a512-239f6459371a,2010-08-24 20:09:45.0,511.0,<estimation>,,
4796,2,2080,a2f069cd-000e-44e6-9008-9360631f273d,2010-08-24 21:27:09.0,919.0,"**De-trending** is fundamental.  This includes regressing against covariates other than time.\\n\\n**Seasonal adjustment** is a version of taking differences but could be construed as a separate technique.\\n\\n**Transformation** of the data implicitly converts a difference operator into something else; e.g., differences of the logarithms are actually ratios.\\n\\nSome **EDA smoothing** techniques (such as removing a moving median) could be construed as non-parametric ways of detrending.  They were used as such by Tukey in his book on EDA.  Tukey continued by detrending the residuals and iterating this process for as long as necessary (until he achieved residuals that appeared stationary and symmetrically distributed around zero).",,
4797,2,2081,845ad8b5-5412-42d2-bee0-d8d6f234fd1c,2010-08-24 22:19:51.0,919.0,"> in that notation, I'm looking for epsilon as a function of m \\n\\nIf I have not misunderstood, this indicates you want to solve an equation in q (= p + epsilon) of the form\\n\\nq ln(p/q) + (1-q) ln((1-p)/(1-q)) = D(q||p) = y; y < 0,\\n\\ngiven p and y.  (y = ln(p)/m reveals the m-dependence.)\\n\\nIt is correct that we do not have a name for the solution, but that does not mean it cannot be found, and fairly easily at that.  A little algebra converts this equation into one of the form\\n\\nH(q) = alpha + beta*q where\\n\\nH(q) = -q ln(q) - (1-q) ln(1-q) and alpha and beta depend only on y (""probability of deviation"") and m (""sample size"").\\n\\nGeometrically this asks for the intersections of a concave downward curve and a line; the curve has endpoints at (0,0) and (1,0) and is symmetric in the unit interval.  There will therefore be up to two solutions.  Newton-Raphson ought to converge rapidly after an initial set of bisection steps finds a point between 0 and the left root and another between the right root and 1 (assume either exist).\\n\\nIf you need them, theoretical properties of the solution(s) could be readily derived from the definitions of H, alpha, and beta.",,
4798,2,2082,5e862279-8f85-4bdf-9ecf-1f1673447db8,2010-08-24 22:38:15.0,603.0,"Difference with another series. i.e. Brent oil prices are not stationary,\\nbut the spread brent-light sweet crude is. A more risky proposition for \\nforecasting is to bet on the existence of a co integration relationship\\nwith another time series.",,
4800,5,1923,deae9e4e-817e-4475-b858-26acf484c5c7,2010-08-25 00:37:40.0,511.0,"Suppose instead of maximizing likelihood I maximize some other function g. Like likelihood, this function decomposes over x's (ie, g({x1,x2})=g({x1})g({x2}), and ""maximum-g"" estimator is consistent. How do I compute asymptotic variance of this estimator?\\n\\n<b>Update 8/24/10</b>: Percy Liang goes through derivation of asymptotic variance in a similar setting in <a href=""http://www.eecs.berkeley.edu/~pliang/papers/asymptotics-icml2008.pdf"">An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators.</a>",added 279 characters in body,
4801,2,2084,7a9c382b-61c6-4557-80b8-46a0eeb26e42,2010-08-25 00:46:17.0,511.0,"""Asymptotic Statistics"" by A. W. van der Vaart seems to serve the purpose",,
4802,2,2085,a72f2347-0b0c-4efb-8a1a-ad8e4ff49817,2010-08-25 01:57:42.0,108.0,"I am modelling a nested mixed-effects model with just the intercept in the random part, of the form:\\n\\n    fit4<-lme(fixed = Stdquad~factor(LayoutN)+factor(nCHIPS.fixed), random = ~1|Class.Ordered/student)\\n\\nWhen trying to check the assumption on the independent, identically distributed (iid) random effects in SPlus using `ranef()`, I keep getting the error:\\n\\n    Problem in sort.list: ordering not defined for mode ""list"": sort.list(x, partial)\\n\\n \\nI suspect the nesting is the problem since if I remove *student*, the plots come up fine. \\n\\nI would like to know how I can check that the random effects are iid and are independent for different groups using SPlus (or R since its very similar).",,
4803,1,2085,a72f2347-0b0c-4efb-8a1a-ad8e4ff49817,2010-08-25 01:57:42.0,108.0,Checking assumptions for random effects in nested mixed-effects models in SPlus,,
4804,3,2085,a72f2347-0b0c-4efb-8a1a-ad8e4ff49817,2010-08-25 01:57:42.0,108.0,<r><mixed-model><error><random-effects-model>,,
4805,2,2086,444ea01a-d427-48ec-8046-65be17d05893,2010-08-25 03:16:23.0,840.0,"I'm looking for correlations between the answers to different questions in a survey (""umm, let's see if answers to question 11 correlate with those of question 78""). All answers are categorical (most of them range from ""very unhappy"" to ""very happy""), but a few have a different set of answers. Most of them can be considered ordinal so let's consider this  case here.\\n\\nSince I don't have access to a commercial statistics program, I must use R.\\n\\nI tried [Rattle][1] (a freeware data mining package for R, very nifty) but unfortunately it doesn't support categorical data. One hack I could use is to import in R the coded version of the survey which has numbers (1..5) instead of ""very unhappy"" ... ""happy"" and let Rattle believe they are numerical data.\\n\\nI was thinking to do a scatter plot and have the dot size proportional to the number of numbers for each pair. After some googling I found http://www.r-statistics.com/2010/04/correlation-scatter-plot-matrix-for-ordered-categorical-data/ but it seems very complicated. \\n\\nI'm not a statistician (but a programmer) but have had some reading in the matter and, if I understand correctly, *Spearman's rho* would be appropriate here. \\n\\nSo the short version of the question for those in a hurry: **is there a way to quickly plot \\nSpearman's rho in R**? A plot is preferable to a matrix of numbers because it's easier to eye ball and also can be included in materials.\\n\\nThank you in advance. \\n\\nPS I pondered for a while whether to post this on the main SO site or here. After searching both sites for R correlation, I felt this site is better suited for the question.\\n\\n\\n  [1]: http://rattle.togaware.com/",,
4806,1,2086,444ea01a-d427-48ec-8046-65be17d05893,2010-08-25 03:16:23.0,840.0,Quickly evaluate (visually) correlations between ordered categorical data in R?,,
4807,3,2086,444ea01a-d427-48ec-8046-65be17d05893,2010-08-25 03:16:23.0,840.0,<r><correlation><categorical-data><data-visualization>,,
4808,2,2087,6fbc6525-050c-429c-9e21-590f05cd1a5f,2010-08-25 03:32:04.0,253.0,"Another good visualization of correlation is offered by the [corrplot][1] package, giving you things like this:\\n![alt text][2]\\n\\nIt is a great package.\\n\\nAlso have a look at the [answer here][3], it might be good for you to know.\\n\\nLastly, if you have suggestions how the code on the post you referred to could be simpler - please let me know.\\n\\n  [1]: http://cran.r-project.org/web/packages/corrplot/index.html\\n  [2]: http://i.stack.imgur.com/IsxzL.png\\n  [3]: http://stats.stackexchange.com/questions/1961/can-you-use-normal-correlation-for-vectors-with-only-2-or-3-ordered-levels",,
4809,5,2086,404ab8b4-7a5e-46fd-bc61-412b0a783272,2010-08-25 04:05:22.0,840.0,"I'm looking for correlations between the answers to different questions in a survey (""umm, let's see if answers to question 11 correlate with those of question 78""). All answers are categorical (most of them range from ""very unhappy"" to ""very happy""), but a few have a different set of answers. Most of them can be considered ordinal so let's consider this  case here.\\n\\nSince I don't have access to a commercial statistics program, I must use R.\\n\\nI tried [Rattle][1] (a freeware data mining package for R, very nifty) but unfortunately it doesn't support categorical data. One hack I could use is to import in R the coded version of the survey which has numbers (1..5) instead of ""very unhappy"" ... ""happy"" and let Rattle believe they are numerical data.\\n\\nI was thinking to do a scatter plot and have the dot size proportional to the number of numbers for each pair. After some googling I found http://www.r-statistics.com/2010/04/correlation-scatter-plot-matrix-for-ordered-categorical-data/ but it seems very complicated (to me).\\n\\nI'm not a statistician (but a programmer) but have had some reading in the matter and, if I understand correctly, *Spearman's rho* would be appropriate here. \\n\\nSo the short version of the question for those in a hurry: **is there a way to quickly plot \\nSpearman's rho in R**? A plot is preferable to a matrix of numbers because it's easier to eye ball and also can be included in materials.\\n\\nThank you in advance. \\n\\nPS I pondered for a while whether to post this on the main SO site or here. After searching both sites for R correlation, I felt this site is better suited for the question.\\n\\n\\n  [1]: http://rattle.togaware.com/",added 7 characters in body,
4810,2,2088,8b3768ae-24a2-4d05-aaf3-ae3255ebe35c,2010-08-25 04:50:17.0,183.0,A couple of additional plotting ideas are:\\n\\n- [Sunflower plot][1]\\n- Scatter plot with a jitter using [base graphics][2] or [ggplot2][3]\\n\\n\\n  [1]: http://addictedtor.free.fr/graphiques/RGraphGallery.php?graph=59\\n  [2]: http://stat.ethz.ch/R-manual/R-patched/library/base/html/jitter.html\\n  [3]: http://had.co.nz/ggplot2/geom_jitter.html,,
4811,2,2089,ee81ff48-bf26-4f85-8eff-e28d13a3511f,2010-08-25 05:19:46.0,1080.0,"First, let me say I agree with John D Cook's answer: Python is not a Domain Specific Language like R, and accordingly, there is a lot more you'll be able to do with it further down the road. Of course, R being a DSL means that the latest algorithms published in JASA will almost certainly be in R. If you are doing mostly ad hoc work and want to experiment with the latest lasso regression technique, say, R is hard to beat. If you are doing more production analytical work, integrating with existing software and environments, and concerned about speed, extensibility and maintainability, Python will serve you much better. \\n\\nSecond, ars gave a great answer with good links. Here are a few more packages that I view as essential to analytical work in Python (note that this is my first post on Statistical Analysis - what an awesome site - so I don't have enough street cred to embed links the nice way):\\n\\n - matplotlib for beautiful, publication quality graphics. See matplotlib.sourceforge.net/gallery.html\\n - IPython for an enhanced, interactive Python console. Importantly, IPython provides a powerful framework for interactive, parallel computing in Python. See ipython.scipy.org/moin/\\n - Cython for easily writing C extensions in Python. This package lets you take a chunk of computationally intensive Python code and easily convert it to a C extension. You'll then be able to load the C extension like any other Python module but the code will run very fast since it is in C. See www.cython.org/\\n - PyIMSL Studio for a collection of hundreds of mathemaical and statistical algorithms that are thoroughly documented and supported. You can call the exact same algorithms from Python and C, with nearly the same API and you'll get the same results. Full disclosure: I work on this product, but I also use it a lot. See www.vni.com/campaigns/pyimslstudioeval/\\n - xlrd for reading in Excel files easily. See pypi.python.org/pypi/xlrd\\n\\nIf you want a more MATLAB-like interactive IDE/console, check out Spyder, or the PyDev plugin for Eclipse. See\\n\\n  \\n\\n - code.google.com/p/spyderlib/\\n - pydev.org/\\n - www.eclipse.org/",,
4812,2,2090,13624a76-d134-4b7a-8a30-e446fca94b13,2010-08-25 05:38:05.0,183.0,ASA Sections on: Statistical Computing Statistical Graphics has a video library:\\n\\n- http://stat-graphics.org/movies/\\n\\nIt contains a large number of interesting videos relevant to statistical computing and graphics. The videos go back as far as the 1960s.,,
4813,16,2062,00000000-0000-0000-0000-000000000000,2010-08-25 08:02:51.0,88.0,,,
4814,16,2090,00000000-0000-0000-0000-000000000000,2010-08-25 08:02:51.0,88.0,,,
4815,16,2061,00000000-0000-0000-0000-000000000000,2010-08-25 08:02:51.0,88.0,,,
4816,2,2091,0147a1eb-72bc-4e8e-9e09-79cc7936ecc7,2010-08-25 08:22:06.0,862.0,"X is uniform random variable in [0,1] and Y=1-X. How do I calculate the distribution function F(X,Y)? I can see that Y is also uniformly distributed and can draw the intervals. But I am unable to compute the distribution function when x+y>1 & x,y in [0,1]. How do I compute it for this specific case?",,
4817,1,2091,0147a1eb-72bc-4e8e-9e09-79cc7936ecc7,2010-08-25 08:22:06.0,862.0,computing probability distribution function for uniform random variables and Y=1-X,,
4818,3,2091,0147a1eb-72bc-4e8e-9e09-79cc7936ecc7,2010-08-25 08:22:06.0,862.0,<distributions><probability>,,
4819,2,2092,bcf7ec68-7aae-4570-9ec4-46007f62142d,2010-08-25 08:33:14.0,862.0,"The waiting times for poisson distribution is an exponential distribution with parameter lambda. But I don't understand it. Poisson models the number of arrivals per unit of time for example. How is this related to exponential distribution? Lets say probability of k arrivals in a unit of time is P(k) (modeled by poisson) and probability of k+1 is P(k+1), how does exponential distribution model the waiting time between them?",,
4820,1,2092,bcf7ec68-7aae-4570-9ec4-46007f62142d,2010-08-25 08:33:14.0,862.0,relationship between poisson and exponential distribution,,
4821,3,2092,bcf7ec68-7aae-4570-9ec4-46007f62142d,2010-08-25 08:33:14.0,862.0,<poisson>,,
4822,2,2093,f1165ca0-9270-40be-a0c2-8ad81ab3298d,2010-08-25 09:32:22.0,937.0,"I've implemented the [Marsaglia polar method][1] to generate random numbers that are normally distributed. Unfortunately the code as shown on this [website][2] returns numbers that are not within the range [0, 1).\\n\\nCode:\\n\\n         double x1, y1, w;\\n \\n         do {\\n                 x1 = 2.0 * ranf() - 1.0;\\n                 y1 = 2.0 * ranf() - 1.0;\\n                 w = x1 * x1 + y1 * y1;\\n         } while ( w >= 1.0 || w == 0 );\\n\\n         w = sqrt( (-2.0 * ln( w ) ) / w );\\n         return x1 * w;\\n\\nwhere ranf() returns values [0, 1).\\n\\nFor example, if (x1, y1) was (-0.43458512585358, -0.07521858582050478), this returns a value of -1.7830255550765148. Obviously not a value within my expected range.\\n\\nI've seen some implementations that multiplies this by the standard deviation and adds the mean. But if I want to get numbers that range from [0, 1), what should I use as my standard deviation and mean? currently, I'm using a standard deviation of 1/8.125 ~= 0.121212... and 0.5 as the mean, but I've only stumbled on this by experimentation. What is the official way for me to get this appropriately ranged? \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Marsaglia_polar_method\\n  [2]: http://www.taygeta.com/random/gaussian.html",,
4823,1,2093,f1165ca0-9270-40be-a0c2-8ad81ab3298d,2010-08-25 09:32:22.0,937.0,"How to make Marsaglia polar method return values [0, 1) ? ",,
4824,3,2093,f1165ca0-9270-40be-a0c2-8ad81ab3298d,2010-08-25 09:32:22.0,937.0,<random-generation><normal-distribution>,,
4825,2,2094,671e1dd9-1468-4c4d-805e-99177840fb3a,2010-08-25 09:43:51.0,,"I will use the following notation to be as consistent as possible with the wiki (in case you want to go back and forth between my answer and the wiki definitions for the [poisson][1] and [exponential][2].)\\n\\n$N_t$: the number of arrivals during time period $t$\\n\\n$X_t$: the time it takes for one additional arrival to arrive assuming that someone arrived at time $t$\\n\\nBy definition, the following conditions are equivalent:\\n\\n$ (X_t > x) \\equiv (N_t = N_{t+x})$\\n\\nThe event on the left captures the event that no one has arrived in the time interval $[t,t+x]$ which implies that our count of the number of arrivals at time $t+x$ is identical to the count at time $t$ which is the event on the right.\\n\\nThus, it follows that:\\n\\n$P(X_t < x) = 1 - P(X_t > x)$\\n\\nUsing the equivalence of the two events that we described above, we can re-write the above as:\\n\\n$P(X_t < x) = 1 - P(N_{t+x} - N_t = 0)$\\n\\nBut,\\n\\n$P(N_{t+x} - N_t = 0) = P(N_x = 0)$ \\n\\nUsing the poisson pmf the above simplifies to:\\n\\n$P(N_{t+x} - N_t = 0) = e^{-\\lambda x}$\\n\\nSubstituting in our original eqn, we have:\\n\\n$P(X_t < x) = 1 - e^{-\\lambda x}$\\n\\nThe above is the cdf of a exponential pdf.\\n \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Poisson_distribution\\n  [2]: http://en.wikipedia.org/wiki/Exponential_distribution",,user28
4826,5,2085,b4ebdb7f-55b8-4891-ad1e-5e9291fc7e7a,2010-08-25 09:53:10.0,108.0,"I am modelling a nested mixed-effects model with just the intercept in the random part, of the form:\\n\\n    fit4<-lme(fixed = Stdquad~factor(LayoutN)+factor(nCHIPS.fixed), random = ~1|Class.Ordered/student)\\n\\nWhen trying to check the assumption on the independent, identically distributed (iid) random effects in SPlus using `ranef()`, I keep getting the error:\\n\\n    Problem in sort.list: ordering not defined for mode ""list"": sort.list(x, partial)\\n\\n \\nI suspect the nesting is the problem since if I remove *student*, the plots come up fine. \\n\\nI would like to know how I can check that the random effects are iid and are independent for different groups using S-Plus (or R since its very similar).",added 1 characters in body; edited title,
4827,4,2085,b4ebdb7f-55b8-4891-ad1e-5e9291fc7e7a,2010-08-25 09:53:10.0,108.0,Checking assumptions for random effects in nested mixed-effects models in R / S-Plus,added 1 characters in body; edited title,
4828,2,2095,ff2a9a0e-a54e-4567-854d-87e3ceb5e29c,2010-08-25 10:01:31.0,,"$F(x,y) = P(X \\le x, Y \\le y)$\\n\\nUsing the fact that $Y= 1-X$ and after simplifying, the above can be re-written as:\\n\\n$F(x,y) = P(1-y \\le X \\le x)$\\n\\nSince, $X$ is a uniform between 0 and 1, it follows that:\\n\\n$F(x,y) = x+y-1$\\n",,user28
4829,5,2093,f4d41f40-1049-4405-b03f-02b46555cf30,2010-08-25 10:37:54.0,937.0,"I've implemented the [Marsaglia polar method][1] to generate random numbers that are normally distributed. Unfortunately the code as shown on this [website][2] returns numbers that are not within the range [0, 1).\\n\\nCode:\\n\\n         double x1, y1, w;\\n \\n         do {\\n                 x1 = 2.0 * ranf() - 1.0;\\n                 y1 = 2.0 * ranf() - 1.0;\\n                 w = x1 * x1 + y1 * y1;\\n         } while ( w >= 1.0 || w == 0 );\\n\\n         w = sqrt( (-2.0 * ln( w ) ) / w );\\n         return x1 * w;\\n\\nwhere ranf() returns values [0, 1).\\n\\nFor example, if (x1, y1) was (-0.43458512585358, -0.07521858582050478), this returns a value of -1.7830255550765148. Obviously not a value within my expected range.\\n\\nI've seen some implementations that multiplies this by the standard deviation and adds the mean. But if I want to get numbers that range from [0, 1), what should I use as my standard deviation and mean? currently, I'm using a standard deviation of 1/8.125 ~= 0.121212... and 0.5 as the mean, but I've only stumbled on this by experimentation. What is the official way for me to get this appropriately ranged? \\n\\nTo answer the question in the comments below: I want to generate normal random variables within the range 0 and 1. Obviously I'd like the mean to be at 0.5 and values no be distributed around this mean.\\n\\nMore for my enlightenment, will the polar method really return numbers over the entire real line? If so, then it should be easy for me to scale the result down to [0, 1), assuming I don't hit floating point underflow due to the division to scale things down.\\n\\n  [1]: http://en.wikipedia.org/wiki/Marsaglia_polar_method\\n  [2]: http://www.taygeta.com/random/gaussian.html",Added more info to answer question in comments.,
4830,2,2096,6b19eff1-ef16-414f-8f72-4c5f75250680,2010-08-25 10:58:52.0,339.0,"For a Poisson process, hits occur at random independent of the past, but with a known long term average rate $\\lambda$ of hits per unit time. The Poisson distribution would let us find the probability of getting some particular number of hits.\\n\\nNow, instead of looking at the number of hits, we look at the random variable $L$ (for Lifetime), the time you have to wait for the first hit.\\n\\nThe probability that the waiting time is more than a given time value is\\n$P(L \\gt t)  = P(\\text{no hits in time t})=\\frac{\\Lambda^0e^{-\\Lambda}}{0!}=e^{-\\lambda t}$ (by the Poisson distribution, where $\\Lambda = \\lambda t$).\\n\\n$P(L \\le t) = 1 - e^{-\\lambda t}$ (the cumulative distribution function). \\nWe can get the density function by taking the derivative of this:\\n\\n<p>$f(x) = \\n\\begin{cases} \\n \\lambda e^{-\\lambda t} &amp; \\mbox{for } t \\ge 0 \\\\ 0 &amp; \\mbox{for } t \\lt 0\\n\\end{cases}$</p>\\n\\nAny random variable that has a density function like this is said to be exponentially distributed.",,
4831,2,2097,d6974362-4728-4d82-a7b9-03e3158458cd,2010-08-25 11:01:38.0,88.0,"It is just impossible, since normal distribution has nonzero probability for the whole $R$. Of course because of the numerical issues it would be not the case for Marsaglia method (this obviously means that it is not creating normally distributed numbers), yet this numerical range is so huge that it is nonsense to scale it to [0,1]. ",,
4832,2,2098,b8d67a48-ce7e-44a2-a72e-5fb81edd15a6,2010-08-25 12:25:39.0,,"Based on the update you want to generate truncated normal random variables. Scaling based on the maximum and minimum will 'destroy' the normal distribution properties of the draws. Instead, you should consider using one of the two methods described below:\\n\\n**[Rejection Sampling][1]**\\n\\nYou draw from the standard normal and then accept the draw only if it is between 0 and 1.\\n\\n**[Inverse Transform Sampling][2]**\\n\\nLet:\\n\\n$\\phi(x)$ be the standard normal cdf.\\n\\nThen a standard normal draw that is restricted to lie between 0 and 1 is given by:\\n\\n$x = \\phi^{-1}(\\phi(0) + (\\phi(1) - \\phi(0)) U )$\\n\\nwhere \\n\\n$U$ is a uniform random draw between 0 and 1.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Rejection_sampling\\n  [2]: http://en.wikipedia.org/wiki/Inverse_transform_sampling",,user28
4833,6,2092,c4456572-e0b5-4c53-be93-9a4b4a06c28e,2010-08-25 12:47:39.0,5.0,<distributions><poisson><exponential>,edited tags,
4834,2,2099,1da4c026-50f7-4010-b2bc-8b2301bcb3ba,2010-08-25 13:10:22.0,666.0,Is there any free desktop software for [treemapping][1]? [Every one I've found][2] appears to have a commercial license. [Many Eyes][3] is free and has treemapping features but what I'm really looking for is downloadable desktop software.\\n\\nThanks in advance.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Treemapping\\n  [2]: http://en.wikipedia.org/wiki/List_of_treemapping_software\\n  [3]: http://manyeyes.alphaworks.ibm.com/manyeyes/,,
4835,1,2099,1da4c026-50f7-4010-b2bc-8b2301bcb3ba,2010-08-25 13:10:22.0,666.0,Free treemapping software,,
4836,3,2099,1da4c026-50f7-4010-b2bc-8b2301bcb3ba,2010-08-25 13:10:22.0,666.0,<data-visualization><software>,,
4837,2,2100,a6aa2227-1e87-4569-b1b2-ee18e07eee79,2010-08-25 13:15:48.0,183.0,[Flowing Data has a tutorial][1] on how to use the `map.market` function in the `portfolio` package in R.\\n\\n\\n  [1]: http://flowingdata.com/2010/02/11/an-easy-way-to-make-a-treemap/,,
4838,5,2057,92e2660b-1dad-46b2-a5e4-78be61707da0,2010-08-25 13:29:28.0,919.0,"I have a relatively simple solution to propose, Hugo.   Because you're forthright about not being a statistician (often a plus ;-) but obviously can handle technical language, I'll take some pains to be technically clear but avoid statistical jargon.\\n\\nLet's start by checking my understanding: you have six series of data (t[j,i], h[j,i]), 1 <= j <= 6, 1 <= i <= n[j], where t[j,i] is the time you measured the entropy h[j,i] for artifact j and n[j] is the number of observations made of artifact j.\\n\\nWe may as well assume t[j,i] <= t[j,i+1] is always the case, but it sounds like you cannot necessarily assume that t[1,i] = ... = t[6,i] for all i (synchronous measurements) or even that t[j,i+1] - t[j,i] is a constant for any given j (equal time increments). We might as well also suppose j=1 designates your special artifact.\\n\\nWe do need a model for the data.  ""Exponential"" versus ""sublinear"" covers a lot of ground, suggesting we should adopt a very broad (non-parametric) model for the behavior of the curves.  One thing that simply distinguishes these two forms of evolution is that the increments h[j,i+1] - h[j,i] in the exponential case will be increasing whereas for concave sublinear growth the increments will decreasing.  Specifically, the *increments of the increments*,\\n\\nd2[j,i] = h[j,i+1] - 2*h[j,i+1] + h[j,i], 1 <= i <= n[j]-2,\\n\\nwill either tend to be positive (for artifact 1) or negative (for the others).\\n\\nA big question concerns the nature of variation: the observed entropies might not exactly fit along any nice curve; they might oscillate, seemingly at random, around some ideal curve.  Because you don't want to do any statistical modeling, we aren't going to learn much about the nature of this variation, but let's hope that the amount of variation for any given artifact j is typically about the same size for all times t[i].  This lets us write each entropy in the form\\n\\nh[j,i] = y[j,i] + e[j,i]\\n\\nwhere y[j,i] is the ""true"" entropy for artifact j at time t[j,i] and e[j,i] is the difference between the observed entropy h[j,i] and the true entropy.  It might be reasonable, as a first cut at this problem, to hope that the e[j,i] act randomly and appear to be statistically independent of each other and of the y[j,i] and t[j,i].\\n\\nThis setup and these assumptions imply that the set of second increments for artifact j, {d2[j,i] | 1 <= i <= n[j]-2}, will not necessarily be entirely positive or entirely negative, but that each such set should look like a bunch of (potentially different) positive or negative numbers plus some fluctuation:\\n\\nd2[j,i] = (y[j,i+2] - 2*y[j,i+1] + y[j,i]) + (e[j,i+2] - 2*e[j,i+1] + e[j,i]).\\n\\nWe're still not in a classic probability context, but we're close if we (incorrectly, but perhaps not fatally) treat the correct second increments (y[j,i+2] - 2*y[j,i+1] + y[j,i]) as if they were numbers drawn randomly from some box.  In the case of artifact 1 your hope is that this is a box of all positive numbers; for the other artifacts, your hope is that it is a box of all negative numbers.\\n\\n**At this point we can apply some standard machinery for hypothesis testing**.  The null hypothesis is that the true second increments are all (or most of them) negative; the alternative hypothesis covers all the other 2^6-1 possibilities concerning the signs of the six batches of second increments.  This suggests **running a t-test separately for each collection of actual second increments to compare them against zero**.  (A non-parametric equivalent, such as a sign test, would be fine, too.)  Use a Bonferroni correction with these planned multiple comparisons; that is, if you want to test at a level of *alpha* (e.g., 5%) to attain a desired ""probability value,"" use the *alpha*/6 critical value for the test.  This can readily be done even in a spreadsheet if you like.  It's fast and straightforward.\\n\\nThis approach is not going to be the best one because among all those that could be conceived: it's one of the less powerful and it still makes some assumptions (such as independence of the errors); but if it works--that is, if you find the second increments for j=1 to be significantly above 0 and all the others to be significantly below 0--then it will have done its job.  If this is not the case, your expectations might still be correct, but it would take a greater statistical modeling effort to analyze the data.  (The next phase, if needed, might be to look at the runs of increments for each artifact to see whether there's evidence that *eventually* each curve becomes exponential or sublinear.  It should also involve a deeper analysis of the nature of variation in the data.)",Fixed typo (d --> d2),
4839,2,2101,ce6b8271-8e05-424d-9374-077629e9e869,2010-08-25 13:31:32.0,5.0,"**Protovis** has an [nice treemap layout][1].  I could try to add this into [webvis][2] if you want to create it from R, but it isn't currently an option.\\n\\n\\n  [1]: http://vis.stanford.edu/protovis/ex/treemap.html\\n  [2]: http://cran.r-project.org/web/packages/webvis/index.html",,
4840,2,2102,85d2181e-91ea-43d0-a9cf-f093a4ee1abc,2010-08-25 13:36:22.0,919.0,"Another possible interpretation is that you are confusing ""normal"" and ""uniform"".  A uniform variate will have a mean of 0.5 and be evenly distributed on [0, 1).  If this interpretation is correct, your code simplifies to\\n\\n    return ranf()",,
4841,5,2057,789a2248-7289-4f01-994c-60000ba5e69c,2010-08-25 13:52:20.0,919.0,"I have a relatively simple solution to propose, Hugo.   Because you're forthright about not being a statistician (often a plus ;-) but obviously can handle technical language, I'll take some pains to be technically clear but avoid statistical jargon.\\n\\nLet's start by checking my understanding: you have six series of data (t[j,i], h[j,i]), 1 <= j <= 6, 1 <= i <= n[j], where t[j,i] is the time you measured the entropy h[j,i] for artifact j and n[j] is the number of observations made of artifact j.\\n\\nWe may as well assume t[j,i] <= t[j,i+1] is always the case, but it sounds like you cannot necessarily assume that t[1,i] = ... = t[6,i] for all i (synchronous measurements) or even that t[j,i+1] - t[j,i] is a constant for any given j (equal time increments). We might as well also suppose j=1 designates your special artifact.\\n\\nWe do need a model for the data.  ""Exponential"" versus ""sublinear"" covers a lot of ground, suggesting we should adopt a very broad (non-parametric) model for the behavior of the curves.  One thing that simply distinguishes these two forms of evolution is that the increments h[j,i+1] - h[j,i] in the exponential case will be increasing whereas for concave sublinear growth the increments will decreasing.  Specifically, the *increments of the increments*,\\n\\nd2[j,i] = h[j,i+1] - 2*h[j,i+1] + h[j,i], 1 <= i <= n[j]-2,\\n\\nwill either tend to be positive (for artifact 1) or negative (for the others).\\n\\nA big question concerns the nature of variation: the observed entropies might not exactly fit along any nice curve; they might oscillate, seemingly at random, around some ideal curve.  Because you don't want to do any statistical modeling, we aren't going to learn much about the nature of this variation, but let's hope that the amount of variation for any given artifact j is typically about the same size for all times t[j,i].  This lets us write each entropy in the form\\n\\nh[j,i] = y[j,i] + e[j,i]\\n\\nwhere y[j,i] is the ""true"" entropy for artifact j at time t[j,i] and e[j,i] is the difference between the observed entropy h[j,i] and the true entropy.  It might be reasonable, as a first cut at this problem, to hope that the e[j,i] act randomly and appear to be statistically independent of each other and of the y[j,i] and t[j,i].\\n\\nThis setup and these assumptions imply that the set of second increments for artifact j, {d2[j,i] | 1 <= i <= n[j]-2}, will not necessarily be entirely positive or entirely negative, but that each such set should look like a bunch of (potentially different) positive or negative numbers plus some fluctuation:\\n\\nd2[j,i] = (y[j,i+2] - 2*y[j,i+1] + y[j,i]) + (e[j,i+2] - 2*e[j,i+1] + e[j,i]).\\n\\nWe're still not in a classic probability context, but we're close if we (incorrectly, but perhaps not fatally) treat the correct second increments (y[j,i+2] - 2*y[j,i+1] + y[j,i]) as if they were numbers drawn randomly from some box.  In the case of artifact 1 your hope is that this is a box of all positive numbers; for the other artifacts, your hope is that it is a box of all negative numbers.\\n\\n**At this point we can apply some standard machinery for hypothesis testing**.  The null hypothesis is that the true second increments are all (or most of them) negative; the alternative hypothesis covers all the other 2^6-1 possibilities concerning the signs of the six batches of second increments.  This suggests **running a t-test separately for each collection of actual second increments to compare them against zero**.  (A non-parametric equivalent, such as a sign test, would be fine, too.)  Use a Bonferroni correction with these planned multiple comparisons; that is, if you want to test at a level of *alpha* (e.g., 5%) to attain a desired ""probability value,"" use the *alpha*/6 critical value for the test.  This can readily be done even in a spreadsheet if you like.  It's fast and straightforward.\\n\\nThis approach is not going to be the best one because among all those that could be conceived: it's one of the less powerful and it still makes some assumptions (such as independence of the errors); but if it works--that is, if you find the second increments for j=1 to be significantly above 0 and all the others to be significantly below 0--then it will have done its job.  If this is not the case, your expectations might still be correct, but it would take a greater statistical modeling effort to analyze the data.  (The next phase, if needed, might be to look at the runs of increments for each artifact to see whether there's evidence that *eventually* each curve becomes exponential or sublinear.  It should also involve a deeper analysis of the nature of variation in the data.)","Fixed typo (t[i] --> t[j,k])",
4842,2,2103,33fe8a63-419d-490b-b2ea-2419951c9b8e,2010-08-25 14:08:50.0,919.0,"I'm interpreting your question as concerning the cdf, which by definition is a function F for which\\n\\nF(x,y) - F(x,0) - F(0,y) + F(0,0) = P(X <= x, Y <= y);\\n\\nF(0,0) = 0.\\n\\nFor x + y < 1, the right hand side is zero and the left hand side becomes a statement that F is a bilinear function, implying the graph of F is part of a plane.  For x + y > 1, the assumption of uniform distributions implies F must be increasing at a unit rate in both x and y, whence the graph of F in this region is a part of a plane of the form x + y == constant.  From the evident restrictions\\n\\nF(x, 0) = F(0, y) = 0,\\n\\n0 <= x <= 1, 0 <= y <= 1,\\n\\nit is geometrically obvious that the first piece of the graph must lie in the xy plane and the second piece must intersect the first along the line segment x + y = 1, 0 <= x <= 1.  The full solution therefore is\\n\\nF(x,y) = 0 if x + y <= 1\\n\\nF(x,y) = x + y - 1 if x + y > 1.",,
4843,6,30,13cb6b13-693e-4bd3-a855-9e569e6e8d66,2010-08-25 14:12:54.0,919.0,<algorithms><hypothesis-testing><random-variable><random-generation>,edited tags,
4844,6,40,a9e550ec-c4d7-46f7-bacf-15de8538f839,2010-08-25 14:13:48.0,919.0,<algorithms><random-variable><random-generation>,edited tags,
4845,2,2104,4cb6c31e-b101-4a56-bf40-93862f0e8835,2010-08-25 14:17:44.0,1084.0,"Thank you for reading. I am trying to get sphericity values, and I understood I need to use mlm, but how do I implement a nested within subject design in mlm? I already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online.\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n\\nOr \\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\n \\nThanks,\\n\\nAdam\\n\\n\\n",,
4846,1,2104,4cb6c31e-b101-4a56-bf40-93862f0e8835,2010-08-25 14:17:44.0,1084.0,How to get Sphericity in R for a nested within subject design?,,
4847,3,2104,4cb6c31e-b101-4a56-bf40-93862f0e8835,2010-08-25 14:17:44.0,1084.0,<r><anova><sphericity>,,
4848,2,2105,e4bcd4c9-966b-450e-a4a7-5bc9f7556619,2010-08-25 14:43:24.0,279.0,"You also might want to consider that perhaps you *don't* want a normally distributed variable on [0,1) but rather a random variable with a bell-shaped distribution naturally restricted to the [0,1) interval. If so, consider the [Beta distribution](http://en.wikipedia.org/wiki/Beta_distribution) or the [Kumaraswamy distribution](http://www.johndcook.com/blog/2009/11/24/kumaraswamy-distribution/).",,
4849,2,2106,be0a1e17-3ccd-498c-9a59-8b60f990b4bf,2010-08-25 15:22:37.0,966.0,"Try:\\n\\n    library(ez)\\n    ezANOVA(data=subset(p12bl, exps==1),\\n      within=.(sucrose, citral),\\n      sid=.(subject),\\n      dv=.(resp)\\n      )\\n\\nand the equivalent aov command, minus sphericity etc, is:\\n\\n    aov(resp ~ sucrose*citral + Error(subject/(sucrose*citral)), \\n        data= subset(p12bl, exps==1))",,
4850,5,2104,aea8474e-97c7-4c05-9b63-57f938ff7bc1,2010-08-25 15:58:23.0,1084.0,"Thank you for reading. I am trying to get sphericity values, and I understood I need to use mlm, but how do I implement a nested within subject design in mlm?\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n\\nOr \\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\nI already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online. EZanova doesn't work for me.",deleted 28 characters in body; added 32 characters in body,
4851,2,2107,b79a599d-b6ed-4601-8016-55209bb99315,2010-08-25 16:31:08.0,705.0,"While I haven't used either of these, I have these two projects bookmarked:\\n\\n - [Treeviz][1] \\n - [JTreeMap][2]\\n\\nBoth are implemented in Java, are free and open source (MIT and Apache 2.0 licenses, respectively). Both appear to be actually libraries, but seem to come with an example application.\\n\\n\\n  [1]: http://www.randelshofer.ch/treeviz/index.html\\n  [2]: http://jtreemap.sourceforge.net/",,
4852,2,2108,7d5e3f9d-9506-4faa-949d-ed55ac1d3fb0,2010-08-25 16:44:21.0,428.0,"I have a web service, which returns a dataset, and I would like to query it from the [R] statistical package. I'd like to know how (or if) this can be done and also how to load the first datatable, of this dataset, into memory.\\n\\nThanks!",,
4853,1,2108,7d5e3f9d-9506-4faa-949d-ed55ac1d3fb0,2010-08-25 16:44:21.0,428.0,Using [R] to connect to a Web Service,,
4854,3,2108,7d5e3f9d-9506-4faa-949d-ed55ac1d3fb0,2010-08-25 16:44:21.0,428.0,<r>,,
4858,2,2110,bbd7e3ad-a94a-4697-a3fc-04f5c100e883,2010-08-25 16:49:37.0,5.0,"If your webservice amounts to calling a URL and getting back XML or json, then you can just use the `XML` or `rjson` packages directly for this.  Possibly with `RCurl` for more elaborate session handling.  See [the `RCurl` paper for an example][1].  You can also look at [our `overflowr` package][2] for an example with `rjson`.\\n\\n\\n  [1]: http://www.omegahat.org/RCurl/RCurlJSS.pdf\\n  [2]: http://code.google.com/p/overflowr/",,
4860,5,2103,fbf721c6-54f3-44c7-bb12-dde1f93d2dab,2010-08-25 17:38:14.0,919.0,"I'm interpreting your question as concerning the cdf, which by definition is a function F for which\\n\\nF(x,y) - F(x,0) - F(0,y) + F(0,0) = P(X <= x, Y <= y);\\n\\nF(0,0) = 0.\\n\\nFor x + y < 1, the right hand side is zero and the left hand side becomes a statement that F is a bilinear function, implying (in conjunction with some of the initial values specified below) that the graph of F is part of a plane.  For x + y > 1, the assumption of uniform distributions implies F must be increasing at a unit rate in both x and y, whence the graph of F in this region is a part of a plane of the form x + y == constant.  From the evident restrictions\\n\\nF(x, 0) = F(0, y) = 0,\\n\\n0 <= x <= 1, 0 <= y <= 1,\\n\\nit is geometrically obvious that the first piece of the graph must lie in the xy plane and the second piece must intersect the first along the line segment x + y = 1, 0 <= x <= 1.  The full solution therefore is\\n\\nF(x,y) = 0 if x + y <= 1\\n\\nF(x,y) = x + y - 1 if x + y > 1.",Clarified one inference,
4861,5,2089,3ddc8de6-e7cf-4973-9365-ed586d51d32e,2010-08-25 17:42:00.0,1080.0,"First, let me say I agree with John D Cook's answer: Python is not a Domain Specific Language like R, and accordingly, there is a lot more you'll be able to do with it further down the road. Of course, R being a DSL means that the latest algorithms published in JASA will almost certainly be in R. If you are doing mostly ad hoc work and want to experiment with the latest lasso regression technique, say, R is hard to beat. If you are doing more production analytical work, integrating with existing software and environments, and concerned about speed, extensibility and maintainability, Python will serve you much better. \\n\\nSecond, ars gave a great answer with good links. Here are a few more packages that I view as essential to analytical work in Python:\\n\\n - [matplotlib][1] for beautiful, publication quality graphics. \\n - [IPython][2] for an enhanced, interactive Python console. Importantly, IPython provides a powerful framework for interactive, parallel computing in Python. \\n - [Cython][3] for easily writing C extensions in Python. This package lets you take a chunk of computationally intensive Python code and easily convert it to a C extension. You'll then be able to load the C extension like any other Python module but the code will run very fast since it is in C.\\n - [PyIMSL Studio][4] for a collection of hundreds of mathemaical and statistical algorithms that are thoroughly documented and supported. You can call the exact same algorithms from Python and C, with nearly the same API and you'll get the same results. Full disclosure: I work on this product, but I also use it a lot. \\n - [xlrd][5] for reading in Excel files easily.\\n\\nIf you want a more MATLAB-like interactive IDE/console, check out [Spyder][6], or the [PyDev][7] plugin for [Eclipse][8].\\n\\n  \\n\\n\\n  [1]: http://matplotlib.sourceforge.net/gallery.html\\n  [2]: http://ipython.scipy.org/moin/\\n  [3]: http://www.cython.org/\\n  [4]: http://www.vni.com/campaigns/pyimslstudioeval/\\n  [5]: http://pypi.python.org/pypi/xlrd\\n  [6]: http://code.google.com/p/spyderlib/\\n  [7]: http://pydev.org/\\n  [8]: http://www.eclipse.org/",Trying to embed links now that I have some points; deleted 39 characters in body; deleted 146 characters in body,
4862,2,2111,3367a496-a5b1-4d75-9d1f-f22701caf364,2010-08-25 18:15:55.0,795.0,"For the selection of predictors in multivariate linear regression with $p$ suitable predictors, what methods are available to find an 'optimal' subset of the predictors without explicitly testing all $2^p$ subsets? In 'Applied Survival Analysis,' Hosmer & Lemeshow make reference to Kuk's method, but I cannot find the original paper. Can anyone describe this method, or, even better, a more modern technique? One may assume normally distributed errors.",,
4863,1,2111,3367a496-a5b1-4d75-9d1f-f22701caf364,2010-08-25 18:15:55.0,795.0,how is best subset linear regression computed?,,
4864,3,2111,3367a496-a5b1-4d75-9d1f-f22701caf364,2010-08-25 18:15:55.0,795.0,<modeling><regression><model-selection><multivariable>,,
4865,2,2112,c86000d7-0cb9-4e7b-9026-adafb5fa4493,2010-08-25 18:30:01.0,930.0,"Did you try the `car` package, from John Fox? It includes the function `Anova()` which is very useful when working with experimental designs. It should give you corrected p-value following Greenhouse-Geisser correction and Huynh-Feldt correction. I can post a quick R example if you wonder how to use it.\\n\\nAlso, there is a nice tutorial on the use of R with repeated measurements and mixed-effects model for [psychology experiments and questionnaires][1]; see Section 6.10 about sphericity.\\n\\nAs a sidenote, the Mauchly's Test of Sphericity is available in `mauchly.test()`, but it doesn't work with `aov` object if I remembered correctly. The [R Newsletter][2] from October 2007 includes a brief description of this topic.\\n\\n\\n  [1]: http://www.psych.upenn.edu/~baron/rpsych/rpsych.html\\n  [2]: http://cran.r-project.org/doc/Rnews/Rnews_2007-2.pdf ",,
4866,2,2113,bac4f602-74ec-4396-b3c1-af85934e6216,2010-08-25 18:32:36.0,,"Go to www.vaultanalytics.com/books\\n\\nThey have written a book on what predictive models are, when to use what tests/models, and how to create them in Excel.  I'm using it every day in my job.  I think it's extremely useful.",,Ryan
4867,16,2113,bac4f602-74ec-4396-b3c1-af85934e6216,2010-08-25 18:32:36.0,-1.0,,,
4868,2,2114,f76ec5d9-33a4-43aa-b754-2b3a944b1ac5,2010-08-25 18:37:15.0,495.0,"I've never heard of Kuk's method, but the hot topic these days is L1 minimisation. The rationale being that if you use a penalty term of the absolute value of the regression coefficients, the unimportant ones should go to zero.\\n\\nThese techniques have some funny names: Lasso, LARS, Dantzig selector. You can read the papers, but a good place to start is with [Elements of Statistical Learning][1], Chapter 3.\\n\\n  [1]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/",,
4869,5,2104,8b0bbe7e-14ac-4687-a99b-7c35e7a9b5ed,2010-08-25 18:54:03.0,1084.0,"Thank you for reading. I am trying to get sphericity values, and I understood I need to use mlm, but how do I implement a nested within subject design in mlm?\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n\\nOr \\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\nI already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online. EZanova doesn't work for me.\\n\\n@Matt\\n\\n    > str(subset(p12bl, exps==1))\\n    'data.frame':	192 obs. of  12 variables:\\n     $ exps     : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ Order    : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ threshold: Factor w/ 2 levels "" Suprathreshold"",..: 1 1 1 1 1 1 1 1 1 1 ...\\n     $ SET      : Factor w/ 2 levels "" A"","" B"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ observ   : Factor w/ 16 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 9 10 ...\\n     $ stim     : chr  ""S1C1"" ""S1C1"" ""S1C1"" ""S1C1"" ...\\n     $ resp     : num  6.01 5.63 0 2.57 6.81 ...\\n     $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\\n     $ X1       : Factor w/ 1 level ""S"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ sucrose  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ X3       : Factor w/ 1 level ""C"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ citral   : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n\\n",added 878 characters in body,
4870,2,2115,6b7d9ef7-82aa-41fb-9625-42000d661fd2,2010-08-25 19:45:27.0,930.0,"It seems you are using the `nlme` package. Maybe it would be worth trying R and the `lme4` instead, although it is not fully comparable wrt. syntax or function call. \\n\\nIn your case, I would suggest to specify the `level` when you called `ranef()`, see `?ranef.lme`:\\n\\n<pre>\\n   level: an optional vector of positive integers giving the levels of\\n          grouping to be used in extracting the random effects from an\\n          object with multiple nested grouping levels. Defaults to all\\n          levels of grouping.\\n</pre>\\n\\nThis is also present in the official [documentation][1] for NLME 3.0 (e.g., p. 17).\\n\\nCheck out Douglas Bates's neat handouts on GLMM. He is also writing a textbook entitled *lme4: Mixed-effects modeling with R*. All are available on [R-forge][2]. \\n\\n\\n  [1]: http://ftp.chg.ru/pub/math/NLME/UGuide.pdf\\n  [2]: http://lme4.R-forge.R-project.org/",,
4871,2,2116,09dbf6f8-259c-41bb-9e52-94e97ddcaeca,2010-08-25 20:09:26.0,840.0,"As someone who needs statistical knowledge but is not a formally trained statistician, I'd find it helpful to have a flowchart (or some kind of decision tree) to help me choose the correct approach to solve a particular problem (eg. ""do you need this and know that and that and consider data to be normally distributed? Use technique X. If data is not normal, use Y or Z"").\\n\\nAfter some [googling][1], I've seen several attempts of various coverage and quality (some not available at the moment). I've also seen similar flowcharts in statistics textbooks I've consulted in libraries.\\n\\nA bonus would be an interactive site that, besides just having a chart, would provide extra info (such as assumptions) and would point to how to perform those techniques in popular stat packages. ""Need to do ANOVA in R? You need package X and here's a tutorial"".\\n\\nI'm asking as a community wiki question in the hope there are better resources I couldn't find. Since statistics is a large subject, I think such a flowchart would be suitable for techniques that can be approached by someone who has beginner or intermediate-level knowledge. Anything more complicated would need someone with formal training.\\n\\n  [1]: http://www.google.ca/#hl=en&q=flowchart+to+select+statistical+test",,
4872,1,2116,09dbf6f8-259c-41bb-9e52-94e97ddcaeca,2010-08-25 20:09:26.0,840.0,Flowcharts to help selecting the proper analysis technique and test?,,
4873,3,2116,09dbf6f8-259c-41bb-9e52-94e97ddcaeca,2010-08-25 20:09:26.0,840.0,<statistical-analysis><best-practices>,,
4874,16,2116,09dbf6f8-259c-41bb-9e52-94e97ddcaeca,2010-08-25 20:09:26.0,840.0,,,
4875,2,2117,20cf1610-f7a4-46e8-9be2-6a7e3c0d2bab,2010-08-25 20:17:24.0,930.0,"These are not really interactive flowcharts, but maybe this could be useful: (1) http://j.mp/cmakYq, (2) http://j.mp/aaxUsz, and (3) http://j.mp/bDMyAR.",,
4876,16,2117,20cf1610-f7a4-46e8-9be2-6a7e3c0d2bab,2010-08-25 20:17:24.0,-1.0,,,
4877,5,2093,95466ee2-7f7a-4b34-9c64-8ca02443a68d,2010-08-25 20:49:17.0,937.0,"I've implemented the [Marsaglia polar method][1] to generate random numbers that are normally distributed. Unfortunately the code as shown on this [website][2] returns numbers that are not within the range [0, 1).\\n\\nCode:\\n\\n         double x1, y1, w;\\n \\n         do {\\n                 x1 = 2.0 * ranf() - 1.0;\\n                 y1 = 2.0 * ranf() - 1.0;\\n                 w = x1 * x1 + y1 * y1;\\n         } while ( w >= 1.0 || w == 0 );\\n\\n         w = sqrt( (-2.0 * ln( w ) ) / w );\\n         return x1 * w;\\n\\nwhere ranf() returns values [0, 1).\\n\\nFor example, if (x1, y1) was (-0.43458512585358, -0.07521858582050478), this returns a value of -1.7830255550765148. Obviously not a value within my expected range.\\n\\nI've seen some implementations that multiplies this by the standard deviation and adds the mean. But if I want to get numbers that range from [0, 1), what should I use as my standard deviation and mean? currently, I'm using a standard deviation of 1/8.125 ~= 0.121212... and 0.5 as the mean, but I've only stumbled on this by experimentation. What is the official way for me to get this appropriately ranged? \\n\\nTo answer the question in the comments below: I want to generate normal random variables within the range 0 and 1. Obviously I'd like the mean to be at 0.5 and values no be distributed around this mean.\\n\\nMore for my enlightenment, will the polar method really return numbers over the entire real line? If so, then it should be easy for me to scale the result down to [0, 1), assuming I don't hit floating point underflow due to the division to scale things down.\\n\\nNew edit:\\nWhat I'm trying to do is take Aniko's advise in his answer to my other [Power Factor Problem question][3]. So my first thought was that simulate my distribution of velocities, was to implement my on GaussianRandom class by deriving from [Random][4] and overriding [Random.Sample()][5] and the 3 other methods as recommended by MSDN. Since Random.Sample() is suppose to return a double in the range [0, 1), then I've hit this stumbling block.\\n\\nI've come to the same conclusion as mbq's answer after staring at the ln(x) graph where x is in (0, 1] and sqrt(x) where x is in (0, 2].\\n\\nFrom a practical/programming standpoint, I can't override Random.Sample(), because later when I actually call GaussianRandom.NextDouble(), there is no way for me to pass in the actual standard deviations for my velocities. It looks like I'll have to use the Marsaglia polar method in the raw instead of wrapped within the Random class. My ""cartridge"" generator will have to <code>return x1 * w * stdDev + mean</code>.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Marsaglia_polar_method\\n  [2]: http://www.taygeta.com/random/gaussian.html\\n  [3]: http://stats.stackexchange.com/questions/1687/applied-statistics-to-find-the-minimum-load-for-power-factor-floor\\n  [4]: http://msdn.microsoft.com/en-us/library/ts6se2ek.aspx\\n  [5]: http://msdn.microsoft.com/en-us/library/system.random.sample.aspx",Added specific problem I'm trying to solve,
4878,5,2097,79f4cc04-9904-417f-9e76-ecc20ffa6832,2010-08-25 21:15:38.0,88.0,"It is just impossible, since normal distribution has nonzero probability for the whole $R$. Of course because of the numerical issues it would be not the case for Marsaglia method (this obviously means that it is not creating normally distributed numbers), yet this numerical range is so huge that it is nonsense to scale it to [0,1]. \\n\\nEDIT: So, as long as I understand, you want this number to be in [0,1) to overload .net Random; you don't need to! Write your own class, say NormalRandom which will hold one Random object and will use its output to generate (using Marsaglia) normal random numbers from $-\\infty$ to $\\infty$ and just use them. Or rather use something already done; Google found http://www.codeproject.com/KB/recipes/Random.aspx .  \\nOn margin, remember that when it is said that something which cannot be negative is ""normally distributed"" (for instance mass) it does mean that it is not from normal but from ""almost-normal"" distribution which looks pretty normal, but still has one tail cut out (nevertheless simple rejection of negative outputs from generator is a fair way to deal with it).   ",added 744 characters in body; added 39 characters in body,
4879,2,2118,b2add0d3-bb70-4029-8e4f-27365b760d58,2010-08-25 21:24:22.0,253.0,"You can look at the solution given on the question [""Statistical models cheat sheet""][1]\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1252/statistical-models-cheat-sheet",,
4880,16,2118,b2add0d3-bb70-4029-8e4f-27365b760d58,2010-08-25 21:24:22.0,-1.0,,,
4881,5,2095,0c1f4e52-fcea-4289-a8bc-83ba0d8b48b4,2010-08-25 21:42:08.0,,"$F(x,y) = P(X \\le x, Y \\le y)$\\n\\nUsing the fact that $Y= 1-X$ and after simplifying, the above can be re-written as:\\n\\n$F(x,y) = P(1-y \\le X \\le x)$\\n\\nSince, $X$ is a uniform between 0 and 1, it follows that:\\n\\n$F(x,y) = x+y-1$\\n\\n**Update**\\n\\nThe above is a bit sloppy as I did not specify the domain of the cdf where it is non-zero appropriately. Consider:\\n\\n$F(x,y) = P(1-y \\le X \\le x)$\\n\\nTwo points about the above equation:\\n\\n1. For the above cdf to be non-zero it must be the case that:\\n\\n $x > 1-y$\\n\\n2. Also, note that probability on the right side of the above equation will be less than 1 if and only if $1-y > 0$ and $x < 1$.\\n\\nThus, the correct way to represent the cdf is:\\n\\n$F(x,y) = 0$ if $x+y \\le 1$\\n\\n$F(x,y) = x+y-1$ if $x+y > 1$\\n\\n$F(x,y) = 1$ if $x>1$ and $y>1$\\n",added 565 characters in body,user28
4882,2,2119,400418e9-69af-49e4-9a64-a78510d5acb8,2010-08-25 21:56:01.0,1356.0,Some guys told me that it's appropriate to use Wald-Wolfowitz Runs Test as a normality test (like Shapiro-Wilk's or Kolmogorov-Smirnov...). Do you think this is good way to test normality assumptions?\\n,,
4883,1,2119,400418e9-69af-49e4-9a64-a78510d5acb8,2010-08-25 21:56:01.0,1356.0,Wald-Wolfowitz Runs Test for Normality Assumptions Testing,,
4884,3,2119,400418e9-69af-49e4-9a64-a78510d5acb8,2010-08-25 21:56:01.0,1356.0,<normality><hypothesis-testing><nonparametric>,,
4885,2,2120,bbc57f31-dd9c-44b8-a6e0-48322db2549a,2010-08-25 21:58:52.0,,"Let me take a stab at this...\\n\\nIf I understand the original sentiment and subsequent response, I think what Gary King is getting at is that noticing a scale invariance effect on your data is a very gross understanding of the phenomena of whats going on.  While this 'birds eye view' of whatever phenomena you are observing could be insightful, one might gloss over useful information at the microscopic level.\\n\\nThis example might not be the best, but consider Conway's 'Game of Life'.  This is completely determined, as in it is a deterministic system.  Consider looking at some statistic of this system, cluster longevity, say, for some appropriate definition of cluster.  For arguments sake lets say this follows a power law (I don't know if it does or not, but just for this example, lets say it does).  This gives a gross high level description of the system but you've washed all the details of how gliders race across the board, how they collide to give glider guns and other useful information that you might be able to use to determine some specifics about your system.\\n\\nI'm not sure this is the best example or even if I've gotten the gist of what Gary King was trying to say, but thats my 2 cents.",,stormygorge
4886,2,2121,452a3a2c-c5a5-4d97-b911-8acf7abb126d,2010-08-25 22:34:45.0,1090.0,"I am using ridge regression on highly multicollinear data. Using OLS I get large standard errors on the coefficients due to the multicollinearity. I know ridge regression is a way to deal with this problem, but in all the implementations of ridge regression that I've looked at, there are no standard errors reported for the coefficients. I would like some way of estimating how much the ridge regression is helping by seeing how much it is decreasing the standard errors of specific coefficients. Is there some way to estimate them in ridge regression?",,
4887,1,2121,452a3a2c-c5a5-4d97-b911-8acf7abb126d,2010-08-25 22:34:45.0,1090.0,How can I estimate coefficient standard errors when using ridge regression?,,
4888,3,2121,452a3a2c-c5a5-4d97-b911-8acf7abb126d,2010-08-25 22:34:45.0,1090.0,<standard-error><ridge-regression>,,
4891,2,2123,573b8647-4605-4f41-8782-009a27adcf1c,2010-08-25 22:54:03.0,795.0,"This is not a great idea. The Wald-Wolfowitz test can detect certain departures from being independently distributed (it detects whether a series of observations is 'too bunchy' or 'too  jittery' with respect to the distribution of positive and negative values), which have nothing to do with normality. It is not sensitive to the shape of the empirical distribution: it will 'pass' Cauchy, shifted uniform, $t$, polluted normal, etc. random variates.  \\n\\nMoreover, the WW test is sensitive to the ordering of the observations. A true test for normality should be invariant to permutations.\\n",,
4892,2,2124,c092a1c4-152e-4b51-8914-faa499f9af55,2010-08-25 22:55:22.0,170.0,"[RapidMiner][1] (Java) [open source]\\n\\n\\n  [1]: http://rapid-i.com/content/view/181/190/lang,en/",,
4893,16,2124,c092a1c4-152e-4b51-8914-faa499f9af55,2010-08-25 22:55:22.0,-1.0,,,
4894,4,2111,98332933-b780-4073-b3ca-49e7a0bdc636,2010-08-25 22:57:35.0,88.0,Computing best subset of predictors for linear regression,Changed title to more informative.,
4895,6,2111,98332933-b780-4073-b3ca-49e7a0bdc636,2010-08-25 22:57:35.0,88.0,<modeling><regression><multivariable><model-selection><feature-selection>,Changed title to more informative.,
4898,2,2125,6e088f23-5fbc-448b-974f-95e0892da563,2010-08-25 23:53:00.0,74.0,"Is it cheating if I know the answer, but ask it anyways?\\n\\nI figure it's good for people to know :)",,
4899,1,2125,6e088f23-5fbc-448b-974f-95e0892da563,2010-08-25 23:53:00.0,74.0,What's the difference between correlation and simple linear regression?,,
4900,3,2125,6e088f23-5fbc-448b-974f-95e0892da563,2010-08-25 23:53:00.0,74.0,<correlation><regression>,,
4901,2,2126,4b219a12-c696-48f8-aa05-f9cf517fc7e1,2010-08-26 00:05:08.0,,"Assuming that the data generating process is an OLS the standard errors for ridge regression is given by:\\n\\n$ \\sigma^2 (A^T A + \\Gamma^T \\Gamma)^{-1} A^T A (A^T A + \\Gamma^T \\Gamma)^{-1}$  \\n\\nThe notation above follows the wiki notation for [ridge regression][1]. Specifically,\\n\\n$A$ is the covraiate matrix,\\n\\n$\\sigma^2$ is the error variance.\\n\\n$\\Gamma$ is the Tikhonov matrix chosen suitably in ridge regression.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Tikhonov_regularization",,user28
4902,5,2126,694ab247-79f3-4e10-a437-fe7359be4a73,2010-08-26 00:30:18.0,,"Assuming that the data generating process follows the standard assumptions behind OLS the standard errors for ridge regression is given by:\\n\\n$ \\sigma^2 (A^T A + \\Gamma^T \\Gamma)^{-1} A^T A (A^T A + \\Gamma^T \\Gamma)^{-1}$  \\n\\nThe notation above follows the wiki notation for [ridge regression][1]. Specifically,\\n\\n$A$ is the covraiate matrix,\\n\\n$\\sigma^2$ is the error variance.\\n\\n$\\Gamma$ is the Tikhonov matrix chosen suitably in ridge regression.\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Tikhonov_regularization",added 34 characters in body,user28
4903,2,2127,75610ab8-3d2d-4ed7-b122-09ce42df7491,2010-08-26 00:56:53.0,521.0,"This is a huge topic. As previously mentioned, Hastie, Tibshirani and Friedman give a good intro in Ch3 of Elements of Statistical Learning.\\n\\nA few points.\\n1) What do you mean by ""best"" or ""optimal""? What is best in one sense may not be best in another. Two common criteria are predictive accuracy (predicting the outcome variable) and producing unbiased estimators of the coefficients. Some methods, such as Lasso & Ridge Regression inevitably produce biased coefficient estimators.\\n\\n2) The phrase ""best subsets"" itself can be used in two separate senses. Generally to refer to the best subset among all predictors which optimises some model building criteria. More specifically it can refer to Furnival and Wilson's efficient algorithm for finding that subset among moderate (~50) numbers of linear predictors (Regressions by Leaps and Bounds. Technometrics, Vol. 16, No. 4 (Nov., 1974), pp. 499-51)\\n\\nhttp://www.jstor.org/stable/1267601\\n\\n\\n",,
4904,2,2128,80518f22-18d9-43b3-a4e4-1b5dd7dbfebc,2010-08-26 02:48:15.0,183.0,"What's the difference between the correlation between $X$ and $Y$ and a linear regression predicting $Y$ from $X$?\\n\\nFirst, some **similarities**:\\n\\n- the standardised regression coefficient is the same as Pearson's correlation coefficient\\n- The square of Pearson's correlation coefficient is the same as the $R^2$ in the simple linear regression\\n- Neither simple linear regression nor correlation answer questions of causality directly. This point is important, because I've met people that think that simple regression can magically allow an inference that X causes Y.\\n\\n\\nSecond, some **differences**:\\n\\n- The regression equation that results (i.e., $a + bX$) can be more used to make predictions on $Y$ based on values of $X$\\n- While correlation typically refers to the linear relationship, it can refer to other forms of dependence, such as polynomial or truly nonlinear relationships\\n- While correlation typically refers to Pearson's correlation coefficient, there are other types of correlation, such as Spearman's.\\n\\n\\n\\n\\n\\n",,
4905,2,2129,65d14819-671a-43d0-bca2-d9da49ee6ce2,2010-08-26 03:21:45.0,25.0,"Here is an answer I [posted on the graphpad.com website][1]:\\n\\nCorrelation and linear regression are not the same. Consider these differences:\\n\\n- Correlation quantifies the degree to which two variables are related. Correlation does not fit a line through the data. \\n- With correlation you don't have to think about cause and effect. You simply quantify how well two variables relate to each other. With regression, you do have to think about cause and effect as the regression line is determined as the best way to predict Y from X.\\n- With correlation, it doesn't matter which of the two variables you call ""X"" and which you call ""Y"". You'll get the same correlation coefficient if you swap the two. With linear regression, the decision of which variable you call ""X"" and which you call ""Y"" matters a lot, as you'll get a different best-fit line if you swap the two. The line that best predicts Y from X is not the same as the line that predicts X from Y (unless you have perfect data with no scatter.)\\n- Correlation is almost always used when you measure both variables. It rarely is appropriate when one variable is something you experimentally manipulate. With linear regression, the X variable is usually something you experimentally manipulate (time, concentration...) and the Y variable is something you measure.\\n\\n\\n  [1]: http://www.graphpad.com/faq/viewfaq.cfm?faq=1141",,
4906,5,2128,4541dfb8-9f48-490c-b17a-77f092c7df7c,2010-08-26 03:42:07.0,183.0,"What's the difference between the correlation between $X$ and $Y$ and a linear regression predicting $Y$ from $X$?\\n\\nFirst, some **similarities**:\\n\\n- the standardised regression coefficient is the same as Pearson's correlation coefficient\\n- The square of Pearson's correlation coefficient is the same as the $R^2$ in  simple linear regression\\n- Neither simple linear regression nor correlation answer questions of causality directly. This point is important, because I've met people that think that simple regression can magically allow an inference that X causes Y.\\n\\n\\nSecond, some **differences**:\\n\\n- The regression equation that results (i.e., $a + bX$) can be more used to make predictions on $Y$ based on values of $X$\\n- While correlation typically refers to the linear relationship, it can refer to other forms of dependence, such as polynomial or truly nonlinear relationships\\n- While correlation typically refers to Pearson's correlation coefficient, there are other types of correlation, such as Spearman's.\\n\\n\\n\\n\\n\\n",deleted 3 characters in body,
4907,2,2130,b8f2d8b1-1798-4a27-b0cb-c264855bf918,2010-08-26 04:33:09.0,1043.0,"the following exponential expansion form:\\n\\n$$ exp\\left[\\sum_{k=1}^\\infty \\gamma_k x^k\\right] = \\sum_{j=0}^\\infty\\delta_j x^j $$\\nwhere $\\gamma_k$'s are known, and $\\delta_0=1$, \\n$$\\delta_{j+1} = \\frac{1}{j+1}\\sum_{i=1}^{j+1}i\\gamma_i\\delta_{j+1-i}$$\\n\\nDoes anyone know how to prove it?\\n\\nThanks!\\n",,
4908,1,2130,b8f2d8b1-1798-4a27-b0cb-c264855bf918,2010-08-26 04:33:09.0,1043.0,How to prove the exponential expansion recursive form?,,
4909,3,2130,b8f2d8b1-1798-4a27-b0cb-c264855bf918,2010-08-26 04:33:09.0,1043.0,<mathematical-statistics>,,
4910,2,2131,2ddbd16f-8746-43f5-b259-6204d17309c3,2010-08-26 04:45:31.0,861.0,"for eg. if I have a class variable credit scoring with two classes good and bad, where\\n#(good) = 700 and #(bad)= 300. I do not want to shorten my data. which technique I should use? I was using SVM but it is giving all bad as good in predictions.",,
4911,1,2131,2ddbd16f-8746-43f5-b259-6204d17309c3,2010-08-26 04:45:31.0,861.0,For a classification problem if class variable has unequal distribution which technique we should use?,,
4912,3,2131,2ddbd16f-8746-43f5-b259-6204d17309c3,2010-08-26 04:45:31.0,861.0,<classification><binary-data>,,
4913,2,2132,f353fd2e-2467-4c29-af98-0426d63fff36,2010-08-26 06:37:23.0,196.0,"In the single predictor case of linear regression, the standardized slope has the same value as the correlation coefficient.  The advantage of the linear regression is that the relationship can be described in such a way that you can predict (based on the relationship between the two variables) the score on the predicted variable given any particular value of the predictor variable.  In particular one piece of information a linear regression gives you that a correlation does not is the intercept, the value on the predicted variable when the predictor is 0.\\n\\nIn short - they produce identical results computationally, but there are more elements which are capable of interpretation in the simple linear regression.  If you are interested in simply characterizing the magnitude of the relationship between two variables, use correlation - if you are interested in predicting or explaining your results in terms of particular values you probably want regression.",,
4914,5,2131,447243aa-2a46-47fa-b42c-2f3ecfd62b0f,2010-08-26 07:02:24.0,88.0,"for eg. if I have a class variable credit scoring with two classes good and bad, where #(good) = 700 and #(bad)= 300. I do not want to shorten my data. which technique I should use? I was using SVM but it is giving all bad as good in predictions.",deleted 1 characters in body,
4915,2,2133,5f7048e7-de49-4eaf-8e73-25f556cec69a,2010-08-26 07:04:58.0,930.0,"Your class sample sizes do not seem so unbalanced since you have 30% of observations in your minority class. Logistic regression should be well performing in your case. Depending on the number of predictors that enter your model, you may consider some kind of penalization for parameters estimation, like ridge (L2) or lasso (L1). For an overview of problems with very unbalanced class, see Cramer (1999), The Statistician, 48: 85-94 ([PDF][1]).\\n\\nI am not familiar with credit scoring techniques, but I found some papers that suggest that you could use SVM with weighted classes, e.g. [Support Vector Machines for Credit Scoring: Extension to Non Standard Cases][2]. As an alternative, you can look at [boosting][3] methods with CART, or Random Forests (in the latter case, it is possible to adapt the sampling strategy so that each class is represented when constructing the classification trees). The paper by Novak and LaDue discuss the pros and cons of [GLM vs Recursive partitioning][4]. I also found this article, [Scorecard construction with unbalanced class sizes][5] by Hand and Vinciotti.\\n\\n\\n  [1]: http://www.tinbergen.nl/discussionpapers/98085.pdf\\n  [2]: http://www.springerlink.com/content/uwx80880224n25w3/\\n  [3]: http://mpra.ub.uni-muenchen.de/8156/1/MPRA_paper_8156.pdf\\n  [4]: http://ageconsearch.umn.edu/bitstream/15129/1/31010109.pdf\\n  [5]: http://fic.wharton.upenn.edu/fic/handpaper.pdf",,
4916,2,2134,30b23fac-2244-4d13-942e-dd4d9732dda7,2010-08-26 08:03:41.0,1091.0,"I'm looking for an R package for estimating the coefficients of logit models with individual fixed-effect (individual intercept) using Chamberlain's 1980 estimator. It is often known as Chamberlain's fixed-effect logit estimator.  \\nIt's a classic esitimator when dealing with binary outcome panel data (at least in econometrics), but I just don't find anything related to it in the CRAN...\\n\\nAny clue ?",,
4917,1,2134,30b23fac-2244-4d13-942e-dd4d9732dda7,2010-08-26 08:03:41.0,1091.0,R package for fixed-effect logistic regression,,
4918,3,2134,30b23fac-2244-4d13-942e-dd4d9732dda7,2010-08-26 08:03:41.0,1091.0,<r><logistic>,,
4919,2,2135,5ea1cf66-b478-48f7-b0c4-484ed4cc1239,2010-08-26 08:45:35.0,339.0,"A popular approach towards solving class imbalance problems is to bias the classifier so that it pays more attention to the positive instances. This can be done, for instance, by increasing the penalty associated with misclassifying the positive class relative to the negative class. Another approach is to preprocess the data by oversampling the majority class or undersampling the minority class in order to create a balanced dataset.\\n\\nHowever, in your case, class imbalancing don't seem to be a problem. Perhaps it is a matter of parameter tuning, since finding the optimal parameters for an SVM classifier can be a rather tedious process. There are two parameters for e.g. in an RBF kernel: $C$ and $\\gamma$. It is not known beforehand which $C$ and $\\gamma$ are best for a given problem; consequently some kind of model selection (parameter search) must be done.\\n\\nIn the data preprocessing phase, remember that SVM requires that each data instance is represented as a vector of real numbers. Hence, if there are categorical attributes, it's recommended to convert them into numeric data, using m numbers to represent an m-category attribute (or replacing it with m new binary variables).\\n\\nAlso, scaling the variables before applying SVM is crucial, in order to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges.\\n\\nCheck out [this paper][1].\\n\\nIf you're working in R, check out the [tune][2] function (package e1071) to tune hyperparameters using a grid search over supplied parameter ranges. Then, using  [plot.tune][3], you can see visually which set of values gives the smaller error rate.\\n\\nThere is a shortcut around the time-consuming parameter search. There is an R package called ""svmpath"" which computes the entire regularization path for a 2-class SVM classifier in one go. Here is [a link to the paper][4] that describes what it's doing.\\n\\nP.S. You may also find this paper interesting: [Obtaining calibrated probability estimates][5]\\n\\n\\n  [1]: http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\\n  [2]: http://www.stat.ucl.ac.be/ISdidactique/Rhelp/library/e1071/html/tune.html\\n  [3]: http://bm2.genes.nig.ac.jp/RGM2/R_current/library/e1071/man/plot.tune.html\\n  [4]: http://jmlr.csail.mit.edu/papers/volume5/hastie04a/hastie04a.pdf\\n  [5]: http://www.google.gr/url?sa=t&source=web&cd=3&ved=0CCwQFjAC&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.29.3039%26rep%3Drep1%26type%3Dpdf&ei=Byh2TMbJM4XN4Abf942yBg&usg=AFQjCNG9_KErIloNqZKPUMEFkjRpEP2Tyg&sig2=pMx4yZ01d2-pT-kt4Wp-gw",,
4920,2,2136,c94451f7-8b9e-4a5a-9479-520a7be8bbe5,2010-08-26 08:59:08.0,930.0,"Conditional logistic regression (I assume that this is what you refered to when talking about Chamberlain's estimator) is available through `clogit()` in the [survival][1] package. I also found this page which contains R code to estimate [conditional logit parameters][2]. The [survey][3] package also includes a lot of wrapper function for GLM and Survival model in the case of complex sampling, but I didn't look at.\\n\\nTry also to look at `logit.mixed` in the [Zelig][4] package, or directly use the [lme4][5] package which provide methods for mixed-effects models with binomial link (see `lmer` or `glmer`).\\n\\nDid you take a look at [Econometrics in R][6], from Grant V. Farnsworth? It seems to provide a gentle overview of applied econometrics in R (with which I am not familiar).\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/survival/index.html\\n  [2]: http://course.wilkes.edu/Merrill/stories/storyReader$24\\n  [3]: http://cran.r-project.org/web/packages/survey/index.html\\n  [4]: http://cran.r-project.org/web/packages/Zelig/index.html\\n  [5]: http://cran.r-project.org/web/packages/lme4/index.html\\n  [6]: http://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf",,
4921,2,2137,02ea71e2-9853-4fe7-82a4-d5028dd3b1b5,2010-08-26 09:36:21.0,930.0,"I think boostrap would the best option to obtain robust SE. There is also a nice paper from Casella on SE computation with penalized model, http://j.mp/c3c4tC. Don't look at the [glmnet][1] package (see Friedman's paper in JSS, [Regularization Paths for Generalized Linear Models via Coordinate Descent][2]), though.\\n\\nHowever, Jelle Goeman who authored the [penalized][3] package discuss this point too. Cannot find the original PDF on the web, so I simply quote his words:\\n\\n> It is a very natural question to ask\\n> for standard errors of regression\\n> coefficients or other estimated\\n> quantities. In principle such standard\\n> errors can easily be calculated, e.g.\\n> using the bootstrap.\\n>\\n> Still, this\\n> package deliberately does not provide\\n> them. The reason for this is that\\n> standard errors are not very\\n> meaningful for strongly biased\\n> estimates such as arise from penalized\\n> estimation methods. Penalized\\n> estimation is a procedure that reduces\\n> the variance of estimators by\\n> introducing substantial bias. The bias\\n> of each estimator is therefore a major\\n> component of its mean squared error,\\n> whereas its variance may contribute\\n> only a small part. \\n>\\n> Unfortunately, in\\n> most applications of penalized\\n> regression it is impossible to obtain\\n> a sufficiently precise estimate of the\\n> bias. Any bootstrap-based cal-\\n> culations can only give an assessment\\n> of the variance of the estimates.\\n> Reliable estimates of the bias are\\n> only available if reliable unbiased\\n> estimates are available, which is\\n> typically not the case in situations\\n> in which penalized estimates are used.\\n>\\n> Reporting a standard error of a\\n> penalized estimate therefore tells\\n> only part of the story. It can give a\\n> mistaken impression of great\\n> precision, completely ignoring the\\n> inaccuracy caused by the bias. It is\\n> certainly a mistake to make confidence\\n> statements that are only based on an\\n> assessment of the variance of the\\n> estimates, such as bootstrap-based\\n> confidence intervals do.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/glmnet/index.html\\n  [2]: http://www.jstatsoft.org/v33/i01/paper\\n  [3]: http://cran.r-project.org/web/packages/penalized/index.html",,
4922,5,2108,5553726a-07dc-40a6-9b77-da77c5306740,2010-08-26 09:52:27.0,428.0,"I have a web service, which returns a dataset, and I would like to query it from the [R] statistical package. I'd like to know how (or if) this can be done and also how to load the first datatable, of this dataset, into memory.\\n\\nThanks!\\n\\nEDIT: It is an XML web service",added 34 characters in body,
4923,2,2138,4abb3fd9-16fe-48d8-9e8a-b2ebd67fbf65,2010-08-26 10:15:37.0,442.0,"As a consequence of the inspiring answers and discussion to my question I constructed the following plots that do not rely on any model based parameters, but present the underlying data.  \\n\\nThe reasons are that independent of whatever kind of standard-error I may choose, the standard-error is a model based parameter. So, why not present the underlying data and thereby transmit more information?  \\n\\nFurthermore, if choosing the SE from the ANOVA, two problems arise for my specific problems.  \\nFirst (at least for me) it is somehow unclear what the SEs from `SPSS` ANOVA Output actually are ([see also this discussion, in the comments][1]). They are somehow related to the MSE but how exactly I don't know.  \\nSecond, they are only reasonable when the underlying assumptions are met. However, as the following plots show, the assumptions of homogeneity of variance is clearly violated.\\n\\nThe plots with boxplots:\\n![alt text][2]\\n\\nThe plots with all data points:\\n![alt text][3]\\n\\nThe means are still plotted in black and the data or boxplots in the background in grey. The differences between the plots on the left and on the right are whether or not the means are also jittered or not.\\n\\n\\nThe question that remains is, which one of the above plots is the one to choose now. I have to think about it and ask the other author of our paper. But right now, I prefer the ""points with jitter"". And I still would be very interested in comments.\\n\\n  [1]: http://www.analysisfactor.com/statchat/why-report-estimated-marginal-means-in-spss-glm/\\n  [2]: http://i.stack.imgur.com/V8GTh.png\\n  [3]: http://i.stack.imgur.com/WYVuh.png",,
4924,2,2139,7f4d4697-e1e4-419a-9359-50d712069cfe,2010-08-26 10:36:17.0,,"A brute-force approach would use [taylor series][1] expansion of $exp(.)$ at 0 and group terms appropriately to demonstrate the relationship. I have not attempted to do so myself but an inspection of the first few terms does indicate that the proposed relationship holds. \\n\\nPerhaps, there is a more elegant approach?\\n\\n$$exp\\left[\\gamma_1 x^1\\right] = 1 + \\frac{\\gamma_1 x^1}{1!} + \\frac{(\\gamma_1 x^1)^2}{2!} + ...$$\\n\\n$$exp\\left[\\gamma_2 x^2\\right] = 1 + \\frac{\\gamma_2 x^2}{1!} + \\frac{(\\gamma_2 x^2)^2}{2!} + ...$$\\n\\n$$exp\\left[\\gamma_3 x^3\\right] = 1 + \\frac{\\gamma_3 x^3}{1!} + \\frac{(\\gamma_3 x^3)^2}{2!} + ...$$\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Taylor_series",,user28
4925,5,2138,363a0b5b-e361-4bc6-a5d7-f741c5b0c0e3,2010-08-26 10:37:19.0,442.0,"As a consequence of the inspiring answers and discussion to my question I constructed the following plots that do not rely on any model based parameters, but present the underlying data.  \\n\\nThe reasons are that independent of whatever kind of standard-error I may choose, the standard-error is a model based parameter. So, why not present the underlying data and thereby transmit more information?  \\n\\nFurthermore, if choosing the SE from the ANOVA, two problems arise for my specific problems.  \\nFirst (at least for me) it is somehow unclear what the SEs from `SPSS` ANOVA Output actually are ([see also this discussion, in the comments][1]). They are somehow related to the MSE but how exactly I don't know.  \\nSecond, they are only reasonable when the underlying assumptions are met. However, as the following plots show, the assumptions of homogeneity of variance is clearly violated.\\n\\nThe plots with boxplots:\\n![alt text][2]\\n\\nThe plots with all data points:\\n![alt text][3]\\n\\nNote that the two groups are dislocated a little to the left or the right: deductive to the left, inductive to the right. \\nThe means are still plotted in black and the data or boxplots in the background in grey. The differences between the plots on the left and on the right are if the means are dislocated the same as the points or boxplots or if they are presented centrally.   \\nSorry for the nonoptimal quality of the graphs and the missing x-axis labels.\\n\\n\\nThe question that remains is, which one of the above plots is the one to choose now. I have to think about it and ask the other author of our paper. But right now, I prefer the ""points with means dislocated"". And I still would be very interested in comments.\\n\\n\\n  [1]: http://www.analysisfactor.com/statchat/why-report-estimated-marginal-means-in-spss-glm/\\n  [2]: http://i.stack.imgur.com/cnMz3.png\\n  [3]: http://i.stack.imgur.com/Fbt4J.png",changed bad english,
4926,2,2140,c706f1f9-a49b-4ecb-81be-eaa8cb9479f8,2010-08-26 11:42:13.0,1094.0,"I'm investgating the repeatability of an experimental process (actually measuring lift in a wind tunnel), which can be measured using a single number, Q. However, I only have limited access to the tunnel so the number of experiments that I can run is restricted.\\n\\nIf I have 5 experiments with the tunnel in one configuration:  Q1 = [1.18, -0.41, -0.66, 0.98, 0.1]\\nand five in another: Q2 = [-0.36 -0.73 -1.47 0.15 -0.31]\\n\\nHow confident can I be that the repeatability of the tunnel in the second configuration is better than the first? i.e. the standard deviation of the data from the first configuration is 0.81 and from the second configuration is 0.60, but these values will clearly change if I do additional experiments. Can I somehow calculate the standard deviation of the standard deviation?\\n\\nAny advice on how I could calculate this would be appreciated and more generally how I can determine when I have enough experiments to say something like I am 95% confident that configuration 2 has better repeatability than configuration 1.",,
4927,1,2140,c706f1f9-a49b-4ecb-81be-eaa8cb9479f8,2010-08-26 11:42:13.0,1094.0,How do I compare the repeatability of two sets of experiments with a low number of samples?,,
4928,3,2140,c706f1f9-a49b-4ecb-81be-eaa8cb9479f8,2010-08-26 11:42:13.0,1094.0,<hypothesis-testing>,,
4929,5,2110,4d976b0d-5f33-414d-aabc-e04e8fefd7ea,2010-08-26 12:36:48.0,5.0,"If your webservice amounts to calling a URL and getting back XML or json, then you can just use the `XML` or `rjson` packages directly for this.  Possibly with `RCurl` for more elaborate session handling.  See [the `RCurl` paper for an example][1].  You can also look at [our `overflowr` package][2] for an example with `rjson`.\\n\\n*Edit*:\\n\\nUsing `RCurl` is not necessary unless you have a complicated request.  Otherwise, R can natively handle pulling in data over http.  Just pass the url into the function (e.g. read.table).  A simple example:\\n\\n    URL <- ""http://ichart.finance.yahoo.com/table.csv?s=SPY""\\n    dat <- read.csv(URL)\\n\\nIn your case, since your data is XML, you can use the `readLines` function in the same manner.\\n\\nHave a look at the `getNYTCongress` function in [the `nytR` package][3] for an example of reading an XML webservice without `RCurl`.  Also look at [this question on StackOverflow][4].\\n\\n\\n  [1]: http://www.omegahat.org/RCurl/RCurlJSS.pdf\\n  [2]: http://code.google.com/p/overflowr/\\n  [3]: http://code.google.com/p/nytr/source/browse/trunk/R/nyt.congress.R\\n  [4]: http://stackoverflow.com/questions/1395528/scraping-html-tables-into-r-data-frames-using-the-xml-package",added 783 characters in body,
4930,2,2141,fdd7b3b6-133f-4add-8a34-08f32f485820,2010-08-26 13:06:34.0,279.0,"Look up the two-sample F-test for variances. With equal sample sizes Q, this test calculates the ratio of the two variances, and then compares the value to the F-distribution with (Q-1, Q-1) degrees of freedom. A main assumption is normality, but that might be OK for random measurement error like this. \\nWith your values, however the difference is not statistically significant (p=0.29). In general, the standard error of the standard deviation is large unless you have lots of samples, and thus detecting differences in variability (repeatability) is difficult.",,
4931,6,2140,f522e36a-c557-4659-ba9b-bdb8e6dabb10,2010-08-26 13:07:19.0,279.0,<hypothesis-testing><variability>,edited tags,
4932,2,2142,45448e39-1150-49f3-a3f3-615be7220d38,2010-08-26 13:07:47.0,,"When performing linear regression, it is often useful to do a transformation such as log-transformation for the dependent variable to achieve better normal distribution conformation. Often it is also useful to inspect beta's from the regression to better assess the effect size/real relevance of the results.\\n\\nThis raises the problem that when using e.g. log transformation, the effect sizes will be in log scale, and I've been told that because of non-linearity of the used scale, back-transforming these beta's will result in non-meaningful values that do not have any real world usage. \\n\\nThis far we have usually performed linear regression with transformed variables to inspect the significance and then linear regression with the original non-transformed variables to determine the effect size.\\n\\nIs there a right/better way of doing this? For the most part we work with clinical data, so a real life example would be to determine how a certain exposure affects continues variables such as height, weight or some laboratory measurement, and we would like to conclude something like ""exposure A had the effect of increasing weight by 2 kg"".\\n",,jay
4933,1,2142,45448e39-1150-49f3-a3f3-615be7220d38,2010-08-26 13:07:47.0,,Linear regression effect sizes when using transformed variables,,jay
4934,3,2142,45448e39-1150-49f3-a3f3-615be7220d38,2010-08-26 13:07:47.0,,<regression><data-transformation><effect-size>,,jay
4935,5,2104,7799916b-022f-4bcd-8e33-e66a1926407e,2010-08-26 13:18:56.0,1084.0,"Thank you for reading. I am trying to get sphericity values, and I understood I need to use mlm, but how do I implement a nested within subject design in mlm?\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n\\nOr \\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\nI already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online. EZanova doesn't work for me.\\n\\n@Matt\\n\\n    > str(subset(p12bl, exps==1))\\n    'data.frame':	192 obs. of  12 variables:\\n     $ exps     : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ Order    : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ threshold: Factor w/ 2 levels "" Suprathreshold"",..: 1 1 1 1 1 1 1 1 1 1 ...\\n     $ SET      : Factor w/ 2 levels "" A"","" B"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ subject   : Factor w/ 16 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 9 10 ...\\n     $ stim     : chr  ""S1C1"" ""S1C1"" ""S1C1"" ""S1C1"" ...\\n     $ resp     : num  6.01 5.63 0 2.57 6.81 ...\\n     $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\\n     $ X1       : Factor w/ 1 level ""S"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ sucrose  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ X3       : Factor w/ 1 level ""C"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ citral   : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n\\n",added 1 characters in body,
4936,2,2143,b0da18ab-43c1-429f-82aa-773e2b2d2e5d,2010-08-26 13:30:32.0,601.0,"SHORT ANSWER: Absolutely correct, the back transformation of the beta value is meaningless.\\n\\nLONG ANSWER:\\n\\nThe meaningfulness of the back transformed value varies but when properly done it usually has some meaning.  Perhaps those who've told you they're meaningless just weren't doing it correctly.\\n\\nIf you have a regression of natural log values on two x predictors with a beta of 0.13, and an intercept of 7.0, then the back transformation of 0.13 (1.14) is pretty much meaningless.  That is correct.  However, the back transformation of 7.13 is going to be a value that can be interpreted with some meaning.  You could then subtract out the back transformation of 7.0 and be left with a remainder value that is your effect in a meaningful scale (152.2).  If you want to look at any predicted value you would need to first calculate it all out in log values and then back-transform.\\n\\nThis is often reasonable to do if your transformation has a relatively small effect on your data.  Log transformation of reaction times are one kind of value that can be back transformed.  When it's done correctly you'll find that the values seem close to median values doing simple calculations on the raw data.  \\n\\nEven then though one must be careful with interactions and non-interactions.  The relative values vary across the scale.  The analysis was sensitive to the log value while the back transformed values may show different patterns that make interactions seem like they shouldn't be there or vice versa.  In other words, you can back transform things that make small changes to the data as long as you're careful.\\n\\nSome changes, like logistic transform of probability, can have quite massive impacts, especially near the end of the scale.  An example of a place you should never back transform is interaction plots near the high or low end of probability.\\n\\n",,
4937,5,2143,c3f6290f-c6d5-47c0-874e-4e800088de6d,2010-08-26 13:44:01.0,601.0,"SHORT ANSWER: Absolutely correct, the back transformation of the beta value is meaningless.  However, you can report the non-linearity as something like.  ""If you weigh 100kg then eating two pieces of cake a day will increase your weight by approximately 2kg in one week.  However, if you weigh 200kg your weight would increase 2.5kg.  See figure 1 for a depiction of this non-linear relationship (figure 1 being a fit of the curve over the raw data).""\\n\\nLONG ANSWER:\\n\\nThe meaningfulness of the back transformed value varies but when properly done it usually has some meaning.\\n\\nIf you have a regression of natural log values on two x predictors with a beta of 0.13, and an intercept of 7.0, then the back transformation of 0.13 (1.14) is pretty much meaningless.  That is correct.  However, the back transformation of 7.13 is going to be a value that can be interpreted with some meaning.  You could then subtract out the back transformation of 7.0 and be left with a remainder value that is your effect in a meaningful scale (152.2).  If you want to look at any predicted value you would need to first calculate it all out in log values and then back-transform.  This would have to be done separately for every predicted value and result in a curve if graphed.\\n\\nThis is often reasonable to do if your transformation has a relatively small effect on your data.  Log transformation of reaction times are one kind of value that can be back transformed.  When it's done correctly you'll find that the values seem close to median values doing simple calculations on the raw data.  \\n\\nEven then though one must be careful with interactions and non-interactions.  The relative values vary across the scale.  The analysis was sensitive to the log value while the back transformed values may show different patterns that make interactions seem like they shouldn't be there or vice versa.  In other words, you can back transform things that make small changes to the data as long as you're careful.\\n\\nSome changes, like logistic transform of probability, can have quite massive impacts, especially near the end of the scale.  An example of a place you should never back transform is interaction plots near the high or low end of probability.\\n\\n",added 343 characters in body; added 33 characters in body,
4938,2,2144,5149b2db-bba1-4894-9bec-1586de1d4a36,2010-08-26 13:49:10.0,364.0,"If you want a non-parametric approach that doesn't assume normality, you could obtain distributions of SDs for each configuration using bootstrapping, then compare these distributions. Comments below the answer to [this question][1] suggest that non-overlapping 84% confidence intervals would indicate a difference in the SDs at an alpha of .05.\\n\\n  [1]: http://stats.stackexchange.com/questions/1169/ci-for-a-difference-based-on-independent-cis",,
4939,5,2144,8eb65ee3-ad41-45ec-a7f2-961fb7d69533,2010-08-26 13:56:57.0,364.0,"If you want a non-parametric approach that doesn't assume normality, you could obtain distributions of SDs for each configuration using bootstrapping, then compare these distributions. Comments below the answer to [this question][1] suggest that non-overlapping 84% confidence intervals would indicate a difference in the SDs at an alpha of .05. Alternatively, on each iteration of the bootstrap you could find the difference between the two pseudo-estimates of the SD then compare the distribution of this difference to zero. Below is R code achieving both approaches; both suggest that you have not observed a difference that should be considered real.\\n\\n	a = c(1.18, -0.41, -0.66, 0.98, 0.1)\\n	b = c(-0.36, -0.73, -1.47, 0.15, -0.31)\\n	library(plyr)\\n	iterations = 1e4\\n	sds = ldply(\\n		.data = 1:iterations\\n		, .fun = function(x){\\n			a_sd = sd(sample(a,replace=T))\\n			b_sd = sd(sample(b,replace=T))\\n			to_return = data.frame(\\n				a_sd = a_sd\\n				, b_sd = b_sd\\n				, diff = a_sd - b_sd\\n			)\\n			return(to_return)\\n		}\\n		, .progress = 'text'\\n	)\\n\\n	quantile(sds$a_sd,c(.08,.92))\\n	quantile(sds$b_sd,c(.08,.92))\\n	#substantial overlap\\n\\n	quantile(sds$diff,c(.025,.975))\\n	#includes zero\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/1169/ci-for-a-difference-based-on-independent-cis",Added code,
4940,5,2104,5b37ed8f-fd5e-4217-b650-c07b0b886dc7,2010-08-26 14:41:32.0,1084.0,"Thank you for reading. I am trying to get sphericity values, and I understood I need to use mlm, but how do I implement a nested within subject design in mlm?\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n\\nOr \\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\nI already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online. EZanova doesn't work for me.\\n\\n@Matt\\n\\n    > str(subset(p12bl, exps==1))\\n    'data.frame':	192 obs. of  12 variables:\\n     $ exps     : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ Order    : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ threshold: Factor w/ 2 levels "" Suprathreshold"",..: 1 1 1 1 1 1 1 1 1 1 ...\\n     $ SET      : Factor w/ 2 levels "" A"","" B"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ subject  : Factor w/ 16 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 9 10 ...\\n     $ stim     : chr  ""S1C1"" ""S1C1"" ""S1C1"" ""S1C1"" ...\\n     $ resp     : num  6.01 5.63 0 2.57 6.81 ...\\n     $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\\n     $ X1       : Factor w/ 1 level ""S"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ sucrose  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ X3       : Factor w/ 1 level ""C"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ citral   : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n\\n",deleted 1 characters in body,
4941,2,2145,39258e9c-0e29-4cef-8245-af70e55d96b7,2010-08-26 14:43:02.0,919.0,"Srikant asks for a ""more elegant approach.""  Perhaps the following will respond to this challenge.\\n\\nLet the argument of the exponential be f(x) (so its power series coefficients are the gammas) and let the right hand side be g(x) (with deltas as its coefficients), so that by definition\\n\\ng(x) = exp(f(x)).\\n\\nDifferentiating both sides and replacing exp(f) with g yields\\n\\ng' = f' * g.\\n\\nWriting this out as power series gives the desired result: the delta comes from g' while the convolution of the gammas and deltas comes from f' * g.\\n\\nYou don't have to worry about convergence (and the whole machinery of Taylor series), by the way: all these calculations can be performed in the ring of formal power series.",,
4942,2,2146,4fecf196-6bfb-40d7-8a59-dca70cfbdb07,2010-08-26 17:26:23.0,1098.0,"I have to compare pairs of audio strems as 1d time series. Looking at the aligned trajectories I need to either cluster them together or assume they arise from independent generators. \\n I remember hearing of Dirichlet processes being applied. Could try to defined a hidden Markov model on the structure, not sure yet how the emission probabilities would work out, but it must be possible.\\n\\nany recommendations?\\nBest",,
4943,1,2146,4fecf196-6bfb-40d7-8a59-dca70cfbdb07,2010-08-26 17:26:23.0,1098.0,"data similar to audio, determine if their are 1 or 2 categories",,
4944,3,2146,4fecf196-6bfb-40d7-8a59-dca70cfbdb07,2010-08-26 17:26:23.0,1098.0,<statistical-analysis><time-series><machine-learning><clustering><classification>,,
4945,2,2147,bbd30dd7-28c0-4a85-9474-a5c9f445ab16,2010-08-26 18:02:43.0,919.0,"A good solution will have several ingredients, including:\\n\\n*  Use a resistant, (perhaps non-parametric) moving window smooth to remove nonstationarity.\\n\\n*  Re-express the original data so that the residuals with respect to the smooth are approximately symmetrically distributed.  Given the nature of your data, it's likely that their square roots or logarithms would give symmetric residuals.\\n\\n*  Apply control chart methods, or at least control chart thinking, to the residuals.\\n\\nAs far as that last one goes, control chart thinking shows that ""conventional"" thresholds like 2 SD or 1.5 times the IQR beyond the quartiles work poorly because they trigger too many false out-of-control signals.  People usually use 3 SD in control chart work, whence 2.5 (or even 3) times the IQR beyond the quartiles would be a good starting point.\\n\\nI have more or less outlined the nature of Rob Hyndman's solution while adding to it two major points: the potential need to re-express the data and the wisdom of being more conservative in signaling an outlier.  I'm not sure that Loess is good for an online detector, though, because it doesn't work well at the endpoints.  You might instead use something as simple as a moving median filter (as in Tukey's resistant smoothing).  If outliers don't come in bursts, you can use a very narrow window (5 data points, perhaps, which will break down only with a burst of 3 or more outliers within a group of 5).\\n\\nOnce you have performed the analysis to determine a good re-expression of the data, it's unlikely you'll need to change the re-expression.  Therefore, your online detector really only needs to reference the most recent values (the latest window) because it won't use the earlier data at all.  If you have really long time series you could go further to analyze autocorrelation and seasonality (such as recurring daily or weekly fluctuations) to improve the procedure.",,
4946,2,2148,c05e7d33-71cd-40cb-aded-cd3adeca2028,2010-08-26 20:43:29.0,401.0,"I would suggest that transformations aren't important to get a normal distribution for your errors. Normality isn't a necessary assumption. If you have ""enough"" data, the central limit theorem kicks in and your standard estimates become asymptotically normal. Alternatively, you can use bootstrapping as a non-parametric means to estimate the standard errors. (Homoskedasticity, a common variance for the observations across units, is required for your standard errors to be right; robust options permit heteroskedasticity).\\n\\nInstead, transformations help to ensure that a linear model is appropriate. To give a sense of this, let's consider how we can interpret the coefficients in transformed models:\\n\\n- outcome is units, predictors is units: A one unit change in the predictor leads to a beta unit change in the outcome.\\n- outcome in units, predictor in log units: A one percent change in the predictor leads to a beta/100 unit change in the outcome.\\n- outcome in log units, predictor in units: A one unit change in the predictor leads to a beta x 100% change in the outcome.\\n- outcome in log units, predictor in log units: A one percent change in the predictor leads to a beta percent change in the outcome.\\n\\nIf transformations are necessary to have your model make sense (i.e., for linearity to hold), then the estimate from this model should be used for inference. An estimate from a model that you don't believe isn't very helpful. The interpretations above can be quite useful in understanding the estimates from a transformed model and can often be more relevant to the question at hand. For example, economists like the log-log formulation because the interpretation of beta is an elasticity, an important measure in economics.\\n\\nI'd add that the back transformation doesn't work because the expectation of a function is not the function of the expectation; the log of the expected value of beta is not the expected value of the log of beta. Hence, your estimator is not unbiased. This throws off standard errors, too.",,
4947,2,2149,e78cd403-07c5-4ac7-8b78-a2053c40eefe,2010-08-26 21:00:11.0,5.0,"A [particle filter][1] and [Kalman filter][2] are both [recursive Bayesian estimators][3].  I often encounter Kalman filters in my field, but very rarely see the usage of a particle filter.  \\n\\nWhen would one be used over the other?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Particle_filter\\n  [2]: http://en.wikipedia.org/wiki/Kalman_filter\\n  [3]: http://en.wikipedia.org/wiki/Recursive_Bayesian_estimation",,
4948,1,2149,e78cd403-07c5-4ac7-8b78-a2053c40eefe,2010-08-26 21:00:11.0,5.0,What is the difference between a particle filter (sequential Monte Carlo) and a Kalman filter?,,
4949,3,2149,e78cd403-07c5-4ac7-8b78-a2053c40eefe,2010-08-26 21:00:11.0,5.0,<bayesian><particle-filter><kalman-filter>,,
4950,5,2125,2e496992-56bf-4a6b-ae48-97f8b52a3090,2010-08-26 21:06:25.0,74.0,"Is it cheating if I know the answer, but ask it anyways?\\n\\nI figure it's good for people to know :)\\n\\nIn particular, I am referring to the Pearson product-moment correlation coefficient",added 87 characters in body,
4951,2,2150,59f0c40b-3f25-4995-a211-03dda498e7ed,2010-08-26 21:24:56.0,74.0,"There's no need to call it Predictive Analytics :) It already has **two** names: statistics, and data mining. \\n\\nBeginner Stats Book: Statistics in Plain English  \\nAdvanced Stats Book: Multivariate Analysis, by Hair  \\nData Mining Book: I still haven't found a great one, but Data Mining by Witten is okay.  \\n  \\nDon't get too confused by all the details. There are only so many things you can accomplish in general:  \\n  \\n1. predict a real number (regression)\\n2. predict a whole number (classification)\\n3. modeling (same as the above two, but the model is understandable by humans)\\n4. group similar observations (clustering)\\n5. group similar factors (factor analysis)\\n5. describe a single factor\\n6. describe the relationship between multiple factors (correlation, association, etc)\\n7. determine if a population value is different from another, based on a sample\\n8. design experiments and calculate sample size\\n\\ngood luck!",,
4952,16,2150,59f0c40b-3f25-4995-a211-03dda498e7ed,2010-08-26 21:24:56.0,-1.0,,,
4953,4,2146,1bde512a-6916-4033-8078-8c53fdb7b6c0,2010-08-26 22:03:48.0,88.0,"For data similar to audio, how to determine if their are 1 or 2 categories?",Changed title.,
4954,2,2151,1c49f71e-9073-453f-aef3-aad2765c2370,2010-08-27 01:56:42.0,,"In other words, instead of having two class problem I am dealing with 4 classes and still would like to assess performance using AUC.",,CLOCK
4955,1,2151,1c49f71e-9073-453f-aef3-aad2765c2370,2010-08-27 01:56:42.0,,How to plot ROC curves in multiclass cla ssification?,,CLOCK
4956,3,2151,1c49f71e-9073-453f-aef3-aad2765c2370,2010-08-27 01:56:42.0,,<data-mining>,,CLOCK
4957,5,2128,7e3cd07a-c13c-4a16-a9bf-8555e4a9d722,2010-08-27 02:27:34.0,183.0,"What's the difference between the correlation between $X$ and $Y$ and a linear regression predicting $Y$ from $X$?\\n\\nFirst, some **similarities**:\\n\\n- the standardised regression coefficient is the same as Pearson's correlation coefficient\\n- The square of Pearson's correlation coefficient is the same as the $R^2$ in  simple linear regression\\n- Neither simple linear regression nor correlation answer questions of causality directly. This point is important, because I've met people that think that simple regression can magically allow an inference that X causes Y.\\n\\n\\nSecond, some **differences**:\\n\\n- The regression equation (i.e., $a + bX$) can be used to make predictions on $Y$ based on values of $X$\\n- While correlation typically refers to the linear relationship, it can refer to other forms of dependence, such as polynomial or truly nonlinear relationships\\n- While correlation typically refers to Pearson's correlation coefficient, there are other types of correlation, such as Spearman's.\\n\\n\\n\\n\\n\\n",deleted 18 characters in body,
4958,5,2151,4b75d2a5-f365-4463-a11f-985c04922dc3,2010-08-27 02:41:15.0,159.0,"In other words, instead of having a two class problem I am dealing with 4 classes and still would like to assess performance using AUC.",added 2 characters in body; edited tags; edited title; edited tags,
4959,4,2151,4b75d2a5-f365-4463-a11f-985c04922dc3,2010-08-27 02:41:15.0,159.0,How to plot ROC curves in multiclass classification?,added 2 characters in body; edited tags; edited title; edited tags,
4960,6,2151,4b75d2a5-f365-4463-a11f-985c04922dc3,2010-08-27 02:41:15.0,159.0,<classification><roc>,added 2 characters in body; edited tags; edited title; edited tags,
4961,6,1241,854276e9-8241-42a2-9fb0-ea22fb10bacf,2010-08-27 02:41:40.0,159.0,<regression><roc-curves>,edited tags,
4962,9,1241,4082f946-f4f6-4b2a-8ed1-353444845f2e,2010-08-27 02:42:25.0,159.0,<regression><roc>,Rollback to [7602a171-07b5-40fa-ad11-cc3b9aa9cba0],
4963,2,2152,fc5b5ab9-ba0b-49a1-b0f9-e8f4860f3cfa,2010-08-27 02:58:26.0,1036.0,"While the math is beyond me this general review article has some references you will likely be interested in, and has a brief description of multi-class ROC graphs.\\n\\nAn introduction to ROC analysis by Tom Fawcett\\nPattern Recognition Letters\\n[Volume 27, Issue 8, June 2006, Pages 861-874][1]\\n\\n\\n  [1]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V15-4HV747X-1&_user=10&_coverDate=06/30/2006&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=bbf137175af13683c9ca504bdb10d977",,
4964,2,2153,777f1962-075d-4bf8-b61c-f220fe4b67de,2010-08-27 06:23:29.0,339.0,"In a linear system with Gaussian noise, the Kalman filter is optimal. In a system that is nonlinear, the Kalman filter can be used for state estimation, but the particle filter may give better results at the price of additional computational effort. In a system that has non-Gaussian noise, the Kalman filter is the optimal *linear* filter, but again the particle filter may perform better. The [unscented Kalman filter][1] (UKF) provides a balance between the low computational effort of the Kalman filter and the high performance of the particle\\nfilter.\\n\\nThe particle filter has some similarities with the UKF in that it transforms a set of points via known nonlinear equations and combines the results to estimate the mean and covariance of the state. However, in the particle filter the points are chosen randomly, whereas in the UKF the points are chosen on the basis of a specific algorithm. Because of this, the number of points used in a particle filter generally needs to be much greater than the number of points in a UKF. Another difference between the two filters is that the estimation error in a UKF does not converge to zero in any sense, but the estimation error in a particle filter does converge to zero as the number of particles (and hence the computational effort) approaches infinity.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter",,
4965,5,2153,eadd0542-6972-41a5-a69e-423f9f4435a6,2010-08-27 06:28:45.0,339.0,"In a linear system with Gaussian noise, the Kalman filter is optimal. In a system that is nonlinear, the Kalman filter can be used for state estimation, but the particle filter may give better results at the price of additional computational effort. In a system that has non-Gaussian noise, the Kalman filter is the optimal *linear* filter, but again the particle filter may perform better. The [unscented Kalman filter][1] (UKF) provides a balance between the low computational effort of the Kalman filter and the high performance of the particle\\nfilter.\\n\\nThe particle filter has some similarities with the UKF in that it transforms a set of points via known nonlinear equations and combines the results to estimate the mean and covariance of the state. However, in the particle filter the points are chosen randomly, whereas in the UKF the points are chosen on the basis of a specific algorithm *****. Because of this, the number of points used in a particle filter generally needs to be much greater than the number of points in a UKF. Another difference between the two filters is that the estimation error in a UKF does not converge to zero in any sense, but the estimation error in a particle filter does converge to zero as the number of particles (and hence the computational effort) approaches infinity.\\n\\n***** See also [this][2] as an example of how the points are chosen in UKF.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter\\n  [2]: http://stats.stackexchange.com/questions/1709/how-to-draw-a-probable-outcome-from-a-distribution/1717#1717",added 200 characters in body,
4966,5,2153,db5c83a3-8155-4097-9b5b-79c0190b85d6,2010-08-27 06:56:54.0,339.0,"In a linear system with Gaussian noise, the Kalman filter is optimal. In a system that is nonlinear, the Kalman filter can be used for state estimation, but the particle filter may give better results at the price of additional computational effort. In a system that has non-Gaussian noise, the Kalman filter is the optimal *linear* filter, but again the particle filter may perform better. The [unscented Kalman filter][1] (UKF) provides a balance between the low computational effort of the Kalman filter and the high performance of the particle filter. \\n\\nThe particle filter has some similarities with the UKF in that it transforms a set of points via known nonlinear equations and combines the results to estimate the mean and covariance of the state. However, in the particle filter the points are chosen randomly, whereas in the UKF the points are chosen on the basis of a specific algorithm *****. Because of this, the number of points used in a particle filter generally needs to be much greater than the number of points in a UKF. Another difference between the two filters is that the estimation error in a UKF does not converge to zero in any sense, but the estimation error in a particle filter does converge to zero as the number of particles (and hence the computational effort) approaches infinity.\\n\\n***** The unscented transformation is a method for calculating the statistics of a random variable which undergoes a nonlinear transformation and uses the intuition (which also applies to the particle filter) that it is easier to approximate a probability distribution than it is to approximate an arbitrary nonlinear function or transformation. See also [this][2] as an example of how the points are chosen in UKF. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kalman_filter#Unscented_Kalman_filter\\n  [2]: http://stats.stackexchange.com/questions/1709/how-to-draw-a-probable-outcome-from-a-distribution/1717#1717",added 341 characters in body,
4967,2,2154,192a26b0-618a-4b9a-a5d6-b37970ed9833,2010-08-27 07:32:57.0,588.0,"(I know it is not quite appropriate to quote it as ""part 2"", but since the question has been dormant for quite a while, I hope by doing this will rise peoples' attention again, you may have a look of part I [here][1].)\\n\\n\\nI have come across an article online talking about the case similar to mine, that most of the time the case count is zero, make sometimes when the case number increases to one, it already shoots above the control level and consider the case as ""out of control"".\\n\\nSince c-chart will be easier for my bosses to read and interpret, I wonder if the method is sound, or did anyone have some more official reference on this method? (I have googled quite a while but I can find nothing)\\n\\nThe article can be found [here][2].\\n\\nAn to further my question, I want to ask one more thing: for the assumption of c-chart that the case count needs to follow a Poisson distribution, is it applicable to all lambda (i.e. mean of case count)?\\n\\nThanks again.\\n\\n  [1]: http://stats.stackexchange.com/questions/1228/how-to-interpret-a-control-chart-containing-a-majority-of-zero-values\\n  [2]: http://www.spcforexcel.com/small-sample-case-for-c-and-u-control-charts",,
4968,1,2154,192a26b0-618a-4b9a-a5d6-b37970ed9833,2010-08-27 07:32:57.0,588.0,How to interpret a control chart containing a majority of zero values? (Part 2),,
4969,3,2154,192a26b0-618a-4b9a-a5d6-b37970ed9833,2010-08-27 07:32:57.0,588.0,<control-chart>,,
4970,2,2155,d5e41c25-6af4-4c69-953b-affd40f67d9d,2010-08-27 08:12:12.0,930.0,"It seems you are looking for multi-class ROC analysis, which is a kind of multi-objective optimization covered in a [tutorial][1] at ICML'04. As in several multi-class problem, the idea is generally to carry out pairwise comparison (one class vs. all other classes, one class vs. another class, see (1) or the *Elements of Statistical Learning*), and there is a recent paper by Landgrebe and Duin on that topic, [Approximating the multiclass ROC by pairwise analysis][2], Pattern Recognition Letters 2007 28: 1747-1758. Now, for visualization purpose, I've seen some papers some time ago, most of them turning around [volume under the ROC surface][3] (VUS) or [Cobweb diagram][4].\\n\\nI don't know, however, if there exists an R implementation of these methods, although I think the `stars()` function might be used for cobweb plot. I just ran across a Matlab toolbox that seems to offer multi-class ROC analysis, [PRSD Studio][5].\\n\\nOther papers that may also be useful as a first start for visualization/computation:\\n \\n* [Visualisation of multi-class ROC surfaces][6]\\n* [A simplified extension of the Area under the ROC to the multiclass domain][7]\\n\\n**References:**  \\n1. Allwein, E.L., Schapire, R.E. and Singer, Y. (2000). Reducing multiclass to binary: A unifying approach for margin classifiers. *Journal of Machine Learning Research*, **1**:113–141.\\n\\n\\n  [1]: http://www.cs.bris.ac.uk/~flach/ICML04tutorial/\\n  [2]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.3250&rep=rep1&type=pdf\\n  [3]: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.2427&rep=rep1&type=pdf\\n  [4]: http://cskku.kkh.go.th/homework/ke/paper/visualization%20and%20analysis%20of%20classifiers%20performance%20in%20multi-class%20medical%20data.pdf\\n  [5]: http://doc.prsdstudio.com/latest/guide/ROC_analysis.html\\n  [6]: http://users.dsic.upv.es/~flip/ROCML2005/papers/fieldsend2CRC.pdf\\n  [7]: http://homepage.tudelft.nl/a9p19/papers/prasa_06_vuc.pdf",,
4971,2,2156,e21e3fa2-3921-40a9-8aef-9c98ed5f3331,2010-08-27 08:22:12.0,930.0,"In case you're interested in further references, an extensive list of papers is available on K.H. Zou's website, [Receiver Operating Characteristic (ROC) Literature Research][1].\\n\\nROC curves are also used when one is interested in comparing different classifiers performance, with wide applications in biomedical research and bioinformatics.\\n\\n\\n  [1]: http://www.spl.harvard.edu/archive/spl-pre2007/pages/ppl/zou/roc.html",,
4972,2,2157,8b07e730-651b-4250-8ea4-4df2f3b3865d,2010-08-27 09:27:37.0,88.0,"C-charts basically works like this: you collect all your observations (this is an approximation of getting only observations from ""normal"" periods) of a case count, then fit Poisson distribution to it (so here you apply all Poisson assumptions, mainly that each case occurance is independent to the others), and finally test all case counts with H0 that they are just from this fitted distribution.\\nSo, if you have majority of zeros, it will just drive $\\lambda$ of fit near zero; for $\\lambda<0.01$ even one case will be something strange (on $p$-value=1%).",,
4973,2,2158,40db8130-3cea-46a0-b669-31b6e5e5d4da,2010-08-27 09:36:24.0,174.0,"[Experiments: Planning, Analysis and Optimization][1] by Wu & Hamada.\\n\\nI'm only a couple of chapters in, so not yet in a position to recommend confidently, but so far it looks like a good graduate text, reasonably detailed, comprehensive and up-to-date. Has more of a ""no nonsense"" feel than the Montgomery.\\n\\n  [1]: http://www.amazon.com/dp/0471699462",,
4974,16,2158,40db8130-3cea-46a0-b669-31b6e5e5d4da,2010-08-27 09:36:24.0,-1.0,,,
4975,2,2159,6595458d-8c48-47dd-859e-bea145790fed,2010-08-27 10:10:35.0,1105.0,"I use Python for statistical analysis and forecasting.  As mentioned by others above, Numpy and Matplotlib are good workhorses.  I also use ReportLab for producing PDF output.\\n\\nI'm currently looking at both Resolver and Pyspread which are Excel-like spreadsheet applications which are based on Python.  Resolver is a commercial product but [Pyspread][1] is still open-source. (Apologies, I'm limited to only one link)\\n\\n  [1]: http://pyspread.sourceforge.net/",,
4976,2,2160,c1cd5cd8-3bd8-4d61-9bdb-81bbf75ae037,2010-08-27 11:15:23.0,930.0,Not really a book but a gentle introduction on DoE in R: [An R companion to Experimental Design][1].\\n\\n\\n  [1]: http://cran.r-project.org/doc/contrib/Vikneswaran-ED_companion.pdf,,
4977,16,2160,c1cd5cd8-3bd8-4d61-9bdb-81bbf75ae037,2010-08-27 11:15:23.0,-1.0,,,
4978,5,2152,7cc0030e-e4fd-41f6-ad52-841c55134e19,2010-08-27 11:53:19.0,1036.0,"While the math is beyond me this general review article has some references you will likely be interested in, and has a brief description of multi-class ROC graphs.\\n\\nAn introduction to ROC analysis by Tom Fawcett\\nPattern Recognition Letters\\n[Volume 27, Issue 8, June 2006, Pages 861-874][1]\\n\\nLink to [pdf][2] as provided by gd047- thanks\\n\\n\\n  [1]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6V15-4HV747X-1&_user=10&_coverDate=06/30/2006&_rdoc=1&_fmt=high&_orig=search&_sort=d&_docanchor=&view=c&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=bbf137175af13683c9ca504bdb10d977\\n  [2]: http://www.google.gr/url?sa=t&source=web&cd=1&ved=0CB8QFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.98.4088%26rep%3Drep1%26type%3Dpdf&ei=nlx3TIq-LJCk4Abn_Li3Bg&usg=AFQjCNHO-_yjWAJrRVnJms7MbcqaJkd8eg&sig2=sqERq2v68UvOhJDMviOklg",added 319 characters in body,
4979,5,2152,37775b31-33a4-4149-89e6-5a1402771488,2010-08-27 13:22:46.0,1036.0,"While the math is beyond me this general review article has some references you will likely be interested in, and has a brief description of multi-class ROC graphs.\\n\\nAn introduction to ROC analysis by Tom Fawcett\\nPattern Recognition Letters\\n[Volume 27, Issue 8, June 2006, Pages 861-874][1]\\n\\nLink to [pdf][2] as provided by gd047- thanks\\n\\n\\n  [1]: http://dx.doi.org/10.1016/j.patrec.2005.10.010\\n  [2]: http://www.google.gr/url?sa=t&source=web&cd=1&ved=0CB8QFjAA&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.98.4088%26rep%3Drep1%26type%3Dpdf&ei=nlx3TIq-LJCk4Abn_Li3Bg&usg=AFQjCNHO-_yjWAJrRVnJms7MbcqaJkd8eg&sig2=sqERq2v68UvOhJDMviOklg",deleted 205 characters in body,
4980,5,2157,c2a52a39-d835-452c-9d6f-68f6d07ce8a4,2010-08-27 13:37:10.0,88.0,"C-charts basically works like this: you collect all your observations (this is an approximation of getting only observations from ""normal"" periods) of a case count, then fit Poisson distribution to it (so here you apply all Poisson assumptions, mainly that each case occurance is independent to the others), and finally test all case counts with H0 that they are just from this fitted distribution.\\nSo, if you have majority of zeros, it will just drive $\\lambda$ of fit near zero; for $\\lambda<0.01$ even one case will be something strange (on $p$-value=1%).\\n\\nEDIT:\\nIn the article you've linked the whole fitting is done just by taking mean as $\\lambda$, while testing just by $3\\sigma$ criterion based on the fact that Poisson's variance is also $\\lambda$. Still it is sufficient for this case.",added 240 characters in body,
4985,2,2163,aebba899-32b9-4e9e-a91b-01ed289e9cda,2010-08-27 14:38:12.0,174.0,"Ronald Fisher's [The Design of Experiments][1] (link is Wikipedia rather than Amazon since it is long out of print) is interesting for historical context. The book is often credited as founding the whole field, and certainly did a lot to promote things like blocking, randomisation and factorial design, though things have moved on a bit since.\\n\\nAs a period document it's quite fascinating, but it's also maddening. In the absence of a common terminology and notation, a lot of time is spent painstakingly explaining things in what now seems comically-stilted English. If you had to use it as a reference to look up how to calculate something you'd probably gnaw your own leg off. But the *terribly polite* hatchet job on some of Galton's analysis is entertaining.\\n\\n(I know, I know -- how the readers of tomorrow will laugh at the archaisms of today's scientific literature...)\\n\\n  [1]: http://en.wikipedia.org/wiki/The_Design_of_Experiments",,
4986,16,2163,aebba899-32b9-4e9e-a91b-01ed289e9cda,2010-08-27 14:38:12.0,-1.0,,,
4988,16,1597,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4989,16,1598,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4990,16,1600,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4991,16,1632,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4992,16,2089,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4993,16,2159,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4994,16,1595,00000000-0000-0000-0000-000000000000,2010-08-27 15:07:18.0,88.0,,,
4998,5,1959,9a95b264-023b-4a8d-92c6-31b664452c3b,2010-08-27 15:21:05.0,919.0,"I still feel negatively about what seems to be a gratuitous insult on King's part but I can see where he might be coming from.  ""Scale-invariance"" is a restriction on a statistical procedure.  Thus, limiting our choice of procedures to scale-invariant ones (or to linear ones or to unbiased ones or minimax ones, etc.) potentially excludes procedures that might perform better.  Whether this is actually the case or not depends.  In many situations, data are reported in units that are essentially independent of what is being studied.  It shouldn't matter whether you measure distances in angstroms or parsecs, for example.  In this context, any procedure that is *not* scale invariant is therefore an *arbitrary* one--and arbitrariness is not a positive attribute in this field.  In other situations, though, there is a natural scale.  The most obvious of these concern counted data.  A procedure that treats counted data as if they were measurements on a continuous scale (e.g., using OLS for a counted response) is potentially inferior to other available procedures and may be (likely is, I suspect) inadmissible in the decision-theoretic sense.  This can be a tricky and subtle point because it's not always obvious when we have counted data.  One example I'm familiar with concerns many chemical or radioactivity measurements, which ultimately originate as counts on some machine.  Said counts get converted by the laboratory into a concentration or activity that forever after is treated as a real number.  (However, attempts to exploit this fact in the chemometrics literature have not yielded superior statistical procedures.)\\n\\nJust to stave off one possible misunderstanding: I wouldn't view a selection of an informative prior for a scale parameter (in a Bayesian analysis) as a scale-dependent procedure.  Such a prior obviously favors some ranges of values over others, but does not affect the scale invariance of the procedure itself.",deleted 130 characters in body,
5000,2,2166,f60975a8-979e-480f-9bf0-9ba063ab32b1,2010-08-27 16:44:31.0,5.0,"I'm not aware that anyone is doing this, but I generally like to try [dimensionality reduction][1] when I have a problem like this.  You might look into a method from manifold learning or [non-linear dimensionality reduction][2].\\n\\nAn example would be a [Kohonen map][3].  A good reference for R is [""Self- and Super-organizing Maps in R: The kohonen Package""][4].\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Dimensionality_reduction\\n  [2]: http://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction\\n  [3]: http://en.wikipedia.org/wiki/Kohonen_maps\\n  [4]: http://www.jstatsoft.org/v21/i05/paper",,
5001,2,2167,3c8d1484-47d8-4fa4-ab81-1ef4ef8824f6,2010-08-27 17:00:11.0,5.0,"The [kernel trick][1] is at the heart of many successful machine learning models (e.g. [SVM][2]).  It was first introduced in the ""Theoretical foundations of the potential function method in pattern recognition learning"" paper in 1964.  \\n\\nThe wikipedia definition says that it is \\n\\n> a method for using a linear classifier\\n> algorithm to solve a non-linear\\n> problem by mapping the original\\n> non-linear observations into a\\n> higher-dimensional space, where the\\n> linear classifier is subsequently\\n> used; this makes a linear\\n> classification in the new space\\n> equivalent to non-linear\\n> classification in the original space.\\n\\nOne example of a linear model that has been extended to non-linear problems is the [kernel PCA][3].  Can the kernel trick be applied to any linear model, or does it have certain restrictions?\\n\\n  [1]: http://en.wikipedia.org/wiki/Kernel_trick\\n  [2]: http://en.wikipedia.org/wiki/Support_vector_machine\\n  [3]: http://en.wikipedia.org/wiki/Kernel_principal_component_analysis",,
5002,1,2167,3c8d1484-47d8-4fa4-ab81-1ef4ef8824f6,2010-08-27 17:00:11.0,5.0,"Applying the ""kernel trick"" to linear methods?",,
5003,3,2167,3c8d1484-47d8-4fa4-ab81-1ef4ef8824f6,2010-08-27 17:00:11.0,5.0,<machine-learning><kernel>,,
5004,2,2168,08bd3dd4-b8e9-4b08-b320-8aada05457f9,2010-08-27 17:16:04.0,881.0,"The kernel trick can only be applied to linear models where the examples in the problem formulation appear as dot products (Support Vector Machines, PCA, etc).",,
5005,5,2104,970411d7-62c9-4003-a63e-a58de7f20b2f,2010-08-27 17:50:23.0,1084.0,"Thank you for reading. I am trying to get sphericity values for a purely within subject design. I have been unable to use ezANOVA, or Anova().\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\nI already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online.\\n\\n@Matt\\n\\n    > str(subset(p12bl, exps==1))\\n    'data.frame':	192 obs. of  12 variables:\\n     $ exps     : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ Order    : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ threshold: Factor w/ 2 levels "" Suprathreshold"",..: 1 1 1 1 1 1 1 1 1 1 ...\\n     $ SET      : Factor w/ 2 levels "" A"","" B"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ subject  : Factor w/ 16 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 9 10 ...\\n     $ stim     : chr  ""S1C1"" ""S1C1"" ""S1C1"" ""S1C1"" ...\\n     $ resp     : num  6.01 5.63 0 2.57 6.81 ...\\n     $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\\n     $ X1       : Factor w/ 1 level ""S"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ sucrose  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ X3       : Factor w/ 1 level ""C"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ citral   : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n\\n    subset(p12b,exps==1)\\n       exps Order       threshold SET observ S1C1 S1C2 S1C3 S1C4 S2C1 S2C2 S2C3 S2C4 S3C1 S3C2 S3C3 S3C4 S4C1 S4C2 S4C3 S4C4\\n    1     1     1  Suprathreshold   A      1  6.0  7.1  7.5  8.6 15.0 15.4 15.0 13.1 16.9   13 13.1 16.5   24   16   21   20\\n    2     1     1  Suprathreshold   A      2  5.6  0.8  4.0  5.6  5.6 11.3 12.9 14.5 18.5   15 12.9 14.5   24   26   29   28\\n    3     1     1  Suprathreshold   A      3  0.0  0.0  1.7  0.0  5.0  8.4  8.4  5.0 11.7   20 18.5 16.8   29   37   37   30\\n    4     1     1  Suprathreshold   A      4  2.6  3.3  9.1 16.3  5.4 10.0  9.6 16.8 13.5   12 22.2 23.1   19   20   22   23\\n    5     1     1  Suprathreshold   A      5  6.8  5.3 15.4 14.5 11.5  8.3 14.5 14.2  8.9   17 11.2 15.1   24   23   19   19\\n    6     1     1  Suprathreshold   A      6  2.6  2.8  2.6  5.2 13.4 15.6 13.7 13.0 13.7   15 16.0 18.9   22   24   25   25\\n    7     1     1  Suprathreshold   A      7  1.3  5.8 10.2  9.8 11.9 12.3 17.7 16.7 11.4   19 19.2 21.1   16   19   18   19\\n    8     1     1  Suprathreshold   A      8  2.0  5.6  3.9  2.0  4.9  5.2  7.5  4.9 20.2   21  8.2  9.5   30   26   32   45\\n    9     1     1  Suprathreshold   A      9  9.4 11.3 11.7 12.1 14.7 13.8 12.6 14.9 15.2   15 15.9 13.9   17   18   15   18\\n    10    1     1  Suprathreshold   A     10  4.5 17.8 18.5 21.6  5.8 10.9 17.0 20.2  6.6   10 17.8 18.7   12   12   16   19\\n    11    1     1  Suprathreshold   A     11  9.8 13.0 16.1 18.0 10.5 11.6 15.4 17.3 10.1   14 15.2 16.7   13   15   15   17\\n    12    1     1  Suprathreshold   A     12  9.6 10.4 13.3 11.3 12.1 12.6 13.6 13.6 14.9   16 15.1 16.3   16   18   18   17\\n\\nSample output\\n\\n    ezANOVA( data = subset(p12bl, exps==1)	, dv= .(resp), sid = .(observ), within = .(sucrose,citral), between = NULL, collapse_within = FALSE)\\n    Note: model has only an intercept; equivalent type-III tests substituted.\\n    $ANOVA\\n              Effect DFn DFd  SSn  SSd     F       p p<.05   pes\\n    1        sucrose   3  33 4953 3263 16.70 9.0e-07     * 0.603\\n    2         citral   3  33  410  553  8.16 3.3e-04     * 0.426\\n    3 sucrose:citral   9  99   56  791  0.77 6.4e-01       0.066\\nWarning messages:\\n1: You have removed one or more Ss from the analysis. Refactoring ""observ"" for ANOVA. \\n2: Too few Ss for Anova(), reverting to aov(). See ""Warning"" section of the help on ezANOVA. \\n\\n",added 2335 characters in body; deleted 2 characters in body,
5006,5,2167,96184e74-3ae9-40c2-b8b9-deacac92778c,2010-08-27 18:03:23.0,5.0,"The [kernel trick][1] is used in several machine learning models (e.g. [SVM][2]).  It was first introduced in the ""Theoretical foundations of the potential function method in pattern recognition learning"" paper in 1964.  \\n\\nThe wikipedia definition says that it is \\n\\n> a method for using a linear classifier\\n> algorithm to solve a non-linear\\n> problem by mapping the original\\n> non-linear observations into a\\n> higher-dimensional space, where the\\n> linear classifier is subsequently\\n> used; this makes a linear\\n> classification in the new space\\n> equivalent to non-linear\\n> classification in the original space.\\n\\nOne example of a linear model that has been extended to non-linear problems is the [kernel PCA][3].  Can the kernel trick be applied to any linear model, or does it have certain restrictions?\\n\\n  [1]: http://en.wikipedia.org/wiki/Kernel_trick\\n  [2]: http://en.wikipedia.org/wiki/Support_vector_machine\\n  [3]: http://en.wikipedia.org/wiki/Kernel_principal_component_analysis",deleted 16 characters in body,
5007,2,2169,fd8082ff-d321-4e73-9c02-2808673abddd,2010-08-27 18:23:40.0,795.0,"I am trying to compute the standard error of the sample [spectral risk measure][1], which is used as a metric for portfolio risk. Briefly, a sample spectral risk measure is defined as \\n$q = \\sum_i w_i x_{(i)}$, where $x_{(i)}$ are the sample order statistics, and $w_i$ is a sequence of monotonically non-increasing non-negative weights that sum to $1$.  I would like to compute the standard error of $q$ (preferrably not via bootstrap). I don't know much about L-estimators, but it looks to me like $q$ is a kind of L-estimator (but with extra restrictions imposed on the weights $w_i$), so this should probably be an easily solved problem. \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Spectral_risk_measure",,
5008,1,2169,fd8082ff-d321-4e73-9c02-2808673abddd,2010-08-27 18:23:40.0,795.0,how to compute the standard error of an L-estimator,,
5009,3,2169,fd8082ff-d321-4e73-9c02-2808673abddd,2010-08-27 18:23:40.0,795.0,<estimation><finance><standard-error>,,
5010,5,2104,b26b10e3-5253-46ce-a0dd-9a67d75dec2d,2010-08-27 18:36:36.0,1084.0,"Thank you for reading. I am trying to get sphericity values for a purely within subject design. I have been unable to use ezANOVA, or Anova(). Anova works if I add a between subject factor, but I have been unable to get sphericity for a purely within subject design. Any advice?\\n\\nI already read the R newsletter, fox chapter appendix, EZanova, and whatever I could find online.\\n\\nMy original ANOVA\\n\\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject, data = p12bl, subset = exps==1)) \\n    anova(aov(resp ~ sucrose*citral, random =~1 | subject/sucrose*citral, data = p12bl, subset = exps==1))\\n\\n    > str(subset(p12bl, exps==1))\\n    'data.frame':	192 obs. of  12 variables:\\n     $ exps     : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ Order    : int  1 1 1 1 1 1 1 1 1 1 ...\\n     $ threshold: Factor w/ 2 levels "" Suprathreshold"",..: 1 1 1 1 1 1 1 1 1 1 ...\\n     $ SET      : Factor w/ 2 levels "" A"","" B"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ subject  : Factor w/ 16 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 9 10 ...\\n     $ stim     : chr  ""S1C1"" ""S1C1"" ""S1C1"" ""S1C1"" ...\\n     $ resp     : num  6.01 5.63 0 2.57 6.81 ...\\n     $ id       : int  1 2 3 4 5 6 7 8 9 10 ...\\n     $ X1       : Factor w/ 1 level ""S"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ sucrose  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ X3       : Factor w/ 1 level ""C"": 1 1 1 1 1 1 1 1 1 1 ...\\n     $ citral   : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 1 1 1 1 1 1 1 1 ...\\n\\n    subset(p12b,exps==1)\\n       exps Order       threshold SET observ S1C1 S1C2 S1C3 S1C4 S2C1 S2C2 S2C3 S2C4 S3C1 S3C2 S3C3 S3C4 S4C1 S4C2 S4C3 S4C4\\n    1     1     1  Suprathreshold   A      1  6.0  7.1  7.5  8.6 15.0 15.4 15.0 13.1 16.9   13 13.1 16.5   24   16   21   20\\n    2     1     1  Suprathreshold   A      2  5.6  0.8  4.0  5.6  5.6 11.3 12.9 14.5 18.5   15 12.9 14.5   24   26   29   28\\n    3     1     1  Suprathreshold   A      3  0.0  0.0  1.7  0.0  5.0  8.4  8.4  5.0 11.7   20 18.5 16.8   29   37   37   30\\n    4     1     1  Suprathreshold   A      4  2.6  3.3  9.1 16.3  5.4 10.0  9.6 16.8 13.5   12 22.2 23.1   19   20   22   23\\n    5     1     1  Suprathreshold   A      5  6.8  5.3 15.4 14.5 11.5  8.3 14.5 14.2  8.9   17 11.2 15.1   24   23   19   19\\n    6     1     1  Suprathreshold   A      6  2.6  2.8  2.6  5.2 13.4 15.6 13.7 13.0 13.7   15 16.0 18.9   22   24   25   25\\n    7     1     1  Suprathreshold   A      7  1.3  5.8 10.2  9.8 11.9 12.3 17.7 16.7 11.4   19 19.2 21.1   16   19   18   19\\n    8     1     1  Suprathreshold   A      8  2.0  5.6  3.9  2.0  4.9  5.2  7.5  4.9 20.2   21  8.2  9.5   30   26   32   45\\n    9     1     1  Suprathreshold   A      9  9.4 11.3 11.7 12.1 14.7 13.8 12.6 14.9 15.2   15 15.9 13.9   17   18   15   18\\n    10    1     1  Suprathreshold   A     10  4.5 17.8 18.5 21.6  5.8 10.9 17.0 20.2  6.6   10 17.8 18.7   12   12   16   19\\n    11    1     1  Suprathreshold   A     11  9.8 13.0 16.1 18.0 10.5 11.6 15.4 17.3 10.1   14 15.2 16.7   13   15   15   17\\n    12    1     1  Suprathreshold   A     12  9.6 10.4 13.3 11.3 12.1 12.6 13.6 13.6 14.9   16 15.1 16.3   16   18   18   17\\n\\nSample output\\n\\n    ezANOVA( data = subset(p12bl, exps==1)	, dv= .(resp), sid = .(observ), within = .(sucrose,citral), between = NULL, collapse_within = FALSE)\\n    Note: model has only an intercept; equivalent type-III tests substituted.\\n    $ANOVA\\n              Effect DFn DFd  SSn  SSd     F       p p<.05   pes\\n    1        sucrose   3  33 4953 3263 16.70 9.0e-07     * 0.603\\n    2         citral   3  33  410  553  8.16 3.3e-04     * 0.426\\n    3 sucrose:citral   9  99   56  791  0.77 6.4e-01       0.066\\nWarning messages:\\n1: You have removed one or more Ss from the analysis. Refactoring ""observ"" for ANOVA. \\n2: Too few Ss for Anova(), reverting to aov(). See ""Warning"" section of the help on ezANOVA. \\n\\n",added 127 characters in body,
5011,5,2169,e2a81605-1033-4206-aa29-c7ac52048a4a,2010-08-27 18:57:49.0,795.0,"I am trying to compute the standard error of the sample [spectral risk measure][1], which is used as a metric for portfolio risk. Briefly, a sample spectral risk measure is defined as \\n$q = \\sum_i w_i x_{(i)}$, where $x_{(i)}$ are the sample order statistics, and $w_i$ is a sequence of monotonically non-increasing non-negative weights that sum to $1$.  I would like to compute the standard error of $q$ (preferrably not via bootstrap). I don't know much about L-estimators, but it looks to me like $q$ is a kind of L-estimator (but with extra restrictions imposed on the weights $w_i$), so this should probably be an easily solved problem. \\n\\n**edit**: per @srikant's question, I should note that the weights $w_i$ are chosen *a priori* by the user, and should be considered independent from the samples $x$.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Spectral_risk_measure","clarify independence of weights, sample.",
5012,5,2146,9703b229-4677-421f-8ec6-64d63b6a86cf,2010-08-27 19:14:19.0,1098.0,"I have to compare pairs of audio strems as 1d time series. Looking at the aligned trajectories I need to either cluster them together or assume they arise from independent generators. \\n I remember hearing of Dirichlet processes being applied. Could try to defined a hidden Markov model on the structure, not sure yet how the emission probabilities would work out, but it must be possible.\\n\\nany recommendations?\\nBest\\n\\nEDITS- set of 1D data time series s in {1...N}, another set of trajectories q in {1...N} all over a discrete time domain t=0...t=T under a uniform interval. Task, determine whether under certain assumptions (eg. iid, normality etc), is the generator under these assumptions a valid null hypothesis to have generated the two categories of trajectories s and q? Or is the null hypothesis abandoned for a more complex model?\\n\\nSo do we use an autoregressive model? a Gaussian process would be hard to interpret the inverse covariance. Some parametric model would seem to be valid. Don't know... any ideas?",adding more info,
5013,6,2146,9703b229-4677-421f-8ec6-64d63b6a86cf,2010-08-27 19:14:19.0,1098.0,<statistical-analysis><machine-learning><time-series><clustering><classification>,adding more info,
5014,2,2170,6a64d4e0-34f5-4cd9-ac1a-f7f0d28e6d21,2010-08-27 20:17:34.0,1036.0,"Say some previous findings identified a curvilinear effect of X on Y, (specifically that X had a positive effect on Y, and that X^2 had a negative effect). You want to see if the same holds for your entirely different sample (although everything else between studies, constructs/measures, are exactly the same). Neither the previous study nor my study are experimental (so I am not manipulating X, merely observing it).  There is no explicit theoretical reason why a curvilinear effect would occur.\\n\\nI will give some examples of how I would currently go about seeing if this is true, but I would like some input and peoples opinions on what they believe would be a preferrable method, if any of my suggestions are innapropriate, and of course if their are other alternatives.\\n\\nExamples:\\n\\n1) Simply examine the bivariate scatterplot and fit some type of smoothing line to the mean of Y over bins of X. Although confounding could be an issue, if a curvilinear effect exists their will likely be some evidence in its distribution.\\n\\n2) Examine the partial residual plot. A partial residual plot is when I have a regression model, predicting Y from X and other confounders. I take the residuals from this model, and they will be the vertical dimension for the points on the plot. I then take X^2 (which is supposed to represent the curvilinear effect) regress the same predictors on X^2 I used for the model predicting Y, and the residuals from this model will be my horizontal dimension. If their is a linear relationship observed in the scatter plot, X^2 has an effect on Y distinct from other variables already included in the model.\\n\\n[Is this a correct description of partial residual plots? Is there a more direct way to observe the actual shape of the effect as oppossed to seeing if X^2 fits a line?]\\n\\n3) Use some type of model selection criteria, and determine whether a model including X^2 is preferred over a model without X^2.\\n\\n4) Include a model with X^2 and see if X^2 has a statistically significant regression coefficient.\\n\\nLike I said any other suggestions are welcome as well. ",,
5015,1,2170,6a64d4e0-34f5-4cd9-ac1a-f7f0d28e6d21,2010-08-27 20:17:34.0,1036.0,Preferred method for identifying curvilinear effect in multi-variate regression framework,,
5016,3,2170,6a64d4e0-34f5-4cd9-ac1a-f7f0d28e6d21,2010-08-27 20:17:34.0,1036.0,<modeling><regression><methodology>,,
5017,2,2171,b0ce71e7-1d12-4109-aefb-86d0b7d087d2,2010-08-27 22:00:32.0,1106.0,"I'm interested in learning how to create the type of visualizations you see at http://flowingdata.com and informationisbeautiful.\\n\\nWhat kinds of tools are used to create these -- is it mostly a lot of Adobe Illustrator/Photoshop? What are good resources (books, websites, etc.) to learn how to use these tools for data visualization in particular?\\n\\nI know *what* I want visualizations to look like (and I'm familiar with design principles, e.g., from Tufte's books), but I have no idea *how* to create them.",,
5018,1,2171,b0ce71e7-1d12-4109-aefb-86d0b7d087d2,2010-08-27 22:00:32.0,1106.0,Resources for learning to create data visualizations?,,
5019,3,2171,b0ce71e7-1d12-4109-aefb-86d0b7d087d2,2010-08-27 22:00:32.0,1106.0,<data-visualization>,,
5020,2,2172,9c2b0edf-1546-45c9-8c59-b294d044b3c5,2010-08-27 22:50:35.0,5.0,"From the horse's mouth: http://flowingdata.com/2009/09/03/what-visualization-toolsoftware-should-you-use-getting-started/\\n\\nSee these questions: http://stats.stackexchange.com/questions/224/visualization-libraries and http://stats.stackexchange.com/questions/216/web-visualization-libraries. \\n\\nIMO, try:\\n\\n1. R and ggplot2\\n2. Processing\\n3. Protovis\\n\\nUse Adobe afterwards to clean it up. ",,
5021,2,2173,249edbbe-cc57-4e4f-8944-d1adfa555699,2010-08-27 23:47:09.0,74.0,"You'll spend a lot of time getting up to speed with R. \\n\\nRapidMiner is free and open source and graphical, and has plenty of good visualizations, and you can export them.\\n\\nIf you have money to spare, or are a university staff/student then JMP is also very freaking nice. It can make some very pretty graphs, very very easily. Can export to flash or PNG or PDF or what have you. ",,
5022,5,2172,21e76f93-e1a3-439d-b2f0-48f2ae506b62,2010-08-28 00:06:51.0,5.0,"Flowing data regularly discusses the tools that he uses.  See, for instance:\\n\\n - [40 Essential Tools and Resources to Visualize Data][1]\\n - [What Visualization Tool/Software Should You Use? – Getting Started][2]\\n\\nHe also shows in great detail how he makes graphics on occasion, such as:\\n\\n - [How to Make a US County Thematic Map Using Free Tools][3]\\n - [How to Make a Graph in Adobe Illustrator][4]\\n - [How to Make a Heatmap – a Quick and Easy Solution][5]\\n\\nThere are also other questions on this site:\\n\\n - http://stats.stackexchange.com/questions/224/visualization-libraries\\n - http://stats.stackexchange.com/questions/216/web-visualization-libraries\\n\\nIMO, try:\\n\\n1. R and ggplot2: this is a [good introductory video][6], but the ggplot2 website has lots of resources.\\n2. Processing: plenty of [good tutorials on the homepage][7].\\n3. Protovis: also a plethora of [great examples on the homepage][8].\\n\\nYou can use Adobe afterwards to clean these up.\\n\\nYou can also look at the R `webvis` package, although it isn't as complete as `ggplot2`.  From R, you can run this command to see the Playfair's Wheat example:\\n\\n    install.packages(""webvis"")\\n    library(webvis)\\n    demo(""playfairs.wheat"")\\n\\nLastly, my favorite commercial applications for interactive visualization are:\\n\\n - [Tableau][9]\\n - [Spotfire][10]\\n - [Qlikview][11]\\n\\n\\n  [1]: http://flowingdata.com/2008/10/20/40-essential-tools-and-resources-to-visualize-data/\\n  [2]: http://flowingdata.com/2009/09/03/what-visualization-toolsoftware-should-you-use-getting-started/\\n  [3]: http://flowingdata.com/2009/11/12/how-to-make-a-us-county-thematic-map-using-free-tools/\\n  [4]: http://flowingdata.com/2008/12/16/how-to-make-a-graph-in-adobe-illustrator/\\n  [5]: http://flowingdata.com/2010/01/21/how-to-make-a-heatmap-a-quick-and-easy-solution/\\n  [6]: http://vimeo.com/13454730\\n  [7]: http://processing.org/learning/\\n  [8]: http://vis.stanford.edu/protovis/ex/\\n  [9]: http://www.tableausoftware.com/\\n  [10]: http://spotfire.tibco.com/\\n  [11]: http://www.qlikview.com/",added 1224 characters in body; added 247 characters in body; added 195 characters in body,
5023,2,2174,7e0139bb-fe6f-4ce5-af08-868fcfbae590,2010-08-28 00:26:17.0,22.0,"Already mentioned processing has a nice set of books available. See: [1][1], [2][2], [3][3], [4][4], [5][5], [6][6], [7][7]\\n\\nYou will find lots of stuff on the web to help you start with R. As next step then ggplot2 has excellent web [documentation][8]. I also found Hadley's [book][9] very helpful.\\n\\nPython might be another way to go. Especially with tools like:\\n\\n - [matplotlib][10]\\n - [NetworkX][11]\\n - [igraph][12]\\n - [Chaco][13]\\n - [Mayavi][14]\\n\\nAll projects are well documented on the web. You might also consider peeking into [some][15] [books][16].\\n\\nLastly, [Graphics of Large Datasets][17] book could be also some help.\\n\\n\\n  [1]: http://www.amazon.com/Processing-Programming-Handbook-Designers-Artists/dp/0262182629/ref=sr_1_1?s=books&ie=UTF8&qid=1282954791&sr=1-1\\n  [2]: http://www.amazon.com/Getting-Started-Processing-Casey-Reas/dp/144937980X/ref=sr_1_3?s=books&ie=UTF8&qid=1282954791&sr=1-3\\n  [3]: http://www.amazon.com/Learning-Processing-Beginners-Programming-Interaction/dp/0123736021/ref=sr_1_4?s=books&ie=UTF8&qid=1282954791&sr=1-4\\n  [4]: http://www.amazon.com/Processing-Creative-Coding-Computational-Foundation/dp/159059617X/ref=sr_1_7?s=books&ie=UTF8&qid=1282954791&sr=1-7\\n  [5]: http://www.amazon.com/Algorithms-Visual-Design-Processing-Language/dp/0470375485/ref=sr_1_8?s=books&ie=UTF8&qid=1282954791&sr=1-8\\n  [6]: http://www.amazon.com/Visualizing-Data-Explaining-Processing-Environment/dp/0596514557/ref=sr_1_5?s=books&ie=UTF8&qid=1282954791&sr=1-5\\n  [7]: http://www.amazon.com/Processing-Visual-Artists-Expressive-Interactive/dp/1568817169/ref=sr_1_9?s=books&ie=UTF8&qid=1282954791&sr=1-9\\n  [8]: http://had.co.nz/ggplot2/\\n  [9]: http://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/0387981403/ref=sr_1_1?ie=UTF8&s=books&qid=1282955050&sr=8-1\\n  [10]: http://matplotlib.sourceforge.net/\\n  [11]: http://networkx.lanl.gov/\\n  [12]: http://cneurocvs.rmki.kfki.hu/igraph/\\n  [13]: http://code.enthought.com/chaco/\\n  [14]: http://code.enthought.com/projects/mayavi/\\n  [15]: http://www.amazon.com/Beginning-Python-Visualization-Transformation-Professionals/dp/1430218436/ref=sr_1_1?s=books&ie=UTF8&qid=1282954157&sr=1-1\\n  [16]: http://www.amazon.com/Matplotlib-Python-Developers-Sandro-Tosi/dp/1847197906/ref=sr_1_1?s=books&ie=UTF8&qid=1282954169&sr=1-1\\n  [17]: http://www.amazon.com/Graphics-Large-Datasets-Visualizing-Statistics/dp/0387329064/ref=sr_1_fkmr0_1?ie=UTF8&qid=1282954448&sr=8-1-fkmr0",,
5024,2,2175,e855de4d-3b89-4f20-bb43-d9244427e4e5,2010-08-28 01:20:33.0,5.0,"
<p>
    Statistical Analysis - Stack Exchange is for <span class=""revision-comment"">statistical analysis, data mining, machine learning, data visualization and statistical computing</span>.
</p>
",,
5025,5,2145,8455164b-7699-4767-afc1-314493975dc7,2010-08-28 01:39:46.0,919.0,"Srikant asks for a ""more elegant approach.""  Perhaps the following will respond to this challenge.\\n\\nLet the argument of the exponential be $f(x)$ (so its power series coefficients are the gammas) and let the right hand side be $g(x)$ (with deltas as its coefficients), so that by definition\\n\\n$g(x) = exp(f(x))$.\\n\\nDifferentiating both sides and replacing $exp(f)$ with $g$ yields\\n\\n$g' = f' * g$.\\n\\nWriting this out as power series gives the desired result: the delta comes from $g'$ while the convolution of the gammas and deltas comes from $f' * g$.\\n\\nYou don't have to worry about convergence (and the whole machinery of Taylor series), by the way: all these calculations can be performed in the ring of formal power series.",Texified the formulas.,
5026,2,2176,68931a15-c23b-4877-902e-458371e9d93c,2010-08-28 02:32:13.0,1107.0,"I generally recommend avoiding these types of sphericity tests altogether by using modern mixed modeling methods.  If you are not working with few subjects this will give you a great deal of flexibility in modeling an appropriate covariance structure, freeing you from the strict assumption of sphericity when necessary.  I infer from the `str` output that you have 16 subjects with 12 observations each (I assume balance b/c you areusing clasical method-of-moments tools) which should be enough data to fit a mixed model with structured covariance matrices via (restricted) maximum likelihood.\\n\\n\\nWithout being close to your data I can't offer specific model recommendations, but a place to start in R would be to replace `aov` in your model specifications with `lme` (after `library(nlme)`). The reason this will work is that you have mistakenly provided an `nlme`-style random argument to `aov` (when, as @Matt Albrecht pointed out, an `Error` term would have been approriate).  In nlme, with the random argument set to `~ 1|<your grouping structure>` \\nand no `correlation` or `weight` arguments, you are specifying a random intercept for each group, implying the response covariance within groups is `ZGZ' + R = 1G1'+ \\sigma^2 I` ==> compound symmetry with between-group variance off-diagonal and between-group variance + within-group variance on the diagonal ==> a spherical structure.  From there you can begin to explore (e.g. using the built-in graphical methods), model, and test (e.g. comparing information criteria or using LRTs for nested models) the various forms of non-sphericity. Some of the tools for the modeling component are:\\n\\n- Using the `weights` argument to model non-constant variance (diagonals) within or between groups (e.g. error variance changes between sucrose levels).\\n- Using the `correlation` argument to model non-constant covariance (off-diagonals) within groups (e.g. a structure within a group where residual errors that are closer together in time (e.g. AR1 structure) or space (e.g. Spherical structure) are more similar).\\n- Modeling random slopes by adding terms to the LHS of the `|` in the `random` formula.\\n\\n\\nThough the process can be complex with many potential pitfalls, I believe it will lead you to think more about the data generating mechanism, and when combined with careful graphical checks (I recommed `lattice` b/c `nlme` has excellent lattice-based plotting methods -- but `ggplot` works well too) you are likely to have not only a better scientific understanding of the process, but also less biased and more efficient estimators with which to draw inferences.",,
5027,5,2170,f922ffa6-4e0f-4e3a-8dda-f56016f75d15,2010-08-28 03:16:50.0,1036.0,"Say some previous findings identified a curvilinear effect of X on Y, (specifically that X had a positive effect on Y, and that X^2 had a negative effect). You want to see if the same holds for your entirely different sample (although everything else between studies, constructs/measures, are exactly the same). Neither the previous study nor my study are experimental (so I am not manipulating X, merely observing it).  There is no explicit theoretical reason why a curvilinear effect would occur.\\n\\nI will give some examples of how I would currently go about seeing if this is true, but I would like some input and peoples opinions on what they believe would be a preferrable method, if any of my suggestions are innapropriate, and of course if their are other alternatives.\\n\\nExamples:\\n\\n1) Simply examine the bivariate scatterplot and fit some type of smoothing line to the mean of Y over bins of X. Although confounding could be an issue, if a curvilinear effect exists their will likely be some evidence in its distribution.\\n\\n2) Examine the partial residual plot. A partial residual plot is when I have a regression model, predicting Y from X and other confounders. I take the residuals from this model, and they will be the vertical dimension for the points on the plot. I then take X^2 (which is supposed to represent the curvilinear effect) regress the same predictors on X^2 I used for the model predicting Y, and the residuals from this model will be my horizontal dimension. If their is a linear relationship observed in the scatter plot, X^2 has an effect on Y distinct from other variables already included in the model.\\n\\n[Is this a correct description of partial residual plots? Is there a more direct way to observe the actual shape of the effect as oppossed to seeing if X^2 fits a line?]\\n\\n3) Use some type of model selection criteria, and determine whether a model including X^2 is preferred over a model without X^2.\\n\\n4) Include a model with X^2 and see if X^2 has a statistically significant regression coefficient.\\n\\nEdit: In this context, my main concern is to identify if the effect of X on Y is best represented in a similar manner to the previous study. While their could be other substantively interesting points to compare between studies (like the magnitude of the effect of X on Y), this is not my main concern.\\n\\nLike I said any other suggestions are welcome as well. ",added 306 characters in body,
5028,5,2170,6ce939bb-30a4-45aa-9a0e-7480995b0440,2010-08-28 03:31:57.0,1036.0,"Say some previous findings identified a curvilinear effect of X on Y, (specifically that X had a positive effect on Y, and that X^2 had a negative effect). You want to see if the same holds for your entirely different sample (although everything else between studies, constructs/measures, are exactly the same). Neither the previous study nor my study are experimental (so I am not manipulating X, merely observing it).  There is no explicit theoretical reason why a curvilinear effect would occur.\\n\\nI will give some examples of how I would currently go about seeing if this is true, but I would like some input and peoples opinions on what they believe would be a preferrable method, if any of my suggestions are innapropriate, and of course if their are other alternatives.\\n\\nExamples:\\n\\n1) Simply examine the bivariate scatterplot and fit some type of smoothing line to the mean of Y over bins of X. Although confounding could be an issue, if a curvilinear effect exists their will likely be some evidence in its distribution.\\n\\n2) Examine the partial residual plot. A partial residual plot is when I have a regression model, predicting Y from other confounders. I take the residuals from this model, and they will be the vertical dimension for the points on the plot. I then regress the same predictors on X I used for the model predicting Y, and the residuals from this model will be my horizontal dimension. This should be synonymous with the bivariate scatter, although it should be the effect of the untransformed X on Y, controlling for all other covariation explained by other confounders. While I can not be sure the effect of X on Y is not due to other unobserved confounders not included in the models, this is an assumption I will have to make for all of my results.\\n\\n3) Use some type of model selection criteria, and determine whether a model including X^2 is preferred over a model without X^2.\\n\\n4) Include a model with X^2 and see if X^2 has a statistically significant regression coefficient.\\n\\nLike I said any other suggestions are welcome as well. \\n\\nEdit: In this context, my main concern is to identify if the effect of X on Y is best represented in a similar manner to the previous study. While their could be other substantively interesting points to compare between studies (like the magnitude of the effect of X on Y), this is not my main concern. I also changed #2 to what I believe is more appropriate for my question than what I had originally.",added 73 characters in body,
5029,16,2171,bfdf8050-0020-479d-bc59-dda377ae93e3,2010-08-28 04:09:07.0,1106.0,,,
5030,5,2171,f18d2281-3361-4cb6-9925-d4eaf1613e55,2010-08-28 04:31:24.0,1106.0,"I'm interested in learning how to create the type of visualizations you see at http://flowingdata.com and informationisbeautiful. EDIT: Meaning, visualizations that are interesting in of themselves -- kinda like the NY Times graphics, as opposed to a quick something for a report.\\n\\nWhat kinds of tools are used to create these -- is it mostly a lot of Adobe Illustrator/Photoshop? What are good resources (books, websites, etc.) to learn how to use these tools for data visualization in particular?\\n\\nI know *what* I want visualizations to look like (and I'm familiar with design principles, e.g., from Tufte's books), but I have no idea *how* to create them.",added 151 characters in body,
5031,2,2177,03d80a33-e7de-4e28-b5e6-d0fdf243a78a,2010-08-28 04:34:44.0,1107.0,"It sounds as though you are interested in formal inference and for that method 4 is best.  Add X^2 to a model containing terms you wish to control for and conduct a test to assess the streght of evidence for the quadratic term given the terms in the model.  Note however that ""absence of evidence is not evidence of absence"" and statistical power will come into play (this is of interest if you fail to reject or the CI contains zero).  You willof course also want to perform diagnostics of model assumptions prior to drawing conclusions.   \\n\\nMethods 1 and 2 are excellent exploratory tools and I would encourage exploring the relationship in as many meaningful ways as you wish (since you know *a priori* what formal test you will conduct -- a test of the quadratic term -- this will not lead to data-driven hypothesis testing).  Other methods of exploration include plotting a fitted LOESS smoother or spline to the (possibly partial) relationships, fitting smoothers or parametric fits within subsets of the data (eg using conditioning plots), a 3d scatterplot with a fitted surface (particularly if you include continuous interactions), etc.  These plots will not only help you understand the data better but can also be used as part of a less formal case for/against the quadratic (keeping in mind that humans are excellent at spotting trends in noise).\\n\\nI'm not sure what model selection methods you refer to in 3, but generally automated model selection and testing do not mix.  If you are referring to using information criteria (AICc, BIC, ...) note that the theory behind these is based on prediction rather than testing.  So, number 4 is the most rigorous way to test the quadratic.\\n\\n\\nFinally, 2 comments on terminology:\\n\\n- 'multivariate' models are those whose response is a matrix, and 'multivariable' models are those with a vector response and multiple terms on the RHS.\\n\\n- [Partial residual plots](http://en.wikipedia.org/wiki/Partial_residual_plot) differ from partial regression plots..",,
5032,16,2172,f51d46d0-7ca8-4f07-a660-26acdda8e6c8,2010-08-28 07:10:15.0,5.0,,,
5033,16,2174,2f13052d-3435-463a-b9b5-799b92da0200,2010-08-28 07:10:59.0,5.0,,,
5034,16,2173,24ccd7c0-2d40-4c27-9231-4a9ce260244f,2010-08-28 07:11:23.0,5.0,,,
5035,2,2178,1bebc306-92d9-4720-a384-e09f376b1919,2010-08-28 07:52:54.0,930.0,"Two further references from [B. Schölkopf][1]:\\n\\n* Schölkopf, B. and Smola, A.J. (2002). [Learning with kernels][2]. The MIT Press.\\n* Schölkopf, B., Tsuda, K., and Vert, J.-P. (2004). [Kernel methods in computational biology][3]. The MIT Press.\\n\\nand a website dedicated to [kernel machines][4].\\n\\n\\n  [1]: http://www.kyb.mpg.de/~bs\\n  [2]: http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=8684\\n  [3]: http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&tid=10338\\n  [4]: http://www.kernel-machines.org/",,
5036,5,2106,ada7ea5f-c9de-4b47-8cba-a80acc2e7269,2010-08-28 09:52:21.0,966.0,"Try:\\n\\n    library(ez)\\n    ezANOVA(data=subset(p12bl, exps==1),\\n      within=.(sucrose, citral),\\n      sid=.(subject),\\n      dv=.(resp)\\n      )\\n\\nand the equivalent aov command, minus sphericity etc, is:\\n\\n    aov(resp ~ sucrose*citral + Error(subject/(sucrose*citral)), \\n        data= subset(p12bl, exps==1))\\n\\nSeeing as ez isn't working for either of us at the moment, here's the equivalent using Anova from car directly:\\n\\n    library(car)\\n    df1<-read.table(""clipboard"", header=T)\\n    sucrose<-factor(rep(c(1:4), each=4))\\n    citral<-factor(rep(c(1:4), 4))\\n    idata<-data.frame(sucrose,citral)\\n\\n    mod<-lm(cbind(S1C1, S1C2, S1C3, S1C4, S2C1, S2C2, S2C3, S2C4, \\n            S3C1, S3C2, S3C3, S3C4, S4C1, S4C2, S4C3, S4C4)~1, data=df1)\\n    av.mod<-Anova(mod, idata=idata, idesign=~sucrose*citral)\\n    summary(av.mod)",added 523 characters in body,
5037,5,2106,80d489da-a271-4ca9-a905-685258d855e7,2010-08-28 10:02:31.0,966.0,"Try:\\n\\n    library(ez)\\n    ezANOVA(data=subset(p12bl, exps==1),\\n      within=.(sucrose, citral),\\n      sid=.(subject),\\n      dv=.(resp)\\n      )\\n\\nand the equivalent aov command, minus sphericity etc, is:\\n\\n    aov(resp ~ sucrose*citral + Error(subject/(sucrose*citral)), \\n        data= subset(p12bl, exps==1))\\n\\nSeeing as ez isn't working for either of us at the moment, (I'm not yet sure what is the problem, it may be other attached packages) here's the equivalent using Anova from car directly:\\n\\n    library(car)\\n    df1<-read.table(""clipboard"", header=T)\\n    sucrose<-factor(rep(c(1:4), each=4))\\n    citral<-factor(rep(c(1:4), 4))\\n    idata<-data.frame(sucrose,citral)\\n\\n    mod<-lm(cbind(S1C1, S1C2, S1C3, S1C4, S2C1, S2C2, S2C3, S2C4, \\n            S3C1, S3C2, S3C3, S3C4, S4C1, S4C2, S4C3, S4C4)~1, data=df1)\\n    av.mod<-Anova(mod, idata=idata, idesign=~sucrose*citral)\\n    summary(av.mod)",added 74 characters in body,
5038,5,2106,36f1f375-ff16-4d82-9035-df61ee5d86e4,2010-08-28 10:08:11.0,966.0,"Try:\\n\\n    library(ez)\\n    ezANOVA(data=subset(p12bl, exps==1),\\n      within=.(sucrose, citral),\\n      sid=.(subject),\\n      dv=.(resp)\\n      )\\n\\nand the equivalent aov command, minus sphericity etc, is:\\n\\n    aov(resp ~ sucrose*citral + Error(subject/(sucrose*citral)), \\n        data= subset(p12bl, exps==1))\\n\\nSeeing as ez isn't working for either of us at the moment, (I'm not yet sure what is the problem, it may be other attached packages) here's the equivalent using Anova from car directly:\\n\\n    library(car)\\n    df1<-read.table(""clipboard"", header=T) #From copying data in the question above\\n    sucrose<-factor(rep(c(1:4), each=4))\\n    citral<-factor(rep(c(1:4), 4))\\n    idata<-data.frame(sucrose,citral)\\n\\n    mod<-lm(cbind(S1C1, S1C2, S1C3, S1C4, S2C1, S2C2, S2C3, S2C4, \\n            S3C1, S3C2, S3C3, S3C4, S4C1, S4C2, S4C3, S4C4)~1, data=df1)\\n    av.mod<-Anova(mod, idata=idata, idesign=~sucrose*citral)\\n    summary(av.mod)",added 41 characters in body,
5039,2,2179,174a7cea-44e2-4fad-b737-2f6e7dac7c4d,2010-08-28 13:34:42.0,88.0,How to obtain a variable (attribute) importance using SVM? ,,
5040,1,2179,174a7cea-44e2-4fad-b737-2f6e7dac7c4d,2010-08-28 13:34:42.0,88.0,Variable importance from SVM,,
5041,3,2179,174a7cea-44e2-4fad-b737-2f6e7dac7c4d,2010-08-28 13:34:42.0,88.0,<machine-learning><feature-selection><svm>,,
5042,2,2180,8fdb8546-3e51-43e7-a400-94364b212270,2010-08-28 14:36:05.0,881.0,"If you use l-1 penalty on the weight vector, it does automatic feature selection as the weights corresponding to irrelevant attributes are automatically set to zero. See [this paper][1]. The (absolute) magnitude of each non-zero weights can give an idea about the importance of the corresponding attribute.\\n\\nAlso look at [this paper][2] which uses criteria derived from SVMs to guide the attribute selection.\\n\\n\\n  [1]: http://books.nips.cc/papers/files/nips16/NIPS2003_AA07.pdf\\n  [2]: http://jmlr.csail.mit.edu/papers/volume3/rakotomamonjy03a/rakotomamonjy03a.pdf",,
5043,2,2181,19877dc8-1401-483f-abe1-884545ec30f2,2010-08-28 17:07:59.0,1356.0,"I'm interested in getting some books about multivariate analysis, and need your recommendations. Free books are always welcome, but if you know about some great non-free MVA book, please, state it.",,
5044,1,2181,19877dc8-1401-483f-abe1-884545ec30f2,2010-08-28 17:07:59.0,1356.0,Book recommendations for multivariate analysis,,
5045,3,2181,19877dc8-1401-483f-abe1-884545ec30f2,2010-08-28 17:07:59.0,1356.0,<books><multivariate-analysis>,,
5046,16,2181,19877dc8-1401-483f-abe1-884545ec30f2,2010-08-28 17:07:59.0,1356.0,,,
5047,2,2182,48c769c8-688d-45ca-9a2e-362115bda62f,2010-08-28 22:34:01.0,,"I need help explaining, and citing basic statistics texts, papers or other references, that it is generally incorrect to use the margin of error (MOE) statistic reported in polling to naively declare a statistical tie.\\n\\nAn example:\\nCandidate A leads Candidate B in a poll, 39 - 31 percent, 4.5 percent margin-of-error for 500 surveyed voters.\\n\\nMy friend reasons like so:\\n\\n> Because of the intricacies of statistical modeling, the margin of error means that A's true support could be as low as 34.5 percent and B's could be as high as 35.5 percent. Therefore, A and B are actually in a statistical dead heat.\\n\\nAll help appreciated in clearly articulating the flaw my friend's reasoning.  I've tried to explain that it is incorrect to naively reject the hypothesis ""A leads B"" if pA-pB < 2*MOE.    \\n",,somebody
5048,1,2182,48c769c8-688d-45ca-9a2e-362115bda62f,2010-08-28 22:34:01.0,,help explaining statistical tie is not naively p1-p2 < 2*MOE,,somebody
5049,3,2182,48c769c8-688d-45ca-9a2e-362115bda62f,2010-08-28 22:34:01.0,,<polling>,,somebody
5050,2,2183,14aefdcb-07bc-4881-8d23-2cbf59769541,2010-08-28 23:20:51.0,,In my opinion the 'correct' way to think of the polling result is as follows:\\n\\n> In a survey of 500 voters the chances that we will see a difference in lead as high as 8% is greater than 5%.\\n\\nWhether you believe that 'A leads B' or 'A ties B' is then dependent on the extent to which you are willing to accept 5% as your cut-off criteria.,,user28
5051,2,2184,b8519ec8-0f1b-4d5a-850a-1c7be2a35e42,2010-08-29 00:12:35.0,601.0,"Not only is that a bad way to term things but that's not even a statistical dead heat.\\n\\nThe MOE is just one side of a confidence interval and you seem to be applying it correct in one aspect.  Candidate B's score is unlikely to be higher than 35.5%.  Since it's usually a 95% confidence interval then Candidate B's score will be higher than 35.5% less than 5% of the time.  Not impossible, just usually considered unlikely.  On the other hand, a score of say 34% also has a cutoff probability.  It's just not reported.  It's probably something lower than random chance, we just don't know what it is from the MOE alone.  Therefore, it's completely incorrect to say that scores with the MOE cutoff are all just randomly equally likely.  Which is the implication of the assertion of statistical dead heat.\\n\\nNot only that, you don't use overlapping confidence intervals that way.  If you really wanted to only say that Candidate A was going to win if the probability of Candidate B winning was less than 5% then Candidate A is definitely in the lead.  The lead is 8%.  The confidence interval of that subtraction score is not double the confidence interval of the individual scores but only sqrt(2) times those scores.  That's because finding the difference between the values would only double the variance.  The confidence interval is based on a sqrt of the variance therefore combining them is the average (4.5) * sqrt(2).  Since the MOE of your 8% lead is approximately 6.4% then Candidate A is in the lead.\\n\\nAs an aside, MOE's are very conservative and based on the 50% choice value. The formula is sqrt(0.25/n) * 2.  The numerator there (0.25) would be less on a strict test of your hypothesis.\\n\\nAny introduction to both confidence intervals and to power would be helpful here.  Even the wikipedia article on MOE looks pretty good.",,
5052,5,2184,b1b8e72b-1404-449b-bac4-b3088e22c5e7,2010-08-29 00:27:19.0,601.0,"Not only is that a bad way to term things but that's not even a statistical dead heat.\\n\\nThe MOE is just one side of a confidence interval and you seem to be applying it correct in one aspect.  Candidate B's score is unlikely to be higher than 35.5%.  Since it's usually a 95% confidence interval then Candidate B's score will be higher than 35.5% less than 5% of the time.  Not impossible, just usually considered unlikely.  On the other hand, a score of say 34% also has a cutoff probability.  It's just not reported.  It's probably something lower than random chance, we just don't know what it is from the MOE alone.  Therefore, it's completely incorrect to say that scores with the MOE cutoff are all just randomly equally likely.  Which is the implication of the assertion of statistical dead heat.\\n\\nNot only that, you don't use overlapping confidence intervals that way.  If you really wanted to only say that Candidate A was going to win if the probability of Candidate B winning was less than 5% then Candidate A is definitely in the lead.  The lead is 8%.  The confidence interval of that subtraction score is not double the confidence interval of the individual scores but only sqrt(2) times those scores.  That's because finding the difference between the values would only double the variance.  The confidence interval is based on a sqrt of the variance therefore combining them is the average (4.5) * sqrt(2).  Since the MOE of your 8% lead is approximately 6.4% then Candidate A is in the lead.\\n\\nAs an aside, MOE's are very conservative and based on the 50% choice value. The formula is sqrt(0.25/n) * 2.  There is a formula for calculating standard errors of difference scores that we could use as well.  We would apply that using the found values rather than the 50% cutoff and that still gives us a significant lead for Candidate A.\\n\\nAny introduction to both confidence intervals and to power would be helpful here.  Even the wikipedia article on MOE looks pretty good.",added 152 characters in body,
5053,5,2183,dacc9a0d-962e-4dab-b855-4b7cccf6413c,2010-08-29 00:32:35.0,,"My first attempt at an answer was flawed (see below for the flawed answer). The reason it is flawed is that the margin of error (MOE) that is reported applies to a candidate's polling percentage but *not* to the difference of the percentages. My second attempt explicitly addresses the question posed by the OP a bit better.\\n\\n**Second Attempt**\\n\\nThe OP's friend reasons as follows:\\n\\n 1. Construct the confidence interval for Candidate A and Candidate B separately using the given MOE.\\n 2. If they overlap we have a statistical dead hear and if they do not then A is currently leading B.\\n\\nThe main issue here is that the first step is invalid. Constructing confidence intervals independently for the two candidates is not a valid step because the true percentages for the two candidates are dependent random variables. In other words, a voter who decides not to vote for A may potentially decide to vote for B instead. Thus, the correct way to assess if the lead is significant or not is to construct a confidence interval for the difference. See the wiki as to how to compute the standard error for the [difference of polling percentages][1] under some assumptions.\\n\\n\\n\\n\\n**Flawed answer below** \\n\\nIn my opinion the 'correct' way to think of the polling result is as follows:\\n\\n> In a survey of 500 voters the chances that we will see a difference in lead as high as 8% is greater than 5%.\\n\\nWhether you believe that 'A leads B' or 'A ties B' is then dependent on the extent to which you are willing to accept 5% as your cut-off criteria.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Margin_of_error#Comparing_percentages",fixed flawed answer,user28
5054,5,2184,24eddd9b-41ee-4d71-bbb3-8f0e68873546,2010-08-29 00:32:54.0,601.0,"Not only is that a bad way to term things but that's not even a statistical dead heat.\\n\\nThe MOE is just one side of a confidence interval and you seem to be applying it correct in one aspect.  Candidate B's score is unlikely to be higher than 35.5%.  Since it's usually a 95% confidence interval then Candidate B's score will be higher than 35.5% less than 5% of the time.  Not impossible, just usually considered unlikely.  On the other hand, a score of say 34% also has a cutoff probability.  It's just not reported.  It's probably something lower than random chance, we just don't know what it is from the MOE alone.  Therefore, it's completely incorrect to say that scores with the MOE cutoff are all just randomly equally likely.  Which is the implication of the assertion of statistical dead heat.\\n\\nNot only that, you don't use overlapping confidence intervals that way.  If you really wanted to only say that Candidate A was going to win if the probability of Candidate B winning was less than 5% then Candidate A is definitely in the lead.  The lead is 8%.  The confidence interval of that subtraction score is not double the confidence interval of the individual scores but only sqrt(2) times those scores.  That's because finding the difference between the values would only double the variance.  The confidence interval is based on a sqrt of the variance therefore combining them is the average (4.5) * sqrt(2).  Since the MOE of your 8% lead is approximately 6.4% then Candidate A is in the lead.\\n\\nAs an aside, MOE's are very conservative and based on the 50% choice value. The formula is sqrt(0.25/n) * 2.  There is a formula for calculating standard errors of difference scores that we could use as well.  We would apply that using the found values rather than the 50% cutoff and that still gives us a significant lead for Candidate A (7.5% MOE).  I believe that, given the questioners comment, and the proximity of that cutoff to the hypothetical one selected, that that was probably what they were looking for.  I like my answer better though because, while not the most correct mathematically in this instance, it is the generally correct way to compare data where confidence intervals are reported for individual values and gives a framework for thinking about why the CI doesn't double.\\n\\nAny introduction to both confidence intervals and to power would be helpful here.  Even the wikipedia article on MOE looks pretty good.",added 177 characters in body; added 279 characters in body,
5055,2,2185,1a168000-6c0a-4921-83ef-a31f93151ce6,2010-08-29 01:35:44.0,1106.0,"Similar to what Mark said, Statistics was historically called **Inverse Probability**, since statistics tries to infer the causes of an event given the observations, while probability tends to be the other way around.",,
5056,5,2183,80ded89a-7821-4c80-8e32-6533f7f1f1d0,2010-08-29 02:31:19.0,,"My first attempt at an answer was flawed (see below for the flawed answer). The reason it is flawed is that the margin of error (MOE) that is reported applies to a candidate's polling percentage but *not* to the difference of the percentages. My second attempt explicitly addresses the question posed by the OP a bit better.\\n\\n**Second Attempt**\\n\\nThe OP's friend reasons as follows:\\n\\n 1. Construct the confidence interval for Candidate A and Candidate B separately using the given MOE.\\n 2. If they overlap we have a statistical dead hear and if they do not then A is currently leading B.\\n\\nThe main issue here is that the first step is invalid. Constructing confidence intervals independently for the two candidates is not a valid step because the polling percentages for the two candidates are dependent random variables. In other words, a voter who decides not to vote for A may potentially decide to vote for B instead. Thus, the correct way to assess if the lead is significant or not is to construct a confidence interval for the difference. See the wiki as to how to compute the standard error for the [difference of polling percentages][1] under some assumptions.\\n\\n\\n\\n\\n**Flawed answer below** \\n\\nIn my opinion the 'correct' way to think of the polling result is as follows:\\n\\n> In a survey of 500 voters the chances that we will see a difference in lead as high as 8% is greater than 5%.\\n\\nWhether you believe that 'A leads B' or 'A ties B' is then dependent on the extent to which you are willing to accept 5% as your cut-off criteria.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Margin_of_error#Comparing_percentages",added 3 characters in body,user28
5057,5,2182,77f02fc0-3942-42aa-ac69-3a6596f39725,2010-08-29 08:26:24.0,88.0,"I need help explaining, and citing basic statistics texts, papers or other references, why it is generally incorrect to use the margin of error (MOE) statistic reported in polling to naively declare a statistical tie.\\n\\nAn example:\\nCandidate A leads Candidate B in a poll, 39 - 31 percent, 4.5 percent margin-of-error for 500 surveyed voters.\\n\\nMy friend reasons like so:\\n\\n> Because of the intricacies of statistical modeling, the margin of error means that A's true support could be as low as 34.5 percent and B's could be as high as 35.5 percent. Therefore, A and B are actually in a statistical dead heat.\\n\\nAll help appreciated in clearly articulating the flaw my friend's reasoning.  I've tried to explain that it is incorrect to naively reject the hypothesis ""A leads B"" if $p_A-p_B < 2MOE$.    \\n",Improved post a little.,
5058,4,2182,77f02fc0-3942-42aa-ac69-3a6596f39725,2010-08-29 08:26:24.0,88.0,Can you explaining why statistical tie is not naively when $p_1-p_2 < 2 MOE$?,Improved post a little.,
5059,2,2186,2d3265e6-cacb-4f12-8367-48e910086139,2010-08-29 08:44:54.0,930.0,"Off the top of my head, I would say that the following general purpose books are rather interesting as a first start:\\n\\n* Izenman, J. [Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning][1]. Springer. [companion website][2]\\n* Tinsley, H. and Brown, S. (2000). [Handbook of Applied Multivariate Statistics and Mathematical Modeling][3]. Academic Press.\\n\\nThere is also many applied textbook, like\\n\\n* Everitt, B.S. (2005). [An R and S-Plus® Companion to Multivariate Analysis][4]. Springer. [companion website][5]\\n\\nIt is difficult to suggest you specific books as there are many ones that are domain-specific (e.g. social sciences, machine learning, categorical data, biomedical data).\\n\\n  [1]: http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-78188-4\\n  [2]: http://astro.temple.edu/~alan/MMST/\\n  [3]: http://www.sciencedirect.com/science/book/9780126913606\\n  [4]: http://www.springer.com/statistics/social+sciences+%26+law/book/978-1-85233-882-4\\n  [5]: http://biostatistics.iop.kcl.ac.uk/publications/everitt/",,
5060,16,2186,2d3265e6-cacb-4f12-8367-48e910086139,2010-08-29 08:44:54.0,-1.0,,,
5061,2,2187,6e8f27e3-f858-432d-8107-94595ac285ee,2010-08-29 08:58:43.0,339.0,"Here are some of my books on that field (in alphabetical order). \\n\\n - AFIFI, A., CLARK, V. Computer-Aided\\n   Multivariate Analysis. CHAPMAN & HALL, 2000\\n - AGRESTI, A. Categorical Data Analysis. WILEY, 2002\\n - HAIR, Multivariate Data Analysis. 6th Ed.\\n - ΗÄRDLE, W., SIMAR, L. Applied Multivariate Statistical Analysis. SPRINGER, 2007.\\n - HARLOW, L. The Essence of Multivariate Thinking. LAWRENCE ERLBAUM ASSOCIATES, INC., 2005\\n - GELMAN, A., HILL, J. Data Analysis\\n   Using Regression and\\n   Multilevel/Hierarchical Models.\\n   CAMBRIDGE UNIVERSITY PRESS, 2007.\\n - IZENMAN, A. J. Modern Multivariate Statistical Techniques. SPRINGER, 2008 \\n - RENCHER, A. Methods of Multivariate analysis. SECOND ED., WILEY-INTERSCIENCE, 2007\\n - TABACHNICK B., FIDELL, L. Using Multivariate Statistics. 5th Ed. Pearson Education. Inc, 2007.\\n - TIMM, N. Applied Multivariate Analysis. SPRINGER, 2002\\n - YANG, K., TREWN, J. Multivariate Statistical Methods in Quality Management. MCGRAW-HILL, 2004\\n\\n",,
5062,16,2187,6e8f27e3-f858-432d-8107-94595ac285ee,2010-08-29 08:58:43.0,-1.0,,,
5063,2,2188,fd943ba8-7290-48cf-a8c8-da63513679d4,2010-08-29 12:04:51.0,930.0,"I have a slight preference for [Random Forests][1] by Leo Breiman & Adele Cutleer for several reasons:\\n\\n* it allows to cope with categorical and continuous predictors, as well as unbalanced class sample size;\\n* as an ensemble/embedded method, cross-validation is embedded and allows to estimate a generalization error;\\n* it is relatively insensible to its tuning parameters (% of variables selected for growing a tree, # of trees built);\\n* it provides an original measure of variable importance and is able to uncover complex interactions between variables (although this may lead to hard to read results).\\n\\nSome authors argued that it performed as well as penalized SVM or Gradient Boosting Machines (see, e.g. Cutler et al., 2009, for the latter point).\\n\\nA complete coverage of its applications or advantages may be off the topic, so I suggest the [Elements of Statistical Learning][5] from Hastie et al. (chap. 15) and Sayes et al. (2007) for further readings.\\n\\nLast but not least, it has a nice implementation in R, with the [randomForest][2] package. Other R packages also extend or use it, e.g. [party][3] and [caret][4].\\n\\n**References:**\\n\\nCutler, A., Cutler, D.R., and Stevens, J.R. (2009). Tree-Based Methods, in *High-Dimensional Data Analysis in Cancer Research*, Li, X. and Xu, R. (eds.), pp. 83-101, Springer.\\n\\nSaeys, Y., Inza, I., and Larrañaga, P. (2007). A review of feature selection techniques in bioinformatics. *Bioinformatics*, **23(19)**: 2507-2517.\\n\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/\\n  [2]: http://cran.r-project.org/web/packages/randomForest/index.html\\n  [3]: http://cran.r-project.org/web/packages/party/index.html\\n  [4]: http://cran.r-project.org/web/packages/caret/index.html\\n  [5]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/",,
5064,16,2188,fd943ba8-7290-48cf-a8c8-da63513679d4,2010-08-29 12:04:51.0,-1.0,,,
5065,2,2189,808ddabf-9963-421e-bdec-8a5aed19e2f9,2010-08-29 14:07:55.0,339.0,"I'm not sure if such a (non-parametric) permutation procedure could be applied here. Anyways, here is my idea:\\n\\n    a <- c(1.18, -0.41, -0.66, 0.98, 0.1)\\n    b <- c(-0.36, -0.73, -1.47, 0.15, -0.31)\\n    total <- c(a,b)\\n    first <- combn(total,length(x))\\n    second <- apply(first,2,function(z) total[is.na(pmatch(total,z))])\\n    var.ratio <- apply(first,2,var) / apply(second,2,var)\\n    # the first element of var.ratio is the one that I'm interested in\\n    (p.value <- length(var.ratio[var.ratio >= var.ratio[1]]) / length(var.ratio))\\n    [1] 0.3055556",,
5066,5,2189,3ea20317-78e3-4404-814a-1460992dbb25,2010-08-29 14:41:44.0,339.0,"I'm not sure if such a (non-parametric) permutation procedure could be applied here. Anyways, here is my idea:\\n\\n    a <- c(1.18, -0.41, -0.66, 0.98, 0.1)\\n    b <- c(-0.36, -0.73, -1.47, 0.15, -0.31)\\n    total <- c(a,b)\\n    first <- combn(total,length(a))\\n    second <- apply(first,2,function(z) total[is.na(pmatch(total,z))])\\n    var.ratio <- apply(first,2,var) / apply(second,2,var)\\n    # the first element of var.ratio is the one that I'm interested in\\n    (p.value <- length(var.ratio[var.ratio >= var.ratio[1]]) / length(var.ratio))\\n    [1] 0.3055556",edited body,
5067,2,2190,6a1f6f01-61f6-4ad5-b0f5-0522022b5841,2010-08-29 16:26:44.0,,I have a great prediction yet I am unsure how to uncover how the results were generated?,,CLOCK
5068,1,2190,6a1f6f01-61f6-4ad5-b0f5-0522022b5841,2010-08-29 16:26:44.0,,How to reconstruct ensemble of trees from random forest?,,CLOCK
5069,3,2190,6a1f6f01-61f6-4ad5-b0f5-0522022b5841,2010-08-29 16:26:44.0,,<classification>,,CLOCK
5070,2,2191,b82aee27-0aef-4e08-9e5f-4083b7a67a9a,2010-08-29 17:15:22.0,174.0,"[Analyzing Multivariate Data][1] by James Lattin, J Douglas Carroll and Paul E Green.\\n\\n  [1]: http://www.amazon.com/dp/0534349749",,
5071,16,2191,b82aee27-0aef-4e08-9e5f-4083b7a67a9a,2010-08-29 17:15:22.0,-1.0,,,
5072,2,2192,4d2c5455-a949-4957-8ae9-a27a46a0dfaa,2010-08-29 17:49:09.0,956.0,"I've used [ZedGraph][1] for .NET. It's open source, and supports all common 2D chart types. \\n\\n\\n  [1]: http://zedgraph.org/",,
5073,6,2190,298c5430-5a0c-4a02-9047-05d526f6fcf4,2010-08-29 18:45:28.0,71.0,<classification><random-forest>,edited tags,
5074,6,224,d31bce4f-4426-41be-a71f-ec8e206fcc96,2010-08-29 18:46:00.0,71.0,<data-visualization><software>,edited tags,
5075,2,2193,019503b9-f1a8-459a-a18a-109509d92d13,2010-08-29 19:47:13.0,1098.0,"From the trees attributed to each class's output you can do a tree search on the similarities. You could do it manually, but that would be as tedious as examining the weights on a Neural network. So you want to find the overlaps in the decision tree structures. This can look for various features depending upon the problem. \\n\\nEg. you can see if there is statistically more preference for certain nodes to be parents of other nodes giving a hierarchical structure. You just search all nodes and find if there is more than average having certain children. Do a distribution test to show its beyond a random chance to have consistently those children and that reveals structure of the problem you analyzed. \\nBest.",,
5076,2,2194,252babc9-b42d-4c25-abcc-4a569a032a80,2010-08-29 19:54:09.0,1098.0,"R is great, but it is not that **R** is difficult to learn it's that the documentation is impossible to search for any other name like Rq would be great. So when you got a problem, searching for a solution is a nightmare, and the documentation is not great either. Matlab or Octave will be great. And to get those plots in R or Matlab would be very very tedious.\\n\\nIMHO post processing visuals is the best route. Alot of them from flowing data are put through Adobe Illustrator or Gimp. It is faster. Once you get the structure of the plot, then change details in an editor. Using R as an editor does not give you the flexibility you want. You will find yourself searching for new packages all the time.",,
5077,16,2194,252babc9-b42d-4c25-abcc-4a569a032a80,2010-08-29 19:54:09.0,-1.0,,,
5078,2,2195,ee0a6c59-d466-47d0-9d24-c57dbe4f0ef6,2010-08-29 20:01:34.0,1098.0,"I don't know about the first point. But for the second one, autoregressive (AR) functions could be simple. I would really chose a parametric method against a non-parametric one. The forecasting in AR is straight forward. And consensus data has lots of samples for each period so you can get robust parameter estimates at each time. And for the association x_n-1 to x_n function is simply a smoothed interpolation of any choice. \\n\\nChanges in zones, well based on empirical data or prior belief?\\n\\nState of the art in consenus? Those methods are arcane. They iterate over generations! Aeons. You could use Gaussian processes which would be quite advanced methodology for these problems. But most in the field stick to older methods given more 'tuning'.\\nBest.\\n",,
5079,2,2196,585e195d-241d-46ef-92cd-19a9410f2fd1,2010-08-29 20:42:33.0,930.0,"You can try *Latent Semantic Analysis*, which basically provides a way to represent in a reduced space your news feeds and any term (in your case, keyword appearing in the title). As it relies on Singular Value Decomposition, I suppose you may then be able to check if there exists a particular association between those two attributes. I know this is used to find documents matching a specific set of criteria, as in information retrieval, or to construct a tree reflecting terms similarity (like a dictionary) based on a large corpus (which here plays the role of the concept space).\\n\\nSee for a gentle introduction [An Introduction to Latent Semantic Analysis][1], by Landauer et al.\\n\\nMoreover, there is an R package that implements this technique, namely [lsa][2].\\n\\n\\n  [1]: http://lsa.colorado.edu/papers/dp1.LSAintro.pdf\\n  [2]: http://cran.r-project.org/web/packages/lsa/index.html",,
5080,5,1947,9aa14c2f-ff69-4950-aa51-2177d834aeb1,2010-08-29 21:38:20.0,223.0,"As **@Ars** said their are no accepted definition (and this is a good point). Their are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer falls into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$  (with $x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{\\alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)). Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can generalize the definition of quantiles in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",deleted 1 characters in body,
5081,2,2197,b6f0cedd-3e59-4400-ba2a-4e98ac428217,2010-08-29 22:50:57.0,1117.0,"Let's say I have a dataset with 1000 observations in 10 variables, ""A"" through ""J."" I have 1000 responses/measures for each of the first 8 variables, through ""H,"" but only the first 500 observations for ""I"" are not missing, and only the last 500 observations for ""J"" are not missing -- there are no observations for which I have measures of both of the last two variables, I and J.\\n\\nThus, if I calculate (pairwise) correlations, I have a full correlation matrix, with only the correlation between I and J missing. Let's say I want to run Principal Component Analysis, or some other such scaling procedure on this correlation matrix.\\n\\nWhat I think I would like to do is: \\n\\n 1. Randomly generate (perhaps from some distribution on [-1, 1], or perhaps via sampling from existing values in the rest of the correlation matrix) an ""invented"" correlation between I and J.\\n 2. Put that in the correlation matrix.\\n 3. Run PCA on the correlation matrix with this invented value.\\n 4. Repeat steps 1 - 3 some large number of times.\\n 5. Assess the collective results of this large number of PCAs, looking at the mean and variance of the loadings, scores, eigenvalues, etc., based on the ""pseudo-bootstrapped"" iterations.\\n\\nQuestions:\\n\\n 1. Is there a better way to handle (a) missing value(s) in the correlation matrix?\\n 2. Is there any precedent for replacing such (a) missing value(s) with random invented values? If so, what is it called?\\n 3. Is this related to the bootstrap?\\n\\nThanks a lot, in advance.",,
5082,1,2197,b6f0cedd-3e59-4400-ba2a-4e98ac428217,2010-08-29 22:50:57.0,1117.0,"Precedent for Bootstrap-like procedure with ""invented"" data?",,
5083,3,2197,b6f0cedd-3e59-4400-ba2a-4e98ac428217,2010-08-29 22:50:57.0,1117.0,<correlation><pca><bootstrap>,,
5084,5,2197,d1e2f46a-bf33-41c7-8683-510acd8d80d7,2010-08-29 23:18:38.0,1117.0,"Let's say I have a dataset with 1000 observations in 10 variables, ""A"" through ""J."" I have 1000 responses/measures for each of the first 8 variables, through ""H,"" but only the first 500 observations for ""I"" are not missing, and only the last 500 observations for ""J"" are not missing -- there are no observations for which I have measures of both of the last two variables, I and J.\\n\\nThus, if I calculate (pairwise) correlations, I have a full correlation matrix, with only the correlation between I and J missing. Let's say I want to run Principal Component Analysis, or some other such scaling procedure on this correlation matrix.\\n\\nWhat I think I would like to do is: \\n\\n 1. Randomly generate (perhaps from some distribution on [-1, 1], or perhaps via sampling from existing values in the rest of the correlation matrix) an ""invented"" correlation between I and J.\\n 2. Put that in the correlation matrix.\\n 3. Run PCA on the correlation matrix with this invented value.\\n 4. Repeat steps 1 - 3 some large number of times.\\n 5. Assess the collective results of this large number of PCAs, looking at the mean and variance of the loadings, scores, eigenvalues, etc., based on the ""pseudo-bootstrapped"" iterations.\\n\\nQuestions:\\n\\n 1. Is there a better way to handle (a) missing value(s) in the correlation matrix?\\n 2. Is there any precedent for replacing such (a) missing value(s) with random invented values? If so, what is it called?\\n 3. Is this related to the bootstrap?\\n\\nThanks a lot, in advance.\\n\\n\\nEdit:\\nQuestion 4. Is this a defensible approach to imputation?",added 69 characters in body,
5085,2,2198,689255b0-e19f-4f24-9184-b59f28df0bb9,2010-08-30 00:54:36.0,862.0,"If given probability of A is a and probability of B is b, how do I find min/max probability of intersection? Max value of intersection would be min(a,b), how do I find the min? ",,
5086,1,2198,689255b0-e19f-4f24-9184-b59f28df0bb9,2010-08-30 00:54:36.0,862.0,find range of possible values for probability of intersection given individual probabilities,,
5087,3,2198,689255b0-e19f-4f24-9184-b59f28df0bb9,2010-08-30 00:54:36.0,862.0,<probability>,,
5088,2,2199,3caa53bf-5e10-4ef8-8215-a56e0cbccf00,2010-08-30 01:22:20.0,74.0,"Tabachnick is the most cited on Google Scholar\\n\\nHair (6th ed) has the most ratings (with a score above 4.5) on Amazon\\n\\nI recommend Hair, as I've read it, and it is written in plain language. \\n\\nIf you are a student or staff at a university, then I would see if your school has an account with SpringerLink, as the Hardle book is on there for free. ",,
5089,16,2199,3caa53bf-5e10-4ef8-8215-a56e0cbccf00,2010-08-30 01:22:20.0,-1.0,,,
5090,2,2200,89aa4b42-6269-47d5-8cec-bbce6efbe0e7,2010-08-30 02:51:15.0,795.0,"The min is the smaller of two values: $\\min(a,b) = a$ if $a < b$ and $b$ otherwise. Though I do not think this is what you are asking for...",,
5091,2,2201,aa5e9fd2-86a1-41fa-9b46-00f1c3fd2677,2010-08-30 03:31:36.0,183.0,"An alternative approach would be to impute the missing raw data using a missing data replacement procedure. You could then run the PCA on the correlation matrix that resulted from the imputed dataset (see also [multiple imputation][1]).\\n\\nHere are a few links on missing data imputation in R:\\n\\n- [Gelman on missing data imputation][2]\\n- [Quick-R][3] has links to R packages such as `Amelia II`, `Mice` and `mitools`\\n\\n\\n  [1]: http://www.stat.psu.edu/~jls/mifaq.html\\n  [2]: http://www.stat.columbia.edu/~gelman/arm/missing.pdf\\n  [3]: http://www.statmethods.net/input/missingdata.html",,
5092,5,2198,aba11be6-4930-4482-b9b1-e3cf27c75311,2010-08-30 03:35:01.0,183.0,"If given probability of $A$ is $a$ and probability of $B$ is $b$, how do I find min/max probability of intersection? Max value of intersection would be $\\min(a,b)$, how do I find the min? ",added 12 characters in body; deleted 1 characters in body,
5093,2,2202,5f5dc9b1-de00-4631-859e-dc99ec9622fc,2010-08-30 03:59:15.0,1112.0,"if $a+b \\le 1$, then presumably one can find disjoint sets $A$ and $B$ with ${\\rm P}A = a$ and\\n${\\rm P}B = b$. so in this case, the min is 0.\\n\\nif $a+b > 1$, we get a smallest intersection by choosing $B$ to contain all of $A^C$, which has probability $1-a$ and then adding to that a piece of $A$ to bring ${\\rm P}B$ up to $b$. so the piece of $A$ added in has to have probability $b - (1-a) = a+b - 1$. this last quantity is then the min probability for the intersection when $a+b > 1$. \\n\\nso in any case. the min is $(a+b-1)^+$.\\n\\n ",,
5094,2,2203,47b9a03d-a9c3-49e0-984e-b9813017748a,2010-08-30 04:55:48.0,1118.0,"JOHNSON R., WICHERN D., [Applied Multivariate Statistical Analysis][1], is what we used in our undergraduate Multivariate class at UC Davis, and it does a pretty good job (though it's a bit pricey).\\n\\n\\n  [1]: http://www.pearsonhighered.com/educator/academic/product/0,3110,0131877151,00.html",,
5095,16,2203,47b9a03d-a9c3-49e0-984e-b9813017748a,2010-08-30 04:55:48.0,-1.0,,,
5096,16,224,610cccec-62f4-4d69-9abd-941aba386134,2010-08-30 05:10:29.0,128.0,,,
5097,4,2198,3a98753a-451e-45df-86aa-159f392a738f,2010-08-30 06:25:16.0,88.0,Find range of possible values for probability of intersection given individual probabilities,edited title,
5098,2,2204,dee4ca04-c224-4c50-b688-9f8bba9152ad,2010-08-30 09:45:59.0,159.0,"You could start with the following references:\\n\\n - [Comte (1999)][1] ""Discrete and continuous time cointegration"", *Journal of Econometrics*.\\n - [Ferstl (2009)][2] ""Cointegration in discrete and continuous time"". Thesis.\\n\\n[Citations of Comte][3] may also be useful.\\n\\n\\n  [1]: http://dx.doi.org/10.1016/S0304-4076(98)00025-6\\n  [2]: http://www-m4.ma.tum.de/Diplarb/DA-Ferstl.pdf\\n  [3]: http://scholar.google.com.au/scholar?cites=2228464727485746115",,
5099,5,949,2ee51180-6db0-408f-93e1-51088e71fefc,2010-08-30 09:48:48.0,159.0,"Take $x \\in {0,1}^d$ and $y \\in \\{0,1\\}$ and suppose we model the task of predicting y given x using logistic regression. When can logistic regression coefficients be written in closed form?\\n\\nOne example is when we use a saturated model.\\n\\nIE, define $P(y|x) \\propto \\exp(\\sum_i w_i f_i(x_i))$, where i indexes sets in the power-set of $\\{x_1,\\ldots,x_d\\}$, and $f_i$ returns 1 if all variables in i'th set are 1, 0 otherwise. Then you can express each $w_i$ in this logistic regression model as a logarithm of a rational function of statistics of the data. \\n\\nAre there other interesting examples when closed form exists?",Latex was broken. I assume this is what was intended.,
5100,2,2205,8f73f85b-00e3-49dc-9edd-906b21edf03a,2010-08-30 11:48:13.0,961.0,"Hastie, T., Tibshirani, R. and Friedman, J.: ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction."", Springer ([book home page][1])\\n\\n\\n  [1]: http://www-stat.stanford.edu/~tibs/ElemStatLearn/",,
5101,16,2205,8f73f85b-00e3-49dc-9edd-906b21edf03a,2010-08-30 11:48:13.0,-1.0,,,
5102,2,2206,a84e0df7-b6d2-4dad-a1b4-96a8bfa5d27e,2010-08-30 11:49:00.0,88.0,"1. I don't know.\\n2. What you've shown is a legitimate Monte Carlo simulation \\n3. Bootstrap is also a Monte Carlo method, but it is more about estimating distributions.\\n4. In general yes, especially if inputation is giving poor results. In special cases when inputation works great, no. In simple words, it will be as good as strongly you are convinced that you cannot say more about I&J correlation that it is in -1..1.",,
5103,5,2170,f34d7ebf-0567-4f6a-91fb-275fe0bdf17e,2010-08-30 12:27:26.0,1036.0,"Say some previous findings identified a curvilinear effect of X on Y, (specifically that X had a positive effect on Y, and that X^2 had a negative effect). You want to see if the same holds for your entirely different sample (although everything else between studies, constructs/measures, are exactly the same). Neither the previous study nor my study are experimental (so I am not manipulating X, merely observing it).  There is no explicit theoretical reason why a curvilinear effect would occur.\\n\\nI will give some examples of how I would currently go about seeing if this is true, but I would like some input and peoples opinions on what they believe would be a preferrable method, if any of my suggestions are innapropriate, and of course if their are other alternatives.\\n\\nExamples:\\n\\n1) Simply examine the bivariate scatterplot and fit some type of smoothing line to the mean of Y over bins of X (e.g. LOESS). Although confounding could be an issue, if a curvilinear effect exists their will likely be some evidence in its distribution.\\n\\n2) Examine the [partial regression plot][1] or other visualization techniques to identify the effect of X on Y independent of other confounding variables.\\n\\n3) Use some type of model selection criteria (e.g. BIC), and determine whether a model including X^2 is preferred over a model without X^2\\n\\n4) Include a model with X^2 and see if X^2 has a statistically significant regression coefficient.\\n\\nLike I said any other suggestions are welcome as well. \\n\\nEdit: In this context, my main concern is to identify if the effect of X on Y is best represented in a similar manner to the previous study. While their could be other substantively interesting points to compare between studies (like the magnitude of the effect of X on Y), this is not my main concern.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Partial_regression_plot",deleted 606 characters in body; edited title,
5104,4,2170,f34d7ebf-0567-4f6a-91fb-275fe0bdf17e,2010-08-30 12:27:26.0,1036.0,Preferred method for identifying curvilinear effect in multi-variable regression framework,deleted 606 characters in body; edited title,
5105,5,973,87943078-4630-461e-90c1-cd0c74c24654,2010-08-30 12:33:04.0,223.0,"What are the **freely available data set for classification with more than 1000 features** (or sample points if it is about curves)? \\n\\nThere is already a community wiki about free data sets:\\nhttp://stats.stackexchange.com/questions/7/locating-freely-available-data-samples/\\n\\nBut here, it would be nice to have a **more focused list that can be used more conveniently**, also I propose the following rules:\\n\\n 1. One post **per dataset** \\n 2. **No** link to set of dataset \\n 3. each data set **must** be associated with\\n\\n    - a **name** (to figure out what it is about)  and a link to the dataset (R datasets can be named with package name)\\n\\n    - the number of features (let say it is **p**) the size of the dataset (let say it is **n**) and the number of labels/class (let say it is **k**)    \\n\\n    - a typical **error rate** from your experience (state the used algorithm in to words) or from the litterature (in this last case link the paper) \\n",deleted 47 characters in body; added 2 characters in body,
5106,2,2207,3bbd6dbd-e0eb-49b3-840c-e41d1ad208e5,2010-08-30 13:44:51.0,1119.0,"In ordinary least squares, the solution to (A'A)^(-1) x = A'b minimizes squared error loss, and is the maximum likelihood solution.\\n\\nSo, largely because the math was easy in this historic case.\\n\\nBut generally people minimize many different loss functions (http://en.wikipedia.org/wiki/Loss_function), such as exponential, logistic, cauchy, laplace, huber, etc.  These more exotic loss functions generally require a lot of computational resources, and don't have closed form solutions (in general), so they're only starting to become more popular now.",,
5107,2,2208,e47146b3-65a7-4b77-a6ec-d80bf5e58ca4,2010-08-30 13:55:03.0,1119.0,"The reason the above works for uncertainty of the mean is because of the central limit theorem.\\n\\nAs long as the central limit theorem holds for your application, so will the above.",,
5108,2,2209,38a2b8f0-5b77-44c0-9c01-9b58f5db2eb6,2010-08-30 14:00:17.0,1119.0,"I don't know if it's a great overall textbook, but David MacKay's Information Theory, Inference, and Learning Algorithms made me intuitively grasp Bayesian statistics better than others - most do the how quite nicely, but I felt MacKay explained why better.",,
5109,16,2209,38a2b8f0-5b77-44c0-9c01-9b58f5db2eb6,2010-08-30 14:00:17.0,-1.0,,,
5110,2,2210,9d07abf0-d0ee-45bb-8506-ab374be1bec1,2010-08-30 14:11:06.0,1119.0,"While they're not mutually exclusive, I think the growing popularity of Bayesian statistics is part of it.  Bayesian statistics can achieve a lot of the same goals through priors and model averaging, and tend to be a bit more robust in practice.",,
5113,6,973,dcf0e800-8506-4922-9ed9-131cf39a539c,2010-08-30 14:16:50.0,223.0,<machine-learning><classification><dataset>,edited tags,
5114,2,2212,e6e8c2ca-a539-422c-af38-f37b14dad265,2010-08-30 14:19:24.0,1119.0,For javascript protovis (http://vis.stanford.edu/protovis/) is very nice.,,
5115,16,2212,e6e8c2ca-a539-422c-af38-f37b14dad265,2010-08-30 14:19:24.0,-1.0,,,
5116,5,145,e4a3bdb7-563e-4a93-8ae5-8a98881b8e55,2010-08-30 15:02:00.0,442.0,> **Possible Duplicate:**  \\n> [Locating freely available data samples](http://stats.stackexchange.com/questions/7/locating-freely-available-data-samples)  \\n\\n<!-- End of automatically inserted text -->\\n\\nWhere can I find freely accessible data sources?\\n\\nI'm thinking of sites like\\n\\n* [http://www2.census.gov/census_2000/datasets/][1]?\\n\\n\\n\\n  [1]: http://www2.census.gov/census_2000/datasets/,changed tags,
5117,6,145,e4a3bdb7-563e-4a93-8ae5-8a98881b8e55,2010-08-30 15:02:00.0,442.0,<dataset>,changed tags,
5118,6,270,12284762-919c-4476-92e5-52d8046f1d30,2010-08-30 15:05:20.0,442.0,<modeling><poisson>,edited tags,
5119,6,641,7d2d124d-7419-4371-9d36-5922b083c728,2010-08-30 15:06:38.0,442.0,<dataset><surveys>,changed tags,
5120,6,645,ff71143a-729d-486d-949b-9db6bace0e5d,2010-08-30 15:11:44.0,442.0,<machine-learning><graphical-techniques><analysis>,edited tags,
5121,6,645,ea13bfef-627b-4c6d-b37f-2f3382be59da,2010-08-30 15:12:10.0,442.0,<statistical-analysis><machine-learning><graphical-techniques>,edited tags,
5122,6,715,b2e5bee6-c491-47fe-ab88-0696177cedd5,2010-08-30 15:13:35.0,442.0,<classification><dataset><multivariable>,edited tags,
5123,6,1358,083de51e-991a-407a-b84e-a8884f739ea7,2010-08-30 15:15:26.0,442.0,<intuition><moments><directional-statistics>,edited tags,
5124,6,1358,930fa28f-7164-4667-ab62-e5e133408f53,2010-08-30 15:17:01.0,442.0,<mathematical-statistics><intuition><moments><directional-statistics>,edited tags,
5125,6,1385,29a5323c-ace3-4d1c-ac56-82cc4b11c678,2010-08-30 15:18:26.0,442.0,<statistical-analysis><fitting><data-generating-process><missing-data>,edited tags,
5126,6,1822,159556a1-1f22-4b24-a239-1cd9d5e521ae,2010-08-30 15:21:15.0,442.0,<statistical-analysis><least-squares>,edited tags,
5127,2,2213,3bbac159-e93c-4ec8-9d97-583d59bc47d0,2010-08-30 15:33:28.0,5.0,What is the difference between a [feed-forward][1] and [recurrent][2] neural network?  Why would you use one over the other?  Do other network topologies exist?  \\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Feedforward_neural_network\\n  [2]: http://en.wikipedia.org/wiki/Recurrent_neural_networks,,
5128,1,2213,3bbac159-e93c-4ec8-9d97-583d59bc47d0,2010-08-30 15:33:28.0,5.0,Feed-forward and recurrent neural networks?,,
5129,3,2213,3bbac159-e93c-4ec8-9d97-583d59bc47d0,2010-08-30 15:33:28.0,5.0,<machine-learning><neural-networks><topologies>,,
5130,2,2214,1fe3ec58-db66-47ed-941a-0900d01ae39d,2010-08-30 16:36:55.0,795.0,"One of the purported uses of L-estimators is the ability to 'robustly' estimate the parameters of a random variable drawn from a given class. One of the downsides of using [Levy $\\alpha$-stable distributions][1] is that it is difficult to estimate the parameters given a sample of observations drawn from the class. Has there been any work in estimating parameters of a Levy RV using L-estimators? There is an obvious difficulty in the fact that the PDF and CDF of the Levy distribution does not have a closed form, but perhaps this could be overcome by some trickery. Any hints?\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Stable_distribution",,
5131,1,2214,1fe3ec58-db66-47ed-941a-0900d01ae39d,2010-08-30 16:36:55.0,795.0,estimating parameters of sum-stable RV via L-estimators,,
5132,3,2214,1fe3ec58-db66-47ed-941a-0900d01ae39d,2010-08-30 16:36:55.0,795.0,<distributions><estimation><parametric>,,
5133,2,2215,cc362549-e732-4242-b2b8-1c7bd645e4f2,2010-08-30 16:54:55.0,1080.0," 1. I think we need to know more about the nature of the data to make recommendations on how to deal with the missing values. An exploratory task that jumps out to me is to look at the behavior of variables A through H when I is present, versus A through H when J is present. Is there anything interesting to take into account for subsequent modeling? Instead of resampling a descriptive statistic, like correlation, I would consider resampling the data itself. For example, you could use the bootstrap to create 500 new (I,J) pairs based upon the 500 values you actually have for these variables. But, again, the exploratory work may inform a resampling scheme beyond a ""naive"", IID approach.\\n\\n 2. In general, as others have noted, filling in missing data goes by ""imputation"" and there are different techniques depending on the context. For example, in one setting I might simply use a median value, or a spline fit, but for a missing data point in a time series I might impute with a value generated from an ARMA time series model.\\n\\n 3. Your outlined solution would be ""bootstrapping"" if you resample from the observed data. I think of *Monte Carlo* as any method that uses probabilistic sampling of data as input into a computation. When the sampling is from a non-parametric or parametric distribution that you use to model how the data was generated, I still call it Monte Carlo. But, when the sampling is done from an empirical distribution (i.e., the observed data itself, not a model of the data generating process) I call it bootstrapping.",,
5134,2,2216,a5996169-fa49-4239-af53-90f0584a2038,2010-08-30 17:42:31.0,1036.0,"Although it may only be of little help, the problem you present to me is synonymous with the ""[Change of Support][1]"" problem encountered when using areal units. Although this work just presents a framework for what you describe as ""reglarize and interpolate"" using a method referred to as ""kriging"". I don't think any of this work will help answer your question of whether estimating your missing values in the series in such a manner will bias error correction estimates, although if some of your samples are in clustered time intervals for both series you may be able to check for yourself. You may also be interested in the technique of ""co-kriging"" from this field, which uses information from one source to estimate the value for another (if your interested I would suggest you check out the work being done by [Pierre Goovaerts][2]).\\n\\nAgain I'm not sure how helpful this will be though. It may be substantially simpler to just use current time-series forecasting techniques to estimate your missing data. It won't help you decide what to estimate either.\\n\\nGood luck, and keep the thread updated if you find any pertinent material. I would be interested, and you would think with the proliferation of data sources online this would become an pertinent issue for at least some research projects.\\n\\n\\n  [1]: http://dx.doi.org/10.1198/016214502760047140\\n  [2]: http://sites.google.com/site/goovaertspierre/pierregoovaertswebsite",,
5135,2,2217,6236ac20-23c1-49e8-85b4-e87aa4882ff6,2010-08-30 18:02:22.0,5.0,Another from XKCD:\\n![alt text][1]\\n\\nMentioned [here][2] and [here][3].\\n\\n\\n  [1]: http://i.stack.imgur.com/3ngU8.png\\n  [2]: http://www.stat.columbia.edu/~cook/movabletype/archives/2009/02/cartoon.html\\n  [3]: http://www.cerebralmastication.com/2009/02/box-plot-vs-violin-plot-in-r/,,
5136,16,2217,6236ac20-23c1-49e8-85b4-e87aa4882ff6,2010-08-30 18:02:22.0,-1.0,,,
5137,2,2218,684f2fba-0fab-4107-9615-9117abf7bc27,2010-08-30 18:23:24.0,339.0,"[**Feed-forward**][1] ANNs allow signals to travel one way only; from input to output. There is no feedback (loops) i.e. the output of any layer does not affect that same layer. Feed-forward ANNs tend to be straight forward networks that associate inputs with outputs. They are extensively used in pattern recognition. This type of organisation is also referred to as bottom-up or top-down.\\n\\n[**Feedback**][2] (or recurrent or interactive) networks can have signals traveling in both directions by introducing loops in the network. Feedback networks are very powerful and can get extremely complicated. Computations derived from earlier input are fed back into the network, which gives them a kind of memory. Feedback networks are dynamic; their 'state' is changing continuously until they reach an equilibrium point. They remain at the equilibrium point until the input changes and a new equilibrium needs to be found.\\n\\n\\nFeedforward neural networks are ideally suitable for modeling relationships between a set of predictor or input variables and one or more response or output variables. In other words, they are appropriate for any functional mapping problem where we want to know how a number of input variables affect the output variable. The multilayer feedforward neural networks, also called multi-layer perceptrons (MLP), are the most widely studied and used neural network model in practice.\\n\\nAs an example of feedback network, I can recall Hopfield’s network. The main use of Hopfield’s network is as associative memory. An associative memory is a device which accepts an input pattern and generates an output as the stored pattern which is most closely associated with the input. The function of the associate memory is to recall the corresponding stored pattern, and then produce a clear version of the pattern at the output. Hopfield networks are typically used for those problems with binary pattern vectors and the input pattern may be a noisy version of one of the stored patterns. In\\nthe Hopfield network, the stored patterns are encoded as the weights of the network.\\n\\n**Kohonen’s self-organizing maps** (SOM) represent another neural network type that is markedly different from the feedforward multilayer networks. Unlike training in the feedforward MLP, the SOM training or learning is often called the unsupervised because there are no known target outputs associated with each input pattern in SOM and during the training process, the SOM processes the input patterns and learns to cluster or segment the data through adjustment of weights (that makes it important neural network model for dimension reduction and data clustering). A two-dimensional map is typically created in such a way that the orders of the interrelationships among inputs are preserved. The number and composition of clusters can be visually determined based on the output distribution generated by the training process. With only input variables in the training sample, SOM aims to learn or discover the underlying structure of the data.\\n\\n\\n\\n  [1]: http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.multil.jpg\\n  [2]: http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.neural2.jpg",,
5138,2,2219,8719a2b5-2e77-4276-afde-c2dc8a0ad87f,2010-08-30 18:42:30.0,401.0,"**General question:** Given a dartboard of unit radius, what's the probability that a dart randomly lands within a circle of radius 1/3 centered inside the dartboard?\\n\\n**Standard answer:** The dart is thrown such that it hits each point with equal likelihood. The probability that it lands within the inner circle is the ratio of the areas of the two circles, which is 1/9.\\n\\n**Other formulation:** Suppose that the dart is thrown directly at the center of the dartboard, but wind comes from a random direction. For any wind direction, the velocity of the wind pushes the dart some distance from the center of the dartboard to its perimeter, with each distance being equally likely. Each wind vector is equally likely and, for each vector, each distance is equally likely. The probability of being less than 1/3 units from the center of the dartboard is 1/3.\\n\\nThe definition of randomness is different for each formulation. In the standard answer, a random vector is chosen from the set $\\{(x,y)\\colon \\; x^2+y^2 \\leq 1\\}$ and we ask the probability that $x^2 + y^2 \\leq \\frac{1}{9}$. In the other formulation, a random vector is chosen $\\{(x,y)\\colon \\; x^2+y^2 = 1\\}$ and, on this vector, a distance $d$ is chosen uniformly on $[0,1]$. We ask the probability that $d\\leq\\frac{1}{3}$.\\n\\nI understand the math of solving this problem, but I don't understand intuitively why these different conceptions of randomness give two different answers. It seems that both are valid means for answering the general question. Intuitively, why do they give different answers?",,
5139,1,2219,8719a2b5-2e77-4276-afde-c2dc8a0ad87f,2010-08-30 18:42:30.0,401.0,Two answers to the dartboard problem,,
5140,3,2219,8719a2b5-2e77-4276-afde-c2dc8a0ad87f,2010-08-30 18:42:30.0,401.0,<probability>,,
5141,2,2220,321b3015-ba39-4012-8e79-26f0ee6f6e96,2010-08-30 18:47:48.0,339.0,"Permutation tests are significance tests based on permutation resamples drawn at random from the original data. Permutation resamples are drawn without replacement, in contrast to bootstrap samples, which are drawn with replacement. Here is [an example I did in R][1] of a simple permutation test. (Your comments are welcome)\\n\\nPermutation tests have great advantages. They do not require specific population shapes such as normality. They apply to a variety of statistics, not just to statistics that have a simple distribution under the null hypothesis. They can give very accurate p-values, regardless of the shape and size of the population (if enough permutations are used). \\n\\nI have also read that it is often useful to give a confidence interval along with a test, which is created using bootstrap resampling rather than permutation resampling.\\n\\nCould you explain (or just give the R code) how a confidence interval is constructed (i.e. for the difference between the means of the two samples in the above example) ? \\n\\n\\n  [1]: http://stackoverflow.com/questions/2449226/randomized-experiments-in-r",,
5142,1,2220,321b3015-ba39-4012-8e79-26f0ee6f6e96,2010-08-30 18:47:48.0,339.0,How do we create a confidence interval for the parameter of a permutation test? ,,
5143,3,2220,321b3015-ba39-4012-8e79-26f0ee6f6e96,2010-08-30 18:47:48.0,339.0,<confidence-interval><bootstrap><permutation>,,
5144,2,2221,fb113a5a-e8d3-43c4-be83-d4b022b57072,2010-08-30 18:53:38.0,795.0,"Intuitively, imagine modeling the second formulation as follows: randomly select an angle to the $x$-axis, calling it $\\theta$, then model the location of the dart as falling uniformly in a very thin rectangle along the line $y = (\\tan\\theta) x$.  Approximately, the dart is in the inner circle with probability $1/3$. However, when you consider the collection of all such thin rectangles (draw them, say), you will see that they have more overlapping area near the center of the dartboard, and less overlap towards the perimeter of the dartboard. This will be more obvious as you draw the rectangles larger and larger (though the approximation will be worse). As you make the rectangles thinner, the approximation gets better, but the same principle applies: you are putting more area around the center of the circle, which increases the probability of hitting the inner circle. ",,
5145,2,2222,79208f36-4e04-4376-95e3-6110d5a9285d,2010-08-30 19:49:10.0,601.0,"It's OK to use permutation resampling.  It really depends on a number of factors.  If your permutations are a relatively low number then your estimation of your confidence interval is not so great with permutations.  Your permutations are in somewhat of a gray area and probably are fine.\\n\\nThe only difference from your prior code is that you'd generate your samples randomly instead of with permutations.  And, you'd generate more of them, let's say 1000 for example.  Get the difference scores for your 1000 replications of your experiment.  Take the cutoffs for the middle 950 (95%).  That's your confidence interval.  It falls directly from the bootstrap.\\n\\nYou've already done most of this in your example.  dif.treat is 462 items long.  Therefore, you need the lower 2.5% and upper 2.5% cut offs (about 11 items in on each end).\\n\\nUsing your code from before... \\n\\n    y <- sort(dif.treat)\\n    ci.lo <- y[11]\\n    ci.hi <- y[462-11]\\n\\nOff hand I'd say that 462 is a little low but you'll find a bootstrap to 10,000 comes out with scores that are little different (likely closer to the mean).",,
5146,2,2223,50136b0a-1e5e-43b6-9ba7-28e3cd33de3a,2010-08-30 19:54:28.0,431.0,I would like to know if anyone could recommend a book that deals more with the practical issues around conducting a meta-analysis?\\n\\nThanking you in advance\\n\\nAndrew Vitiello ,,
5147,1,2223,50136b0a-1e5e-43b6-9ba7-28e3cd33de3a,2010-08-30 19:54:28.0,431.0,Books covering how to conduct a meta-anlysis,,
5148,3,2223,50136b0a-1e5e-43b6-9ba7-28e3cd33de3a,2010-08-30 19:54:28.0,431.0,<meta-analysis>,,
5149,5,2222,24525c0a-2de8-40ab-940f-998cf6bf0eb6,2010-08-30 20:24:38.0,601.0,"It's OK to use permutation resampling.  It really depends on a number of factors.  If your permutations are a relatively low number then your estimation of your confidence interval is not so great with permutations.  Your permutations are in somewhat of a gray area and probably are fine.\\n\\nThe only difference from your prior code is that you'd generate your samples randomly instead of with permutations.  And, you'd generate more of them, let's say 1000 for example.  Get the difference scores for your 1000 replications of your experiment.  Take the cutoffs for the middle 950 (95%).  That's your confidence interval.  It falls directly from the bootstrap.\\n\\nYou've already done most of this in your example.  dif.treat is 462 items long.  Therefore, you need the lower 2.5% and upper 2.5% cut offs (about 11 items in on each end).\\n\\nUsing your code from before... \\n\\n    y <- sort(dif.treat)\\n    ci.lo <- y[11]\\n    ci.hi <- y[462-11]\\n\\nOff hand I'd say that 462 is a little low but you'll find a bootstrap to 10,000 comes out with scores that are little different (likely closer to the mean).\\n\\nThought I'd also add in some simple code requiring the boot library (based on your prior code).\\n\\n    diff <- function(x,i) mean(x[i[6:11]]) - mean(x[i[1:5]])\\n    b <- boot(total, diff, R = 1000)\\n    boot.ci(b)",added boot based code,
5150,2,2224,b8f07041-3ccb-4e5e-b6ae-c2e521c96fbf,2010-08-30 20:33:07.0,919.0,"I asked this question last week and obtained two excellent answers.  The question is readily accessible through links on your ""meta-analysis"" tag.  Here's the URL:\\n\\nhttp://stats.stackexchange.com/questions/1963/looking-for-good-introductory-treatment-of-meta-analysis",,
5151,2,2225,c455693a-a24b-4a09-9fb8-167144323b30,2010-08-30 21:07:57.0,88.0,"Think of the board as a filter -- it just converts the positions on board into an id of a field that dart hit. So that the output will be only a deterministically converted input -- and thus it is obvious that different realization of throwing darts will result in distribution of results.   \\nThe paradox itself is purely linguistic -- ""random throwing"" seems ok, while the true is that it misses crucial information about how the throwing is realized.",,
5152,2,2226,b7bdb6a6-e1af-415e-a973-2f2be85967c7,2010-08-30 22:37:36.0,,"It seems to me that the fundamental issue is that the two scenarios assume different data generating process for the position of a dart which results in different probabilities. \\n\\nThe first situation's data generating process looks like so: (a) Pick a $x \\in U[-1,1]$ and (b) Pick a $y$ uniformly subject to the constraint that $x^2+y^2 \\le 1$. Then the required probability is $P(x^2 + y^2  \\le \\frac{1}{9})= \\frac{1}{9}$. \\n\\nThe second situation's data generating process is as described in the question: (a) Pick an angle $\\theta \\in [0,2\\pi]$ and (b) Pick a point on the diameter that is at an angle $\\theta$ to the x-axis. Under this data generating process the required probability is $\\frac{1}{3}$ as mentioned in the question.\\n\\nAs articulated by mbq, the issue is that the phrase 'randomly lands on the dartboard' is not precise enough as it leaves the meaning of 'random' ambiguous. This is similar to asking what is the probability of coin landing heads on a random toss. The answer can be 0.5 if we assume that the coin is a fair coin but it can be anything else (say, 0.8) if the coin is biased towards heads.",,user28
5153,2,2227,13aa2139-b23e-4abd-9b9f-18f4fd7048d5,2010-08-30 23:04:11.0,881.0,There is this one on Bayesian learning:\\n\\n![alt text][1]\\n\\n\\n  [1]: http://i.stack.imgur.com/64Pe4.jpg,,
5154,16,2227,13aa2139-b23e-4abd-9b9f-18f4fd7048d5,2010-08-30 23:04:11.0,-1.0,,,
5155,2,2228,12550657-16d9-4f0b-833c-96ae61dfc254,2010-08-31 00:03:12.0,364.0,"ez has now been updated to version 2.0. Among other improvements, the bug that caused it to fail to work for this example has been fixed.",,
5156,2,2229,b766c41d-1602-44cb-bb5d-de5dd2cb8a70,2010-08-31 00:32:34.0,1122.0,"There is no single exact confidence interval for the ratio of two proportions. Generally speaking, an exact 95% confidence interval is any interval-generating procedure that guarantees at least 95% coverage of the true ratio, irrespective of the values of the underlying proportions.\\n\\nAn interval formed by the Fisher Exact Test is probably overly conservative -- in that it has MORE than 95% coverage for most values of the parameters. It's not wrong but it's also wider than it has to be.\\n\\nThe interval used by the StatXact software with the default settings would be a better choice here -- I believe it uses some variety of Chan interval (i.e. an extremum-searching interval using the Berger-Boos procedure and a standardized statistic), but would need to check the manual to be sure.\\n\\n(Footnote: Some authors reserve the word ""exact"" to apply only to intervals and tests where false-positives are controlled at exactly alpha, instead of merely bounded by alpha. Taken in this sense, there simply isn't a deterministic exact confidence interval for the ratio of two proportions, period. All of the deterministic intervals are necessarily approximate. Of course, even so some intervals and tests do unconditionally control Type I error and some don't.)",,
5157,5,2229,ee1dea36-ca95-4594-88ac-5385e012296d,2010-08-31 02:24:01.0,1122.0,"There is no single exact confidence interval for the ratio of two proportions. Generally speaking, an exact 95% confidence interval is any interval-generating procedure that guarantees at least 95% coverage of the true ratio, irrespective of the values of the underlying proportions.\\n\\nAn interval formed by the Fisher Exact Test is probably overly conservative -- in that it has MORE than 95% coverage for most values of the parameters. It's not wrong but it's also wider than it has to be.\\n\\nThe interval used by the StatXact software with the default settings would be a better choice here -- I believe it uses some variety of Chan interval (i.e. an extremum-searching interval using the Berger-Boos procedure and a standardized statistic), but would need to check the manual to be sure.\\n\\nWhen you ask for the ""how and why"" -- does this answer your question? I think we could certainly expound further about the definition of confidence intervals and how to construct one from scratch if that's what you were looking for. Or does it do the trick just to say that this is a Fisher Exact Test-based interval, one (but not the only and not the most powerful) of the confidence intervals that guarantees its coverage unconditionally?\\n\\n(Footnote: Some authors reserve the word ""exact"" to apply only to intervals and tests where false-positives are controlled at exactly alpha, instead of merely bounded by alpha. Taken in this sense, there simply isn't a deterministic exact confidence interval for the ratio of two proportions, period. All of the deterministic intervals are necessarily approximate. Of course, even so some intervals and tests do unconditionally control Type I error and some don't.)",added 444 characters in body,
5158,5,2106,94c9a73f-06d8-4792-84ab-af1c8b367655,2010-08-31 03:28:42.0,966.0,"Try:\\n\\n    library(ez)\\n    ezANOVA(data=subset(p12bl, exps==1),\\n      within=.(sucrose, citral),\\n      sid=.(subject),\\n      dv=.(resp)\\n      )\\n\\nand the equivalent aov command, minus sphericity etc, is:\\n\\n    aov(resp ~ sucrose*citral + Error(subject/(sucrose*citral)), \\n        data= subset(p12bl, exps==1))\\n\\nHere's the equivalent using Anova from car directly:\\n\\n    library(car)\\n    df1<-read.table(""clipboard"", header=T) #From copying data in the question above\\n    sucrose<-factor(rep(c(1:4), each=4))\\n    citral<-factor(rep(c(1:4), 4))\\n    idata<-data.frame(sucrose,citral)\\n\\n    mod<-lm(cbind(S1C1, S1C2, S1C3, S1C4, S2C1, S2C2, S2C3, S2C4, \\n            S3C1, S3C2, S3C3, S3C4, S4C1, S4C2, S4C3, S4C4)~1, data=df1)\\n    av.mod<-Anova(mod, idata=idata, idesign=~sucrose*citral)\\n    summary(av.mod)",ez package now working!,
5159,2,2230,ad1012c2-52e1-4673-9c50-aff3cddeda70,2010-08-31 03:57:21.0,1106.0,"I've never really grokked the difference between these two measures of convergence. (Or, in fact, any of the different types of convergence, but I mention these two in particular because of the Weak and Strong Laws of Large Numbers.)\\n\\nSure, I can quote the definition of each and give an example where they differ, but I still don't quite get it.\\n\\nWhat's a good way to understand the difference? Why is the difference important? Is there a particularly memorable example where they differ?",,
5160,1,2230,ad1012c2-52e1-4673-9c50-aff3cddeda70,2010-08-31 03:57:21.0,1106.0,Convergence in probability vs. Almost sure convergence,,
5161,3,2230,ad1012c2-52e1-4673-9c50-aff3cddeda70,2010-08-31 03:57:21.0,1106.0,<probability><random-variable>,,
5162,2,2231,a02ad957-a2ec-487c-8635-3dea03fe8d76,2010-08-31 04:39:45.0,,"I understand it as follows,\\n\\n**Convergence in probability**\\n\\nThe probability that the sequence of random variables equals the target value is asymptotically decreasing and approaches 0 but never actually attains 0.\\n\\n**Almost Sure Convergence**\\n\\nThe sequence of random variables will equal the target value asymptotically but you cannot predict at what point it will happen.\\n\\nAlmost sure convergence is a stronger condition on the behavior of a sequence of random variables because it states that ""something will definitely happen"" (we just don't know when). In contrast, convergence in probability states that ""while something is likely to happen"" the likelihood of ""something *not* happening"" decreases asymptotically but never actually reaches 0. (something $\\equiv$ a sequence of random variables converging to a particular value). \\n\\nThe [wiki][1] has some examples of both which should help clarify the above (in particular see the example of the archer in the context of convergence in prob and the example of the charity in the context of almost sure convergence).\\n\\nFrom a practical standpoint, convergence in probability is enough as we do not particularly care about very unlikely events. As an example, consistency of an estimator is essentially convergence in probability. Thus, when using a consistent estimate, we implicitly acknowledge the fact that in large samples there is a very small probability that our estimate is far from the true value. We live with this 'defect' of convergence in probability as we know that asymptotically the probability of the estimator being far from the truth is vanishingly small.\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Convergence_of_random_variables",,user28
5163,2,2232,a3585168-bc59-40e9-b1e6-935c9ff2de0e,2010-08-31 06:53:32.0,352.0,"From my point of view the difference is important, but largely for philosophical reasons.  Assume you have some device, that improves with time.  So, every time you use the device the probability of it failing is less than before.  \\n\\nConvergence in probability says that the chance of failure goes to zero as the number of usages goes to infinity.  So, after using the device a large number of times, you can be very confident of it working correctly, it still might fail, it's just very unlikely.\\n\\nConvergence almost surely is a bit stronger.  It says that the total number of failures is *finite*. That is, if you count the number of failures as the number of usages goes to infinity, you will get a finite number.  The impact of this is as follows:  As you use the device more and more, you will, after some finite number of usages, exhaust all failures.  From then on the device will work perfectly. \\n\\nAs Srikant points out, you don't actually know when you have exhausted all failures, so from a purely practical point of view, there is not much difference between the two modes of convergence.\\n\\nHowever, personally I am very glad that, for example the strong law of large numbers exists, as opposed to just the weak law.  Because now, a scientific experiment to obtain, say, the speed of light, is justified in taking averages.  At least in theory, after obtaining enough data, you can get arbitrarily close to the true speed of light.  There wont be any failures (however improbable) in the averaging process.\\n\\n   ",,
5164,2,2233,846c26df-76ed-42a6-b57c-57c6e730468c,2010-08-31 07:47:14.0,930.0,"Check out the R [Epi][1] and [epitools][2] packages, which include many functions for computing exact and approximate CIs/p-values for various measures of association found in epidemiological studies.\\n\\nBy ""exact"" tests, we generally mean tests/CIs not relying on an asymptotic distribution, like the chi-square or standard normal; e.g. in the case of an RR, an 95% CI may be approximated as\\n\\n    exp[ log(rr) - 1.96*sqrt(Var(log(rr))) ], exp[ log(rr) + 1.96*sqrt(Var(log(rr))) ], \\nwhere `Var(log(rr))=1/a - 1/(a+b) + 1/c - 1/(c+d)` (assuming a 2-way cross-classification table, with `a`, `b`, `c`, and `d` denoting cell frequency)\\n\\nFor more details on the calculation of CIs in epidemiology, I would suggest to look at Rothman and Greenland's textbook, [Modern Epidemiology][3] (now in it's 3rd edition), or [Statistical analyses of the relative risk][4], from J.J. Gart (1979). The explanations given by @Keith are, however, very insightful. You will generally get similar results with `fisher.test()`, as pointed by @gd047.\\n\\n**Note:** I don't check your Excel file, for the reason advocated by @csgillespie.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/Epi/index.html\\n  [2]: http://cran.r-project.org/web/packages/epitools/index.html\\n  [3]: http://www.lww.com/product/?978-0-7817-5564-1\\n  [4]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1637917/",,
5165,5,2232,2aff4170-8e46-482b-96cf-d750aec4095e,2010-08-31 07:51:12.0,352.0,"From my point of view the difference is important, but largely for philosophical reasons.  Assume you have some device, that improves with time.  So, every time you use the device the probability of it failing is less than before.  \\n\\nConvergence in probability says that the chance of failure goes to zero as the number of usages goes to infinity.  So, after using the device a large number of times, you can be very confident of it working correctly, it still might fail, it's just very unlikely.\\n\\nConvergence almost surely is a bit stronger.  It says that the total number of failures is *finite*. That is, if you count the number of failures as the number of usages goes to infinity, you will get a finite number.  The impact of this is as follows:  As you use the device more and more, you will, after some finite number of usages, exhaust all failures.  From then on the device will work *perfectly*. \\n\\nAs Srikant points out, you don't actually know when you have exhausted all failures, so from a purely practical point of view, there is not much difference between the two modes of convergence.\\n\\nHowever, personally I am very glad that, for example, the strong law of large numbers exists, as opposed to just the weak law.  Because now, a scientific experiment to obtain, say, the speed of light, is justified in taking averages.  At least in theory, after obtaining enough data, you can get arbitrarily close to the true speed of light.  There wont be any failures (however improbable) in the averaging process.\\n\\n   ",added 3 characters in body,
5167,4,2230,14725c47-82ef-4753-aa82-f672bd46d9b7,2010-08-31 08:21:26.0,88.0,Convergence in probability vs. almost sure convergence,edited title,
5168,5,2233,56c0592b-383d-4b3d-83eb-314e72898ba0,2010-08-31 08:40:58.0,930.0,"Check out the R [Epi][1] and [epitools][2] packages, which include many functions for computing exact and approximate CIs/p-values for various measures of association found in epidemiological studies.\\n\\nBy ""exact"" tests, we generally mean tests/CIs not relying on an asymptotic distribution, like the chi-square or standard normal; e.g. in the case of an RR, an 95% CI may be approximated as\\n$\\exp[ \\log(\\text{rr}) - 1.96\\sqrt{\\text{Var}(\\log(\\text{rr}))} ], \\exp[ \\log(\\text{rr}) + 1.96\\sqrt{\\text{Var}(\\log(\\text{rr}))} ]$, \\nwhere $\\text{Var}(\\log(\\text{rr}))=1/a - 1/(a+b) + 1/c - 1/(c+d)$ (assuming a 2-way cross-classification table, with $a$, $b$, $c$, and $d$ denoting cell frequencies)\\n\\nFor more details on the calculation of CIs in epidemiology, I would suggest to look at Rothman and Greenland's textbook, [Modern Epidemiology][3] (now in it's 3rd edition), or [Statistical analyses of the relative risk][4], from J.J. Gart (1979). The explanations given by @Keith are, however, very insightful. You will generally get similar results with `fisher.test()`, as pointed by @gd047.\\n\\n**Note:** I don't check your Excel file, for the reason advocated by @csgillespie.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/Epi/index.html\\n  [2]: http://cran.r-project.org/web/packages/epitools/index.html\\n  [3]: http://www.lww.com/product/?978-0-7817-5564-1\\n  [4]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1637917/",format using LaTeX,
5169,5,2233,e548277d-d048-4711-9b53-8fc8c7835b01,2010-08-31 09:04:24.0,930.0,"Check out the R [Epi][1] and [epitools][2] packages, which include many functions for computing exact and approximate CIs/p-values for various measures of association found in epidemiological studies, including relative risk (RR). I know there is also [PropCIs][3], but I never tried it. Bootstraping is also an option, but generally these are exact or approximated CIs that are provided in epidemiological papers, although most of the explanatory studies rely on GLM, and thus make use of odds-ratio (OR) instead of RR (although, wrongly it is often the RR that is interpreted because it is easier to understand, but this is another story).\\n\\nYou can also check your results with online calculator, like on [statpages.org][7], or [Relative Risk and Risk Difference Confidence Intervals][8]. The latter explains how computations are done.\\n\\nBy ""exact"" tests, we generally mean tests/CIs not relying on an asymptotic distribution, like the chi-square or standard normal; e.g. in the case of an RR, an 95% CI may be approximated as\\n$\\exp\\left[ \\log(\\text{rr}) - 1.96\\sqrt{\\text{Var}\\big(\\log(\\text{rr})\\big)} \\right], \\exp\\left[ \\log(\\text{rr}) + 1.96\\sqrt{\\text{Var}\\big(\\log(\\text{rr})\\big)} \\right]$, \\nwhere $\\text{Var}\\big(\\log(\\text{rr})\\big)=1/a - 1/(a+b) + 1/c - 1/(c+d)$ (assuming a 2-way cross-classification table, with $a$, $b$, $c$, and $d$ denoting cell frequencies). The explanations given by @Keith are, however, very insightful. \\n\\nFor more details on the calculation of CIs in epidemiology, I would suggest to look at Rothman and Greenland's textbook, [Modern Epidemiology][4] (now in it's 3rd edition), [Statistical Methods for Rates and Proportions][5], from Fleiss et al., or [Statistical analyses of the relative risk][6], from J.J. Gart (1979). \\n\\nYou will generally get similar results with `fisher.test()`, as pointed by @gd047, although in this case this function will provide you with a 95% CI for the odds-ratio (which in the case of a disease with low prevalence will be very close to the RR).\\n\\n**Notes:** \\n\\n1. I don't check your Excel file, for the reason advocated by @csgillespie.\\n2. Michael E Dewey provides an interesting summary of [confidence intervals for risk ratios][9], from a digest of posts on the R mailing-list.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/Epi/index.html\\n  [2]: http://cran.r-project.org/web/packages/epitools/index.html\\n  [3]: http://cran.r-project.org/web/packages/PropCIs\\n  [4]: http://www.lww.com/product/?978-0-7817-5564-1\\n  [5]: http://www.wiley.com/remtitle.cgi?0471526290\\n  [6]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1637917/\\n  [7]: http://statpages.org/ctab2x2.html\\n  [8]: http://www.phsim.man.ac.uk/risk/\\n  [9]: http://www.zen103156.zen.co.uk/rr.pdf",add further references and rephrase some paragraphs,
5170,5,2220,26a09545-1d2f-4860-bb59-1ea15a0e987d,2010-08-31 09:46:54.0,339.0,"Permutation tests are significance tests based on permutation resamples drawn at random from the original data. Permutation resamples are drawn without replacement, in contrast to bootstrap samples, which are drawn with replacement. Here is [an example I did in R][1] of a simple permutation test. (Your comments are welcome)\\n\\nPermutation tests have great advantages. They do not require specific population shapes such as normality. They apply to a variety of statistics, not just to statistics that have a simple distribution under the null hypothesis. They can give very accurate p-values, regardless of the shape and size of the population (if enough permutations are used). \\n\\nI have also read that it is often useful to give a confidence interval along with a test, which is created using bootstrap resampling rather than permutation resampling.\\n\\nCould you explain (or just give the R code) how a confidence interval is constructed (i.e. for the difference between the means of the two samples in the above example) ? \\n\\n**EDIT**\\n\\nAfter some googling I found [this interesting reading][2].\\n\\n\\n  [1]: http://stackoverflow.com/questions/2449226/randomized-experiments-in-r\\n  [2]: http://bcs.whfreeman.com/ips5e/content/cat_080/pdf/moore14.pdf",added 145 characters in body,
5171,2,2234,09b1a83f-3ed5-46f1-9cac-edd84337e3f8,2010-08-31 10:02:07.0,253.0,"I would like as many algorithms that perform the same task as logistic regression.  That is  algorithms/models that can give a prediction to a binary response (Y) with some explanatory variable (X).\\n\\nI would be glad if after you name the algorithm, if you would also show how to implement it in R.  Here is a code that can be updated with other models:\\n\\n    set.seed(55)\\n    n <- 100\\n    x <- c(rnorm(n), 1+rnorm(n))\\n    y <- c(rep(0,n), rep(1,n))\\n    r <- glm(y~x, family=binomial)\\n    plot(y~x)\\n    abline(lm(y~x),col='red',lty=2)\\n    xx <- seq(min(x), max(x), length=100)\\n    yy <- predict(r, data.frame(x=xx), type='response')\\n    lines(xx,yy, col='blue', lwd=5, lty=2)\\n    title(main='Logistic regression with the ""glm"" function')",,
5172,1,2234,09b1a83f-3ed5-46f1-9cac-edd84337e3f8,2010-08-31 10:02:07.0,253.0,Alternatives to logistic regression ? (in R),,
5173,3,2234,09b1a83f-3ed5-46f1-9cac-edd84337e3f8,2010-08-31 10:02:07.0,253.0,<r><regression><logistic><predictive-models>,,
5174,16,2234,09b1a83f-3ed5-46f1-9cac-edd84337e3f8,2010-08-31 10:02:07.0,253.0,,,
5175,2,2235,23f1e6d9-ca2b-4f10-acb1-d6981772dd38,2010-08-31 10:13:21.0,1119.0,"Popular right now are randomForest and gbm (called MART or Gradient Boosting in machine learning literature), rpart for simple trees.  Also popular is bayesglm, which uses MAP with priors for regularization.\\n\\n    install.packages(c(""randomForest"", ""gbm"", ""rpart"", ""arm""))\\n    \\n    library(randomForest)\\n    library(gbm)\\n    library(rpart)\\n    library(arm)\\n    \\n    r1 <- randomForest(y~x)\\n    r2 <- gbm(y~x)\\n    r3 <- rpart(y~x)\\n    r4 <- bayesglm(y ~ x, family=binomial)\\n    \\n    yy1 <- predict(r1, data.frame(x=xx))\\n    yy2 <- predict(r2, data.frame(x=xx))\\n    yy3 <- predict(r3, data.frame(x=xx))\\n    yy4 <- predict(r4, data.frame(x=xx), type=""response"")\\n\\n",,
5176,16,2235,23f1e6d9-ca2b-4f10-acb1-d6981772dd38,2010-08-31 10:13:21.0,-1.0,,,
5177,4,1531,22f9e552-0c8d-4fce-9721-923ff2395c01,2010-08-31 10:56:24.0,88.0,"How to calculate the ""exact confidence interval"" for relative risk?",edited title,
5178,5,2233,fce57869-a983-4110-9d2f-a1a40510ffc4,2010-08-31 11:14:57.0,930.0,"Check out the R [Epi][1] and [epitools][2] packages, which include many functions for computing exact and approximate CIs/p-values for various measures of association found in epidemiological studies, including relative risk (RR). I know there is also [PropCIs][3], but I never tried it. Bootstraping is also an option, but generally these are exact or approximated CIs that are provided in epidemiological papers, although most of the explanatory studies rely on GLM, and thus make use of odds-ratio (OR) instead of RR (although, wrongly it is often the RR that is interpreted because it is easier to understand, but this is another story).\\n\\nYou can also check your results with online calculator, like on [statpages.org][7], or [Relative Risk and Risk Difference Confidence Intervals][8]. The latter explains how computations are done.\\n\\nBy ""exact"" tests, we generally mean tests/CIs not relying on an asymptotic distribution, like the chi-square or standard normal; e.g. in the case of an RR, an 95% CI may be approximated as\\n$\\exp\\left[ \\log(\\text{rr}) - 1.96\\sqrt{\\text{Var}\\big(\\log(\\text{rr})\\big)} \\right], \\exp\\left[ \\log(\\text{rr}) + 1.96\\sqrt{\\text{Var}\\big(\\log(\\text{rr})\\big)} \\right]$, \\nwhere $\\text{Var}\\big(\\log(\\text{rr})\\big)=1/a - 1/(a+b) + 1/c - 1/(c+d)$ (assuming a 2-way cross-classification table, with $a$, $b$, $c$, and $d$ denoting cell frequencies). The explanations given by @Keith are, however, very insightful. \\n\\nFor more details on the calculation of CIs in epidemiology, I would suggest to look at Rothman and Greenland's textbook, [Modern Epidemiology][4] (now in it's 3rd edition), [Statistical Methods for Rates and Proportions][5], from Fleiss et al., or [Statistical analyses of the relative risk][6], from J.J. Gart (1979). \\n\\nYou will generally get similar results with `fisher.test()`, as pointed by @gd047, although in this case this function will provide you with a 95% CI for the odds-ratio (which in the case of a disease with low prevalence will be very close to the RR).\\n\\n**Notes:** \\n\\n1. I didn't check your Excel file, for the reason advocated by @csgillespie.\\n2. Michael E Dewey provides an interesting summary of [confidence intervals for risk ratios][9], from a digest of posts on the R mailing-list.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/Epi/index.html\\n  [2]: http://cran.r-project.org/web/packages/epitools/index.html\\n  [3]: http://cran.r-project.org/web/packages/PropCIs\\n  [4]: http://www.lww.com/product/?978-0-7817-5564-1\\n  [5]: http://www.wiley.com/remtitle.cgi?0471526290\\n  [6]: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1637917/\\n  [7]: http://statpages.org/ctab2x2.html\\n  [8]: http://www.phsim.man.ac.uk/risk/\\n  [9]: http://www.zen103156.zen.co.uk/rr.pdf",fix typo,
5179,2,2236,783a106c-2c57-40bb-a2b3-32e8981b528f,2010-08-31 13:02:43.0,5.0,"I agree with Joe, and would add:\\n\\nAny classification method could in principle be used, although it will depend on the data/situation.  For instance, you could also use a SVM, possibly with the popular C-SVM model.  Here's an example from kernlab using a radial basis kernel function:\\n\\n    library(kernlab)\\n    x <- rbind(matrix(rnorm(120),,2),matrix(rnorm(120,mean=3),,2))\\n    y <- matrix(c(rep(1,60),rep(-1,60)))\\n\\n    svp <- ksvm(x,y,type=""C-svc"")\\n    plot(svp,data=x)\\n\\n",,
5180,16,2236,783a106c-2c57-40bb-a2b3-32e8981b528f,2010-08-31 13:02:43.0,-1.0,,,
5181,16,2224,00000000-0000-0000-0000-000000000000,2010-08-31 13:03:46.0,5.0,,,
5182,16,2223,00000000-0000-0000-0000-000000000000,2010-08-31 13:03:46.0,5.0,,,
5183,6,2234,765335e5-da13-4fc8-8684-b8b2fe010328,2010-08-31 13:06:01.0,5.0,<r><regression><logistic><classification><predictive-models>,edited tags,
5184,6,2219,79c8e5d9-cd10-44ac-9031-8df97da2939f,2010-08-31 13:43:39.0,8.0,<probability><games>,edited tags,
5185,2,2237,df7f77a2-df68-4156-889d-200cfb2c5201,2010-08-31 14:26:41.0,443.0,"My question is based on the ""*forecast*""  package for R used in *Forecasting with Exponential Smoothing. The State Space Approach* - Hyndman et al. 2008. I am using the ""*ets*"" function to estimate the parameters of a model. Is there a way to obtain standard errors for the estimates of the smoothing parameters? \\n\\nThanks!\\n\\nLavinia",,
5186,1,2237,df7f77a2-df68-4156-889d-200cfb2c5201,2010-08-31 14:26:41.0,443.0,Standard errors for estimates of smoothing parameters,,
5187,3,2237,df7f77a2-df68-4156-889d-200cfb2c5201,2010-08-31 14:26:41.0,443.0,<forecasting>,,
5188,2,2238,3e525b05-342a-431b-a7cb-ca994c9b6b68,2010-08-31 14:30:58.0,334.0,"Not all methods lead to analytic expressions (preferably based on proper asymptotic results) that provides this.\\n\\nBut the bootstrap allows you to approximate this via simulation. In essence, you generate (lots of) surrogate 'fake' data sets, employ your estimator on each of these and then use the population of your estimates to make inferences.  However, doing bootstrapping in a time series context has its own challenges...",,
5189,5,2106,fa8bd6a4-5941-45ca-8bce-95b2b62f7969,2010-08-31 15:25:02.0,966.0,"Try:\\n\\n    library(ez)\\n    ezANOVA(data=subset(p12bl, exps==1),\\n      within=.(sucrose, citral),\\n      wid=.(subject),\\n      dv=.(resp)\\n      )\\n\\nand the equivalent aov command, minus sphericity etc, is:\\n\\n    aov(resp ~ sucrose*citral + Error(subject/(sucrose*citral)), \\n        data= subset(p12bl, exps==1))\\n\\nHere's the equivalent using Anova from car directly:\\n\\n    library(car)\\n    df1<-read.table(""clipboard"", header=T) #From copying data in the question above\\n    sucrose<-factor(rep(c(1:4), each=4))\\n    citral<-factor(rep(c(1:4), 4))\\n    idata<-data.frame(sucrose,citral)\\n\\n    mod<-lm(cbind(S1C1, S1C2, S1C3, S1C4, S2C1, S2C2, S2C3, S2C4, \\n            S3C1, S3C2, S3C3, S3C4, S4C1, S4C2, S4C3, S4C4)~1, data=df1)\\n    av.mod<-Anova(mod, idata=idata, idesign=~sucrose*citral)\\n    summary(av.mod)",updated ez package changed sid to wid in ezANOVA code,
5190,2,2239,9cbaaea6-137b-4e1d-b82e-6be272df1fa2,2010-08-31 15:30:46.0,1124.0,"Actually, that depends on what you want to obtain. If you perform logistic regression only for the predictions, you can use any supervised classification method suited for your data. Another possibility : discriminant analysis ( lda() and qda() from package MASS)\\n\\n    r <- lda(y~x) # use qda() for quadratic discriminant analysis\\n    \\n    xx <- seq(min(x), max(x), length=100)\\n    pred <- predict(r, data.frame(x=xx), type='response')\\n    yy <- pred$posterior[,2]\\n    \\n    color <- c(""red"",""blue"")\\n    \\n    plot(y~x,pch=19,col=color[pred$class])\\n    abline(lm(y~x),col='red',lty=2)\\n    lines(xx,yy, col='blue', lwd=5, lty=2)\\n    title(main='lda implementation')\\n\\nOn the other hand, if you need confidence intervals around your predictions or standard errors on your estimates, most classification algorithms ain't going to help you. You could use generalized additive (mixed) models, for which a number of packages are available. I often use the mgcv package of Simon Wood. Generalized additive models allow more flexibility than logistic regression, as you can use splines for modelling your predictors.\\n\\n    set.seed(55)\\n    require(mgcv)\\n    n <- 100\\n    x1 <- c(rnorm(n), 1+rnorm(n))\\n    x2 <- sqrt(c(rnorm(n,4),rnorm(n,6)))\\n    y <- c(rep(0,n), rep(1,n))\\n    r <- gam(y~s(x1)+s(x2),family=binomial)\\n    \\n    xx <- seq(min(x1), max(x1), length=100)\\n    xxx <- seq(min(x2), max(x2), length=100)\\n    yy <- predict(r, data.frame(x1=xx,x2=xxx), type='response')\\n    \\n    color=c(""red"",""blue"")\\n    clustering <- ifelse(r$fitted.values < 0.5,1,2)\\n    \\n    plot(y~x1,pch=19,col=color[clustering])\\n    abline(lm(y~x1),col='red',lty=2)\\n    lines(xx,yy, col='blue', lwd=5, lty=2)\\n    title(main='gam implementation')\\n\\nThere's a whole lot more to do :\\n\\n    op <- par(mfrow=c(2,1))\\n    plot(r,all.terms=T)\\n    par(op)\\n\\n    summary(r)\\n    anova(r)\\n\\n    r2 <- gam(y~s(x1),family=binomial)\\n    anova(r,r2,test=""Chisq"")\\n\\n...\\n\\nI'd recommend the [book of Simon Wood about Generalized Additive Models][1]\\n\\n\\n  [1]: http://www.amazon.ca/Generalized-Additive-Models-Introduction-R/dp/1584884746",,
5191,16,2239,9cbaaea6-137b-4e1d-b82e-6be272df1fa2,2010-08-31 15:30:46.0,-1.0,,,
5192,2,2240,7c9da6d0-2e90-47ed-8010-411c5d2bf4ff,2010-08-31 15:55:59.0,1124.0,"As a permutation test is an **exact** test, giving you an exact p-value. Bootstrapping a permutation test doesn't make sense. \\n\\nNext to that, determining a confidence interval around a test statistic doesn't make sense either, as it is calculated based on your sample and not an estimate. You determine confidence intervals around estimates like means and the likes, but not around test statistics.\\n\\nPermutation tests should not be used on datasets that are so big you can't calculate all possible permutations any more. If that's the case, use a bootstrap procedure to determine the cut-off for the test statistic you use. But again, this has little to do with a 95% confidence interval. \\n\\nAn example : I use here the classic T-statistic, but use a simple approach to bootstrapping for calculation of the empirical distribution of my statistic. Based on that, I calculate an empirical p-value :\\n\\n    x <- c(11.4,25.3,29.9,16.5,21.1)\\n    y <- c(23.7,26.6,28.5,14.2,17.9,24.3)\\n    \\n    t.sample <- t.test(x,y)$statistic\\n    t.dist <- apply(\\n          replicate(1000,sample(c(x,y),11,replace=F)),2,\\n          function(x){t.test(x[1:5],x[6:11])$statistic})\\n    \\n    # two sided testing\\n    center <- mean(t.dist)\\n    t.sample <-abs(t.sample-center)\\n    t.dist <- abs(t.dist - center)\\n    p.value <- sum( t.sample < t.dist ) / length(t.dist)\\n    p.value",,
5193,2,2241,1b1e6e50-ae0d-45d1-9604-b05e87611db8,2010-08-31 15:58:42.0,1080.0,"I found this [from a NoSQL presentation][1], but the cartoon can be found directly at\\n\\n[http://browsertoolkit.com/fault-tolerance.png][2]\\n\\n\\n![alt text][3]\\n\\n\\n  [1]: http://www.erlang-factory.com/upload/presentations/282/neo4j-is-not-erlang-but-i-still-heart-you-2010-06-10.pdf\\n  [2]: http://browsertoolkit.com/fault-tolerance.png\\n  [3]: http://i.stack.imgur.com/MV8HX.png",,
5194,16,2241,1b1e6e50-ae0d-45d1-9604-b05e87611db8,2010-08-31 15:58:42.0,-1.0,,,
5195,5,2240,43d9bb57-200e-42c1-aad7-4d8e99fd3d34,2010-08-31 16:01:19.0,1124.0,"As a permutation test is an **exact** test, giving you an exact p-value. Bootstrapping a permutation test doesn't make sense. \\n\\nNext to that, determining a confidence interval around a test statistic doesn't make sense either, as it is calculated based on your sample and not an estimate. You determine confidence intervals around estimates like means and the likes, but not around test statistics.\\n\\nPermutation tests should not be used on datasets that are so big you can't calculate all possible permutations any more. If that's the case, use a bootstrap procedure to determine the cut-off for the test statistic you use. But again, this has little to do with a 95% confidence interval. \\n\\nAn example : I use here the classic T-statistic, but use a simple approach to bootstrapping for calculation of the empirical distribution of my statistic. Based on that, I calculate an empirical p-value :\\n\\n    x <- c(11.4,25.3,29.9,16.5,21.1)\\n    y <- c(23.7,26.6,28.5,14.2,17.9,24.3)\\n    \\n    t.sample <- t.test(x,y)$statistic\\n    t.dist <- apply(\\n          replicate(1000,sample(c(x,y),11,replace=F)),2,\\n          function(x){t.test(x[1:5],x[6:11])$statistic})\\n    \\n    # two sided testing\\n    center <- mean(t.dist)\\n    t.sample <-abs(t.sample-center)\\n    t.dist <- abs(t.dist - center)\\n    p.value <- sum( t.sample < t.dist ) / length(t.dist)\\n    p.value\\n\\nTake into account that this 2-sided testing only works for symmetrical distributions. Non-symmetrical distributions are typically only tested one-sided.",added 156 characters in body,
5196,2,2242,00cb9b37-52ff-4567-ad7c-1d8d45be6ea9,2010-08-31 16:32:21.0,1124.0,"More mathematically : Assuming the scores values are independent (which they aren't, but with the right setting it should be close), the chance that A has as low as 34.5 and B as high as 35.5 equals p(A<=34.5)*p(B>=35.5) = 0.0025.\\n\\n ",,
5197,2,2243,d6a7c2fb-c8f2-435b-bf2d-23832a0cff2b,2010-08-31 17:41:55.0,795.0,"![Bush and Gorbachev in a statistical golf cart][1] My favorite was created by Emanuel Parzen, appearing in [IMA preprint 663][2], but this illustrates my degenerate sense of humor. \\n\\nGorbachev says to Bush: ""that's a very nice golfcart, Mr. President. Can it change how statistics is practiced?"" etc. hahahah.\\n\\n\\n  [1]: http://i.stack.imgur.com/tuNFK.jpg\\n  [2]: http://www.ima.umn.edu/preprints/July90Series/663.pdf",,
5198,16,2243,d6a7c2fb-c8f2-435b-bf2d-23832a0cff2b,2010-08-31 17:41:55.0,-1.0,,,
5199,2,2244,6e45d3a2-cd3b-4d4b-a7d4-cb2dceb0b54e,2010-08-31 18:56:09.0,1088.0,What is the best package to to do some survival analysis and plots in R? I have tried some tutorials but I couldn't find a definite answer.\\n\\nTIA,,
5200,1,2244,6e45d3a2-cd3b-4d4b-a7d4-cb2dceb0b54e,2010-08-31 18:56:09.0,1088.0,Kaplan-meier and survival analysis and plot in R,,
5201,3,2244,6e45d3a2-cd3b-4d4b-a7d4-cb2dceb0b54e,2010-08-31 18:56:09.0,1088.0,<r><data-visualization><survival-analysis>,,
5202,2,2245,779c0508-b335-4dbb-ab5c-4fa21f2d11a3,2010-08-31 19:13:04.0,5.0,"In his 1984 paper [""Statistics and Causal Inference""][1], Paul Holland raised one of the most fundamental questions in statistics:\\n\\n> What can a statistical model say about\\n> causation?\\n\\nThis led to his motto:\\n\\n> NO CAUSATION WITHOUT MANIPULATION\\n\\nwhich emphasized the importance of restrictions around experiments that consider causation. \\nAndrew Gelman makes [a similar point][2]:\\n\\n> ""To find out what happens when you change something, it is necessary to change it.""...There are things you learn from perturbing a system that you'll never find out from any amount of passive observation.\\n\\nHis ideas are summarized in [this article][3].\\n\\nWhat considerations should be made when making a causal inference from a statistical model?\\n\\n  [1]: http://www-unix.oit.umass.edu/~stanek/pdffiles/causal-holland.pdf\\n  [2]: http://www.stat.columbia.edu/~cook/movabletype/archives/2010/08/no_understandin.html\\n  [3]: http://www.stat.columbia.edu/~gelman/research/published/causalreview4.pdf",,
5203,1,2245,779c0508-b335-4dbb-ab5c-4fa21f2d11a3,2010-08-31 19:13:04.0,5.0,Statistics and causal inference?,,
5204,3,2245,779c0508-b335-4dbb-ab5c-4fa21f2d11a3,2010-08-31 19:13:04.0,5.0,<statistical-analysis><causal-inference>,,
5205,2,2246,23deeb43-70d5-403c-a803-5db445cd0121,2010-08-31 19:41:55.0,88.0,Try CRAN Task View: http://cran.at.r-project.org/web/views/Survival.html,,
5206,2,2247,3bf02e33-1e0c-4f10-aac2-e5019913c6cf,2010-08-31 19:48:40.0,5.0,"I think that it's fair to say that the [survival][1] package is the ""recommended"" package in general, as it's included in base R (i.e. does not need to be installed separately).  There are many good tutorials online for this.  But you need to be more specific to get a more specific answer.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/survival/",,
5207,4,2244,6af2e477-a7ed-4c6b-9c6b-abb6aa9689e7,2010-08-31 19:50:47.0,1088.0,"Kaplan-meier, survival analysis and plot in R",edited title,
5208,2,2248,9d530231-a9d3-48ae-911d-3eda38834dcf,2010-08-31 20:57:50.0,,"I have a series of observations that fall into bins (or ""scores""); that is, the data can be 0, 1, 2, 3 or 4.  There are two groups of such data, control and treated.  I know the number of individuals with each score for each group.  What is the best way to determine whether these groups are different or not?  A colleague suggested just arranging the data as individual data points with the given score, and doing the analysis on those two columns of data.  Since there are ten individuals per group, this is not difficult, but I do not believe that I am getting a valid answer.  Thanks for your help.",,John
5209,1,2248,9d530231-a9d3-48ae-911d-3eda38834dcf,2010-08-31 20:57:50.0,,What is the proper way to analyze discrete data?,,John
5210,3,2248,9d530231-a9d3-48ae-911d-3eda38834dcf,2010-08-31 20:57:50.0,,<nonparametric><statistical-significance><discrete-data>,,John
5211,2,2249,1392e250-b1ed-4ff8-b494-945d7a3937d2,2010-08-31 21:33:24.0,561.0,"Three things come to mind:\\n\\n 1. Contingency table analysis using Fisher's exact test or Chi Square (but will only tell you that somewhere in the table there is a difference that is significant. You'd have to visualize your date or do post-hoc tests to know where this difference is.) Not my preferred solution.\\n 2. A non-parametric method such the Mann Whitney test. This will rank all of your scores within each group. A good method, but may be underpowered.\\n 3. A parametric method (such as a t test). Disadvantage is that the assumptions of this method may be violated, especially with such a small sample. Also, the difference between 0 and 1 is not likely to be the same (depending on what you're measuring) as the difference between 3 and 4. The good news is that the t test is relatively robust to the assumptions you are supposed to ensure are true before using the test. However, as I said, the sample size is fairly small.\\n\\nThe best bet may be the Mann Whitney test.\\n\\n",,
5212,2,2250,1f288156-a333-4534-bd8a-f23810b2f9b8,2010-08-31 21:49:30.0,930.0,"What you are looking for seems to be a test for comparing two groups where observations are kind of ordinal data. In this case, I would suggest to apply a trend test to see if there are any differences between the CTL and TRT group, or a proportional odds-ratio test (check the `polr()` function in the `MASS` R package). \\n\\nUsing a t-test would not acknowledge the fact your data are discrete, and the Gaussian assumption may be seriously violated if scores distribution isn't symmetric as if often the case with Likert scores (such as the ones you seem to report). Don't know if these data come from a case-control study or not, but you might also apply rank-based method as suggested by @propfol: If it is not a matched design, the Wilcoxon-Mann-Whitney test (`wilcox.test()` in R) is fine, and ask for an exact p-value although you may encounter problem with tied observations.\\n\\nCheck also those related questions:\\n\\n* [Group differences on a five point Likert item][1]\\n* [Under what conditions should Likert scales be used as ordinal or interval data?][2]\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/203/group-differences-on-a-five-point-likert-item/1888#1888\\n  [2]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data/1887#1887",,
5213,6,2248,0d8f1de5-a59a-490e-a167-84ee70987c14,2010-08-31 21:49:50.0,930.0,<nonparametric><statistical-significance><discrete-data><scales>,edited tags,
5214,5,2250,e2c49265-1a16-460e-8f98-c868d31e247f,2010-08-31 22:12:30.0,930.0,"What you are looking for seems to be a test for comparing two groups where observations are kind of ordinal data. In this case, I would suggest to apply a trend test to see if there are any differences between the CTL and TRT group, or a proportional odds-ratio test (check the `polr()` function in the `MASS` R package). \\n\\nUsing a t-test would not acknowledge the fact your data are discrete, and the Gaussian assumption may be seriously violated if scores distribution isn't symmetric as is often the case with Likert scores (such as the ones you seem to report). Don't know if these data come from a case-control study or not, but you might also apply rank-based method as suggested by @propfol: If it is not a matched design, the Wilcoxon-Mann-Whitney test (`wilcox.test()` in R) is fine, and ask for an exact p-value although you may encounter problem with tied observations.\\n\\nGiven your sample size, you may also consider applying a permutation test (see the [perm][1] or [coin][2] R packages).\\n\\nCheck also those related questions:\\n\\n* [Group differences on a five point Likert item][3]\\n* [Under what conditions should Likert scales be used as ordinal or interval data?][4]\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/perm/index.html\\n  [2]: http://cran.r-project.org/web/packages/coin/index.html\\n  [3]: http://stats.stackexchange.com/questions/203/group-differences-on-a-five-point-likert-item/1888#1888\\n  [4]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data/1887#1887","fix typo, add comment about permutation test",
5215,2,2251,0a952049-d5ac-4bf8-bc97-77ffc4ee8708,2010-08-31 23:26:46.0,1107.0,"\\nThis is a broad question, but given the Box, Hunter and Hunter quote is true I think what it comes down to is\\n\\n1) The quality of the experimental design\\n    - how randomization was carried out, sample sizes, control of confounders, efficiency ...\\n\\n2) The quality of the implementation of the design.\\n    - adherance to protocol, measurement error, data handling, ...\\n\\n3) The quality of the model to accurately reflect the design.\\n    - blocking structures are accurately represented, proper degrees of freedom are associated with effects, estimators are unbiased and efficient, ...\\n\\n\\nAt the risk of stating the obvious I'll try to hit on the key points of each:\\n\\n(1) is a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we ""control what we can and randomize what we can't"", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').  \\n\\n(2) is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)\\n\\n(3) is another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...\\n\\nAnyway, hopefully some of the above helps in thinking about considerations in making causal inference from a model.  Did I forget anything big?",,
5216,6,534,e66352a6-f242-47b5-a03a-d1b0fba5a433,2010-08-31 23:50:10.0,159.0,<correlation><causal-inference>,edited tags,
5217,4,2244,b35e698e-6aa4-4a8e-b20d-e9393cf4c0be,2010-08-31 23:52:01.0,159.0,"Kaplan-Meier, survival analysis and plots in R",edited title,
5218,2,2252,19e49adc-0747-411e-a97b-45c3e98bb2f9,2010-09-01 00:00:38.0,1107.0,"If you enjoy visual explanations, there was a nice 'Teacher's Corner' article on this subject in the American Statistician (cite below).  As a bonus, the authors included an [R package](http://www.biostatisticien.eu/ConvergenceConcepts/) to facilitate learning.\\n \\n\\n@article{lafaye09,\\n  title={Understanding Convergence Concepts: A Visual-Minded and Graphical Simulation-Based Approach},\\n  author={Lafaye de Micheaux, P. and Liquet, B.},\\n  journal={The American Statistician},\\n  volume={63},\\n  number={2},\\n  pages={173--178},\\n  year={2009},\\n  publisher={ASA}\\n}\\n",,
5219,5,2251,434fb502-b839-4e7f-8b97-7057f4cd1d42,2010-09-01 00:08:44.0,1107.0,"This is a broad question, but given the Box, Hunter and Hunter quote is true I think what it comes down to is\\n\\n1) The quality of the experimental design\\n    -  randomization, sample sizes, control of confounders,...\\n\\n2) The quality of the implementation of the design.\\n    - adherance to protocol, measurement error, data handling, ...\\n\\n3) The quality of the model to accurately reflect the design.\\n    - blocking structures are accurately represented, proper degrees of freedom are associated with effects, estimators are unbiased, ...\\n\\n\\nAt the risk of stating the obvious I'll try to hit on the key points of each:\\n\\n(1) is a large sub-field of statistics, but in it's most basic form I think it comes down to the fact that when making causal inference we ideally start with identical units that are monitored in identical environments other than being assigned to a treatment.  Any systematic differences between groups after assigment are then logically attributable to the treatment (we can infer cause).  But, the world isn't that nice and units differ prior to treatment and evironments during experiments are not perfectly controlled.  So we ""control what we can and randomize what we can't"", which helps to insure that there won't be systematic bias due to the confounders that we controlled or randomized.  One problem is that experiments tend to be difficult (to impossible) and expensive and a large variety of designs have been developed to efficiently extract as much information as possible in as carefully controlled a setting as possible, given the costs.  Some of these are quite rigorous (e.g. in medicine the double-blind, randomized, placebo-controlled trial) and others less so (e.g. various forms of 'quasi-experiments').  \\n\\n(2) is also a big issue and one that statisticians generally don't think about...though we should.  In applied statistical work I can recall incidences where 'effects' found in the data were spurious results of inconsistency of data collection or handling.  I also wonder how often information on true causal effects of interest is lost due to these issues (I believe students in the applied sciences generally have little-to-no training about ways that data can become corrupted - but I'm getting off topic here...)\\n\\n(3) is another large technical subject, and another necessary step in objective causal inference.  To a certain degree this is taken care of because the design crowd develop designs and models together (since inference from a model is the goal, the attributes of the estimators drive design). But this only gets us so far because in the 'real world' we end up analysing experimental data from non-textbook designs and then we have to think hard about things like the appropriate controls and how they should enter the model and what associated degrees of freedom should be and whether assumptions are met if if not how to adjust of violations and how robust the estimators are to any remaining violations and...\\n\\nAnyway, hopefully some of the above helps in thinking about considerations in making causal inference from a model.  Did I forget anything big?",deleted 47 characters in body,
5220,5,2252,b0e146d7-961e-42ca-8293-0760a0803059,2010-09-01 00:23:54.0,159.0,"If you enjoy visual explanations, there was a nice ['Teacher's Corner' article][1] on this subject in the American Statistician (cite below).  As a bonus, the authors included an [R package](http://www.biostatisticien.eu/ConvergenceConcepts/) to facilitate learning.\\n \\n\\n    @article{lafaye09,\\n      title={Understanding Convergence Concepts: A Visual-Minded and Graphical Simulation-Based Approach},\\n      author={Lafaye de Micheaux, P. and Liquet, B.},\\n      journal={The American Statistician},\\n      volume={63},\\n      number={2},\\n      pages={173--178},\\n      year={2009},\\n      publisher={ASA}\\n    }\\n\\n\\n  [1]: http://dx.doi.org/doi:10.1198/tas.2009.0032",added 99 characters in body,
5221,2,2253,358c6940-1969-4694-9034-8b7d1f2ed7b9,2010-09-01 00:27:21.0,919.0,"This question is a little unusual because the nature of ""different"" is unspecified.  This response is formulated in the spirit of trying to detect as many kinds of differences as possible, not just changes of location (""trend"").\\n\\nOne approach that might have more power than most, while remaining agnostic about the relative magnitudes represented by the five groups (e.g., adopting a multinomial model) yet retaining the ordering of the groups, is a Kolmogorov-Smirnov test: as a test statistic use the size of the largest deviation between the two empirical cdfs.  This is easy and quick to compute and it would also be easy to bootstrap a p-value by pooling the two sets of results.\\n\\nSpecifically, let the count in bin $j$ for group $i$ be $k_{ij}$.  Then the empirical cdf for group $i$ is essentially the vector $\\left( 0 = m_{i0}, m_{i1}, \\ldots, m_{i5}=n_i \\right) / n_i$ where $m_{i,j} = m_{i-1,j} + k_{ij}, 1 \\le i \\le 5$.  The test statistic is the sup norm of the difference of these two vectors.\\n\\nCritical values ($\\alpha = 0.05$) with two groups of ten individuals are going to be around 0.2 - 0.4, with the higher values occurring when the 20 values are spread evenly between the two extremes.",,
5222,5,2232,005d65f7-4c67-496f-8474-88c02f39d61e,2010-09-01 01:05:20.0,352.0,"From my point of view the difference is important, but largely for philosophical reasons.  Assume you have some device, that improves with time.  So, every time you use the device the probability of it failing is less than before.  \\n\\nConvergence in probability says that the chance of failure goes to zero as the number of usages goes to infinity.  So, after using the device a large number of times, you can be very confident of it working correctly, it still might fail, it's just very unlikely.\\n\\nConvergence almost surely is a bit stronger.  It says that the total number of failures is *finite*. That is, if you count the number of failures as the number of usages goes to infinity, you will get a finite number.  The impact of this is as follows:  As you use the device more and more, you will, after some finite number of usages, exhaust all failures.  From then on the device will work *perfectly*. \\n\\nAs Srikant points out, you don't actually know when you have exhausted all failures, so from a purely practical point of view, there is not much difference between the two modes of convergence.\\n\\nHowever, personally I am very glad that, for example, the strong law of large numbers exists, as opposed to just the weak law.  Because now, a scientific experiment to obtain, say, the speed of light, is justified in taking averages.  At least in theory, after obtaining enough data, you can get arbitrarily close to the true speed of light.  There wont be any failures (however improbable) in the averaging process.\\n\\nLet me clarify what I mean by ''failures (however improbable) in the averaging process''.  Choose some $\\delta > 0$ arbitrarily small. You obtain $n$ estimates $X_1,X_2,\\dots,X_n$ of the speed of light (or some other quantity) that has some `true' value, say $\\mu$. You compute the average \\n$$S_n = \\frac{1}{n}\\sum_{k=1}^N X_k.$$\\nAs we obtain more data ($n$ increases) we can compute $S_n$ for each $n = 1,2,\\dots$.  The weak law says (under some assumptions about the $X_n$) that the probability\\n$$P(|S_n - \\mu| > \\delta) \\rightarrow 0$$\\nas $n$ goes to $\\infty$. The strong law says that the number of times that $|S_n - \\mu|$ is larger than $\\delta$ is finite (with probability 1).  That is, if we define the indicator function $I(|S_n - \\mu| > \\delta)$ that returns one when $|S_n - \\mu| > \\delta$ and zero otherwise, then \\n$$\\sum_{n=1}^{\\infty}I(|S_n - \\mu| > \\delta)$$\\nconverges. This gives you considerable confidence in the value of $S_n$, because it guarantees (i.e. with probability 1) the existence of some finite $n_0$ such that $|S_n - \\mu| < \\delta$ for all $n > n_0$ (i.e. the average never *fails* for $n > n_0$).  Notice that the weak law gives no such guarantee.\\n   ",added 1187 characters in body,
5223,5,2249,2e8dd776-f9e7-4ae4-9eea-1c00969c5ffe,2010-09-01 01:57:27.0,561.0,"Three things come to mind:\\n\\n 1. Contingency table analysis using Fisher's exact test or Chi Square (but will only tell you that somewhere in the table there is a difference that is significant. You'd have to visualize your data or do post-hoc tests to know where this difference is.) Not my preferred solution.\\n 2. A non-parametric method such the Mann Whitney test. This will rank all of your scores within each group. A good method, but may be underpowered.\\n 3. A parametric method (such as a t test). Disadvantage is that the assumptions of this method may be violated, especially with such a small sample. Also, the difference between 0 and 1 is not likely to be the same (depending on what you're measuring) as the difference between 3 and 4. The good news is that the t test is relatively robust to the assumptions you are supposed to ensure are true before using the test. However, as I said, the sample size is fairly small.\\n\\nThe best bet may be the Mann Whitney test.\\n\\n",edited body,
5224,2,2254,67e24368-38e9-4c86-b943-29712e2a26b4,2010-08-30 10:31:13.0,88.0,Arrrr!,,
5226,2,2255,4a453c47-e584-4565-ae1a-2a08b61680f1,2010-09-01 03:35:33.0,1122.0,"This is probably a stupid answer (I am new here), but if you want to estimate the hazard function from observations of an initial population that slowly died away (i.e. had events and then were censored), isn't that what the Nelson-Aalen estimator was built to do?\\n\\nWe could have another conversation about the reliability of the available classical confidence intervals -- my understanding is that there basically do not exist functioning exact confidence intervals that guarantee their coverage even over small sample sizes, since such an interval would need to work over all distributions of censoring time. (Maybe the problem is simpler when individuals are always censored exactly after their first event.) And mapping out the coverage of an approximate interval precisely would take work.\\n\\nBut if you just need a point estimate, the Nelson-Aalen estimator seems to do the trick. (It's a lot like the Kaplan-Meier estimate for the survival function...)\\n\\nIf you want to calculate an a posteriori distribution on a whole family of possible hazard functions, and your prior is that they are drawn from the Gaussian processes with certain statistics, can you explain further what the difficulty is? If there isn't agreement on the covariance matrix, then that needs to be part of the prior -- that the covariance matrix is drawn from some distribution. You're not going to get around having to state a prior if the goal is a posterior.",,
5227,2,2256,4fea9640-eb6c-4073-87ff-a02731cd79e0,2010-09-01 05:55:30.0,18462.0,"I have a dataset forwhich i have performed an mds and visualized the results using scatterplot3d library. However i would like to see the names of the points on the 3d plot. How do i accomplish that? Each column belongs to a certain group i would like to see which points belong to which groups on the 3dplot.\\n\\n    #generate a distance matrix of the data\\n    d <- dist(data)\\n    \\n    #perform the MDS on  3 dimensions and include a Goodness-of-fit (GOF)\\n    \\n    fit.mds <- cmdscale(d,eig=TRUE, k=3) # k is the number of dimensions; 3 in this case\\n    \\n    #Assign names x,y,z to the result vectors (dimension numbers)\\n    x <- fit.mds$points[,1]\\n    y <- fit.mds$points[,2]\\n    z <- fit.mds$points[,3]\\n    \\n    plot3d <- scatterplot3d(x,y,z,highlight.3d=TRUE,xlab="""",ylab="""",pch=16,main=""Multidimensional Scaling 3-D Plot"",col.axis=""blue"")",,
5228,1,2256,4fea9640-eb6c-4073-87ff-a02731cd79e0,2010-09-01 05:55:30.0,18462.0,adding labels to points using mds  and scatter3d package with R ,,
5229,3,2256,4fea9640-eb6c-4073-87ff-a02731cd79e0,2010-09-01 05:55:30.0,18462.0,<r><multidimensional-scaling>,,
5230,2,2257,fa7697c0-7d25-4156-a594-9fd17858e2bb,2010-09-01 06:11:00.0,930.0,"Basically, what you need is to store your `scatterplot3d` in a variable and reuse it like this:\\n\\n    x <- replicate(10,rnorm(100))\\n    x.mds <- cmdscale(dist(x), eig=TRUE, k=3)\\n    s3d <- scatterplot3d(x.mds$points[,1:3])\\n    text(s3d$xyz.convert(0,0,0), labels=""Origin"")\\n\\nReplace the coordinates and text by whatever you want to draw. You can also use a color vector to highlight the groups of interest.\\n\\nThe `R.basic` package, from H[enrik Bengtsson][1], seems to provide additional facilities to customize 3D plots, but I never tried it.\\n\\n\\n  [1]: http://www.braju.com/R/",,
5231,2,2258,a4e6cb6a-b609-4c27-8159-9278bb50bce5,2010-09-01 07:09:34.0,862.0,"Given a function mapping between two sample spaces S1 and S2, if S2,F2 is measurable how do I show that preimage of S2,F2 in S1 is measurable set?\\n\\nP.S. How do I insert latex here? I have tried everything from using latex, tex and $ enclosing but nothing works.\\n\\nTHanks",,
5232,1,2258,a4e6cb6a-b609-4c27-8159-9278bb50bce5,2010-09-01 07:09:34.0,862.0,How to prove that preimage of measurable space is measurable?,,
5233,3,2258,a4e6cb6a-b609-4c27-8159-9278bb50bce5,2010-09-01 07:09:34.0,862.0,<probability>,,
5234,2,2259,1bf4ca8f-99de-4524-8ad9-32b80eeae108,2010-09-01 07:23:52.0,183.0,"Imagine that \\n\\n- responses were collected on a 20 item scale which was designed to measure 4 factors with 5 items on each scale. \\n- participants were drawn from two groups (Group 1) and (Group 2) with sample size $n_1 = 150$ and $n_2 = 150$.\\n- a researcher wanted to assess the factor structure of the scale\\n\\nCommon scenarios that I see in my consulting:\\n\\n- Group 1 are first year psychology students and Group 2 is sampled from the general community\\n- Group 1 are sampled at one period of time and Group 2 is sampled several years later\\n- Group 1 is a normal population and Group 2 is a clinical population\\n\\n\\n**Question**\\n\\n1. Under what circumstances would it be appropriate to collapse across groups?\\n2. How would these circumstances be assessed?\\n\\n\\n**My initial Thoughts:**\\n\\nMy own initial thoughts were as follows:\\n\\n1. **Theoretical assessment**: assess the degree to which the two groups were sampled or measured in ways that would alter the means, sds, or correlation between the items\\n2. **Empirical assessment:** Examine differences between means, sds, and intercorrelations on the scales and optionally on other relevant variables (e.g., demographics); perform a [two-group confirmatory factor analysis][1] to assess the consistency of the factor structure across groups.\\n\\nEssentially, if the empirical evidence suggests that the groups are similar and the theoretical assessment suggests that they are similar, then it should be reasonable to combine.\\n\\n**Conclusion**\\n\\n- Does the approach above seem reasonable?\\n- Do you have alternative strategies?\\n- Are there any references that provide recommendations or examples regarding best practice in this situation?\\n\\n\\n  [1]: http://www.biostat.umn.edu/~melanie/PH7435/DATA/MultipleGroupCFAinAMOS.pdf",,
5235,1,2259,1bf4ca8f-99de-4524-8ad9-32b80eeae108,2010-09-01 07:23:52.0,183.0,When is it acceptable to collapse across groups when performing a factor analysis?,,
5236,3,2259,1bf4ca8f-99de-4524-8ad9-32b80eeae108,2010-09-01 07:23:52.0,183.0,<factor-analysis><sampling>,,
5237,2,2260,a9c768f4-03cb-40b0-90b4-fbf2dd7ff916,2010-09-01 07:55:30.0,196.0,"It might be a little fly by night, but your theory may suggest whether the two groups have the same factor structure or not.  If your theory suggests they do, and there is no reason to doubt the theory, I'd suggest you could go right ahead and trust that they have the same factor structure.  \\n\\nYour empirical assessment would probably be a good route to go just to spot check the theoretical assessment as whether they are likely to share the same structure.  However, I don't intuitively see why mean differences between items would imply they have a different underlying factor structure.  It seems to me that might just suggest that one group has higher or lower scores on a given factor.",,
5238,5,2242,696c6888-6580-4c24-b0fd-f747968d87d5,2010-09-01 08:19:57.0,1124.0,"More mathematically : Assuming the scores values are independent (which they aren't, but with the right setting it should be close), the chance that A has as low as 34.5 and B as high as 35.5 equals p(A<=34.5)*p(B>=35.5) = 0.000625.\\n\\nEDIT : corrected the calculation. P(A<=34.5) is 0.025 and not 0.05 Same goes for p(B>=35.5)\\n\\n ",added 97 characters in body,
5239,5,2240,7f84ef1d-ffed-4471-ae6a-3748ede0804f,2010-09-01 08:28:51.0,1124.0,"As a permutation test is an **exact** test, giving you an exact p-value. Bootstrapping a permutation test doesn't make sense. \\n\\nNext to that, determining a confidence interval around a test statistic doesn't make sense either, as it is calculated based on your sample and not an estimate. You determine confidence intervals around estimates like means and the likes, but not around test statistics.\\n\\nPermutation tests should not be used on datasets that are so big you can't calculate all possible permutations any more. If that's the case, use a bootstrap procedure to determine the cut-off for the test statistic you use. But again, this has little to do with a 95% confidence interval. \\n\\nAn example : I use here the classic T-statistic, but use a simple approach to bootstrapping for calculation of the empirical distribution of my statistic. Based on that, I calculate an empirical p-value :\\n\\n    x <- c(11.4,25.3,29.9,16.5,21.1)\\n    y <- c(23.7,26.6,28.5,14.2,17.9,24.3)\\n    \\n    t.sample <- t.test(x,y)$statistic\\n    t.dist <- apply(\\n          replicate(1000,sample(c(x,y),11,replace=F)),2,\\n          function(x){t.test(x[1:5],x[6:11])$statistic})\\n    \\n    # two sided testing\\n    center <- mean(t.dist)\\n    t.sample <-abs(t.sample-center)\\n    t.dist <- abs(t.dist - center)\\n    p.value <- sum( t.sample < t.dist ) / length(t.dist)\\n    p.value\\n\\nTake into account that this 2-sided testing only works for symmetrical distributions. Non-symmetrical distributions are typically only tested one-sided.\\n\\nEDIT :\\n\\nOK, I misunderstood the question. If you want to calculate a confidence interval on the estimate of the difference, you can use the code mentioned [here][1] for bootstrapping within each sample. Mind you, this is a biased estimate: generally this gives a CI that is too small.\\n\\n\\n  [1]: http://stackoverflow.com/questions/3615718/bootstrapping-to-compare-two-groups/3615880#3615880",added 397 characters in body,
5240,2,2261,12fdf058-feb0-4fb8-b447-966ec74726c4,2010-09-01 08:44:54.0,1124.0,"The approach you mention seems reasonable, but you'd have to take into account that you cannot see the total dataset as a single population. So theoretically, you should use any kind of method that can take differences between those groups into account, similar to using ""group"" as a random term in an ANOVA or GLM approach.\\n\\nAn alternative for empirical evaluation would be to check formally whether an effect of group can be found on the answers. To do that, you could create a binary dataset with following columns :\\n\\nyes/no  -  item  -  participant  -  group\\n\\nWith this you can use item as a random term, participant nested in group and test the fixed effect of group using e.g. a glm with a logit link. You can just ignore participant too if you lose too many df.\\n\\nThis is an approximation of the truth, but if the effect of group is significant, I wouldn't collapse the dataset.",,
5241,5,2240,21cb5d3d-ef60-4f6f-8fe7-2409d434606b,2010-09-01 08:54:35.0,1124.0,"As a permutation test is an **exact** test, giving you an exact p-value. Bootstrapping a permutation test doesn't make sense. \\n\\nNext to that, determining a confidence interval around a test statistic doesn't make sense either, as it is calculated based on your sample and not an estimate. You determine confidence intervals around estimates like means and the likes, but not around test statistics.\\n\\nPermutation tests should not be used on datasets that are so big you can't calculate all possible permutations any more. If that's the case, use a bootstrap procedure to determine the cut-off for the test statistic you use. But again, this has little to do with a 95% confidence interval. \\n\\nAn example : I use here the classic T-statistic, but use a simple approach to bootstrapping for calculation of the empirical distribution of my statistic. Based on that, I calculate an empirical p-value :\\n\\n    x <- c(11.4,25.3,29.9,16.5,21.1)\\n    y <- c(23.7,26.6,28.5,14.2,17.9,24.3)\\n    \\n    t.sample <- t.test(x,y)$statistic\\n    t.dist <- apply(\\n          replicate(1000,sample(c(x,y),11,replace=F)),2,\\n          function(x){t.test(x[1:5],x[6:11])$statistic})\\n    \\n    # two sided testing\\n    center <- mean(t.dist)\\n    t.sample <-abs(t.sample-center)\\n    t.dist <- abs(t.dist - center)\\n    p.value <- sum( t.sample < t.dist ) / length(t.dist)\\n    p.value\\n\\nTake into account that this 2-sided testing only works for symmetrical distributions. Non-symmetrical distributions are typically only tested one-sided.\\n\\nEDIT :\\n\\nOK, I misunderstood the question. If you want to calculate a confidence interval on the estimate of the difference, you can use the code mentioned [here][1] for bootstrapping within each sample. Mind you, this is a biased estimate: generally this gives a CI that is too small. Also see the example given there as a reason why you have to use a different approach for the confidence interval and the p-value.\\n\\n\\n  [1]: http://stackoverflow.com/questions/3615718/bootstrapping-to-compare-two-groups/3615880#3615880",added 131 characters in body,
5242,2,2262,9958c2d7-e3d9-4062-895c-a74b48e9348d,2010-09-01 09:46:35.0,1133.0,I am currently into a situation that i don't really know how to solve by myself.\\n\\nI need to calculate the AUC of each peak and then compare these areas in relation to each other. The problem is that the peaks are not completely separated and the only information i got is the mean and the SD of each peak.\\n\\nDoes anyone know how to do this? Any hint or guess would already be really cool.\\n\\nThanks.\\n,,
5243,1,2262,9958c2d7-e3d9-4062-895c-a74b48e9348d,2010-09-01 09:46:35.0,1133.0,Area Under Curve (AUC) - given peak mean and standard deviation (SD),,
5244,3,2262,9958c2d7-e3d9-4062-895c-a74b48e9348d,2010-09-01 09:46:35.0,1133.0,<normal-distribution>,,
5245,2,2263,acde6633-bf0f-4180-8125-066c05ec78c7,2010-09-01 10:10:54.0,1124.0,"That really depends on the form and the height of the curve. If you assume the curves are all gaussian and you know the heights, then you can calculate the area under the curve by using the normal density function. In R this would become:\\n\\n    heights <- 1\\n    avg <- 3\\n    sdev <- 2\\n    \\n    AUC <- heights/dnorm(avg,avg,sd) # the density function at the mean\\n\\nAs the value of the density function at the mean is only determined by the sd, this information suffices for calculation of the AUC, given the assumptions are correct. If all heights are the same, the AUC is proportional to the sd only. \\n\\nWithout information about the shape of the curve and the heights, you simply cannot calculate the AUC as far as I know.",,
5246,2,2264,a580eb41-4126-4ad5-b8ca-7d7830e546c0,2010-09-01 10:20:34.0,1134.0,"I've got product ratings for a few thousand products. The number of ratings for each product varies from zero to about fifty. I want to find the expected value of product rating for each product. If there are lots of ratings for the product I'd expect the ev to be the average of the ratings for the product, but if there are only a few I'd expect the ev to be closer to the average of all ratings. How do I calculate the true ev? Please be gentle: I'm no statistician or mathematician. ",,
5247,1,2264,a580eb41-4126-4ad5-b8ca-7d7830e546c0,2010-09-01 10:20:34.0,1134.0,Expected value of small sample,,
5248,3,2264,a580eb41-4126-4ad5-b8ca-7d7830e546c0,2010-09-01 10:20:34.0,1134.0,<statistics>,,
5249,5,2258,e514cb7c-5fd1-4239-a0cd-21f3255a8394,2010-09-01 10:56:13.0,,"Given a function mapping between two sample spaces $S_1$ and $S_2$, if $S_2$,$F_2$ is measurable how do I show that preimage of $S_2$,$F_2$ in $S_1$ is measurable set?\\n\\nP.S. How do I insert latex here? I have tried everything from using latex, tex and $ enclosing but nothing works.\\n\\nTHanks",texified notation,user28
5250,2,2265,f0376ca8-6fb3-4efd-8ccd-61166fc39ad4,2010-09-01 11:01:01.0,930.0,"There seems to be two cases to consider, depending on whether your scale was already validated using standard psychometric methods (from classical test or item response theory). In what follows, I will consider the first case where I assume preliminary studies have demonstrated construct validity and scores reliability for your scale.\\n\\nIn this case, there is no formal need to apply exploratory factor analysis, unless you want to examine the pattern matrix within each group (but I generally do it, just to ensure that there are no items that unexpectedly highlight low factor loading or cross-load onto different factors); in order to be able to pool all your data, you need to use a multi-group factor analysis (hence, a confirmatory approach as you suggest), which basically amount to add extra parameters for testing a group effect on factor loading (1st order model) or factor correlation (2nd order model, if this makes sense) which would impact measurement invariance across subgroups of respondents. This can be done using [Mplus][1] (see the discussion about CFA [there][2]) or [Mx][3] (e.g. [Conor et al.][4], 2009), not sure about [Amos][5] as it seems to be restricted to simple factor structure. The Mx software has been redesigned to work within the R environment, [OpenMx][6]. The wiki is well responding so you can ask questions if you encounter difficulties with it. There is also a more recent package, [lavaan][7], which appears to be a promising package for SEMs.\\n\\nAlternatives models coming from IRT may also be considered, including a Latent Regression Rasch Model (for each scale separately, see De Boeck and Wilson, 2004), or a Multivariate Mixture Rasch Model (von Davier and Carstensen, 2007). You can take a look at [Volume 20][8] of the [Journal of Statistical Software][9], entirely devoted to psychometrics in R, for further information about IRT modeling with R.\\nYou may be able to reach similar tests using Structural Equation Modeling, though. \\n\\nIf factor structure proves to be equivalent across the two groups, then you can aggregate the scores (on your four summated scales) and report your statistics as usual. \\nHowever, it is always a challenging task to use CFA since not rejecting H0 does by no mean allow you to check that your postulated theoretical model is correct in the true world, but just that there is no reason to reject it on statistical grounds; on the other hand, rejecting the null would lead to accept the alternative, which is generally left unspecified, unless you apply sequential testing of nested models. Anyway, this is the way we go in cross-cultural settings, especially when we want to assess whether a given questionnaire (e.g., on Patients Reported Outcomes) measures what it purports to do whatever the population it is administered to.\\n\\nNow, regarding the apparent differences between the two groups -- one is drawn from a population of students, the other is a clinical sample, assessed at a later date -- it depends very much on your own considerations: Does mixing of these two samples makes sense from the literature surrounding the questionnaire used (esp., it should have shown temporal stability and applicability in a wide population), do you plan to generalize your findings over a larger population (obviously, you gain power by increasing sample size). At first sight, I would say that you need to ensure that both groups are comparable with respect to the characteristics thought to influence one's score on this questionnaire (e.g., gender, age, SES, biomedical history, etc.), and this can be done using classical statistics for two-groups comparison (on raw scores). It is worth noting that in clinical studies, we face the reverse situation: We usually want to show that scores differ between different clinical subgroups (or between treated and naive patients), which is often refered to as *know-group validity*.\\n\\n\\n**Reference:**\\n\\n1. De Boeck, P. and Wilson, M. (2004). *Explanatory Item Response Models. A Generalized Linear and Nonlinear Approach*. Springer.\\n2. von Davier, M. and Carstensen, C.H. (2007). *Multivariate and Mixture Distribution Rasch Models*. Springer.\\n\\n\\n  [1]: http://www.statmodel.com/\\n  [2]: http://www.statmodel.com/discussion/messages/9/9.html\\n  [3]: http://www.vcu.edu/mx/\\n  [4]: http://www.eric.ed.gov:80/ERICWebPortal/search/detailmini.jsp?_nfpb=true&_&ERICExtSearch_SearchValue_0=EJ857035&ERICExtSearch_SearchType_0=no&accno=EJ857035\\n  [5]: http://www.spss.com/amos/\\n  [6]: http://openmx.psyc.virginia.edu/\\n  [7]: http://lavaan.ugent.be/\\n  [8]: http://www.jstatsoft.org/index.php?vol=20\\n  [9]: http://www.jstatsoft.org/",,
5251,2,2266,adf74bb9-f304-4ef5-bd4b-09011f4f92a2,2010-09-01 11:26:03.0,1124.0,"The true expected value cannot be calculated. You can estimate it using the mean of the ratings for each product, and get an idea about the position by calculating the 95% confidence interval (CI) on the mean.\\n\\nThis is done by\\n\\n$CI = avg \\pm 1.96 * \\frac{SD}{n}$\\n\\nwith n being the number of ratings, SD the standard deviation and avg the average.\\n\\nThere's a chance of 95% this confidence interval contains the true expected value. You assume here that the distribution of the average is normal. If you have a very low amount of ratings, the SD can be estimated wrongly. \\n\\nIn that case, you could estimate an ""overall"" standard deviation on the scoring, and use that to calculate the CI. But keep in mind that this way you assume that the standard deviation is the same for every product. \\n\\nIn extremis, you could resort to bootstrapping to calculate the CI for every product. This will increase the calculation time substantially, and won't be adding any value for products with enough ratings. ",,
5252,2,2267,5202d534-e522-4e6d-8e25-8e9b920a607e,2010-09-01 11:31:31.0,930.0,You can also take a look at [Task views][1] on CRAN and see if something suit your needs. I agree with @Jeromy for these must-have packages (for data manipulation and plotting).\\n\\n\\n  [1]: http://cran.r-project.org/web/views/,,
5253,16,2267,5202d534-e522-4e6d-8e25-8e9b920a607e,2010-09-01 11:31:31.0,-1.0,,,
5254,5,2266,20b7a248-8bdc-42ba-b962-2ca93131390a,2010-09-01 11:44:54.0,1124.0,"The true expected value cannot be calculated. You can estimate it using the mean of the ratings for each product, and get an idea about the position by calculating the 95% confidence interval (CI) on the mean.\\n\\nThis is done by\\n\\n$CI \\approx avg \\pm 2 * \\frac{SD}{n}$\\n\\nwith n being the number of ratings, SD the standard deviation and avg the average. More correct would be to use the T-distribution, where you use the 2.5% and 97.5% quantile of the T-distribution with degrees of freedom equal to number of observations minus one.\\n\\n$CI = avg \\pm T_{(p=0.975,df=n-1)} * \\frac{SD}{n}$\\n\\nFor 10 ratings, $T_{(p=0.975,df=n-1)}$ is 2.26. For 50 ratings, it is 2.01.\\n\\nThere's a chance of 95% this confidence interval contains the true expected value. You assume here that the distribution of the average is normal. If you have a very low amount of ratings, the SD can be estimated wrongly. \\n\\nIn that case, you could estimate an ""overall"" standard deviation on the scoring, and use that to calculate the CI. But keep in mind that this way you assume that the standard deviation is the same for every product. \\n\\nIn extremis, you could resort to bootstrapping to calculate the CI for every product. This will increase the calculation time substantially, and won't be adding any value for products with enough ratings. ",added 215 characters in body; added 101 characters in body,
5255,2,2268,191fd64c-1f57-4149-a853-299c7701f8eb,2010-09-01 11:57:18.0,183.0,"I haven't looked into it much, but this article on [Bayesian rating systems][1] looks interesting.\\n\\n\\n  [1]: http://www.thebroth.com/blog/118/bayesian-rating",,
5256,2,2269,c170886f-0993-495f-ad85-dc955f861c40,2010-09-01 12:07:28.0,1135.0,How can I access tables created in SAS Enterprise Guide Client into SAS Enterprise Miner Client?,,
5257,1,2269,c170886f-0993-495f-ad85-dc955f861c40,2010-09-01 12:07:28.0,1135.0,Access tables created in SAS Enterprise Guide Client into SAS Enterprise Miner Client?,,
5258,3,2269,c170886f-0993-495f-ad85-dc955f861c40,2010-09-01 12:07:28.0,1135.0,<sas>,,
5259,2,2270,76b49e0f-e10c-4019-b1ed-c7ea104b38be,2010-09-01 12:21:52.0,266.0,"I'll add an independent recommendation for Jeromy's blog post, and second the suggestions of James DeCoster's notes and the Borenstein textbook (propofols' no. 2).\\n\\nAt risk of indulging in self-promotion, I recently published a methods paper entitled [Getting Started with Meta-analysis][1].  It's aimed at ecologists and evolutionary biologists, so the examples are taken from these fields, but I hope it will be useful for those working in other areas. \\n\\n\\n  [1]: http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00056.x/abstract",,
5260,16,2270,76b49e0f-e10c-4019-b1ed-c7ea104b38be,2010-09-01 12:21:52.0,-1.0,,,
5261,10,2269,81e54b8d-1cd5-4734-9dd0-e3afea36587d,2010-09-01 12:41:40.0,88.0,"{""Voters"":[{""Id"":88,""DisplayName"":""mbq""}]}",2,
5265,2,2272,a605e966-31f6-4328-b6a6-776704d94e07,2010-09-01 13:53:07.0,71.0,Joris and Srikant's exchange [here][1] got me wondering (again) if my internal explanations for the the difference between confidence intervals and credible intervals were the correct ones.  How you would explain the difference?\\n\\n\\n  [1]: http://stats.stackexchange.com/questions/2182/can-you-explaining-why-statistical-tie-is-not-naively-when-p-1-p-2-2-moe/2242#2242,,
5266,1,2272,a605e966-31f6-4328-b6a6-776704d94e07,2010-09-01 13:53:07.0,71.0,What's the difference between a confidence interval and a credible interval?,,
5267,3,2272,a605e966-31f6-4328-b6a6-776704d94e07,2010-09-01 13:53:07.0,71.0,<bayesian><confidence-interval><frequentist><credible-interval>,,
5269,2,2274,123db9b4-6de9-4b21-85a4-38b45452ff0f,2010-09-01 14:09:09.0,930.0,"Given how your plot looks like, I would suggest rather to fit a mixture of gaussians and get their respective densities. Look at the [mclust][1] package; basically this is refered to model-based clustering (you are seeking groups of points belonging to a given distribution, that is to be estimated, whose location parameter -- but also shape -- varies along a common dimension). A full explanation of MClust is available [here][2].\\n\\nIt seems the [delt][3] package offers an alternative way to fit 1D data with a mixture of gaussians, but I didn't get into details. \\n\\nAnyway, I think this is the best way to get automatic estimates and avoid cutting your x-scale at arbitrary locations.\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/mclust/index.html\\n  [2]: http://www.google.fr/url?sa=t&source=web&cd=5&ved=0CD0QFjAE&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.156.6814%26rep%3Drep1%26type%3Dpdf&rct=j&q=Mclust%201D%20mixture%20of%20gaussian&ei=Cl1-TI_YAovOswb3zIiUCQ&usg=AFQjCNHRwRIYnWEvDbdMqBj6s2LmpCSouw&sig2=gkbsyHKQMi9KiontFQ0E1g\\n  [3]: http://cran.r-project.org/web/packages/delt/index.html",,
5270,2,2275,bcc9638f-2a8c-4890-bffd-66e08c40eed7,2010-09-01 14:29:02.0,1137.0,"I do not study statistics but engineering, but this is a statistics question, and I hope you can lead me to what I need to learn to solve this problem.  \\n\\nI have this situation where I calculate probabilities of 1000's of things happening in like 30 days.  If in 30 days I see what actually happened, how can I test to see how accurately I predicted?  These calculations result in probabilities and in actual values (ft).  What is the method for doing this?\\nThanks,\\nCP",,
5271,1,2275,bcc9638f-2a8c-4890-bffd-66e08c40eed7,2010-09-01 14:29:02.0,1137.0,Newbie question: How can I determine accuracy of past probability calculations?,,
5272,3,2275,bcc9638f-2a8c-4890-bffd-66e08c40eed7,2010-09-01 14:29:02.0,1137.0,<probability>,,
5273,5,2250,df828bd5-148e-4450-a033-4b27bddf4059,2010-09-01 14:32:30.0,930.0,"What you are looking for seems to be a test for comparing two groups where observations are kind of ordinal data. In this case, I would suggest to apply a trend test to see if there are any differences between the CTL and TRT group. \\n\\nUsing a t-test would not acknowledge the fact your data are discrete, and the Gaussian assumption may be seriously violated if scores distribution isn't symmetric as is often the case with Likert scores (such as the ones you seem to report). Don't know if these data come from a case-control study or not, but you might also apply rank-based method as suggested by @propfol: If it is not a matched design, the Wilcoxon-Mann-Whitney test (`wilcox.test()` in R) is fine, and ask for an exact p-value although you may encounter problem with tied observations. The efficiency of the WMW test is $3/\\pi$ with respect to the t-test if normality holds but it may even be better otherwise, I seem to remember.\\n\\nGiven your sample size, you may also consider applying a permutation test (see the [perm][1] or [coin][2] R packages).\\n\\nCheck also those related questions:\\n\\n* [Group differences on a five point Likert item][3]\\n* [Under what conditions should Likert scales be used as ordinal or interval data?][4]\\n\\n\\n  [1]: http://cran.r-project.org/web/packages/perm/index.html\\n  [2]: http://cran.r-project.org/web/packages/coin/index.html\\n  [3]: http://stats.stackexchange.com/questions/203/group-differences-on-a-five-point-likert-item/1888#1888\\n  [4]: http://stats.stackexchange.com/questions/10/under-what-conditions-should-likert-scales-be-used-as-ordinal-or-interval-data/1887#1887","suppress reference to ordinal logistic regression, following comment of Joris Meys; add comment on WMW efficiency",
5277,2,2277,274a87a2-8d83-4b21-8fe1-169d5df74395,2010-09-01 14:52:06.0,919.0,"It is critical to know how the peak heights and sds were calculated.  (I take ""mean"" in the question to be a mistaken way of referring to a height.  Without the heights, the problem is hopeless; it would be like requesting a formula for the area of a rectangle given only its width and location.)\\n\\nOne would expect, as Joris Meys' answer and its commentary suggest, that the area could be estimated as a sum of Gaussians.  Actually, we don't need to assume a Gaussian shape; almost any standard (preferably unimodal, continuous) shape will do, because the area will be proportional to the peak height (a y-scale factor) and the sd (an x-scale factor), whence the total estimated area ought to be a constant times the sum of height*SD and the relative contribution of each peak will equal its height*SD divided by this sum.  But this all assumes the heights and sds were fit to the curve with such an application in mind.\\n\\nI realize there are many problems with such a formula, but let's not get carried away by all the detail in the example graph: the problem as posed says that the ""means"" and SDs are the *only* information available.",,
5279,2,2278,4a0f3b23-f60d-45b2-8ce2-642eb5ee5336,2010-09-01 15:02:00.0,442.0,"And another one from xkcd.  \\n\\nTitle: **Self-Description**\\n\\n![alt text][1]\\n\\nThe mouseover text:  \\n> The contents of any one panel are\\n> dependent on the contents of every\\n> panel including itself. The graph of\\n> panel dependencies is complete and\\n> bidirectional, and each node has a\\n> loop. The mouseover text has two\\n> hundred and forty-two characters.\\n\\n  [1]: http://i.stack.imgur.com/T1Hep.png\\n\\n",,
5280,16,2278,4a0f3b23-f60d-45b2-8ce2-642eb5ee5336,2010-09-01 15:02:00.0,-1.0,,,
5281,2,2279,66a91ef2-4852-46d9-8272-56da978c7668,2010-09-01 15:07:38.0,919.0,"Strictly speaking, this is trivial: the preimage of $(S_2, F_2)$ is all of $S_1$ (by definition), which is measurable (by definition).\\n\\nPerhaps you want to conclude that the preimage of any *measurable subset* of $S_2$ is measurable: that is a nice property of a function.  However, this conclusion is not true in general, either.  For example, let $(S_1, F_1)$ contain a nonmeasurable set and let $(S_2, F_2)$ contain two disjoint measurable atoms.  Map every element of the nonmeasurable set to one of the atoms and map every element of the complement of the nonmeasurable set to the other atom.  The preimage of the first atom is not measurable, whence this map is not measurable.\\n\\n",,
5282,5,2266,f3650792-2c81-40de-b212-835157075c08,2010-09-01 15:39:17.0,1124.0,"The true expected value cannot be calculated. You can estimate it using the mean of the ratings for each product, and get an idea about the position by calculating the 95% confidence interval (CI) on the mean.\\n\\nThis is done by\\n\\n$CI \\approx avg \\pm 2 * \\frac{SD}{\\sqrt{n}}$\\n\\nwith n being the number of ratings, SD the standard deviation and avg the average. More correct would be to use the T-distribution, where you use the 2.5% and 97.5% quantile of the T-distribution with degrees of freedom equal to number of observations minus one.\\n\\n$CI = avg \\pm T_{(p=0.975,df=n-1)} * \\frac{SD}{\\sqrt{n}}$\\n\\nFor 10 ratings, $T_{(p=0.975,df=n-1)}$ is 2.26. For 50 ratings, it is 2.01.\\n\\nThere's a chance of 95% this confidence interval contains the true expected value. You assume here that the distribution of the average is normal. If you have a very low amount of ratings, the SD can be estimated wrongly. \\n\\nIn that case, you could estimate an ""overall"" standard deviation on the scoring, and use that to calculate the CI. But keep in mind that this way you assume that the standard deviation is the same for every product. \\n\\nIn extremis, you could resort to bootstrapping to calculate the CI for every product. This will increase the calculation time substantially, and won't be adding any value for products with enough ratings. ",added 14 characters in body,
5283,2,2280,771741af-d402-4955-a4c5-5cbaf84ebc0d,2010-09-01 16:01:33.0,1107.0,"Incorporating a prior is one way to 'make up' for small samples.  Another is to use a mixed model, with an intercept for the mean structure and a random intercept for each product.  The estimate of the population mean plus the predicted random effect (BLUP) then offers a form of shrinkage, where values for products with less information are shrunk more toward the population mean than those based on more information.  This method is common in, for example, Small Area Estimation in survey sampling.",,
5284,2,2281,955e34cf-4793-4625-a645-938ba3b1cca9,2010-09-01 16:01:43.0,,"My understanding is as follows:\\n\\n**Background**\\n\\nSuppose that you have some data $x$ and you are trying to estimate $\\theta$. You have a data generating process that describes how $x$ is generated conditional on $\\theta$. In other words you know the distribution of $x$ (say, $f(x|\\theta)$.\\n\\n**Inference Problem**\\n\\nYour inference problem is: What values of $\\theta$ are reasonable given the observed data $x$ ?\\n\\n**Confidence Intervals**\\n\\nConfidence intervals are a classical answer to the above problem. In this approach, you assume that there is *true, fixed* value of $\\theta$. Given this assumption, you use the data $x$ to get to an estimate of $\\theta$ (say, $\\hat{\\theta}$). Once you have your estimate you want to assess where the true value is in relation to your estimate.\\n\\nNotice that under this approach the true value is *not* a random variable. It is a fixed but unknown quantity. In contrast, your estimate *is* a random variable as it depends on your data $x$ which was generated from your data generating process. Thus, you realize that you get different estimates each time you repeat your study.\\n\\nThe above understanding leads to the following methodology to assess where the true parameter is in relation to your estimate. Define an interval, $I \\equiv [lb(x), ub(x)]$ with the following property:\\n\\n$P(\\theta \\in I) = 0.95$\\n\\nAn interval constructed like the above is what is called a confidence interval. Since, the true value is unknown but fixed, the true value is either in the interval or outside the interval. The confidence interval then is a statement about the likelihood that the interval we obtain actually has the true parameter value. Thus, the probability statement is about the interval (i.e., the chances that interval which has the true value or not) rather than about the location of the true parameter value.\\n\\nIn this paradigm, it is meaningless to speak about the probability that a true value is less than or greater than some value as the true value is *not* a random variable.\\n\\n**Credible Intervals** \\n\\nIn contrast to the classical approach, in the bayesian approach we assume that the true value is a random variable. Thus, we capture the our uncertainty about the true parameter value by a imposing a prior distribution on the true parameter vector (say $f(\\theta)$).\\n\\nUsing bayes theorem, we construct the posterior distribution for the parameter vector by blending the prior and the data we have (briefly the posterior is $f(\\theta|-) \\propto f(\\theta) f(x|\\theta)$).\\n\\nWe then arrive at a point estimate using the posterior distribution (e.g., use the mean of the posterior distribution). However, since under this paradigm, the true parameter vector is a random variable, we also want to know the extent of uncertainty we have in our point estimate. Thus, we construct an interval such that the following holds:\\n\\n$P(l(\\theta) \\le {\\theta} \\le ub(\\theta)) = 0.95$\\n\\nThe above is a credible interval. \\n\\n**Summary**\\n\\nCredible intervals capture our current uncertainty in the location of the parameter values and thus can be interpreted as probabilistic statement about the parameter. \\n\\nIn contrast, confidence intervals capture the uncertainty about the interval we have obtained (i.e., whether it contains the true value or not). Thus, they cannot be interpreted as a probabilistic statement about the true parameter values.\\n\\n",,user28
5285,2,2282,9ee7f853-6e3a-40de-a26f-078772872392,2010-09-01 16:04:03.0,253.0,"Following [this question][1], I wish to have some way of counting how many times I am using a package in my daily work.\\n\\nIs there a function/package to do that?\\n\\n\\nIn case there isn't, how would you construct such a capability?\\n\\n\\nThe way I would do that is by changing it so that at the end of any R session, a log file of the commands would be saved to some location.  On that file, I would grep out all instances of ""library"" and ""require"".  Then save the results into a file (with some time stamp).\\n\\nLastly, I might want some function to (once in X time) send this file to a remote FTP location - so that other R users could analyse the results.  (is there a way to do that with R ?!)\\n\\nIf someone wants to try and construct such a machanism - I'd be glad to help by providing a relevant FTP account and by spreading the word on it on [""R bloggers""][2] for the good of the community.\\n\\np.s (mainly for Shane): I wasn't sure if this question should go on stackoverflow or here.  If this type of question wasn't debated yet on the meta.stat - it should be.  If it was, I'll be glad to know what the conclusion of that discussion was. \\n\\n  [1]: http://stats.stackexchange.com/questions/1676/i-just-installed-the-latest-version-of-r-what-packages-should-i-obtain\\n  [2]: http://R-bloggers.com",,
5286,1,2282,9ee7f853-6e3a-40de-a26f-078772872392,2010-09-01 16:04:03.0,253.0,Counting how many times a package has been loaded in R ?,,
5287,3,2282,9ee7f853-6e3a-40de-a26f-078772872392,2010-09-01 16:04:03.0,253.0,<r><recommendations>,,
5288,2,2283,b4893a4a-d480-42ae-b286-4d2c091581a6,2010-09-01 16:13:47.0,334.0,"Overload `library()` and `require()` so that they report what they do (whichever way: append to a text file, say) and have those replacement functions loaded first at startup.",,
5289,6,2264,d40d85d9-d5a5-4deb-98b6-136431647270,2010-09-01 16:41:08.0,88.0,<statistics><expected-value>,edited tags,
5290,2,2284,45726977-eca2-4c37-a6f2-8f9599cc6fb0,2010-09-01 16:49:30.0,919.0,"In their classic book on the *Federalist* papers, Mosteller and Wallace argue for a log penalty function: you penalize yourself $-\\log(p)$ when you predict an event with probability $p$ and it occurs; the penalty for it not occurring equals $-\\log(1-p)$.  Thus, the penalty is high when whatever happens is unexpected according to your prediction.\\n\\nTheir argument in favor of this function rests on a simple natural criterion: ""the penalty function should encourage the prediction of the correct probabilities if they are known.""  Assuming the total penalty is summed over all predictions and there will be three or more of them, M&W claim that the log penalty function is the *only* one (up to affine transformation) for which the ""expected penalty is minimized over all predictions"" by the correct probabilities.\\n\\nFollowing this, then, a good test for you to use is to track your accumulated log penalties.  If, after a long time (or by means of some independent oracle), you obtain accurate estimates of what the probabilities actually were, you can compare your penalty with the minimum possible one.  The average of that difference measures your long-run predictive performance (the lower the better).  This is an excellent way to compare two or more competing predictors, too.",,
5291,10,2282,e1423b24-85a1-4a8d-a753-1eddd79c04a1,2010-09-01 16:59:06.0,-1.0,"{""Voters"":[{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":253,""DisplayName"":""Tal Galili""},{""Id"":5,""DisplayName"":""Shane""}]}",2,
5293,2,2285,8966b36f-e469-4106-89ae-08242bce3b87,2010-09-01 17:30:32.0,495.0,"What you're looking for are called **Scoring Rules**, which are ways of evaluating probabilistic forecasts. They were invented in the 1950s by weather forecasters, and there has been a been a bit of work on them in the statistics community, but I don't know of any books on the topic.\\n\\nOne thing you could do would be to bin the forecasts by probability range (e.g.: 0-5%, 5%-10%, etc.) and look at how many predicted events in that range occurred (if there are 40 events in the 0-5% range, and 20 occur, then your might have problems). If the events are independent, then you could compare these numbers to a binomial distribution.\\n",,
5294,2,2286,fbc2d12b-fd9d-4da7-8df9-2096c0d7a052,2010-09-01 18:23:56.0,1118.0,Here's a groaner:\\nQ: What do you call 100 statisticians at a tea party?\\nA: A Z-Party.,,
5295,16,2286,fbc2d12b-fd9d-4da7-8df9-2096c0d7a052,2010-09-01 18:23:56.0,-1.0,,,
5296,2,2287,fa9c9a35-6c0f-442a-8a80-959ad9301c10,2010-09-01 18:46:23.0,1122.0,"I agree completely with Srikant's explanation. To give more heuristic spin on it:\\n\\nClassical approaches generally posit that the world is one way (e.g., a parameter has one particular true value), and try to conduct experiments whose resulting conclusion -- no matter the true value of the parameter -- will be correct with at least some minimum probability.\\n\\nAs a result, to express uncertainty in our knowledge after an experiment, the frequentist approach uses a ""confidence interval"" -- a range of values designed to include the true value of the parameter with some minimum probability, say 95%. A frequentist will design the experiment and 95% confidence interval procedure so that out of every 100 experiments run start to finish, at least 95 of the resulting confidence intervals will be expected to include the true value of the parameter. The other 5 might be slightly wrong, or they might be complete nonsense -- formally speaking that's ok as far as the approach is concerned, as long as 95 out of 100 inferences are correct. (Of course we would prefer them to be slightly wrong, not total nonsense.)\\n\\nBayesian approaches formulate the problem differently. Instead of saying the parameter has one true value, a Bayesian method says the value is chosen from some probability distribution -- known as the prior probability distribution. This ""prior"" might be known (imagine trying to estimate the size of a truck, if we know the overall distribution of truck sizes from the DMV) or it might be an assumption drawn out of thin air. The Bayesian inference is simpler -- we collect some data, and then calculate the probability of different values of the parameter GIVEN the data. This new probability distribution is called the ""a posteriori probability"" or simply the ""posterior."" Bayesian approaches can summarize their uncertainty by giving a range of values on the posterior probability distribution that includes 95% of the probability -- this is called a ""95% credibility interval.""\\n\\nA Bayesian partisan might criticize the frequentist confidence interval like this: ""So what if 95 out of 100 experiments yield a confidence interval that includes the true value? I don't care about 99 experiments I DIDN'T DO; I care about this experiment I DID DO. Your rule allows 5 out of the 100 to be complete nonsense [negative values, impossible values] as long as the other 95 are correct; that's ridiculous.""\\n\\nA frequentist die-hard might criticize the Bayesian credibility interval like this: ""So what if 95% of the posterior probability is included in this range? What if the true value is, say, 0.37? If it is, then your method, run start to finish, will be WRONG 75% of the time. Your response is, 'Oh well, that's ok because according to the prior it's very rare that the value is 0.37,' and that may be so, but I want a method that works for ANY possible value of the parameter. I don't care about 99 values of the parameter that IT DOESN'T HAVE; I care about the one true value IT DOES HAVE. Oh also, by the way, your answers are only correct if the prior is correct. If you just pull it out of thin air because it feels right, you can be way off.""\\n\\nIn a sense both of these partisans are correct in their criticisms of each others' methods, but I would urge you to think mathematically about the distinction -- as Srikant explains.\\n\\nI have a talk on this subject that goes into more detail, including an extended example of where confidence intervals and credibility intervals diverge; see http://groups.csail.mit.edu/mac/users/gjs/6.945/readings/winstein-inference-slides.pdf .",,
5297,5,2264,a4100716-78e1-4ce5-a918-900411f874b3,2010-09-01 19:54:55.0,1134.0,"I've got product ratings for a few thousand products. The number of ratings for each product varies from zero to about fifty. I want to find the expected value of product rating for each product. If there are lots of ratings for the product I'd expect the ev to be the average of the ratings for the product, but if there are only a few I'd expect the ev to be closer to the average of all ratings. How do I calculate the true ev? Please be gentle: I'm no statistician or mathematician. \\n\\nEdit 1: Joris's answer below maintains I can't calculate ev because by definition that means I must have the entire population. In that case please can you tell me how to calculate the quantity that is similar to ev in spirit, does not require the entire population, and can make use of prior information.\\n\\nEdit 2: I would expect that if each product's ratings have low variance ratings, or if there is a very high variance between different products' ratings, then the measured ratings are more significant.",1. Broadened question outside of narrow definition of expected value. 2. Added features of expected solution.,
5298,2,2288,d1f7cbe5-d634-4121-8f44-68d5bfaa275c,2010-09-01 19:58:58.0,795.0,"It looks like I am probably stuck with a bootstrap. One interesting possibility here is to compute the 'exact bootstrap covariance', as outlined by [Hutson & Ernst][1]. Presumably the bootstrap covariance gives a good estimate of the standard error, asymptotically. However, the approach of Hutson & Ernst requires computation of the covariance of each pair of order statistics, and so this method is quadratic in the number of samples. Maybe I should just stick with the bootstrap!\\n\\n\\n  [1]: http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00221/abstract",,
5299,5,1947,6f7cebff-e70a-4f27-ae58-f1a0f60b753a,2010-09-01 20:06:24.0,223.0,"As **@Ars** said there are no accepted definition (and this is a good point). There are general alternatives famillies of ways to generalize quantiles on $\\mathbb{R}^d$, I think the most significant are:\\n\\n -  [**Generalize quantile process**][1] Let $P_n(A)$ be your empirical measure (=the proportion of observation in $A$). Then, with $\\mathbb{A}$ a well chosen subset of the borel set in $\\mathbb{R}^d$ and $\\lambda$ a real valued measure,\\n you can define the empirical quantile function:\\n\\n $U_n(t)=\\inf (\\lambda(A) : P_n(A)\\geq t A\\in\\mathbb{A})$\\n\\n Suppose you can find one $A_{t}$ that gives you the minimum. Then the set (or a point somewhere in it) $A_{1/2-\\epsilon}\\cap A_{1/2+\\epsilon}$ gives you the median when $\\epsilon$ is made small enough. The definition of the median is recovered when using $\\mathbb{A}=(]-\\infty,x] x\\in\\mathbb{R})$  and $\\lambda(]-\\infty,x])=x$. **Ars** answer falls into that framework I guess...  **tukey's half space location** may be obtained using $\\mathbb{A}(a)=( H_{x}=(t\\in \\mathbb{R}^d :\\; \\langle a, t \\rangle \\leq x ) $ and  $\\lambda(H_{x})=x$  (with $x\\in \\mathbb{R}$, $a\\in\\mathbb{R}^d$).\\n\\n - [**variational definition and M-estimation**][2]\\nThe idea here is that the  $\\alpha$-quantile $Q_{\\alpha}$ of a random variable $Y$ in $\\mathbb{R}$ can be defined through a variational equality. \\n  - The most common definition is using the **quantile regression function**   $\\rho_{\\alpha}$ (also known as pinball loss, guess why ? )  $Q_{\\alpha}=arg\\inf_{x\\in \\mathbb{R}}\\mathbb{E}[\\rho_{\\alpha}(Y-x)]$. The case $\\alpha=1/2$ gives $\\rho_{1/2}(y)=|y|$ and you can generalize that to higher dimension using $l^1$ distances as done in **@Srikant Answer**. This is theoretical median but gives you empirical median if you replace expectation by empirical expectation (mean).\\n\\n  - But [Kolshinskii][2] proposes to use  Legendre-Fenchel transform: since $Q_{\\alpha}=Arg\\sup_s (s\\alpha-f(s))$\\nwhere $f(s)=\\frac{1}{2}\\mathbb{E} [|s-Y|-|Y|+s]$ for $s\\in \\mathbb{R}$.\\n He gives a lot of deep reasons for that (see the paper ;)). Generalizing this to higher dimensions require working with a vectorial $\\alpha$ and replacing $s\\alpha$ by $\\langle s,\\alpha\\rangle$ but you can take $\\alpha=(1/2,\\dots,1/2)$. \\n - [**Partial ordering**][3] You can generalize the definition of quantiles in $\\mathbb{R}^d$ as soon as you can create a partial order (with equivalence classes)... \\n\\nObviously there are bridges between the different formulations. They are not all obvious...\\n\\n  [1]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1176348670\\n  [2]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1031833659\\n[3]: http://www.ams.org/mathscinet-getitem?mr=55:26",edited body,
5300,2,2289,3c4dac4e-9cee-43b0-872a-3eb873c08220,2010-09-01 20:42:40.0,919.0,"In an article in *The American Statistician*, Wolkewitz *et al.* use packages *Epi*, *mvna*, and *survival*.  See *Two Pitfalls in Survival Analyses of Time-Dependent Exposure: A Case Study in a Cohort of Oscar Nominees*, v. 64 no. 3 (August 2010) pp 205-211.  This exposition introduces multistate survival models and focuses on the use of a ""Lexis diagram"" to assess possible forms of bias.",,
5301,2,2290,7a3cdb4e-1858-45e5-ac2c-e1e1a968a1fd,2010-09-01 21:06:34.0,743.0,"This is a follow-up to the [repeated measures sample size](http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova) question.\\n\\nI am planning a repeated measures experiment. We record energy usage for 12 months, then give (a randomly assigned) half of the customers continuous information about their energy usage (perform the treatment), and record their energy usage for another 12 months. A similar study performed in the past showed a 5% reduction in energy usage.\\n\\nI want to estimate the required sample size using $\\alpha=0.05, \\beta=0.1$. G*Power 3 has a tool for repeated measures power analysis. However, it requires two inputs that I am not entirely familiar with:\\n\\n- $\\lambda$ - the noncentrality parameter (How do I estimate this?)\\n- $f$ - the effect size (I believe that this is the square root of Cohen's $f^2$)\\n\\nAccording to Wikipedia's effect size page:\\n> Cohen's $f^2= {R^2_{AB} - R^2_A \\over 1 - R^2_{AB}}$ where $R^2_A$ is the variance accounted for by a set of one or more independent variables $A$, and $R^2_{AB}$ is the combined variance accounted for by $A$ and another set of one or more independent variables $B$.\\n\\nHowever, my expected 5% change in energy consumption does not tell me how much variability will be explained. If there any way to make this conversion?\\n\\nIf you know of a way to do this power analysis in R, I would love to hear it. I am planning to simulate some data and try using lmer from the lme4 package.",,
5302,1,2290,7a3cdb4e-1858-45e5-ac2c-e1e1a968a1fd,2010-09-01 21:06:34.0,743.0,Determination of effect size for a repeated measures ANOVA power analysis.,,
5303,3,2290,7a3cdb4e-1858-45e5-ac2c-e1e1a968a1fd,2010-09-01 21:06:34.0,743.0,<r><repeated-measures><power-analysis>,,
5304,5,2287,38c53734-ab50-470c-8d2c-83767f65c45e,2010-09-01 21:35:43.0,1122.0,"I agree completely with Srikant's explanation. To give a more heuristic spin on it:\\n\\nClassical approaches generally posit that the world is one way (e.g., a parameter has one particular true value), and try to conduct experiments whose resulting conclusion -- no matter the true value of the parameter -- will be correct with at least some minimum probability.\\n\\nAs a result, to express uncertainty in our knowledge after an experiment, the frequentist approach uses a ""confidence interval"" -- a range of values designed to include the true value of the parameter with some minimum probability, say 95%. A frequentist will design the experiment and 95% confidence interval procedure so that out of every 100 experiments run start to finish, at least 95 of the resulting confidence intervals will be expected to include the true value of the parameter. The other 5 might be slightly wrong, or they might be complete nonsense -- formally speaking that's ok as far as the approach is concerned, as long as 95 out of 100 inferences are correct. (Of course we would prefer them to be slightly wrong, not total nonsense.)\\n\\nBayesian approaches formulate the problem differently. Instead of saying the parameter has one true value, a Bayesian method says the value is chosen from some probability distribution -- known as the prior probability distribution. This ""prior"" might be known (imagine trying to estimate the size of a truck, if we know the overall distribution of truck sizes from the DMV) or it might be an assumption drawn out of thin air. The Bayesian inference is simpler -- we collect some data, and then calculate the probability of different values of the parameter GIVEN the data. This new probability distribution is called the ""a posteriori probability"" or simply the ""posterior."" Bayesian approaches can summarize their uncertainty by giving a range of values on the posterior probability distribution that includes 95% of the probability -- this is called a ""95% credibility interval.""\\n\\nA Bayesian partisan might criticize the frequentist confidence interval like this: ""So what if 95 out of 100 experiments yield a confidence interval that includes the true value? I don't care about 99 experiments I DIDN'T DO; I care about this experiment I DID DO. Your rule allows 5 out of the 100 to be complete nonsense [negative values, impossible values] as long as the other 95 are correct; that's ridiculous.""\\n\\nA frequentist die-hard might criticize the Bayesian credibility interval like this: ""So what if 95% of the posterior probability is included in this range? What if the true value is, say, 0.37? If it is, then your method, run start to finish, will be WRONG 75% of the time. Your response is, 'Oh well, that's ok because according to the prior it's very rare that the value is 0.37,' and that may be so, but I want a method that works for ANY possible value of the parameter. I don't care about 99 values of the parameter that IT DOESN'T HAVE; I care about the one true value IT DOES HAVE. Oh also, by the way, your answers are only correct if the prior is correct. If you just pull it out of thin air because it feels right, you can be way off.""\\n\\nIn a sense both of these partisans are correct in their criticisms of each others' methods, but I would urge you to think mathematically about the distinction -- as Srikant explains.\\n\\nI have a talk on this subject that goes into more detail, including an extended example of where confidence intervals and credibility intervals diverge; see http://groups.csail.mit.edu/mac/users/gjs/6.945/readings/winstein-inference-slides.pdf .",added 2 characters in body,
5305,5,2280,61e11ca4-23bc-462e-b09b-26980f7dd085,2010-09-01 22:53:22.0,1107.0,"Incorporating a prior is one way to 'make up' for small samples.  Another is to use a mixed model, with an intercept for the mean structure and a random intercept for each product.  The estimate of the population mean plus the predicted random effect (BLUP) then offers a form of shrinkage, where values for products with less information are shrunk more toward the overall sample mean than those based on more information.  This method is common in, for example, Small Area Estimation in survey sampling.\\n\\nEdit:  The R code might look like:\\n\\n    library(nlme)\\n    f <- lme(score ~ 1, data = yourData, random = ~1|product)\\n    p <- predict(f)\\n\\nIf you go this route the assumptions are:\\n\\n - independent, normal errors with expected value 0 and constant variance for all observations\\n - normal random effects with expected value 0\\n\\nThese violations can generally be modeled, but course with that comes added complexity...\\n\\n\\n",added 430 characters in body; added 7 characters in body,
5306,5,1375,dfb55fe7-3004-4dd3-8a26-f32bae4a4f3e,2010-09-01 23:10:22.0,556.0,"Here is a list of many fun statistics jokes ([link][1])\\n\\nHere are just a few:\\n\\n------\\n\\nDid you hear the one about the statistician? Probably....\\n\\n------\\nIt is proven that the celebration of birthdays is healthy. Statistics show that those people who celebrate the most birthdays become the oldest. -- S. den Hartog, Ph D. Thesis Universtity of Groningen.\\n\\n------\\n\\nA statistician is a person who draws a mathematically precise line from an unwarranted assumption to a foregone conclusion.\\n\\n\\n------\\n\\n\\nThe average statistician is just plain mean.\\n\\n------\\n\\n\\nAnd there is also the one from a TED talk:\\n\\n""A friend asked my wife what I do.  She answered that I model.  Model what, she was asked - he models genes, she answered.""\\n\\n\\n  [1]: http://www.btinternet.com/~se16/hgb/statjoke.htm",added 2 characters in body,
5307,5,2280,557f64be-3132-4c23-b9c2-fd0100b49a20,2010-09-01 23:12:40.0,1107.0,"Incorporating a prior is one way to 'make up' for small samples.  Another is to use a mixed model, with an intercept for the mean structure and a random intercept for each product.  The estimate of the population mean plus the predicted random effect (BLUP) then offers a form of shrinkage, where values for products with less information are shrunk more toward the overall sample mean than those based on more information.  This method is common in, for example, Small Area Estimation in survey sampling.\\n\\nEdit:  The R code might look like:\\n\\n    library(nlme)\\n    f <- lme(score ~ 1, data = yourData, random = ~1|product)\\n    p <- predict(f)\\n\\nIf you go this route the assumptions are:\\n\\n - independent, normal errors with expected value 0 and constant variance for all observations\\n - normal random effects with expected value 0\\n\\nViolations of these can generally be modeled, but of course with that comes added complexity...\\n\\n\\n",added 6 characters in body,
5308,2,2291,de2d608e-1eae-4ffb-b7aa-21aab24c7e98,2010-09-01 23:57:06.0,138.0,"I'm working on a small (200M) corpus of text, which I want to explore with some cluster analysis. What books or articles on that subject would you recommend?  \\n\\n",,
5309,1,2291,de2d608e-1eae-4ffb-b7aa-21aab24c7e98,2010-09-01 23:57:06.0,138.0,Recommended books or articles as introduction to Cluster Analysis?,,
5310,3,2291,de2d608e-1eae-4ffb-b7aa-21aab24c7e98,2010-09-01 23:57:06.0,138.0,<machine-learning><statistics><clustering><recommendations>,,
5311,4,2275,2d62ead4-7af7-48e8-9bd3-d38dffc60ea5,2010-09-02 00:12:45.0,159.0,How can I determine accuracy of past probability calculations?,edited title,
5312,2,2292,9e64b3ed-76cc-4020-afe3-949e07a52d4c,2010-09-02 00:30:48.0,159.0,"The ets() function uses maximum likelihood estimation. So it would be possible to obtain standard errors based on the Hessian matrix in the usual way. However, in forecasting, the value of the model parameters is usually of very limited interest -- what we care about are the forecasts and their variances. \\n\\nI can't think of a situation where you might want a confidence interval for a smoothing parameter, for example. What could you do with the information that the ""true"" value of alpha (whatever that means) lies between 0.2 and 0.4? \\n\\nConsequently, I have not included the calculation of the standard errors of parameters in the package.\\n\\n",,
5313,5,2258,a0f17bd6-6e47-4403-af11-03bc8f6d21f4,2010-09-02 00:32:21.0,159.0,"Given a function mapping between two sample spaces $S_1$ and $S_2$, if $S_2$,$F_2$ is measurable how do I show that preimage of $S_2$,$F_2$ in $S_1$ is measurable set?\\n",deleted 125 characters in body,
5314,2,2293,8e06f5c0-465c-47bb-8f38-b58c70922fe0,2010-09-02 01:10:36.0,,NIPS:  http://nips.cc/,,
5315,16,2293,8e06f5c0-465c-47bb-8f38-b58c70922fe0,2010-09-02 01:10:36.0,-1.0,,,
5316,2,2294,ac2e276a-e3fb-41a4-b3e0-a760fbe83426,2010-09-02 01:14:21.0,1144.0,"I have a four-state, discrete time Markov process with time-dependent transition matrices such that after a given time T the matrices become constant. The idea is people in a program leaving the program in a variety of ways. Everyone starts in state 1, and states 2, 3 and 4 are absorbing, but state 4 represents the fairly small percentage of people who are 'lost in the system' - in other words state 4 represents our ignorance of what happens to people rather than a genuine outcome.\\n\\nI would like to use lumping to put those in state 4 in with those in state 1 and run this as a three-state system, and compare this with the naive approach of running this as a four-state system then apportioning those who are asymptotically in state 4 into states 2 and 3 according to their relative proportions. (in other words, p_2/(p_2 + p_3) of those in state 4 go into state 2 after the system is run to infinite time and similar for state 3)\\n\\nFrom some rough scribblings it doesn't seem that these two methods give the same results, so it would be good to get an idea on the error involved. To this end, here's my question:\\n\\nCan I have pointers to the literature on lumping in Markov chains (or related) that would apply - even roughly - in this example? Or otherwise some words of advice on how to approach this.",,
5317,1,2294,ac2e276a-e3fb-41a4-b3e0-a760fbe83426,2010-09-02 01:14:21.0,1144.0,Lumping in Markov process with absorbing states,,
5318,3,2294,ac2e276a-e3fb-41a4-b3e0-a760fbe83426,2010-09-02 01:14:21.0,1144.0,<modeling><asymptotics><markov-process>,,
5320,2,2296,c9619488-a014-45e7-84e9-68bfd08229e6,2010-09-02 02:00:45.0,1146.0,"I am interested in fitting a factor analysis-like model on asset returns or other similar latent variable models. What are good papers to read on this topic? I am particularly interested in how to handle the fact that a factor analysis model is identical under a sign change for the ""factor loadings"".",,
5321,1,2296,c9619488-a014-45e7-84e9-68bfd08229e6,2010-09-02 02:00:45.0,1146.0,Papers on Bayesian factor analysis?,,
5322,3,2296,c9619488-a014-45e7-84e9-68bfd08229e6,2010-09-02 02:00:45.0,1146.0,<bayesian><pca><factor-analysis>,,
5323,2,2297,fd9854b1-bcda-45db-b134-616b13da953c,2010-09-02 02:19:37.0,530.0,"Some references to help you out. \\n\\n 1. Tipping, M. E. & Bishop, C. M.\\n    Probabilistic principal component\\n    analysis Journal of the Royal\\n    Statistical Society (Series B),\\n    1999, 21, 611-622\\n 2. Tom Minka. Automatic choice of\\n    dimensionality for PCA. NIPS 2000\\n    url:\\n\\n    http://research.microsoft.com/en-us/um/people/minka/papers/pca/\\n 3. Šmídl, V. & Quinn, A. On Bayesian\\n    principal component analysis\\n    Computational Statistics & Data\\n    Analysis, 2007, 51, 4101-4123\\n\\nIf you are familiar with information theoretic model selection (MML, MDL, etc.), I highly recommend checking out:\\n\\n 1. Wallace, C. S. & Freeman, P. R.\\n    Single-Factor Analysis by Minimum\\n    Message Length Estimation Journal of\\n    the Royal Statistical Society\\n    (Series B), 1992, 54, 195-209\\n 2. C. S. Wallace. Multiple Factor\\n    Analysis by MML Estimation.\\n    \\n http://www.allisons.org/ll/Images/People/Wallace/Multi-Factor/    \\n Tech report:\\n    http://www.allisons.org/ll/Images/People/Wallace/Multi-Factor/TR95.218.pdf\\n\\n",,
5324,2,2298,4523181b-c2d9-4e3a-bb46-ab9d8f81faab,2010-09-02 03:32:30.0,795.0,"This is somewhat vague, but suppose you have a black box function $f(x_1,x_2,\\ldots,x_k)$, for which you have code, and you are interested in the behaviour of $f$ when the $x_i$ are i.i.d. standard Gaussian random variables. What are some good ways to visualize this function? To make it easier, we may assume that $k$ is smallish, say less than 10. \\n\\nOne particular relationship of interest is how $f$ varies with one of the input, say $x_i$. An easy way to visualize this relationship would be to sample the function for fixed values of $x_i$ while varying the other input (either in a structured way, or randomly, say), then box-plotting, which could show how the mean trend is affected by $x_i$, but also whether the scatter is affected (i.e. heteroskedasticity). However, interaction between $x_i$ and the levels of the other input might be masked by this approach.\\n\\nWhat I am looking for is somewhat open-ended. I do not have a particular hypothesis that I am testing, but rather am looking for new ways of visualizing the response which might reveal peculiarities of the function.",,
5325,1,2298,4523181b-c2d9-4e3a-bb46-ab9d8f81faab,2010-09-02 03:32:30.0,795.0,visualization of multivariate function,,
5326,3,2298,4523181b-c2d9-4e3a-bb46-ab9d8f81faab,2010-09-02 03:32:30.0,795.0,<data-visualization><computing>,,
5327,2,2299,9f91e1f4-3473-4d3a-937c-e9756a85955a,2010-09-02 04:01:56.0,795.0,"What broad methods are there to detect fraud, anomalies, fudging, etc. in scientific works produced by a third party? (I was motivated to ask this by the recent [Marc Hauser affair][1].) Usually for election and accounting fraud, some variant of [Benford's Law][2] is cited. I am not sure how this could be applied to _e.g._ the Marc Hauser case, because Benford's Law requires numbers to be approximately log uniform. \\n\\nAs a concrete example, suppose a paper cited the p-values for a large number of statistical tests. Could one transform these to log uniformity, then apply Benford's Law? It seems like there would be all kinds of problems with this approach (_e.g._ some of the null hypotheses might legitimately be false, the statistical code might give p-values which are only approximately correct, the tests might only give p-values which are uniform under the null asymptotically, etc.)\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Marc_Hauser#Scientific_misconduct\\n  [2]: http://en.wikipedia.org/wiki/Benfords_law",,
5328,1,2299,9f91e1f4-3473-4d3a-937c-e9756a85955a,2010-09-02 04:01:56.0,795.0,statistical forensics: Benford and beyond.,,
5329,3,2299,9f91e1f4-3473-4d3a-937c-e9756a85955a,2010-09-02 04:01:56.0,795.0,<meta-analysis>,,
5330,2,2300,ed2e12fd-b102-4520-a6e8-0941e2ebffb1,2010-09-02 04:13:24.0,795.0,"A decent overview of factor analysis is [Latent Variable Methods and Factor Analysis][1] by Bartholomew and Knott. They write about the interpretation of latent factors. This book is not as algorithmically-oriented as I would like, but their description of _e.g._ partial factor analysis is decent.\\n\\n\\n  [1]: http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470711108.html",,
5331,2,2301,14b53287-ab92-4f09-8d75-a8d09228a1e9,2010-09-02 04:23:58.0,485.0,Cluster Analysis by Brian S. Everitt is a nice book length applied treatment of Cluster Analysis.,,
5332,2,2302,3ca786e3-e03b-478d-a5d5-2e12994c4ad9,2010-09-02 05:24:13.0,5.0,"This chapter of <a HREF=""http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf"">Introduction to Data Mining</a> is available online and gives a nice overview. ",,
5333,2,2303,906ed2dc-a571-4b82-9fbf-660bdde76174,2010-09-02 05:45:28.0,183.0,"Great Question!\\n\\nIn the scientific context there are various kinds of problematic reporting and problematic behaviour:\\n\\n- **Fraud**: I'd define fraud as a deliberate intention on the part of the author or analyst to misrepresent the results and where the misrepresentation is of a sufficiently grave nature. The main example being complete fabrication of raw data or summary statistics.\\n- **Error**: Data analysts can make errors at many phases of data analysis from data entry, to data manipulation, to analyses, to reporting, to interpretation.\\n- **Inappropriate behaviour**: There are many forms of inappropriate behaviour. In general, it can be summarised by an orientation which seeks to confirm a particular position rather than search for the truth. \\n\\nCommon examples of inappropriate behaviour include: \\n\\n- Examining a series of possible dependent variables and only reporting the one that is statistically significant\\n- Not mentioning important violations of assumptions\\n- Performing data manipulations and outlier removal procedures without mentioning it, particularly where these procedures are both inappropriate and chosen purely to make the results look better\\n- Presenting a model as confirmatory which is actually exploratory\\n- Omitting important results that go against the desired argument\\n- Choosing a statistical test solely on the basis that it makes the results look better\\n- Running a series of five or ten under-powered studies where only one is statistically significant (perhaps at p = .04) and then reporting the study without mention of the other studies\\n\\n\\nIn general, I'd hypothesise that **incompetence** is related to all three forms of problematic behaviour. A researcher who does not understand how to do good science but otherwise wants to be successful will have a greater incentive to misrepresent their results, and is less likely to respect the principles of ethical data analysis.\\n\\nThe above distinctions have implications for detection of problematic behaviour.\\nFor example, if you manage to discern that a set of reported results are wrong, it still needs to be ascertained as to whether the results arose from fraud, error or inappropriate behaviour. Also, I'd assume that various forms of inappropriate behaviour are far more common than fraud.\\n\\nWith regards to detecting problematic behaviour, I think it is largely a skill that comes from experience working with **data**, working with a **topic**, and working with **researchers**. All of these experiences strengthen your expectations about what data should look like. Thus, major deviations from expectations start the process of searching for an explanation. Experience with researchers gives you a sense of the kinds of inappropriate behaviour which are more or less common. In combination this leads to the generation of hypotheses. For example, if I read a journal article and I am surprised with the results, the study is underpowered, and the nature of the writing suggests that the author is set on making a point, I generate the hypothesis that the results perhaps should not be trusted.\\n\\n**Other Resources**\\n\\n- Robert P. Abelson [Statistics as a Principled Argument][1] has a chapter titled ""On Suspecting Fishiness"" \\n\\n\\n  [1]: http://www.amazon.com/Statistics-Principled-Argument-Robert-Abelson/dp/0805805281",,
5334,2,2304,ef91937a-4d3e-4b68-84c4-cb993f343679,2010-09-02 06:33:11.0,183.0,"Just a thought, although I've never tried it.\\n\\n- you could obtain a large number of values from the function across different parameter values\\n- take a tour of the resulting data in [ggobi][1] (check out [Mat Kelcey's video][2])\\n\\n\\n  [1]: http://www.ggobi.org/\\n  [2]: http://vimeo.com/12292239",,
5335,5,880,160eeb5b-396a-4988-86b6-27edc279ad1b,2010-09-02 07:13:57.0,223.0,"My question is about cross validation when there are many more variables than observations. To fix ideas, I propose to restrict to the classification framework in very high dimension (more features than observation).\\n\\n **Problem:** Assume that for each variable $i=1,\\dots,p$ you have a measure of importance $T[i]$ than exactly measure the interest of feature $i$ for the classification problem. The problem of selecting a subset of feature to reduce optimally the classification error is then reduced to that of finding the number of features. \\n\\n**Question:** What is the most efficient way to run cross validation in this case (cross validation scheme)? My question is not about how to write the code but on the version of cross validation to use when trying to find the number of selected feature (to minimize the classification error) but how to deal with the high dimension when doing cross validation (hence the problem above may be a bit like a 'toy problem' to discuss CV in high dimension). \\n\\n\\n**Notations:**  $n$ is the size of the learning set, p the number of features (i.e. the dimension of the feature space).  By very high dimension I mean p>>n (for example $p=10000$ and $n=100$). ",added 119 characters in body,
5336,16,2304,00000000-0000-0000-0000-000000000000,2010-09-02 07:18:38.0,88.0,,,
5337,16,2298,00000000-0000-0000-0000-000000000000,2010-09-02 07:18:38.0,88.0,,,
5338,4,2298,44534baa-c979-405c-b745-28aa609d882c,2010-09-02 07:50:54.0,88.0,Visualization of a multivariate function,edited title,
5339,5,683,c61fac4b-3fee-4789-9de4-a42521cd349d,2010-09-02 09:21:53.0,223.0,"If your interested in the mathematical statistic around entropy, you may consult this book \\n\\nhttp://www.renyi.hu/~csiszar/Publications/Information_Theory_and_Statistics:_A_Tutorial.pdf\\n\\n\\nit is freely available ! ",added 9 characters in body,
5340,16,2301,00000000-0000-0000-0000-000000000000,2010-09-02 09:35:37.0,88.0,,,
5341,16,2302,00000000-0000-0000-0000-000000000000,2010-09-02 09:35:37.0,88.0,,,
5342,16,2291,00000000-0000-0000-0000-000000000000,2010-09-02 09:35:37.0,88.0,,,
5343,5,1993,65b7b70c-2ac8-4622-9dcc-1191518834f8,2010-09-02 09:45:08.0,8.0,"We wrote a paper explaining how to use R/Bioconductor when analysing microarray data. The paper was written in Sweave and all the code used to generate the graphs is included as supplementary material.\\n\\nGillespie, C. S., Lei, G., Boys, R. J., Greenall, A. J., Wilkinson, D. J., 2010. [Analysing yeast time course microarray data using BioConductor: a case study using yeast2 Affymetrix arrays][1] BMC Research Notes,  3:81.\\n\\n\\n  [1]: http://www.mas.ncl.ac.uk/~ncsg3/microarray/",Fix typo in the link,
5344,5,338,aaebaf34-8d82-4c1b-b011-7f88b6381ad3,2010-09-02 09:52:44.0,8.0,"Cover and Thomas's book [Elements of Information Theory][1] is a good source on entropy and its applications, although I don't know that it addresses exactly the issues you have in mind.\\n\\n\\n  [1]: http://www.amazon.com/Elements-Information-Theory-Thomas-Cover/dp/0471062596",Adding amazon link to the book,
5345,2,2305,fd9acf1f-7cf0-4596-82c6-e8a9ce70feb0,2010-09-02 10:25:32.0,930.0,"It may be worth looking at M.W. Berry's books: \\n\\n1. *Survey of Text Mining I: Clustering, Classification, and Retrieval* (2003)\\n2. *Survey of Text Mining II: Clustering, Classification, and Retrieval* (2008)\\n\\nThey consist of series of applied and review papers. The latest seems to be available as PDF at the following address: http://bit.ly/deNeiy.\\n\\nHere are few links related to CA as applied to text mining: \\n\\n* [Document Topic Generation in Text Mining by Using Cluster Analysis with EROCK][1]\\n* [An Approach to Text Mining using Information Extraction][2]\\n\\nYou can also look at *Latent Semantic Analysis*, but see my response there: [Working through a clustering problem][3]. \\n\\n\\n  [1]: http://www.cscjournals.org/csc/manuscript/Journals/IJCSS/volume4/Issue2/IJCSS-271.pdf\\n  [2]: http://www.google.fr/url?sa=t&source=web&cd=8&ved=0CFoQFjAH&url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.21.8862%26rep%3Drep1%26type%3Dpdf&ei=P3h_TJXfAdT34AbIuN3TCw&usg=AFQjCNHaVKyo45HtjNKiEPvFnPuvD2MvmQ&sig2=vPQ1BmIz8vb16q1Oysi8_g\\n  [3]: http://stats.stackexchange.com/questions/369/working-through-a-clustering-problem/2196#2196",,
5346,16,2305,fd9acf1f-7cf0-4596-82c6-e8a9ce70feb0,2010-09-02 10:25:32.0,-1.0,,,
5347,2,2306,f72627d9-c87b-405a-8215-2381ce1e20e1,2010-09-02 10:25:42.0,1150.0,"I am getting a bit confused about feature selection and machine learning\\nand I was wondering if you could help me out.  I have a dataset that is\\nclassified into two groups and my aim is to get a small number of genes (my features)\\n(10-20) in a signature that I will in theory be able to apply to\\nother datasets to optimally classify the samples.  As I do not have a test\\nand training set I am using Leave-one-out cross-validation to help\\ndetermine the robustness.  I have read that one should perform feature\\nselection for each split of the samples i.e.\\n\\n1) Select one group as the test set\\n2) On the remaining samples select genes\\n3) Apply machine learning algorithm\\n4) Test whether the test set is correctly classified\\n5) Go to one\\n\\nIf you do this, you might get different genes each time, so how do you\\nget your ""final"" optimal gene classifier?\\n\\nWhat I mean by optimal is the collection of genes that any further studies\\nshould use.  For example, say I have a cancer/normal dataset and I want\\nto find the top 10 genes that will classify the tumour type according to\\nan SVM.  I would like to know the set of genes plus SVM parameters that\\ncould be used in further experiments to see if it could be used as a\\ndiagnostic test.\\n\\n",,
5348,1,2306,f72627d9-c87b-405a-8215-2381ce1e20e1,2010-09-02 10:25:42.0,1150.0,"Feature selection for ""final"" model when performing cross-validation in machine learning",,
5349,3,2306,f72627d9-c87b-405a-8215-2381ce1e20e1,2010-09-02 10:25:42.0,1150.0,<machine-learning><classification><feature-selection>,,
5350,5,2306,4146966c-2a0d-412f-827d-84040c5972a6,2010-09-02 10:35:19.0,1150.0,"I am getting a bit confused about feature selection and machine learning\\nand I was wondering if you could help me out.  I have a microarray dataset that is\\nclassified into two groups and has 1000s of features.  My aim is to get a small number of genes (my features) (10-20) in a signature that I will in theory be able to apply to\\nother datasets to optimally classify those samples.  As I do not have that many samples (<100), I am not using a test and training set but using Leave-one-out cross-validation to help\\ndetermine the robustness.  I have read that one should perform feature selection for each split of the samples i.e.\\n\\n 1. Select one sample as the test set\\n 2. On the remaining samples perform feature selection\\n 3. Apply machine learning algorithm to remaining samples using the features selected\\n 4. Test whether the test set is correctly classified\\n 5. Go to 1.\\n\\nIf you do this, you might get different genes each time, so how do you\\nget your ""final"" optimal gene classifier? i.e. what is step 6.\\n\\nWhat I mean by optimal is the collection of genes that any further studies\\nshould use.  For example, say I have a cancer/normal dataset and I want\\nto find the top 10 genes that will classify the tumour type according to\\nan SVM.  I would like to know the set of genes plus SVM parameters that\\ncould be used in further experiments to see if it could be used as a\\ndiagnostic test.\\n\\n",added 72 characters in body; added 1 characters in body; added 89 characters in body,
5351,2,2307,8e08247a-6365-43af-b918-04254e72248e,2010-09-02 10:46:12.0,930.0,"This is a very good question that I faced myself when working with SNPs data... And I didn't find any obvious answer through the literature. \\n\\nWhether you use LOO or K-fold CV, you'll end up with different features since the cross-validation iteration must be the most outer loop, as you said. You can think of some kind of voting scheme which would rate the n-vectors of features you got from your LOO-CV (can't remember the paper but I'll look at it). Or, alternatively, you can use embedded methods which provide you with features ranking through a measure of variable importance, e.g. like in [Random Forests][1] (RF). As cross-validation is included in RFs, you don't have to worry about the $n\\ll p$ case or curse of dimensionality. Here are nice papers of their applications in gene expression studies:\\n\\n1. Cutler, A., Cutler, D.R., and Stevens, J.R. (2009). Tree-Based Methods, in *High-Dimensional Data Analysis in Cancer Research*, Li, X. and Xu, R. (eds.), pp. 83-101, Springer.\\n2. Saeys, Y., Inza, I., and Larrañaga, P. (2007). A review of feature selection techniques in bioinformatics. *Bioinformatics*, **23(19)**: 2507-2517.\\n3. Díaz-Uriarte, R., Alvarez de Andrés, S. (2006). Gene selection and classification of microarray data using random forest. *BMC Bioinformatics*, **7**:3.\\n4. Diaz-Uriarte, R. (2007). GeneSrF and varSelRF: a web-based tool and R package for gene selection and classification using random forest. *BMC Bioinformatics*, **8**: 328\\n\\nSince you are talking of SVM, you can look for *penalized SVM*.\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/",,
5352,5,2307,eed6bf80-55d7-4711-8662-98820ead9ff9,2010-09-02 10:52:08.0,930.0,"This is a very good question that I faced myself when working with SNPs data... And I didn't find any obvious answer through the literature. \\n\\nWhether you use LOO or K-fold CV, you'll end up with different features since the cross-validation iteration must be the most outer loop, as you said. You can think of some kind of voting scheme which would rate the n-vectors of features you got from your LOO-CV (can't remember the paper but I'll look at it). In the absence of a new test sample, what is usually done is to re-apply the ML algorithm to the whole sample once you have found its optimal cross-validated parameters. But proceeding this way, you cannot ensure that there is no overfitting (since the sample was laready used for model optimization).\\n\\nOr, alternatively, you can use embedded methods which provide you with features ranking through a measure of variable importance, e.g. like in [Random Forests][1] (RF). As cross-validation is included in RFs, you don't have to worry about the $n\\ll p$ case or curse of dimensionality. Here are nice papers of their applications in gene expression studies:\\n\\n1. Cutler, A., Cutler, D.R., and Stevens, J.R. (2009). Tree-Based Methods, in *High-Dimensional Data Analysis in Cancer Research*, Li, X. and Xu, R. (eds.), pp. 83-101, Springer.\\n2. Saeys, Y., Inza, I., and Larrañaga, P. (2007). A review of feature selection techniques in bioinformatics. *Bioinformatics*, **23(19)**: 2507-2517.\\n3. Díaz-Uriarte, R., Alvarez de Andrés, S. (2006). Gene selection and classification of microarray data using random forest. *BMC Bioinformatics*, **7**:3.\\n4. Diaz-Uriarte, R. (2007). GeneSrF and varSelRF: a web-based tool and R package for gene selection and classification using random forest. *BMC Bioinformatics*, **8**: 328\\n\\nSince you are talking of SVM, you can look for *penalized SVM*.\\n\\n  [1]: http://www.stat.berkeley.edu/~breiman/RandomForests/",added 305 characters in body,
5353,2,2308,d61e982f-1357-4834-a27f-7cbf498ddcd9,2010-09-02 11:15:56.0,319.0,[Irreproducibility of NCI60 Predictors of Chemotherapy][1]\\n\\nThis is a reproducible analysis showing the lack of reproducibility of a paper that has been the center of a great deal of controversy.\\n\\n\\n  [1]: http://bioinformatics.mdanderson.org/Supplements/ReproRsch-Chemo/,,
5354,2,2309,33b01936-5361-4fb8-919f-85983a3312b6,2010-09-02 11:29:47.0,1124.0,"To add to chl: When using support vector machines, a highly recommended penalization method is the elastic net. This method will shrink coefficients towards zero, and in theory retains the most stable coefficients in the model. Initially it was used in a regression framework, but it is easily extended for use with support vector machines.\\n\\n[The original publication][1] : Zou and Hastie (2005) : Regularization and variable selection via the elastic net. J.R.Statist.Soc. B, 67-2,pp.301-320\\n\\n[Elastic net for SVM][2] : Zhu & Zou (2007): Variable Selection for the Support Vector Machine : Trends in Neural Computation, chapter 2 (Editors: Chen and Wang) \\n\\n[improvements on the elastic net][3] Jun-Tao and Ying-Min(2010): An Improved Elastic Net for Cancer Classification and Gene Selection : Acta Automatica Sinica, 36-7,pp.976-981\\n\\n\\n  [1]: http://www.math.mtu.edu/~shuzhang/MA5761/model_selection2.pdf\\n  [2]: http://www.springerlink.com/content/517605v67gg75338/fulltext.pdf\\n  [3]: http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B8H0V-50M1N7Y-2&_user=794998&_coverDate=07%2F31%2F2010&_rdoc=1&_fmt=high&_orig=search&_origin=search&_sort=d&_docanchor=&view=c&_searchStrId=1448587725&_rerunOrigin=google&_acct=C000043466&_version=1&_urlVersion=0&_userid=794998&md5=7e56402b32c944bc33c9373013f60ae8&searchtype=a",,
5355,5,2290,fb9dfd32-92b0-4823-baa2-d6de7874a4be,2010-09-02 12:15:16.0,183.0,"This is a follow-up to the [repeated measures sample size](http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova) question.\\n\\nI am planning a repeated measures experiment. We record energy usage for 12 months, then give (a randomly assigned) half of the customers continuous information about their energy usage (perform the treatment), and record their energy usage for another 12 months. A similar study performed in the past showed a 5% reduction in energy usage.\\n\\nI want to estimate the required sample size using $\\alpha=0.05, \\beta=0.1$. G*Power 3 has a tool for repeated measures power analysis. However, it requires two inputs that I am not entirely familiar with:\\n\\n- $\\lambda$ - the noncentrality parameter (How do I estimate this?)\\n- $f$ - the effect size (I believe that this is the square root of Cohen's $f^2$)\\n\\nAccording to Wikipedia's effect size page:\\n> Cohen's $f^2= {R^2_{AB} - R^2_A \\over 1 - R^2_{AB}}$ where $R^2_A$ is the variance accounted for by a set of one or more independent variables $A$, and $R^2_{AB}$ is the combined variance accounted for by $A$ and another set of one or more independent variables $B$.\\n\\nHowever, my expected 5% change in energy consumption does not tell me how much variability will be explained. Is there any way to make this conversion?\\n\\nIf you know of a way to do this power analysis in R, I would love to hear it. I am planning to simulate some data and try using lmer from the lme4 package.",edited body,
5356,2,2310,19cf3b8a-9e79-4e2a-8f6f-3133e8566f33,2010-09-02 12:27:52.0,183.0,"Assuming you are going to average the first 12 months to form a baseline measure and the second 12 months to form as a follow-up measure, your problem reduces to a repeated measures t-test.\\n\\n**G*Power**\\n\\n You might want to check out the following menu in G*Power 3:\\n`Tests - Means - Two Dependent Groups (matched pairs)`.\\n Use A priori, $\\alpha=.05$, Power = 0.90.\\nUse the `Determine` button to determine effect size. This requires that you can estimate  time 1 and 2 means, sds, and correlation between time points.\\n\\n\\n**R**\\n\\n[UCLA has a tutorial][1] on doing a power analysis on a repeated measures t-test using R.\\n\\n**Side point**\\n\\nAs a side point, you might want to consider having a control group.\\n\\n\\n\\n  [1]: http://www.ats.ucla.edu/stat/R/dae/t_test_power3.htm",,
5361,2,2312,d7291991-98ff-4e59-a170-1cef8fe8351e,2010-09-02 13:10:52.0,582.0,"Not specifically about text-mining, but I quite liked [""Exploratory Data Analysis with MATLAB""][1] by Martinez and Martinez.\\n\\n[1]: http://www.amazon.com/Exploratory-Analysis-Chapman-Computer-Science/dp/1584883669/ref=sr_1_cc_1?ie=UTF8&qid=1283433014&sr=1-1-catcorr",,
5362,16,2312,d7291991-98ff-4e59-a170-1cef8fe8351e,2010-09-02 13:10:52.0,-1.0,,,
5365,6,2299,7def4df0-16d6-4fbe-8f18-f80f56936bb7,2010-09-02 14:30:38.0,183.0,<meta-analysis><fraud>,edited tags,
5366,11,2269,eeabf12c-25b1-45d4-8632-ee5bc11220fe,2010-09-02 14:36:03.0,-1.0,"{""Voters"":[{""Id"":74,""DisplayName"":""el chief""},{""Id"":28,""DisplayName"":""Srikant Vadali""},{""Id"":5,""DisplayName"":""Shane""}]}",,
5367,5,2308,2d4b81c2-68f1-4e50-bc36-aeac1075ef31,2010-09-02 14:57:59.0,319.0,"[Irreproducibility of NCI60 Predictors of Chemotherapy][1]\\n\\nThis is a reproducible analysis showing the lack of reproducibility of a paper that has been in the news.  A clinical trial based on the false conclusions of the irreproducible paper was suspended, re-instated, suspended again, ... It's a good example of reproducible analysis in the news.  \\n\\n\\n  [1]: http://bioinformatics.mdanderson.org/Supplements/ReproRsch-Chemo/",added 156 characters in body,
5368,2,2314,7fc4dc8f-a334-4988-8bd4-f42082a171fc,2010-09-02 15:56:37.0,279.0,"As step 6 (or 0) you run the feature detection algorithm on the entire data set.\\n\\nThe logic is the following: you have to think of cross-validation as a method for finding out the properties of the procedure you are using to select the features. It answers the question: ""if I have some data and perform this procedure, then what is the error rate for classifying a new sample?"". Once you know the answer, you can use the procedure (feature selection + classification rule development) on the entire data set. People like leave-one-out because the predictive properties usually depend on the sample size, and $n-1$ is usually close enough to $n$ not to matter much.",,
5370,2,2315,b74c0581-2c23-436e-bf4d-30f6f3807dcb,2010-09-02 16:12:52.0,132.0,"A previous user asked this question specifically for R.  I'd like to know what, if any, other software can do this.",,
5371,1,2315,b74c0581-2c23-436e-bf4d-30f6f3807dcb,2010-09-02 16:12:52.0,132.0,What software allows non-parametric repeated-measures multi-way Anova?,,
5372,3,2315,b74c0581-2c23-436e-bf4d-30f6f3807dcb,2010-09-02 16:12:52.0,132.0,<anova><nonparametric>,,
5373,2,2316,3c2b9d21-035e-4eb0-920b-026c4a035155,2010-09-02 16:58:19.0,1134.0,"Ha! I've answered my own question. Simon Funk figured this out for the Netflix challenge [here][1]. See the paragraph commencing ""However, even this isn't quite as simple as it appears"". But I'm having difficulty proving it algebraically: maybe you guys would like to take that on. \\n\\n\\n  [1]: http://sifter.org/~simon/journal/20061211.html",,
5374,2,2317,e8692bc7-2e67-457d-adcf-e5c2e887538e,2010-09-02 17:53:16.0,887.0,"In principle:\\n\\nMake your predictions using a single model trained on the entire dataset (so there is only one set of features).  The cross-validation is only used to estimate the predictive performance of the single model trained on the whole datset.  It is VITAL in using cross-validation that in each fold you repeat the entire procedure used to fit the primary model, as otherwise you can end up with a substantial optimistic bias in performance.\\n\\nTo see why this happens, consider a binary classification problem with 1000 binary features but only 100 patterns, where the targets and features are all purely random, so there is no statistical relationship between the inputs and the targets whatsoever.  If we train a primary model on the full dataset, we can always achieve zero error on the training set as there are more features than patterns.  We can even find a subset of ""informative"" features (that happen to be correllated to the target by chance).  I we then adopt the naughty approach and perform cross-validation using only those features, we will get an estimate of performance that is better than random guessing.  The reason is that in each fold of the cross-validation procedure there is some information about the held-out patterns used for testing as the features were chosen because they were good for predicting the targets, all of them, including those held out.  Of course the true error rate will be 0.5.\\n\\nIf we adopt the proper procedure, and perform feature selection in each fold, there is no longer any information about the target of the held out pattern in the choice of features used in that fold.  If you use the proper procedure, in this case, you will get an error rate of about 0.5 (although it will vary a bit for difference realisations of the dataset).\\n\\nGood papers to read are:\\n\\nChristophe Ambroise, Geoffrey J. McLachlan, ""Selection bias in gene extraction on the basis of microarray gene-expression data"", PNAS http://www.pnas.org/content/99/10/6562.abstract\\n\\nwhich is highly relevant to the OP and\\n\\nGavin C. Cawley, Nicola L. C. Talbot, ""On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation"", JMLR 11(Jul):2079−2107, 2010 http://jmlr.csail.mit.edu/papers/v11/cawley10a.html\\n\\nwhich demonstrates that the same thing can easily ocurr in model selection (e.g. tuning the hyper-parameters of an SVM, which also need to be repeated in each iteration of the CV procedure).\\n\\nIn practice:\\n\\nI would recommend using Bagging, and using the out-of-bag error for estimating performance.    You will get a committee model using many features, but that is actually a good thing.  If you only use a single model, it will be likely that you will over-fit the feature selection criterion, and end up with a model that gives poorer predictions than a model that uses a larger number of features.\\n\\nAlan Millers book on subset selection in regression (Chapman and Hall monographs on statistics and applied probability, volume 95) gives the good bit of advice (page 221) that if predictive performance is the most important thing, then don't do any feature selection, just use ridge regression instead.  And that is in a book on subset selection!!! ;o)\\n",,
5375,2,2318,3e59a228-5929-4130-b5ba-1b0169a28827,2010-09-02 18:07:50.0,887.0,"Isabelle Guyon, André Elisseeff, ""An Introduction to Variable and Feature Selection"", JMLR, 3(Mar):1157-1182, 2003. http://jmlr.csail.mit.edu/papers/v3/guyon03a.html\\n\\nis well worth reading, it will give a good overview of approaches and issues.  The one thing I would add is that feature selection doesn't necessarily improve predictive performance, and can easily make it worse (beacuse it is easy to over-fit the feature selection criterion).  One of the advantages of (especially linear) SVMs is that they work well with large numbers of features (providing you tune the regularisation parameter properly), so there is often no need if you are only interested in prediction.\\n",,
5376,2,2319,ec99bc52-a048-4e6a-840b-93c033cdcce0,2010-09-02 18:56:34.0,1080.0,"Given that you are at the initial, exploratory stages of the analysis I would start simple. Consider sampling your inputs using a [Latin Hypercube][1] strategy. Then, a tornado chart can be used to get a quick assessment of the multiple,one-way sensitivities f() has to the various input variables. Here is an example chart (from [here][2])\\n\\n\\n![alt text][3]\\n\\n\\nThis chart is not that interesting, but an interpretation would be ""NPV is most sensitive to Shipments, all other things being equal. But, the sensitivity is mostly on the upside, which is good. The Escalation variable induces sensitivity into NPV, but what looks to be skewed negatively a bit..."". \\n\\nYou could do something similar for Mean(f) on the X-axis as well as Var(f)\\n\\nGiven what you find from some first glance visualizations like this, you could then slice and dice more and focus on specific variables or relationships between variables. Maybe you can revisit this thread in coming months and post the visualizations you found useful :)\\n\\n  [1]: http://en.wikipedia.org/wiki/Latin_hypercube_sampling\\n  [2]: http://www.add-ins.com/sensitivity_chart_creator.htm\\n  [3]: http://i.stack.imgur.com/DQCby.gif",,
5377,16,2319,ec99bc52-a048-4e6a-840b-93c033cdcce0,2010-09-02 18:56:34.0,-1.0,,,
5378,2,2320,a639ce02-aa5f-4400-b647-3012ca06b720,2010-09-02 19:22:14.0,364.0,"The [ez][1] package, of which I am the author, has a function called ezPerm() which computes a permutation test, but probably doesn't do interactions properly (the documentation admits as much). The latest version has a function called ezBoot(), which lets you do bootstrap resampling that takes into account repeated measures (by resampling subjects, then within subjects), either using traditional cell means as the prediction statistic or using mixed effects modelling to make predictions for each cell in the design. I'm still not sure how ""non-parametric"" the bootstrap CIs from mixed effects model predictions are; my intuition is that they might reasonably be considered non-parametric, but my confidence in this area is low given that I'm still learning about mixed effects models. \\n\\n\\n  [1]: http://cran.r-project.org/web/packages/ez/index.html\\n",,
5379,2,2321,b5b84b8f-5ff9-4acb-9b1a-8e1a19b4b818,2010-09-02 21:56:32.0,,"You could apply some sort of dimensionality reduction technique like principal components and plot the value of the function as you vary the first, second, third etc. principal components, holding all others fixed.  This would show you how the function varies in the directions of the maximal variance of the inputs.",,
5380,16,2321,b5b84b8f-5ff9-4acb-9b1a-8e1a19b4b818,2010-09-02 21:56:32.0,-1.0,,,
5381,2,2322,b4d49d64-d3a4-4e18-a42c-299549a454de,2010-09-02 23:05:55.0,795.0,"I'm not sure about classification problems, but in the case of feature selection for regression problems, Jun Shao showed [that Leave-One-Out CV is asymptotically inconsistent][1], i.e. the probability of selecting the proper subset of features does not converge to 1 as the number of samples increases. From a practical point of view, Shao recommends a Monte-Carlo cross-validation, or leave-many-out procedure.\\n\\n\\n  [1]: http://www.jstor.org/pss/2290328",,
5383,2,2323,201dfeac-6dba-45af-89a4-adcd677789d0,2010-09-03 00:34:44.0,1167.0,"I think that dynamic pricing algorithms (used in aviation and ticketing industry) is very statistical based, anyone here has experience with those algorithms with references for it?\\n\\nRegards\\n\\nCamilo",,
5384,1,2323,201dfeac-6dba-45af-89a4-adcd677789d0,2010-09-03 00:34:44.0,1167.0,What are good references for dynamic pricing?,,
5385,3,2323,201dfeac-6dba-45af-89a4-adcd677789d0,2010-09-03 00:34:44.0,1167.0,<time-series>,,
5386,2,2324,56289211-daf2-49fb-b200-e6cf69db5bc5,2010-09-03 02:15:18.0,74.0,"This article is highly cited:\\n\\nhttp://www.jstor.org/stable/25061571\\n\\n""Yield Management at American Airlines""",,
5387,2,2325,5994dac4-9536-499e-89ed-086507263c47,2010-09-03 02:27:00.0,10229.0,"On Pearsons residuals,\\n----------------------\\n\\nThe Pearson residual is the difference between the observed and estimated probabilities divided by the binomial standard deviation of the estimated probability. Therefore standardizing the residuals.\\nFor large samples the standardized residuals should have a normal distribution.\\n\\nFrom Menard, Scott (2002). Applied logistic regression analysis, 2nd Edition. Thousand Oaks, CA: Sage Publications. Series: Quantitative Applications in the Social Sciences, No. 106. First ed., 1995. See Chapter 4.4\\n",,
5388,5,2324,09d78e56-70e3-46f8-ba3c-5622de9175b1,2010-09-03 02:58:41.0,183.0,"This article is highly cited:\\n\\n\\n\\n""Yield Management at American Airlines"" ([Free PDF][1] or  [JSTOR][2])\\n\\n\\n  [1]: http://www2.haut.edu.cn/jpkc2006/orsite/c/case/c28.pdf\\n  [2]: http://www.jstor.org/stable/25061571",added free link,
5389,2,2326,091b381f-8e5f-48ba-be35-4dbb33b3d08b,2010-09-03 03:25:40.0,1114.0,"I am looking at setting up an experiment concerning a hobby of mine, basically measuring a variety of parameters 'before' and 'after' and see which one, if any, gives the most reliable prediction of a final parameter i.e. do they have a linear relationship, etc.  The object being to save some time and effort later *not* sorting by the parameters that have little actual bearing on the final results, and to see if some of the more tedious sorting methods are actually as useful as thought.\\n\\nI'm starting off with one box of 100 pieces of a particular component that is non-consumable.  Initial testing will be primarily of the direct measure type: weight, thickness, volume, etc.  Later testing will require some combination with other components (which are consumable i.e. one-time usage only) to be able to test the final parameter - which is only measurable via non-contact devices of... well, consumer-grade construction would be the nicest way to put it.  % error numbers for the test device are tough to come by, etc.\\n\\nI'd planned on collecting the various data and attempting some sort of stepwise regression in R to develop a model of which parameters matter and which do not.  That is a bit beyond the intro stats course I took in college a few years ago, so I thought I might run it by here and see if there are a) any massive errors with my plan or b) suggested tweaks or improvements.\\n\\nThanks,\\n\\nMonte",,
5390,1,2326,091b381f-8e5f-48ba-be35-4dbb33b3d08b,2010-09-03 03:25:40.0,1114.0,Setting up experiment for statistical analysis,,
5391,3,2326,091b381f-8e5f-48ba-be35-4dbb33b3d08b,2010-09-03 03:25:40.0,1114.0,<r><statistical-analysis><correlation><experiment><experiment-design>,,
5392,2,2327,9488817d-4687-47d6-a1d1-44748b11b4f4,2010-09-03 03:49:48.0,10229.0,"In my opinion 16 are too many reasons, too fine of a specification and sort of overlap at times. Instead I would personally streamline into  broad groups. We can classify study objectives in 3 main categories: single hypothesis testing, exploratory study and to predict.",,
5393,16,2327,9488817d-4687-47d6-a1d1-44748b11b4f4,2010-09-03 03:49:48.0,-1.0,,,
5394,2,2328,ae88a5fb-0edc-444e-a351-7d3fb233db28,2010-09-03 04:53:37.0,861.0,"What should be the ratio of number of observations and number of variables?\\nHow to detect overfitting in the neural network model and what are the ways to avoid overfitting?\\nIf I want to perform classification with Neural Network, do the classes should have equal frequency?\\nPlease help me out.",,
5395,1,2328,ae88a5fb-0edc-444e-a351-7d3fb233db28,2010-09-03 04:53:37.0,861.0,How to perform Neural Network modelling effectively?,,
5396,3,2328,ae88a5fb-0edc-444e-a351-7d3fb233db28,2010-09-03 04:53:37.0,861.0,<neural-networks>,,
5397,5,2328,c0aa0195-22a0-4067-9c19-aaaed1d17c21,2010-09-03 04:59:10.0,183.0,"What should be the ratio of number of observations and number of variables?\\nHow to detect overfitting in the neural network model and what are the ways to avoid overfitting?\\nIf I want to perform classification with Neural Network, should the classes  have equal frequency?\\nPlease help me out.",grammar,
